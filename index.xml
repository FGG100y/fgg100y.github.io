<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>fgg blog</title><link>/</link><description>fgg blog</description><generator>Hugo -- gohugo.io</generator><language>zh</language><managingEditor>1522009317@qq.com
(fmh)</managingEditor><lastBuildDate>Wed, 11 Sep 2024 18:28:25 +0800</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>ÁâõÂÆ¢Âà∑È¢ò‰πãÁÅ´ËΩ¶ËøõÂá∫Á´ô--Ê†à.md</title><link>/posts/personals/momentsoflife/2024-09-11-%E7%89%9B%E5%AE%A2%E5%88%B7%E9%A2%98%E4%B9%8B%E7%81%AB%E8%BD%A6%E8%BF%9B%E5%87%BA%E7%AB%99%E6%A0%88/</link><pubDate>Wed, 11 Sep 2024 18:28:25 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/personals/momentsoflife/2024-09-11-%E7%89%9B%E5%AE%A2%E5%88%B7%E9%A2%98%E4%B9%8B%E7%81%AB%E8%BD%A6%E8%BF%9B%E5%87%BA%E7%AB%99%E6%A0%88/</guid><description>&lt;p>È¢òÁõÆÔºö&lt;/p>
&lt;p>ÊèèËø∞&lt;/p>
&lt;p>ÁªôÂÆö‰∏Ä‰∏™Ê≠£Êï¥Êï∞N‰ª£Ë°®ÁÅ´ËΩ¶Êï∞ÈáèÔºå0&amp;lt;N&amp;lt;10ÔºåÊé•‰∏ãÊù•ËæìÂÖ•ÁÅ´ËΩ¶ÂÖ•Á´ôÁöÑÂ∫èÂàóÔºå‰∏ÄÂÖ±NËæÜÁÅ´ËΩ¶ÔºåÊØèËæÜÁÅ´ËΩ¶‰ª•
Êï∞Â≠ó1-9ÁºñÂè∑ÔºåÁÅ´ËΩ¶Á´ôÂè™Êúâ‰∏Ä‰∏™ÊñπÂêëËøõÂá∫ÔºåÂêåÊó∂ÂÅúÈù†Âú®ÁÅ´ËΩ¶Á´ôÁöÑÂàóËΩ¶‰∏≠ÔºåÂè™ÊúâÂêéËøõÁ´ôÁöÑÂá∫Á´ô‰∫ÜÔºåÂÖàËøõ
Á´ôÁöÑÊâçËÉΩÂá∫Á´ô„ÄÇ&lt;/p>
&lt;p>Ë¶ÅÊ±ÇËæìÂá∫ÊâÄÊúâÁÅ´ËΩ¶Âá∫Á´ôÁöÑÊñπÊ°àÔºå‰ª•Â≠óÂÖ∏Â∫èÊéíÂ∫èËæìÂá∫„ÄÇ&lt;/p>
&lt;p>Êï∞ÊçÆËåÉÂõ¥Ôºö1‚â§n‚â§10&lt;/p>
&lt;p>ËøõÈò∂ÔºöÊó∂Èó¥Â§çÊùÇÂ∫¶ÔºöO(n!) ÔºåÁ©∫Èó¥Â§çÊùÇÂ∫¶ÔºöO(n)&lt;/p>
&lt;p>ËæìÂÖ•ÊèèËø∞Ôºö
Á¨¨‰∏ÄË°åËæìÂÖ•‰∏Ä‰∏™Ê≠£Êï¥Êï∞NÔºà0 &amp;lt; N &amp;lt;= 10ÔºâÔºåÁ¨¨‰∫åË°åÂåÖÊã¨N‰∏™Ê≠£Êï¥Êï∞ÔºåËåÉÂõ¥‰∏∫1Âà∞10„ÄÇ&lt;/p>
&lt;p>ËæìÂá∫ÊèèËø∞Ôºö
ËæìÂá∫‰ª•Â≠óÂÖ∏Â∫è‰ªéÂ∞èÂà∞Â§ßÊéíÂ∫èÁöÑÁÅ´ËΩ¶Âá∫Á´ôÂ∫èÂàóÂè∑ÔºåÊØè‰∏™ÁºñÂè∑‰ª•Á©∫Ê†ºÈöîÂºÄÔºåÊØè‰∏™ËæìÂá∫Â∫èÂàóÊç¢Ë°åÔºåÂÖ∑‰ΩìËßÅsample„ÄÇ&lt;/p>
&lt;p>ËæìÂÖ•Ôºö&lt;br>
3&lt;br>
1 2 3&lt;/p>
&lt;p>ËæìÂá∫Ôºö&lt;br>
1 2 3&lt;br>
1 3 2&lt;br>
2 1 3&lt;br>
2 3 1&lt;br>
3 2 1&lt;/p>
&lt;p>ËØ¥ÊòéÔºö
Á¨¨‰∏ÄÁßçÊñπÊ°àÔºö1Ëøõ„ÄÅ1Âá∫„ÄÅ2Ëøõ„ÄÅ2Âá∫„ÄÅ3Ëøõ„ÄÅ3Âá∫&lt;br>
Á¨¨‰∫åÁßçÊñπÊ°àÔºö1Ëøõ„ÄÅ1Âá∫„ÄÅ2Ëøõ„ÄÅ3Ëøõ„ÄÅ3Âá∫„ÄÅ2Âá∫&lt;br>
Á¨¨‰∏âÁßçÊñπÊ°àÔºö1Ëøõ„ÄÅ2Ëøõ„ÄÅ2Âá∫„ÄÅ1Âá∫„ÄÅ3Ëøõ„ÄÅ3Âá∫&lt;br>
Á¨¨ÂõõÁßçÊñπÊ°àÔºö1Ëøõ„ÄÅ2Ëøõ„ÄÅ2Âá∫„ÄÅ3Ëøõ„ÄÅ3Âá∫„ÄÅ1Âá∫&lt;br>
Á¨¨‰∫îÁßçÊñπÊ°àÔºö1Ëøõ„ÄÅ2Ëøõ„ÄÅ3Ëøõ„ÄÅ3Âá∫„ÄÅ2Âá∫„ÄÅ1Âá∫&lt;br>
ËØ∑Ê≥®ÊÑèÔºå[3,1,2]Ëøô‰∏™Â∫èÂàóÊòØ‰∏çÂèØËÉΩÂÆûÁé∞ÁöÑ„ÄÇ&lt;/p>
&lt;hr>
&lt;p>ÂàöÁúãÈ¢òÁõÆÂ∞±‰∏çËÉΩÊòéÁôΩ‰∏∫‰ªÄ‰πàËæìÂÖ•&amp;quot;1 2 3&amp;quot;‰∏çÂ∞±ÊòØ&amp;quot;3 2 1&amp;quot;‰∏Ä‰∏™Á≠îÊ°àÂêóÔºüÔºàÈ¢òÁõÆ‰∏≠Ëøô‰∏ÄÂè•Ôºö‚ÄúÊé•‰∏ãÊù•ËæìÂÖ•ÁÅ´ËΩ¶ÂÖ•Á´ôÁöÑÂ∫èÂàóÔºå‚ÄùÁõ¥Êé•Ë¶Å‰∫Ü‰ø∫ÁöÑÂëΩ‰∫ÜÔºâÔºå
ËæìÂÖ•&amp;quot;1 2 3&amp;quot;, ‰πüÂ∞±ÊòØÁºñÂè∑1„ÄÅ2„ÄÅ3Ëøô‰∏âËæÜÁÅ´ËΩ¶ËøõÁ´ô‰∫ÜÔºà‰ΩÜ‰∫∫ÂÆ∂ÂèØÊ≤°ÊúâËØ¥ÊòØ‰∏ÄÊ¨°Êù•‰∫Ü‰∏âËæÜÔºâÔºåÊâÄ‰ª•Âë¢ÔºåÂæóÁåúÔºåÁåúÂá∫È¢òËÄÖÂà∞Â∫ïËØ¥‰∫Ü‰ªÄ‰πàÊÑèÊÄù„ÄÇ
‚ÄúËØ¥Êòé‚ÄùÊâÄ‰∏æ‰æãÁöÑÂÜÖÂÆπÂ∞±ÊòØ‰∏ÄÁßçÊèêÁ§∫ÔºöÊàëÔºàÂá∫È¢òËÄÖÔºâÂ∞±ÊòØËøô‰πàËßÑÂÆöÈ¢òÁõÆÊÑèÊÄùÁöÑ„ÄÇÊÇ®Ë¶ÅÊòØÊ≤°ÁúãÊòéÁôΩÔºåÊòØÊÇ®Ëá™Â∑±ÁöÑÊ∞¥Âπ≥ÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;blockquote>
&lt;p>ÊçÆËØ¥ÔºåÈáëÂú£ÂèπËÆ∞ËΩΩÁöÑÂè≤‰∏äÊúÄÁü≠Áßë‰∏æËØïÈ¢ò‚Äú‰∫å‚ÄùÔºåËøô‰∏™È¢òÁõÆÂè™Êúâ‰∏Ä‰∏™Â≠óÔºåË¶ÅÊ±ÇËÄÉÁîüÂõ¥ÁªïËøô‰∏™Êï∞Â≠óÂ±ïÂºÄËÆ∫Ëø∞ÔºåÂÖ∂ËçíËØûÊÄß‰∏çË®ÄËÄåÂñª„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>ÊçÆËΩΩÔºåÈáëÂú£ÂèπÂú®Â∫îËØïÊó∂ÈÅáÂà∞ÁöÑÈ¢òÁõÆ‚ÄúÂêæÂõõÂçÅËÄå‰∏çÂä®ÂøÉ‚ÄùÔºå‰ªñÂú®ËØïÂç∑‰∏äÂÜô‰∫Ü‰∏âÂçÅ‰πù‰∏™‚ÄúÂä®ÂøÉ‚Äù&amp;hellip;&lt;/p>
&lt;/blockquote>
&lt;p>Âì•‰ª¨ÈÇ£39‰∏™‚ÄúÂä®ÂøÉ‚ÄùÂÄíÊòØÊúâÁÇπÁêÜÂ∑•Áõ¥Áî∑ÁöÑÈªëËâ≤ÂπΩÈªòÔºå‰∏çÁÆ°ÊÄé‰πàËØ¥ÔºåÊàëÊÄªËßâÂæó‰ªñÂ§öÂ∞ëÊòØÂú®ÊàèÂºÑËÄÉÂÆò„ÄÇÂìàÂìà„ÄÇËøòÊòØËøô‰ΩçÈáëÊÄßÊÄ™ËÄÅÂ§¥Áé©ÂæóËµ∑„ÄÇ&lt;/p>
&lt;p>È¢òÁõÆÊÄªÂΩíÊòØÊ≤°ÊúâÈîôÁöÑÔºàÊêû‰∏çÂ•ΩÊòØÊâìÁùÄ‚ÄúÂú£‰∏ä‚ÄùÊóóÂè∑Âá∫È¢òÁöÑÔºåË∞ÅÊï¢ËØ¥ÂÆÉÈîô‰∫ÜÔºüÔºâËÄå‰∏îÂá∫È¢òËåÉÂõ¥ÂÜÖÔºåÈ¢òÁõÆÂè™
ÊúâÈÇ£‰πàÂ§öÔºåÊØèÂπ¥ÈÉΩË¶ÅËÄÉÔºåÊâÄ‰ª•Ë¶Å‰πàÂ∞ÜÂÖ∂ÂèòÊõ≤ÊäòÂ§çÊùÇÔºåË¶Å‰πàÂèòÂê´Á≥äÂÖ∂ËæûÔºåË¶Å‰πàËØ¥ÁÇπÊÇÑÊÇÑËØù&amp;hellip;ÊÄªËÄåË®Ä‰πãÔºåÂ¶ÇÊûúÊè£Êë©‰∏çÈÄè‚Äú‰∏äÊÑè‚ÄùÔºåËÇØÂÆöÊòØËá™Â∑±Ê∞¥Âπ≥ÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;p>ÂÖ∂ÂÆû‰πüÂ∞±ÊÑèÂë≥ÁùÄÔºåÊÇ®‰∏çÂú®Âá∫È¢òËÄÖÊ∑∑ÁöÑÂúàÂ≠êÈáåÔºåËÄåÂèàÈùûË¶ÅÂéªÂ∫îËØïÔºåÊòØÊÇ®ÊúÄÂ§ßÁöÑÈîô„ÄÇ&lt;/p>
&lt;p>&lt;img alt="IMG_‰Ω†ÁªôÈ¢ÜÂØº‰∫åÈÄâ‰∏ÄÈ¢ÜÂØºÂõûÁ≠îÂØπ" src="images/%E4%BD%A0%E7%BB%99%E9%A2%86%E5%AF%BC%E4%BA%8C%E9%80%89%E4%B8%80%E9%A2%86%E5%AF%BC%E5%9B%9E%E7%AD%94%E5%AF%B9.png">&lt;/p>
&lt;hr>
&lt;p>Âï∞ÂîÜÂ§ö‰∏§Âè•Ôºö&lt;br>
ÂõûÂà∞ËøôÈÅìÈ¢òÔºåÊàë‰ªø‰ΩõÁúãÂà∞‰∫ÜÁßë‰∏æËÄÉËØïÁöÑÈ¨ºÂΩ±Âú®Ë∑≥Ëàû„ÄÇÊÉ≥‰∏çÂà∞ÈÇ£‰∫õÂºØÂºØÁªïÂΩìÁÑ∂ÊòØËá™Â∑±Á¨®&amp;hellip;ÂêóÔºü&lt;/p>
&lt;p>ÊâçÂàö&lt;code>git push&lt;/code>ËøôÁØáÂçöÂÆ¢ÔºåÂ∞±Âà∑Âà∞‚ÄúÈÅìË∑ØÂäû‰Ωï‰∏ª‰ªª‚ÄùÁöÑÂàöÂàöÂèëÂ∏ÉÁöÑ&lt;a href="https://mp.weixin.qq.com/s/tiUXtTPaTHYCARzQ5f_xTQ">‰∏∫‰ªÄ‰πàÊúâ‰∫∫ÂñúÊ¨¢ËÑ±Ë£§Â≠êÊîæÂ±Å&lt;/a>ÔºåÂæàÂñúÊ¨¢ËøôÂì•‰ª¨ÁöÑÊñáÁ´†„ÄÇ&lt;/p>
&lt;p>‰ª£Á†ÅÔºà‰∏çÁªô‰ª£Á†ÅÂ∞±ÊòØÁûéÂìîÂìî‰∏™Â±ÅÔºå‰ΩÜÊòØËøô‰∏™Âπ¥Â§¥ÔºåÁî®ÊúÄÂñúÁà±ÁöÑ&lt;ruby>‚ÄúÂç°Â∏ïË•ø‚Äù&lt;rt>Andrej Karpathy&lt;/rt>&lt;/ruby>ËÄÅÂ∏àÁöÑËØùÊù•ËÆ≤Â∞±ÊòØÔºöÁºñÂÜô‰ª£Á†ÅÂç≥Â∞ÜÂ∞±ÊòØtab tab tabÔºâÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> sys
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">train_out_sequences&lt;/span>(n, trains):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">dfs&lt;/span>(in_index, stack, out_sequence):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Â¶ÇÊûúÊâÄÊúâÁÅ´ËΩ¶ÈÉΩÂá∫Á´ôÔºåËÆ∞ÂΩïÂΩìÂâçÊñπÊ°à&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(out_sequence) &lt;span style="color:#ff6ac1">==&lt;/span> n:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> result&lt;span style="color:#ff6ac1">.&lt;/span>append(out_sequence[:])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># ÈÄâÊã©ÂÖ•Á´ôÊìç‰Ωú&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> in_index &lt;span style="color:#ff6ac1">&amp;lt;&lt;/span> n:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stack&lt;span style="color:#ff6ac1">.&lt;/span>append(trains[in_index]) &lt;span style="color:#78787e"># ÁÅ´ËΩ¶ÂÖ•Á´ô&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfs(in_index &lt;span style="color:#ff6ac1">+&lt;/span> &lt;span style="color:#ff9f43">1&lt;/span>, stack, out_sequence) &lt;span style="color:#78787e"># ÈÄíÂΩíÂ§ÑÁêÜ‰∏ã‰∏ÄËæÜËΩ¶&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stack&lt;span style="color:#ff6ac1">.&lt;/span>pop() &lt;span style="color:#78787e"># ÂõûÊ∫ØÔºåÊí§ÈîÄÂÖ•Á´ôÊìç‰Ωú&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># ÈÄâÊã©Âá∫Á´ôÊìç‰Ωú&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> stack:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> out_sequence&lt;span style="color:#ff6ac1">.&lt;/span>append(stack&lt;span style="color:#ff6ac1">.&lt;/span>pop()) &lt;span style="color:#78787e"># Ê†àÈ°∂ÁÅ´ËΩ¶Âá∫Á´ô&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfs(in_index, stack, out_sequence) &lt;span style="color:#78787e"># ÈÄíÂΩíÂ§ÑÁêÜ&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> stack&lt;span style="color:#ff6ac1">.&lt;/span>append(out_sequence&lt;span style="color:#ff6ac1">.&lt;/span>pop()) &lt;span style="color:#78787e"># ÂõûÊ∫ØÔºåÊí§ÈîÄÂá∫Á´ôÊìç‰Ωú&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> result &lt;span style="color:#ff6ac1">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfs(&lt;span style="color:#ff9f43">0&lt;/span>, [], [])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># ÂØπÊâÄÊúâÂá∫Á´ôÊñπÊ°àÊåâÂ≠óÂÖ∏Â∫èÊéíÂ∫è&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> result&lt;span style="color:#ff6ac1">.&lt;/span>sort()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> result
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ins &lt;span style="color:#ff6ac1">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> line &lt;span style="color:#ff6ac1">in&lt;/span> sys&lt;span style="color:#ff6ac1">.&lt;/span>stdin:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> a &lt;span style="color:#ff6ac1">=&lt;/span> line&lt;span style="color:#ff6ac1">.&lt;/span>strip()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ins&lt;span style="color:#ff6ac1">.&lt;/span>append(a)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>n, trains &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">int&lt;/span>(ins[&lt;span style="color:#ff9f43">0&lt;/span>]), &lt;span style="color:#ff5c57">list&lt;/span>(&lt;span style="color:#ff5c57">map&lt;/span>(&lt;span style="color:#ff5c57">int&lt;/span>, ins[&lt;span style="color:#ff9f43">1&lt;/span>]&lt;span style="color:#ff6ac1">.&lt;/span>split()))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Ëé∑ÂèñÊâÄÊúâÂêàÊ≥ïÁöÑÂá∫Á´ôÊñπÊ°à&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sequences &lt;span style="color:#ff6ac1">=&lt;/span> train_out_sequences(n, trains)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ÊâìÂç∞ÊØè‰∏™Âá∫Á´ôÊñπÊ°à&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> seq &lt;span style="color:#ff6ac1">in&lt;/span> sequences:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">&amp;#34; &amp;#34;&lt;/span>&lt;span style="color:#ff6ac1">.&lt;/span>join(&lt;span style="color:#ff5c57">map&lt;/span>(&lt;span style="color:#ff5c57">str&lt;/span>, seq)))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>expectation_maximization</title><link>/posts/optimazationmethods/em/expectation_maximization/</link><pubDate>Fri, 30 Aug 2024 13:58:13 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/optimazationmethods/em/expectation_maximization/</guid><description>&lt;p>Ê¶ÇÁéáÊ®°ÂûãÊúâÊó∂Êó¢Êúâ &lt;ruby>‚ÄúËßÇÊµãÂèòÈáè‚Äù&lt;rt>Ôºàobservable variableÔºâ&lt;/rt>&lt;/ruby> ÔºåÂèàÂê´Êúâ
&lt;ruby>‚ÄúÈöêÂèòÈáèÊàñÊΩúÂú®ÂèòÈáè‚Äù&lt;rt>Ôºàlatent variableÔºâ&lt;/rt>&lt;/ruby> „ÄÇÂΩìÂ≠òÂú®ÂêéËÄÖÊó∂Ôºå‰∏çËÉΩÁõ¥Êé•‰ΩøÁî®
MLEËøôÊ†∑ÁöÑÊñπÊ≥ïËøõË°åÂèÇÊï∞‰º∞ËÆ°ÔºåÈúÄË¶ÅÂºïÂÖ•EMËøõË°åÂèÇÊï∞‰º∞ËÆ°„ÄÇ
EMÁÆóÊ≥ïÂ∞±ÊòØÂê´ÊúâÈöêÂèòÈáèÁöÑÊ¶ÇÁéáÊ®°ÂûãÂèÇÊï∞ÁöÑÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°Ê≥ïÔºàÊàñÊúÄÂ§ßÂêéÈ™åÊ¶ÇÁéá‰º∞ËÆ°Ê≥ïÔºâ„ÄÇ&lt;/p>
&lt;h2 id="‰æãÂ≠ê----‰∏âÁ°¨Â∏ÅÊ®°Âûã" >
&lt;div>
&lt;a href="#%e4%be%8b%e5%ad%90----%e4%b8%89%e7%a1%ac%e5%b8%81%e6%a8%a1%e5%9e%8b">
#
&lt;/a>
‰æãÂ≠ê &amp;ndash; ‰∏âÁ°¨Â∏ÅÊ®°Âûã
&lt;/div>
&lt;/h2>
&lt;p>ÂÅáËÆæÊúâ3ÊûöÁ°¨Â∏ÅÔºåÂàÜÂà´ËÆ∞‰Ωú A, B, C„ÄÇÂÖ∂ÊäõÂá∫Ê≠£Èù¢ÁöÑÊ¶ÇÁéáÂàÜÂà´ÊòØ $\pi$, $p$ Âíå $q$ „ÄÇËøõË°åÂ¶Ç‰∏ãÊäõÁ°¨
Â∏ÅËØïÈ™åÔºöÂÖàÊäõÁ°¨Â∏ÅAÔºåÊ†πÊçÆÂÖ∂ÁªìÊûúÈÄâÊã©Á°¨Â∏ÅBÊàñËÄÖÁ°¨Â∏ÅCÔºàÊ≠£Èù¢ÈÄâÁ°¨Â∏ÅBÔºåÂèçÈù¢ÈÄâÁ°¨Â∏ÅAÔºâÔºõÁÑ∂ÂêéÊäõÈÄâÂá∫
ÁöÑÁ°¨Â∏ÅÔºåÂÖ∂ÊäõÁ°¨Â∏ÅÁöÑÁªìÊûúÔºåÂá∫Áé∞Ê≠£Èù¢ËÆ∞‰Ωú1ÔºåÂá∫Áé∞ÂèçÈù¢ËÆ∞‰Ωú0ÔºõÁã¨Á´ãÈáçÂ§ç $n$ Ê¨°ËØïÈ™å„ÄÇËßÇÊµãÁªìÊûúÂ¶Ç‰∏ã
Ôºà$n=10$ÔºâÔºö&lt;/p>
&lt;pre>&lt;code>1,1,0,1,0,0,1,0,1,1
&lt;/code>&lt;/pre>
&lt;p>ÂÅáËÆæÂè™ËÉΩËßÇÂØüÂà∞ÊäõÁ°¨Â∏ÅÁöÑÁªìÊûúÔºå‰∏çËÉΩËßÇÊµãÊäõÁ°¨Â∏ÅËøáÁ®ã„ÄÇÈóÆÔºöÂ¶Ç‰Ωï‰º∞ËÆ°‰∏âÊûöÁ°¨Â∏ÅÊ≠£Èù¢Âá∫Áé∞ÁöÑÊ¶ÇÁéáÔºåÂç≥‰∏â
Á°¨Â∏ÅÊ®°ÂûãÁöÑÂèÇÊï∞„ÄÇ&lt;/p>
&lt;h3 id="Âª∫Ê®°" >
&lt;div>
&lt;a href="#%e5%bb%ba%e6%a8%a1">
##
&lt;/a>
Âª∫Ê®°
&lt;/div>
&lt;/h3>
&lt;p>‰∏âÁ°¨Â∏ÅÊ®°ÂûãÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö&lt;/p>
&lt;p>$$
\begin{eqnarray}
P(y|\theta)
&amp;amp;=&amp;amp; \sum_{z} P(y, z | \theta) = \sum_{z} P(z|\theta)P(y|z,\theta) \\
&amp;amp;=&amp;amp; \pi p^{y}(1-p)^{1-y} + (1 - \pi) q^{y}(1-q)^{1-y}
\end{eqnarray}
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠ÔºåÈöèÊú∫ÂèòÈáè $y$ ÊòØËßÇÊµãÂèòÈáèÔºåË°®Á§∫‰∏ÄÊ¨°ËØïÈ™åËßÇÊµãÁöÑÁªìÊûúÊòØ1Êàñ0ÔºõÈöèÊú∫ÂèòÈáè $z$ ÊòØÈöêÂèòÈáèÔºåË°®Á§∫
Êú™ËßÇÊµãÂà∞ÁöÑÊäõÁ°¨Â∏ÅAÁöÑÁªìÊûúÔºõ$\theta=(\pi, p, q)$ ÊòØÊ®°ÂûãÂèÇÊï∞„ÄÇËøô‰∏ÄÊ®°ÂûãÊòØÂâçËø∞$n$Ê¨°ËØïÈ™åÊï∞ÊçÆÁöÑ
ÁîüÊàêÊ®°Âûã„ÄÇ&lt;/p>
&lt;p>Â∞ÜËßÇÊµãÊï∞ÊçÆË°®Á§∫‰∏∫ $Y = (Y_1, Y_2, \dots, Y_n)$ÔºåÂ∞ÜÊú™ËßÇÊµãÊï∞ÊçÆË°®Á§∫‰∏∫ $Z = (Z_1, Z_2, \dots,
Z_n)$ÔºåÂàôËßÇÊµãÊï∞ÊçÆÁöÑ&lt;strong>‰ººÁÑ∂ÂáΩÊï∞&lt;/strong>‰∏∫&lt;/p>
&lt;p>$$
\begin{eqnarray}
P(Y|\theta)
&amp;amp;=&amp;amp; \sum_{z} P(Z|\theta)P(Y|Z,\theta) \\
&amp;amp;=&amp;amp; \prod^{n}_{j=1}[\pi p^{y_j}(1-p)^{1-y_j} + (1 - \pi) q^{y_j}(1-q)^{1-y_j}
\end{eqnarray}
$$&lt;/p>
&lt;p>Ê±ÇÊ®°ÂûãÂèÇÊï∞ $\theta=(\pi, p, q)$ ÁöÑÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºåÂç≥&lt;/p>
&lt;p>$$
\hat{\theta} = \arg\max_{\theta} \log P(Y|\theta)
$$&lt;/p>
&lt;p>Ëøô‰∏™ÈóÆÈ¢òÊ≤°ÊúâËß£ÊûêËß£ÔºåÈúÄË¶ÅÈÄöËøáEMÁÆóÊ≥ïËøõË°åËø≠‰ª£Ê±ÇËß£„ÄÇ&lt;/p>
&lt;h3 id="emÁÆóÊ≥ïËø≠‰ª£Ê≠•È™§" >
&lt;div>
&lt;a href="#em%e7%ae%97%e6%b3%95%e8%bf%ad%e4%bb%a3%e6%ad%a5%e9%aa%a4">
##
&lt;/a>
EMÁÆóÊ≥ïËø≠‰ª£Ê≠•È™§
&lt;/div>
&lt;/h3>
&lt;p>EMÁÆóÊ≥ïÈÄöËøáËø≠‰ª£Ê±Ç $L(\theta) = \log P(Y|\theta)$ ÁöÑÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°„ÄÇÊØèÊ¨°Ëø≠‰ª£ÂåÖÂê´‰∏§Ê≠•ÔºöEÊ≠•ÔºåÊ±ÇÊúüÊúõÔºõMÊ≠•ÔºåÊ±ÇÊúÄÂ§ßÂåñ„ÄÇ&lt;/p>
&lt;hr>
&lt;p>EMÁÆóÊ≥ïÊ≠•È™§Ê∏ÖÂçï&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>ÔºöËßÇÊµãÂèòÈáèÊï∞ÊçÆ $Y$ÔºåÈöêÂèòÈáèÊï∞ÊçÆ $Z$ÔºåËÅîÂêàÂàÜÂ∏É $P(Y,Z|\theta)$ÔºåÊù°‰ª∂ÂàÜÂ∏É $P(Z|Y, \theta)$Ôºõ&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>ÔºöÊ®°ÂûãÂèÇÊï∞ $\theta$ „ÄÇ&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ÈÄâÊã©ÂèÇÊï∞ÂàùÂßãÂÄº $\theta^{(0)}$ ÔºåÂºÄÂßãËø≠‰ª£Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>EÊ≠•ÔºöËÆ∞ $\theta^{(i)}$ ‰∏∫Á¨¨$i$Ê¨°Ëø≠‰ª£ÂèÇÊï∞ $\theta$ ÁöÑ‰º∞ËÆ°ÂÄºÔºåÂú®Á¨¨$i+1$Ê¨°Ëø≠‰ª£ÁöÑEÊ≠•ÔºåËÆ°ÁÆó
$$
\begin{eqnarray}
Q(\theta, \theta^{(i)})
&amp;amp;=&amp;amp; E_{z} [\log P(Y,Z|\theta) | Y, \theta^{(i)}] \\
&amp;amp;=&amp;amp; \sum_{z} \log P(Y,Z|\theta) P(Z|Y, \theta^{(i)})
\end{eqnarray}
$$
ËøôÈáåÔºå$P(Z|Y, \theta^{(i)}$ ÊòØÂú®ÁªôÂÆöËßÇÊµãÊï∞ÊçÆ $Y$ ÂíåÂΩìÂâçÁöÑÂèÇÊï∞‰º∞ËÆ° $\theta^{(i)}$ ‰∏ãÈöêÂèòÈáèÊï∞ÊçÆ $Z$ ÁöÑÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÔºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>MÊ≠•ÔºöÊ±Ç‰ΩøÂæó $Q(\theta, \theta^{(i)})$ ÊúÄÂ§ßÂåñÁöÑ $\theta$ ÔºåÁ°ÆÂÆöÁ¨¨ $i+1$ Ê¨°Ëø≠‰ª£ÁöÑÂèÇÊï∞ÁöÑ‰º∞ËÆ°ÂÄº $\theta^{(i)}$
$$
\theta^{(i)} = \arg\max_{\theta} Q(\theta, \theta^{(i)})
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÈáçÂ§çÁ¨¨2Ê≠•ÂíåÁ¨¨3Ê≠•ÔºåÁõ¥Âà∞Êî∂Êïõ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr></description></item><item><title>maximum_likelihood_estimate</title><link>/posts/optimazationmethods/mle/maximum_likelihood_estimate/</link><pubDate>Fri, 30 Aug 2024 10:17:41 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/optimazationmethods/mle/maximum_likelihood_estimate/</guid><description>&lt;p>ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºàMaximum Likelihood Estimation, MLEÔºâÊòØ‰∏ÄÁßçÂ∏∏Áî®ÂèÇÊï∞‰º∞ËÆ°ÊñπÊ≥ïÔºåÂú®ÁªüËÆ°Â≠¶ÂíåÊú∫
Âô®Â≠¶‰π†‰∏≠Ë¢´ÂπøÊ≥õ‰ΩøÁî®„ÄÇÂÆÉÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÊòØÊâæÂà∞‰∏ÄÁªÑÂèÇÊï∞ÂÄºÔºå‰ΩøÂæóËßÇÂØüÂà∞ÁöÑÊï∞ÊçÆÂá∫Áé∞ÁöÑÊ¶ÇÁéáÊúÄÂ§ß„ÄÇ&lt;/p>
&lt;h3 id="Ê¶ÇÂøµ" >
&lt;div>
&lt;a href="#%e6%a6%82%e5%bf%b5">
##
&lt;/a>
Ê¶ÇÂøµ
&lt;/div>
&lt;/h3>
&lt;p>ÂÅáËÆæÊàë‰ª¨Êúâ‰∏ÄÁªÑ&lt;ruby>‚ÄúÁã¨Á´ãÂêåÂàÜÂ∏É‚Äù&lt;rt>Ôºàindependent identity distribution, i.i.d.Ôºâ&lt;/rt>&lt;/ruby>
ÁöÑËßÇÊµãÊï∞ÊçÆ $X_1, X_2, \ldots, X_n$ÔºåËøô‰∫õÊï∞ÊçÆÊù•Ëá™Êüê‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉÊóèÔºåÂÖ∂Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ÊàñÊ¶ÇÁéáË¥®Èáè
ÂáΩÊï∞‰∏∫ $f(x|\theta)$ÔºåÂÖ∂‰∏≠$\theta$ ÊòØÊú™Áü•ÂèÇÊï∞„ÄÇÁõ¥ËßÇ‰∏äÁúãÔºåÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÁöÑÁõÆÊ†áÂ∞±ÊòØËØïÂõæÂú®
$\theta$ ÊâÄÊúâÂèØËÉΩÁöÑÂèñÂÄº‰∏≠ÔºåÊâæÂà∞‰∏Ä‰∏™ËÉΩ‰ΩøËøô‰∏ÄÁªÑËßÇÊµãÊï∞ÊçÆÂá∫Áé∞ÁöÑ‚ÄúÂèØËÉΩÊÄß‚ÄùÊúÄÂ§ßÁöÑÂÄº„ÄÇ&lt;/p>
&lt;p>ÂΩ¢ÂºèÂåñÂú∞ËÆ≤ÔºåÁªôÂÆöËßÇÊµãÊï∞ÊçÆ $x_1, x_2, \ldots, x_n$ÔºåÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ° $\hat{\theta}$ ÂÆö‰πâ‰∏∫Ôºö
$$
\hat{\theta} = \arg\max_\theta L(\theta|x_1, x_2, \ldots, x_n)
$$
ÂÖ∂‰∏≠ $L(\theta|x_1, x_2, \ldots, x_n)$ Áß∞‰∏∫‰ººÁÑ∂ÂáΩÊï∞ÔºåÂÆö‰πâ‰∏∫Ôºö
$$
L(\theta|x_1, x_2, \ldots, x_n) = f(x_1, x_2, \ldots, x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)
$$&lt;/p>
&lt;p>Âï∞ÂîÜÂ§ö‰∏§Âè•ÔºöÊâÄË∞ì‚Äú‰ººÁÑ∂‚ÄùÔºåÂíå‚ÄúÊ¶ÇÁéá‚ÄùÂà∞Â∫ïÊúâ‰ªÄ‰πàÂå∫Âà´Âë¢ÔºüÁî®ÊäõÁ°¨Â∏Å‰∏æ‰æãÔºåÂå∫Âà´Âú®‰∫éÔºö&lt;/p>
&lt;ul>
&lt;li>ÂΩìÊàë‰ª¨Ë∞àËÆ∫‚ÄúÊ¶ÇÁéá‚ÄùÊó∂ÔºåÂÆûÈôÖÂ∞±ÊòØÂú®ÈóÆÔºöÂ∑≤Áü•Ê®°ÂûãÔºàp=0.6ÔºåÊäõÂá∫Ê≠£Èù¢ÁöÑÊ¶ÇÁéáÔºâÔºåÂàôÊäõÁ°¨Â∏Å10Ê¨°‰∏≠Êúâ7
Ê¨°‰∏∫Ê≠£Èù¢ÁöÑÊ¶ÇÁéáÊòØÂ§öÂ§ßÔºü($P^{7}*(1-p)^{10-7}$)&lt;/li>
&lt;li>ËÄåË∞àËÆ∫‚Äú‰ººÁÑ∂‚ÄùÊó∂ÔºåÂÖ∂ÂÆûÊòØÂèçËøáÊù•ÈóÆÔºöÂ∑≤ÁªèËßÇÂØüÂà∞Êäõ10Ê¨°Á°¨Â∏ÅÂá∫Áé∞‰∫Ü7Ê¨°Ê≠£Èù¢ÔºåÂàôÁ°¨Â∏ÅÊäõÂá∫Ê≠£Èù¢ÁöÑÊ¶Ç
ÁéáÊòØÊüê‰∏™ÂÄºÁöÑÂèØËÉΩÊúâÂ§öÂ§ßÔºüÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°Â∞±ÊòØÊâæÂà∞ËøôÊ†∑‰∏Ä‰∏™ÂèÇÊï∞ÔºåÊ≠§ÂèÇÊï∞‰ΩøÂæóÂá∫Áé∞ËøôÊ†∑ÁöÑÊï∞ÊçÆÔºà7
Ê≠£Èù¢3ÂèçÈù¢ÔºâÁöÑÂèØËÉΩÊúÄÈ´ò„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="Ê±ÇËß£ÊñπÊ≥ï----Áõ¥Êé•Ê±ÇÂØºÊ≥ï" >
&lt;div>
&lt;a href="#%e6%b1%82%e8%a7%a3%e6%96%b9%e6%b3%95----%e7%9b%b4%e6%8e%a5%e6%b1%82%e5%af%bc%e6%b3%95">
##
&lt;/a>
Ê±ÇËß£ÊñπÊ≥ï &amp;ndash; &lt;strong>Áõ¥Êé•Ê±ÇÂØºÊ≥ï&lt;/strong>Ôºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>ÂÜôÂá∫‰ººÁÑ∂ÂáΩÊï∞ $L(\theta|x_1, x_2, \ldots, x_n)$„ÄÇ&lt;/li>
&lt;li>ÂØπ‰ººÁÑ∂ÂáΩÊï∞ÂèñÂØπÊï∞ÂæóÂà∞ÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞ $l(\theta|x_1, x_2, \ldots, x_n) = \log L(\theta|x_1, x_2, \ldots, x_n)$„ÄÇ&lt;/li>
&lt;li>ÂØπÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞Ê±ÇÂØºÔºåÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÊ±Ç‰∏ÄÈò∂ÂØºÊï∞ÔºåÂπ∂‰ª§ÂÖ∂Á≠â‰∫éÈõ∂Êù•ÊâæÂà∞ÊûÅÂÄºÁÇπ„ÄÇ&lt;/li>
&lt;li>Ê£ÄÊü•‰∫åÈò∂ÂØºÊï∞ÊàñËÄÖÁõ¥Êé•ÈÄöËøáÁõ¥ËßÇÂà§Êñ≠ÊòØÂê¶‰∏∫ÊûÅÂ§ßÂÄºÁÇπ„ÄÇ&lt;/li>
&lt;/ul>
&lt;h4 id="Á§∫‰æãÊúâËß£ÊûêËß£" >
&lt;div>
&lt;a href="#%e7%a4%ba%e4%be%8b%e6%9c%89%e8%a7%a3%e6%9e%90%e8%a7%a3">
###
&lt;/a>
Á§∫‰æãÔºàÊúâËß£ÊûêËß£Ôºâ
&lt;/div>
&lt;/h4>
&lt;p>‰ª•Ê≠£ÊÄÅÂàÜÂ∏É‰∏∫‰æãÔºåÂÅáËÆæ $X_1, X_2, \ldots, X_n$ Êúç‰ªéÂùáÂÄº‰∏∫ $\mu$„ÄÅÊñπÂ∑Æ‰∏∫ $\sigma^2$ ÁöÑÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÈÇ£‰πà‰ººÁÑ∂ÂáΩÊï∞‰∏∫Ôºö
$$
L(\mu, \sigma^2|x_1, x_2, \ldots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)
$$&lt;/p>
&lt;p>ÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞‰∏∫Ôºö
$$
l(\mu, \sigma^2|x_1, x_2, \ldots, x_n) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2
$$&lt;/p>
&lt;p>ÈÄöËøáÊ±ÇÂØºÂπ∂ËÆæÁΩÆ‰∏∫Èõ∂ÂèØ‰ª•ÂæóÂà∞ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÂÄº $\hat{\mu}$ Âíå $\hat{\sigma}^2$Ôºö
$$
\hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i
$$
$$
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \hat{\mu})^2
$$&lt;/p>
&lt;h3 id="Ê±ÇËß£ÊñπÊ≥ï----Êï∞ÂÄº‰ºòÂåñÊñπÊ≥ï" >
&lt;div>
&lt;a href="#%e6%b1%82%e8%a7%a3%e6%96%b9%e6%b3%95----%e6%95%b0%e5%80%bc%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95">
##
&lt;/a>
Ê±ÇËß£ÊñπÊ≥ï &amp;ndash; &lt;strong>Êï∞ÂÄº‰ºòÂåñÊñπÊ≥ï&lt;/strong>Ôºö
&lt;/div>
&lt;/h3>
&lt;p>ÂΩìÁõ¥Êé•Ê±ÇÂØºÊ≥ïÈöæ‰ª•Â∫îÁî®Êó∂Ôºà‰æãÂ¶ÇÈùûÁ∫øÊÄßÈóÆÈ¢ò„ÄÅÂ§öÁª¥ÈóÆÈ¢òÁ≠âÔºâÔºåÂèØ‰ª•ÈááÁî®Êï∞ÂÄº‰ºòÂåñÊñπÊ≥ïÊù•ÂØªÊâæ‰ººÁÑ∂ÂáΩÊï∞
ÁöÑÊúÄÂ§ßÂÄº„ÄÇÂ∏∏ËßÅÊï∞ÂÄº‰ºòÂåñÁÆóÊ≥ïÊúâÔºö‰∏ÄÈò∂ÂØºÊï∞ÔºöÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºõ‰∫åÈò∂ÂØºÊï∞ÔºöÁâõÈ°ø-ÊãâÂ§çÁîüÊ≥ï„ÄÅÊãüÁâõÈ°øÊ≥ïÁ≠â„ÄÇ&lt;/p>
&lt;h4 id="Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï" >
&lt;div>
&lt;a href="#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95">
###
&lt;/a>
Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï
&lt;/div>
&lt;/h4>
&lt;p>Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊòØ‰∏ÄÁßçËø≠‰ª£‰ºòÂåñÁÆóÊ≥ïÔºåÂÆÉÂü∫‰∫éÂáΩÊï∞Âú®Êüê‰∏ÄÁÇπÂ§ÑÁöÑÊ¢ØÂ∫¶ÊñπÂêëÔºåÊ≤øÁùÄÊ¢ØÂ∫¶ÁöÑÂèçÊñπÂêëÊõ¥Êñ∞ÂèÇÊï∞ÔºåÁõ¥Âà∞ËææÂà∞‰∏Ä‰∏™Â±ÄÈÉ®ÊúÄÂ∞èÂÄºÁÇπ„ÄÇ&lt;/p>
&lt;p>‰∏ÄËà¨Ê≠•È™§Ôºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ËÆæÁΩÆÁõÆÊ†áÂáΩÊï∞Ôºàaka, ÊçüÂ§±ÂáΩÊï∞ÔºâÔºöÂú®MLE‰∏≠ÁõÆÊ†áÂáΩÊï∞Â∞±ÊòØÊúÄÂ§ßÂåñÂØπÊï∞‰ººÁÑ∂Ôºàlog-likelihoodÔºâÔºå
ÁÑ∂ËÄåÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÈÄöÂ∏∏ÊòØÊ±ÇÂáΩÊï∞ÊúÄÂ∞èÂÄºÔºåÊâÄ‰ª•ÈÄöÂ∏∏Âú®MLE‰∏≠ÊúÄÂ∞èÂåñË¥üÊï∞ÂØπÊï∞‰ººÁÑ∂Ôºànegative
log-likelihoodÔºâÔºåËøô‰∏™Âè™ÊòØÂá∫‰∫éÊï∞ÂÄºËÆ°ÁÆóÊñπ‰æøÁöÑËÄÉËôëÔºåÂπ∂‰∏îÂÖ∂Á≠â‰ª∑‰∫éÊúÄÂ§ßÂåñÂØπÊï∞‰ººÁÑ∂„ÄÇ&lt;/p>
&lt;ul>
&lt;li>ËÆæ $\mathcal{l}(\theta)$ ‰∏∫‚ÄúÂØπÊï∞‰ººÁÑ∂‚ÄùÔºåÂàô&lt;/li>
&lt;li>ÁõÆÊ†áÂáΩÊï∞‰∏∫ $-\mathcal{l}(\theta)$ „ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>ËÆ°ÁÆóÊ¢ØÂ∫¶ÔºöËÆ°ÁÆóË¥üÊï∞ÂØπÊï∞‰ººÁÑ∂ÁöÑÊ¢ØÂ∫¶ÔºàÂçïÁª¥Â∫¶Êó∂Êàê‰∏∫‚ÄúÂØºÊï∞‚ÄùÔºåÂ§ö‰∏™Áª¥Â∫¶Êó∂Â∞±Âè´‚ÄúÊ¢ØÂ∫¶‚ÄùÔºåÂÆûË¥®ÈÉΩÊòØ
ÊåáÂèòÂåñÁéáÔºâÔºå‰πüÂ∞±ÊòØÔºöÊ¢ØÂ∫¶ÊåáÁ§∫‰∫ÜÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞‰∏≠Âù°Â∫¶ÊúÄÈô°ÁöÑÊñπÂêë,
$$
\nabla_{\theta}(\mathcal{l}(\theta)) = - \frac{\partial \mathcal{l}(\theta)}{\partial \theta}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ëø≠‰ª£Êõ¥Êñ∞ÂèÇÊï∞ÔºöÊ¢ØÂ∫¶‰∏ãÈôçÂ∞±ÊòØÂú®Âù°Â∫¶ÊúÄÈô°Â≥≠ÁöÑÊñπÂêë‰∏äÔºåÊØèÊ¨°Âè™ÂâçËøõ‰∏ÄÂ∞èÊ≠•„ÄÇÂ¶ÇÊûúÂ∞ÜÁ¨¨tÊ¨°Ëø≠‰ª£ÁöÑÂèÇ
Êï∞ËÆæ‰∏∫$\theta^{(t)}$ÔºåÂàôÂèÇÊï∞Êõ¥Êñ∞ÁöÑËßÑÂàô‰∏∫Ôºö
$$
\theta{t+1} = \theta{(t)} - \eta \cdot \nabla_{\theta}(\mathcal{l}(\theta^{(t)}))
$$
ÂÖ∂‰∏≠Ôºå$\eta$ ÊòØÂ≠¶‰π†ÁéáÔºåÁî®‰∫éÊéßÂà∂ÊØèÊ¨°Ëø≠‰ª£ÂâçËøõÁöÑ‰∏ÄÂ∞èÊ≠•Âà∞Â∫ïÊúâÂ§öÂ∞èÔºàÈÄöÂ∏∏ËÆæ‰∏∫ 0.001Á≠âÔºâ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Êî∂ÊïõÔºöËøô‰∏™Ëø≠‰ª£ËøáÁ®ã‰∏ÄÁõ¥ÊåÅÁª≠Âà∞ÂèÇÊï∞ÁöÑÂèòÂåñÂèòÂæóÈùûÂ∏∏Â∞èÔºàÈÄöÂ∏∏ÊÑèÂë≥ÁùÄÊ¢ØÂ∫¶ÂÄºÊé•Ëøë0ÔºâÔºå‰πüÂ∞±ÊòØÁõÆÊ†á
ÂáΩÊï∞ÂÄºÊî∂Êïõ‰∫éÊ≠§ÊúÄÂ∞èÂÄº„ÄÇÔºàÊ¢ØÂ∫¶‰∏ãÈôçËøáÁ®ã‰∏ÄÂÆöÊòØÊî∂ÊïõÁöÑÂêóÔºü‰∏ç„ÄÇËøúÁöÑ‰∏çËØ¥ÔºåÂçïÊääÂ≠¶‰π†ÁéáÁöÑÂÄºË∞ÉÈ´òÔºå
Â∞±ÊúâÂèØËÉΩÂØºËá¥ÂèëÊï£„ÄÇÔºâ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊúâËÆ∏Â§öÂèòÁßçÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é" >
&lt;div>
&lt;a href="#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%9c%89%e8%ae%b8%e5%a4%9a%e5%8f%98%e7%a7%8d%e5%8c%85%e6%8b%ac%e4%bd%86%e4%b8%8d%e9%99%90%e4%ba%8e">
###
&lt;/a>
Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊúâËÆ∏Â§öÂèòÁßçÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>ÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôçÔºàBatch Gradient DescentÔºâÔºöÊØèÊ¨°Ëø≠‰ª£Êó∂‰ΩøÁî®ÊâÄÊúâËÆ≠ÁªÉÊ†∑Êú¨ËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ&lt;/li>
&lt;li>ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÔºàStochastic Gradient Descent, SGDÔºâÔºöÊØèÊ¨°Ëø≠‰ª£‰ªÖ‰ΩøÁî®‰∏Ä‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ&lt;/li>
&lt;li>Â∞èÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôçÔºàMini-batch Gradient DescentÔºâÔºöÊØèÊ¨°Ëø≠‰ª£‰ΩøÁî®‰∏ÄÂ∞èÈÉ®ÂàÜËÆ≠ÁªÉÊ†∑Êú¨ËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ&lt;/li>
&lt;/ul></description></item><item><title>merge_exists_and_not_empty_directory</title><link>/posts/thecli/gitops/2024-08-21-merge_exists_and_not_empty_directory/</link><pubDate>Wed, 21 Aug 2024 13:54:50 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/gitops/2024-08-21-merge_exists_and_not_empty_directory/</guid><description>&lt;h2 id="‰∏∫‰ªÄ‰πà" >
&lt;div>
&lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88">
#
&lt;/a>
‰∏∫‰ªÄ‰πà
&lt;/div>
&lt;/h2>
&lt;p>‰∏∫‰ªÄ‰πà‰ºöÊúâËøôÁßçÈúÄÊ±ÇÔºüË¶ÅÊää‰ªìÂ∫ìÂíåÂè¶‰∏Ä‰∏™ÈùûÁ©∫ÁõÆÂΩïÁªôÂêàÂπ∂Ëµ∑Êù•Ôºü&lt;/p>
&lt;p>Âõ†‰∏∫ÔºåÈ¶ñÂÖàËøôÊòØÂÆûÈ™åÊÄßË¥®ÁöÑÊìç‰Ωú„ÄÇÂÖ∂Ê¨°ÔºåÊàëÊúâ‰∏Ä‰∏™Â∑≤ÁªèÂºÄÂèëÁöÑÈ°πÁõÆÔºåÁÑ∂ÂêéÂèëÁé∞
&lt;a href="https://github.com/drivendataorg/cookiecutter-data-science">cookiecutter-data-science&lt;/a>
Êõ¥Êñ∞Âà∞ v2 ÁâàÊú¨‰∫ÜÔºåÊàëÊÉ≥Ë¶ÅËøô‰∏™ÁâàÊú¨ÁöÑÁõÆÂΩïÁªìÊûÑ„ÄÇÊúÄÂêéÔºåÊàëÊÉ≥Áü•ÈÅìËÉΩÂê¶ÂÆûÁé∞Ëøô‰∏™Êìç‰Ωú„ÄÇÁ≠îÊ°àÊòØÔºöÂèØ‰ª•Ôºå
ËÄå‰∏îÊå∫ÁÆÄÂçï„ÄÇ&lt;/p>
&lt;h2 id="ÂÆû‰Ωú" >
&lt;div>
&lt;a href="#%e5%ae%9e%e4%bd%9c">
#
&lt;/a>
ÂÆû‰Ωú
&lt;/div>
&lt;/h2>
&lt;p>1: Â∞Ü‰ªìÂ∫ìÁõÆÂΩï‰∏ãÁöÑ &lt;code>.git&lt;/code> Â§çÂà∂Âà∞ÊÉ≥Ë¶ÅÂêàÂπ∂ÁöÑ‚ÄúÈùûÁ©∫ÁõÆÂΩï‚Äù&lt;/p>
&lt;p>2: &lt;code>git reset --hard&lt;/code>&lt;/p>
&lt;p>3: Ê≤°Êúâ‰∫ÜÔºåÂ∞±ËøôÊ†∑Â∞±ÂèØ‰ª•„ÄÇ‰∏çÁî®ÁÇπÂºÄÂÖ®Êñá„ÄÇ&lt;/p></description></item><item><title>Class-Importance Weighted F2 Score (F2-CIW)</title><link>/posts/notes4resume/2024-08-14-f2-score/</link><pubDate>Wed, 14 Aug 2024 12:35:38 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/notes4resume/2024-08-14-f2-score/</guid><description>&lt;h2 id="why-use-f2-ciw" >
&lt;div>
&lt;a href="#why-use-f2-ciw">
#
&lt;/a>
Why Use F2-CIW?
&lt;/div>
&lt;/h2>
&lt;p>&lt;strong>Economic Impact&lt;/strong>: In the SewerML dataset, certain sewer defects may be more costly or
dangerous if undetected, so the model should prioritize accurately classifying these
defects.&lt;/p>
&lt;p>&lt;strong>Handling Class Imbalance&lt;/strong>: In datasets where some classes are underrepresented but
critically important, F2-CIW ensures that these classes are not overshadowed by more
frequent, less important classes.&lt;/p>
&lt;h2 id="traditional-f2-score" >
&lt;div>
&lt;a href="#traditional-f2-score">
#
&lt;/a>
Traditional F2 Score
&lt;/div>
&lt;/h2>
&lt;p>The F2 score is a variant of the F1 score, which is the harmonic mean of precision and
recall, but it gives more weight to recall. The F2 score is defined as:&lt;/p>
&lt;p>$$
F2 = \frac{5 \times \text{Precision} \times \text{Recall}}{4 \times \text{Precision} + \text{Recall}}
$$&lt;/p>
&lt;p>The F2 score is particularly useful in situations where you care more about capturing as
many true positives as possible, even if it means allowing more false positives.&lt;/p>
&lt;h2 id="class-importance-weighted-f2-score-f2-ciw" >
&lt;div>
&lt;a href="#class-importance-weighted-f2-score-f2-ciw">
#
&lt;/a>
Class-Importance Weighted F2 Score (F2-CIW)
&lt;/div>
&lt;/h2>
&lt;p>The Class-Importance Weighted F2 Score (F2-CIW) is a metric that adapts the traditional
F2 score to account for the economic or practical importance of different classes in a
classification task. This is particularly useful in imbalanced datasets or scenarios
where some classes have a higher impact on the overall outcome than others, such as in
the SewerML dataset.&lt;/p>
&lt;p>In tasks like those involving the SewerML dataset, different classes (e.g., types of
sewer defects) may have different levels of importance based on their economic impact or
other practical considerations. The F2-CIW metric adjusts the F2 score by assigning a
weight to each class that reflects its importance.&lt;/p>
&lt;h3 id="steps-to-compute-f2-ciw" >
&lt;div>
&lt;a href="#steps-to-compute-f2-ciw">
##
&lt;/a>
Steps to Compute F2-CIW:
&lt;/div>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Assign Weights to Classes&lt;/strong>: Each class $c$ is assigned a weight $w_c$ based on
its importance. The weights are typically derived from domain knowledge, such as the
economic impact of misclassifying that class.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Compute Per-Class F2 Scores&lt;/strong>: For each class, compute the F2 score $F2_c$ using
the standard formula.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Weighted Average of F2 Scores&lt;/strong>: The overall F2-CIW is computed as a weighted
average of the per-class F2 scores:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>$$
F2\text{-CIW} = \frac{\sum_{c} w_c \times F2_c}{\sum_{c} w_c}
$$&lt;/p>
&lt;p>This approach ensures that classes with higher importance have a more significant
influence on the final score.&lt;/p>
&lt;p>Overall, F2-CIW provides a more nuanced evaluation metric that aligns the model&amp;rsquo;s
performance with the practical importance of different outcomes.&lt;/p></description></item><item><title>pipe_table_and_md_to_pdf</title><link>/posts/thecli/md2pdf_using_pandoc/</link><pubDate>Mon, 05 Aug 2024 15:29:29 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/md2pdf_using_pandoc/</guid><description>&lt;h2 id="three-steps-to-produce-good-format-of-pdf-from-md" >
&lt;div>
&lt;a href="#three-steps-to-produce-good-format-of-pdf-from-md">
#
&lt;/a>
three steps to produce good format of .pdf from .md
&lt;/div>
&lt;/h2>
&lt;p>Â¶ÇÊûú‰Ω†ÁöÑ markdown ÊñáÊ°£‰∏çÊòØÂÖ®Ëã±ÊñáÔºåÂ¶ÇÊûú‰Ω†ÁöÑË°®Ê†ºÊòØ &lt;code>pipe table&lt;/code> Á±ªÂûãÔºàÁÆÄÂçïÊù•ËØ¥Â∞±ÊòØÁî® &amp;ldquo;|&amp;rdquo;
Ëøô‰∏™Á¨¶Âè∑Êù•Âõ¥ÊàêÔºåÊ¥ã‰∫∫Â∞±ÁÆ°ÂÆÉÂè´ pipe tableÔºâÔºåÁÑ∂ÂêéË°®Ê†º‰∏≠Êüê‰∫õÊ†ºÂ≠ê‰∏≠ÁöÑÂÜÖÂÆπÁâπÂà´ÈïøÔºåÂ¶ÇÊûúÂØÑÂ∏åÊúõ
Áõ¥Êé•Áî®pandoc ÈªòËÆ§ÂëΩ‰ª§ËøõË°åÊñáÊ°£Ê†ºÂºèËΩ¨Êç¢ÔºåÂàôÂ§ßÊ¶ÇÁéá‰∏ç‰ºöÁ´ãÂç≥ÂæóÂà∞‚ÄúÁæéËßÇ‚ÄùÁöÑpdfÊñáÊ°£„ÄÇÈúÄË¶ÅËøõË°å‰∏ÄÂÆö
ÁöÑÂ§ÑÁêÜÔºö&lt;/p>
&lt;ol>
&lt;li>ÊîæÂú® &lt;code>md&lt;/code> ÊñáÊ°£ÊúÄÂºÄÂ§¥ÁöÑÂÖÉÊï∞ÊçÆÔºàmeta dataÔºâÔºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>output:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pdf_document:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> latex_engine: xelatex
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> keep_tex: true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra_dependencies: [&amp;#34;fontspec&amp;#34;]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>header-includes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \usepackage{xeCJK}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \usepackage{tabularx}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \usepackage{fontspec}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \setmainfont{Noto Sans CJK SC}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \setmonofont{Noto Sans Mono CJK SC}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>table_preamble.tex Êñá‰ª∂&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">\u&lt;/span>sepackage&lt;span style="color:#ff6ac1">{&lt;/span>xeCJK&lt;span style="color:#ff6ac1">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">\u&lt;/span>sepackage&lt;span style="color:#ff6ac1">{&lt;/span>tabularx&lt;span style="color:#ff6ac1">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>ÂëΩ‰ª§Ë°åÂëΩ‰ª§&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>pandoc your_cool.md -o document.pdf --pdf-engine&lt;span style="color:#ff6ac1">=&lt;/span>xelatex --include-in-header&lt;span style="color:#ff6ac1">=&lt;/span>table_preamble.tex
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- ![tb01](images/blog_test_tb01_Screenshot_20240805_175815.png)![tb02](images/blog_test_tb02_Screenshot_20240805_180004.png) -->
&lt;hr>
&lt;h2 id="table-format-expected" >
&lt;div>
&lt;a href="#table-format-expected">
#
&lt;/a>
table format expected
&lt;/div>
&lt;/h2>
&lt;div style="display: flex; justify-content: space-between;">
&lt;img src="images/blog_test_tb01_Screenshot_20240805_175815.png" alt="Image 1" style="width: 45%;"/>
&lt;img src="images/blog_test_tb02_Screenshot_20240805_180004.png" alt="Image 2" style="width: 45%;"/>
&lt;/div>
&lt;hr>
&lt;h2 id="table-rendering-testing" >
&lt;div>
&lt;a href="#table-rendering-testing">
#
&lt;/a>
table rendering testing
&lt;/div>
&lt;/h2>
&lt;p>Ë°®1: ÊµãËØïË°®1 (pipe table syntax)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>| | time | describe | s2c | w2c |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| -- | ---- | ------- | -- | -- |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| 80891 | 2022-03-21 | xxÂå∫xxxxxxxxxÂçóË∑ØÂçóÂçÅ‰∏âÂ∑∑6Âè∑ÊóÅÔºå‰∏ÄÊöóÊ∏†Áõ¥ÊéíÊ±°Ê∞¥Ëá≥Èõ®Ê∞¥Ê≤üÔºåÊ∞¥Ë¥®ÊµëÊµäÔºåÊ∞îÂë≥ÈöæÈóªÔºåÂ∫ïÈÉ®... | H | L |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| 9191 | 2020-10-07 | Ê≤≥ÊÆµÂ∫∑ÈôÇË∑Ø5Âè∑ÂæÄ‰∏äÊ∏∏900Á±≥Ê≤≥Â∫äÊúâÊºÇÊµÆÁâ©,ÊéíÂá∫Ê±°Ê∞¥ | H | L |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| 765 | 2022-05-10 | ËîóÂü∫Ê∞¥Èó∏Âè£ÊúâÂ§ßÈáèÊºÇÊµÆÁâ©ÂíåÊ≠ªÈ±º | L | H |
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">time&lt;/th>
&lt;th style="text-align: left">describe&lt;/th>
&lt;th style="text-align: left">s2c&lt;/th>
&lt;th style="text-align: left">w2c&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">80891&lt;/td>
&lt;td style="text-align: left">2022-03-21&lt;/td>
&lt;td style="text-align: left">xxÂå∫xxxxxxxxxÂçóË∑ØÂçóÂçÅ‰∏âÂ∑∑6Âè∑ÊóÅÔºå‰∏ÄÊöóÊ∏†Áõ¥ÊéíÊ±°Ê∞¥Ëá≥Èõ®Ê∞¥Ê≤üÔºåÊ∞¥Ë¥®ÊµëÊµäÔºåÊ∞îÂë≥ÈöæÈóªÔºåÂ∫ïÈÉ®&amp;hellip;&lt;/td>
&lt;td style="text-align: left">H&lt;/td>
&lt;td style="text-align: left">L&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">9191&lt;/td>
&lt;td style="text-align: left">2020-10-07&lt;/td>
&lt;td style="text-align: left">Ê≤≥ÊÆµÂ∫∑ÈôÇË∑Ø5Âè∑ÂæÄ‰∏äÊ∏∏900Á±≥Ê≤≥Â∫äÊúâÊºÇÊµÆÁâ©,ÊéíÂá∫Ê±°Ê∞¥&lt;/td>
&lt;td style="text-align: left">H&lt;/td>
&lt;td style="text-align: left">L&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">765&lt;/td>
&lt;td style="text-align: left">2022-05-10&lt;/td>
&lt;td style="text-align: left">ËîóÂü∫Ê∞¥Èó∏Âè£ÊúâÂ§ßÈáèÊºÇÊµÆÁâ©ÂíåÊ≠ªÈ±º&lt;/td>
&lt;td style="text-align: left">L&lt;/td>
&lt;td style="text-align: left">H&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Ë°®2: ÊµãËØïË°®2 (tabularx table syntax)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>\begin{tabularx}{\textwidth}{|X|X|X|}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\hline
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Header 1 &amp;amp; Header 2 &amp;amp; Header 3 \\
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\hline
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Long content that should wrap &amp;amp; ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÂçóË∑ØÂçóÂçÅ‰∏âÂ∑∑6Âè∑ÊóÅÔºå‰∏ÄÊöóÊ∏†Áõ¥ÊéíÊ±°Ê∞¥Ëá≥Èõ®Ê∞¥Ê≤üÔºåÊ∞¥Ë¥®ÊµëÊµäÔºåÊ∞îÂë≥ÈöæÈóªÔºåÂ∫ïÈÉ®... &amp;amp; More content \\
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\hline
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\end{tabularx}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>\begin{tabularx}{\textwidth}{|X|X|X|}
\hline
Header 1 &amp;amp; Header 2 &amp;amp; Header 3 \
\hline
Long content that should wrap &amp;amp; ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÈùûÂ∏∏ÈïøÁöÑ‰∏ÄÊÆµËØù ÂçóË∑ØÂçóÂçÅ‰∏âÂ∑∑6Âè∑ÊóÅÔºå‰∏ÄÊöóÊ∏†Áõ¥ÊéíÊ±°Ê∞¥Ëá≥Èõ®Ê∞¥Ê≤üÔºåÊ∞¥Ë¥®ÊµëÊµäÔºåÊ∞îÂë≥ÈöæÈóªÔºåÂ∫ïÈÉ®&amp;hellip; &amp;amp; More content \
\hline
\end{tabularx}&lt;/p></description></item><item><title>matplotlibIssue</title><link>/posts/py101/pyplot/matplotlibissue/</link><pubDate>Thu, 01 Aug 2024 17:58:28 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/py101/pyplot/matplotlibissue/</guid><description>&lt;p>matplotlibÊòØ‰∏™Â•ΩÂ∑•ÂÖ∑Ôºå‰ΩÜËøòÊ≤°Â•ΩÈÄèÔºåÂÅöÂá∫Êù•ÁöÑÂõæ‰∏≠‰∏≠ÊñáÂèòÊ°ÜÊ°Ü„ÄÇÈúÄË¶ÅÈ¢ùÂ§ñËÆæÁΩÆ„ÄÇ&lt;/p>
&lt;ol>
&lt;li>Ëé∑Âèñ matplotlibrc&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Êü•ÁúãmatplotlibÂÆâË£ÖÂú®Âì™ÈáåÔºö&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python -c &lt;span style="color:#5af78e">&amp;#34;import matplotlib;print(matplotlib.__file__)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># your-venv/lib/python3.x/site-packages/matplotlib/mpl-data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Â§çÂà∂ matplotlibrc Âà∞Áî®Êà∑ÈÖçÁΩÆÁõÆÂΩï&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv your-venv/lib/python3.x/site-packages/matplotlib/mpl-data/matplotlibrc ~/.config/matplotlib/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>‰øÆÊîπ matplotlibrc&lt;/li>
&lt;/ol>
&lt;p>ÁºñËæëÂô®‰∏≠ÊâìÂºÄ matplotlibrc Êñá‰ª∂ÔºåÊâæÂà∞Âπ∂‰øÆÊîπ‰ª•‰∏ã‰∏§‰∏™Âú∞ÊñπÔºö&lt;/p>
&lt;p>&amp;lsquo;#font.family: sans-serif&amp;rsquo; &amp;raquo; font.family: sans-serif
&amp;lsquo;#font.sans-serif: DejaVu Sans,&amp;hellip;&amp;rsquo; &amp;raquo; font.sans-serif: DejaVu Sans,&amp;hellip;&lt;/p>
&lt;p>ÁÑ∂ÂêéÊ∑ªÂä†‰∏≠ÊñáÂ≠ó‰ΩìÔºàttf Êñá‰ª∂ÁöÑÊñá‰ª∂ÂêçÔºåÂ¶Ç‰∏ãÊñáÁöÑ &lt;code>simfang&lt;/code> ‰ªøÂÆãÂ≠ó‰ΩìÔºâ
font.sans-serif: &lt;code>simfang&lt;/code>, DejaVu Sans,&amp;hellip;&lt;/p>
&lt;p>ÊúÄÂêéÂ∞Ü &lt;code>simfang.ttf&lt;/code> Â≠ó‰ΩìÊñá‰ª∂Âä†ÂÖ•Âà∞ &lt;code>mpl-data/fonts/ttf/&lt;/code> ÁõÆÂΩï‰∏≠„ÄÇ&lt;/p>
&lt;p>Ê≥®ÔºöÊõ¥ÂÆπÊòìÁª¥Êä§ÁöÑ‰∏ÄÁßçÈÖçÁΩÆÊñπÂºèÂ∫îËØ•ÊòØËøôÊ†∑Ôºö
‰ΩøÁî® &lt;code>ln -s&lt;/code> ÂëΩ‰ª§Â∞Ü‰∏™‰∫∫Â≠ó‰ΩìÂ∫ì (&lt;code>.local/share/fonts/&lt;/code>) ‰∏≠ÁöÑÊüê‰∏™Â≠ó‰ΩìÊò†Â∞ÑÂà∞‰∏äËø∞ÁõÆÂΩï&lt;/p>
&lt;p>Ê≥®2ÔºöÂèØËÉΩÈúÄË¶ÅÂà†Èô§matplotlibÁöÑÁºìÂ≠ò&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>rm -rf /home/user/.cache/matplotlib
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>&lt;code>mpl.rcParams&lt;/code> ‰ª£Á†ÅÂùó&lt;/li>
&lt;/ol>
&lt;p>Âú®‰ΩúÂõæÁöÑÊ®°Âùó‰∏≠Ê∑ªÂä†‰ª•‰∏ã‰ª£Á†ÅÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> matplotlib &lt;span style="color:#ff6ac1">as&lt;/span> mpl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> matplotlib.pyplot &lt;span style="color:#ff6ac1">as&lt;/span> plt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ‰∏≠ÊñáÂáèÂè∑ÊòæÁ§∫ÈóÆÈ¢ò&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mpl&lt;span style="color:#ff6ac1">.&lt;/span>rcParams[&lt;span style="color:#5af78e">&amp;#39;axes.unicode_minus&amp;#39;&lt;/span>] &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ‰∏≠ÊñáÂ≠óÁ¨¶ÊòæÁ§∫ÈóÆÈ¢ò&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ÂêçÂ≠ó‰∏éttfÊñá‰ª∂ÁöÑÂêçÂ≠ó‰∏çÂêå: simfang.ttf &amp;gt;&amp;gt; &amp;#39;FangSong&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mpl&lt;span style="color:#ff6ac1">.&lt;/span>rcParams[&lt;span style="color:#5af78e">&amp;#39;font.sans-serif&amp;#39;&lt;/span>] &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#39;FangSong&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÊúÄÂêéÔºåÂ¶ÇÊûúÊúâÂ§ö‰∏™ venv ÁéØÂ¢ÉÔºåÈöæÈÅìË¶Å‰∏Ä‰∏™‰∏Ä‰∏™Âéª‰øÆÊîπÂêóÔºü
&lt;code>pipx&lt;/code> Â§ßÊ≥ïÊòØËß£ËçØ„ÄÇ&lt;/p></description></item><item><title>inspect_substrings_in_file_using_sed_awk_jq</title><link>/posts/thecli/awk_sed_cat_head_etc/awk_use_cases/</link><pubDate>Wed, 17 Jul 2024 19:12:45 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/awk_sed_cat_head_etc/awk_use_cases/</guid><description>&lt;p>Âú®ÂëΩ‰ª§Ë°åÂ∑•‰ΩúËøúÈùûÂÆåÁæéÔºå‰ΩÜÂÖ∑ÊúâÊûÅÈ´òÁöÑËá™Áî±Â∫¶„ÄÇÊØîÂ¶ÇÔºå‰Ω†ÊÉ≥Ë¶ÅÊü•ÁúãÊüê‰∏™Êñá‰ª∂ÂÜÖ(ËøôÈáåÁªùÂØπ‰∏çÊòØÊåá
doc/docx ËøôÁ±ªÊÄ™ËÉéÔºåËÄåÊòØ text/csv/tsv/json Á≠âÁ≠âÊ≠£Â∏∏ÁöÑÊñá‰ª∂) ÁöÑÊüê‰∏ÄË°åÁöÑÂÜÖÂÆπÔºåÂ¶ÇÊûúÈÇ£‰∏ÄË°åÊúâÂæà
Â§öÂàóÔºåËÄåÂÖ∂‰∏≠Ë¶ÅÊòØÊüê‰∏ÄÂàóÁöÑÂÜÖÂÆπÁâπÂà´ÈïøÔºàÂ∞±ÊòØ&lt;code>sed&lt;/code>Âá∫Êù•Âç†ÊçÆ‰∫ÜÊï¥‰∏™Â±èÂπïËøôÁßçÈïøÂ∫¶ÔºâÔºåÈÇ£‰πàÔºå‰Ω†ÊÉ≥Ë¶ÅÂØπ
Ëøô‰∏™Êñá‰ª∂ÁöÑÂÜÖÂÆπÊúâÊâÄ‰∫ÜËß£ÔºåÂèØËÉΩÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËΩØ‰ª∂Êù•ÊâìÂºÄÊ≠§Êñá‰ª∂ÔºåÂÜçÂ•ΩÂ•ΩÊ¨£Ëµè‰πã„ÄÇÊàñËÄÖÔºåÂÖ∂ÂÆûÂú®ÂëΩ‰ª§Ë°å
Â∞±ÂèØ‰ª•Âë¢Ôºü&amp;ndash; &lt;code>awk&lt;/code> ‰∫ÜËß£‰∏Ä‰∏ã„ÄÇ&lt;/p>
&lt;p>&lt;img alt="is_value_missing" src="https://fgg100y.github.io/posts/thecli/awk_sed_cat_head_etc/awk_use_cases/images/awk-to-the-rescue.png">&lt;/p>
&lt;p>ÂÉèÊàëÂ∞±ÊòØÁ¢∞‰∏äËøô‰πà‰∏™ÊÉÖÂÜµÔºåÂ∞ÜÂõæÁâáËΩ¨Êç¢Êàê base64 ÁºñÁ†ÅÁöÑ byte-data ÂêéÔºåÂú® Jupyterlab ÊâìÂºÄÂèëÁé∞
Êúâ‰∫õË°åÊÄé‰πàÊòØÁ©∫Ë°åÔºüÔºàÈ¶ñÂÖàÊàë‰∏çÊòØÂéªË¥®Áñë‰ª£Á†ÅÔºåËÄåÊòØË¥®Áñë‚ÄúÁúºËßÅ‰∏∫ÂÆû‚ÄùÔºâ&lt;/p>
&lt;p>ÊâÄ‰ª•ÔºåÂøÖÈ°ªË¶ÅÊü•ÁúãÊüê‰∏ÄË°åÁöÑÂÜÖÂÆπÔºåÂπ∂‰∏îË¶ÅÊà™Êñ≠ËøáÈïøÁöÑÂ≠ó‰∏≤ÔºåËÄÅ‰ºôËÆ° GPT4o ÂæàÂø´ÁªôÂá∫Á≠îÊ°àÔºö&lt;/p>
&lt;h2 id="tsv" >
&lt;div>
&lt;a href="#tsv">
#
&lt;/a>
TSV
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and truncate the 3rd column to 50 characters&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#5af78e">&amp;#39;NR==10 { $3=substr($3, 1, 50) &amp;#34;...&amp;#34;; print }&amp;#39;&lt;/span> &lt;span style="color:#ff5c57">FS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> &lt;span style="color:#ff5c57">OFS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> file.tsv
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ËøôËøòÊ≤°ÂÆåÔºåÂõ†‰∏∫ base64 ÁºñÁ†ÅÁöÑÁºòÊïÖÔºà‰ª•ÂèäËæìÂÖ•ÂõæÁâáÊØîËæÉÁõ∏‰ººÔºâÂØºËá¥ &lt;code>awk&lt;/code> Âà∞ÁöÑÁªìÊûúÈÉΩÊòØ‰∏ÄÊ®°‰∏ÄÊ†∑
ÁöÑËµ∑ÂßãÂ≠ó‰∏≤ÔºåËá™ÁÑ∂ËÄåÁÑ∂Â∞±ÊÄÄÁñëËøôÊòØÂ∑ßÂêàËøòÊòØÈîôËØØÔºåÊâÄ‰ª•ÔºåÂøÖÈ°ªÊü•ÁúãÂ≠ó‰∏≤Â∞æÈÉ®ÂÜÖÂÆπÊòØÂê¶‰πü‰∏ÄÊ†∑, ËøôÂΩìÁÑ∂
Èöæ‰∏çÂÄí‰∏ÄÁÇπËÑæÊ∞î‰πü‰∏ç‰ºöÊúâÁöÑ GPT4o ËÄÅÂì•Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># print the last 50 characters of the string in the 3rd column of the 10th row&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#5af78e">&amp;#39;NR==10 { len=length($3); print substr($3, len-50, 50) }&amp;#39;&lt;/span> &lt;span style="color:#ff5c57">FS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> file.tsv
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="jsonl" >
&lt;div>
&lt;a href="#jsonl">
#
&lt;/a>
JSONL
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and the field names of json object&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sed -n &lt;span style="color:#5af78e">&amp;#39;10p&amp;#39;&lt;/span> file.jsonl | jq &lt;span style="color:#5af78e">&amp;#39;keys&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and the first 10 chars of strings or 10 elements of array&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sed -n &lt;span style="color:#5af78e">&amp;#39;10p&amp;#39;&lt;/span> file.jsonl | jq &lt;span style="color:#5af78e">&amp;#39;.field_name[:10]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;p>ËØùÁó®Â§ö‰∏§Âè•Ôºö&lt;/p>
&lt;p>Ëá≥‰∫éÂèÇÊï∞‰ª£Ë°®Âï•ÊÑèÊÄùÔºåÊàëÈÄöÂ∏∏‰∏çËÆ© GPT4o Â§öË¥πÂîáËàå„ÄÇ&lt;/p>
&lt;p>ÔºàÊØïÁ´üÂ∞±ÁÆóËß£Èáä‰∫ÜÊàëËøòÊòØ‰ºöÂä®ÊâãÈ™åËØÅ‰∏ÄÁï™ÔºåÂÜçËÄÖÔºåÊúÄÂÖ≥ÈîÆÊòØÊàëËøòÊ≤°ÂºÄÈÄö plus ‰ºöÂëòÔºåÊâÄ‰ª•ÂæóÂê¨ÊüØÊôØËÖæ
‰ªñËÄÅÂ¶àÁöÑÈÇ£Âè•ÂäùÔºöË¶ÅÁúÅÁùÄÁÇπÁî®Âì¶ :Ôºâ&lt;/p></description></item><item><title>wsl2_and_unicode</title><link>/posts/wsl2_and_unicode/</link><pubDate>Tue, 16 Jul 2024 21:27:30 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/wsl2_and_unicode/</guid><description>&lt;h2 id="ÂÖ≥‰∫é-localeunicodejsondump-‰ª•ÂèäË¢´ÂÜ§ÊûâÁöÑ-wsl2" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e-localeunicodejsondump-%e4%bb%a5%e5%8f%8a%e8%a2%ab%e5%86%a4%e6%9e%89%e7%9a%84-wsl2">
#
&lt;/a>
ÂÖ≥‰∫é localeÔºåunicodeÔºåjson.dump() ‰ª•ÂèäË¢´ÂÜ§ÊûâÁöÑ WSL2
&lt;/div>
&lt;/h2>
&lt;p>WSL2ÁöÑ &lt;code>locale&lt;/code> ‰∏ÄÁõ¥ÊòØÈªòËÆ§ËÆæÁΩÆÂÄº &amp;ndash; &amp;ldquo;C.UTF-8&amp;rdquo;„ÄÇÁÑ∂Âêé‰ªäÂ§©Áî® python Ëß£ÊûêË∑ØÂæÑÊñá‰ª∂‰∏≠ÊñáÂêçÊó∂Ôºå
ÊúÄÁªà‰øùÂ≠ò‰∏ãÊù•ÁöÑÁ´üÊòØ unicode (&amp;quot;\u5c0f\u6728\u4ed4-\u7761\u7740\u4e862&amp;quot;)ÔºåËøôÂÄíÊòØÁ†¥Â§©ËçíÁ¨¨‰∏Ä
Ê¨°Á¢∞‰∏ä :&amp;rsquo;) ‰∏∫‰∫ÜÁúÅ‰∫ãÔºåÂøÖÈ°ªÂéªÊâæ GPT ÈòÅ‰∏ã„ÄÇ&lt;/p>
&lt;p>Êõ¥Êñ∞Ôºö‰∏ãÊñáÊòØÂÆåÂÖ®Áªï‰∫ÜËøúË∑Ø‰∫ÜÁöÑ„ÄÇÈÄ†Êàê‰∏äËø∞ÊÉÖÂÜµÁöÑÂÆûÈôÖÂéüÂõ†Â∫îËØ•ÊòØÔºö&lt;code>json.dump()&lt;/code> ÈªòËÆ§ÂèÇÊï∞‰∏≠ÁöÑ
&lt;code>ensure_ascii=True&lt;/code>ÔºåËøôÂ∞±ÂÜ≥ÂÆö‰∫ÜÂÜôÂÖ•Êñá‰ª∂ÁöÑÂ≠óÁ¨¶‰∏ç‰ºöË∂ÖÂá∫ASCIIÂ≠óÁ¨¶ÈõÜ„ÄÇ&lt;/p>
&lt;p>ÊâÄ‰ª•ÔºåÊ≠£Á°ÆÁöÑ‰øÆÊîπÊñπÂºè‰∏çÊòØÂ§ßË¥πÂë®Á´†ÁöÑÊîπÂÜôÁ≥ªÁªü &lt;code>locale&lt;/code> ËÄåÊòØÂ∞ÜÂèÇÊï∞Êîπ‰∏∫ &lt;code>ensure_ascii=False&lt;/code>„ÄÇ&lt;/p>
&lt;p>ÂÆûÈôÖ‰∏äÔºåjson.dump() ËøôÈÉ®ÂàÜÁöÑÂáΩÊï∞‰πüÊòØ GPT ÈòÅ‰∏ãÂÜôÁöÑÔºåÂÆÉÁêÜÊâÄÂΩìÁÑ∂Âú∞ËÆ§‰∏∫ÊòØÂú®Ëã±ÊñáËØ≠Â¢É‰∏≠Ôºå‰ΩøÁî®
ÈªòËÆ§ÁöÑÂèÇÊï∞‰∏ç‰∏∫ËøáÂêß„ÄÇüòÇ&lt;/p>
&lt;p>‰ΩÜ GPT4o ÊúÄÂÖàË¶ÅÊàëÁ°ÆËÆ§ÊòØÂê¶ÊòØÁªàÁ´ØÁöÑÁºñÁ†ÅÈóÆÈ¢òÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ËøôÂ∞±ÊòØÂéüÂéüÊú¨Êú¨ copy ÂæóÂà∞ÁöÑunicodeÂ≠óÁ¨¶‰∏≤&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">echo&lt;/span> -e &lt;span style="color:#5af78e">&amp;#34;\u5c0f\u6728\u4ed4-\u7761\u7740\u4e862&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># output: Â∞èÊú®‰ªî-Áù°ÁùÄ‰∫Ü2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÂÖ∂Ê¨°Ôºå‰ΩøÁî® &lt;code>file -bi&lt;/code> ÂëΩ‰ª§Êü•ÁúãÊñá‰ª∂ÁöÑÁºñÁ†ÅÊ†ºÂºèÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>file -bi path/to/the/suspicious_file.jsonl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#output: application/json; charset=us-ascii&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ËøôÂ∞±ÊÑèÂë≥ÁùÄÁ®ãÂ∫èÂú®ÂÜôÂÖ•Êú¨Âú∞Êñá‰ª∂Êó∂ÔºåÂè™‰ΩøÁî®‰∫Ü ascii Â≠óÁ¨¶„ÄÇ&lt;/p>
&lt;h2 id="ÂÖ≥‰∫é-escape-sequences-ÂèäÂÖ∂Ëß£Êûê" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e-escape-sequences-%e5%8f%8a%e5%85%b6%e8%a7%a3%e6%9e%90">
#
&lt;/a>
ÂÖ≥‰∫é escape sequences ÂèäÂÖ∂Ëß£Êûê
&lt;/div>
&lt;/h2>
&lt;p>ÁÑ∂Âêé GPT4o ÂæàËØ¶ÁªÜÁöÑÂëäÁü•Â∫îËØ•Â¶Ç‰ΩïÂú®ÂëΩ‰ª§Ë°åÁªàÁ´ØÁîüÊàêÂíåËÆæÁΩÆ &amp;ldquo;zh_CN.UTF-8&amp;rdquo;Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂú® python ‰∏≠ËøõË°å
ËΩ¨‰πâÁ≥ªÂàóÁöÑÈáçÁºñÁ†ÅÂíåËß£Á†ÅÊù•ÊòæÁ§∫‰∏≠ÊñáÂ≠ó‰∏≤Ôºå‰ΩÜÂ∞ùËØï‰πãÂêéÊ≤°ÊàêÂäü„ÄÇÂéüÊù•ÊòØÊàëÁêÜËß£ÊúâËØØ„ÄÇ&lt;/p>
&lt;p>GPT4o:
It appears there may be a misunderstanding regarding the purpose and usage of the
unicode-escape encoding. The unicode-escape codec is used to interpret escape sequences
in strings, but it does not process normal Unicode escape sequences (\uXXXX) that are
already present in a Unicode string.&lt;/p>
&lt;p>If you have a string that looks like a Unicode escape sequence and you need to convert
it into actual Unicode characters, you can use the unicode-escape codec. However, your
string needs to be encoded in a way that the codec understands.&lt;/p>
&lt;p>Here&amp;rsquo;s the corrected approach to convert a string with Unicode escape sequences into
actual Chinese characters:&lt;/p>
&lt;pre>&lt;code>Given String: You have a string with Unicode escape sequences.
Decode: Use unicode-escape to decode the string.
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Given string with Unicode escape sequences&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>unicode_str &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u5c0f&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u6728&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u4ed4-&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u7761&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u7740&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u4e862&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Decode the Unicode escape sequences&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>decoded_str &lt;span style="color:#ff6ac1">=&lt;/span> unicode_str&lt;span style="color:#ff6ac1">.&lt;/span>encode(&lt;span style="color:#5af78e">&amp;#39;utf-8&amp;#39;&lt;/span>)&lt;span style="color:#ff6ac1">.&lt;/span>decode(&lt;span style="color:#5af78e">&amp;#39;unicode-escape&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(decoded_str)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Explanation&lt;/p>
&lt;pre>&lt;code>The string unicode_str contains Unicode escape sequences.
The encode('utf-8') part ensures the string is in a byte format.
The decode('unicode-escape') part interprets the byte format as Unicode escape sequences and converts them into actual characters.
&lt;/code>&lt;/pre>
&lt;p>This should correctly output the Chinese characters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>Â∞èÊú®‰ªî-Áù°ÁùÄ‰∫Ü2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note&lt;/p>
&lt;p>Ensure that your string uses double backslashes (\uXXXX) if you&amp;rsquo;re defining it directly
in the script. This is necessary because a single backslash (\uXXXX) would be
interpreted as an actual Unicode character during the string definition phase, not as an
escape sequence.&lt;/p>
&lt;hr>
&lt;p>‰∏ÄÁÇπÁâ¢È™öÔºöÂèàÂ≠¶‰∫Ü‰∏Ä‰∫õÊ≤°Áî®ÁöÑÁü•ËØÜÔºåÊØïÁ´üÔºöÈ¶ñÂÖà‰Ω†‰∏∫‰ªÄ‰πàË¶ÅÁî®WSLËøôÁ†¥ÁÉÇÁé©ÊÑèÂÑøÂë¢ÔºüÔºüÔºü&lt;/p>
&lt;p>&lt;del>Âõ†‰∏∫ËøôÊú¨Êù•ÊòØ‰π∞ÁªôÈùûÁªàÁ´ØÁà±Â•ΩËÄÖ‰∫∫Â£´‰ΩøÁî®ÁöÑÁîµËÑë&lt;/del>&lt;/p>
&lt;p>&lt;del>ÂèØ‰∫∫ÂÆ∂ÊØïÁ´ü‰πüÊ≤°ÊúâÁî®ÔºåËøòÊòØ‰Ω†Ëá™‰∏™Èú∏Âç†ÁùÄ„ÄÇ
Ââç‰∏ç‰πÖÂèëÁé∞Â∑®Á°¨ÂèàÂú®Á≥ªÁªüÊòéÁÅ´Êâß‰ªóÂú∞ÊêûÂÅ∑Á™•Ê¥ªÂä®ÔºàËÆæÁΩÆ&amp;gt;ÈöêÁßÅÂíåÂÆâÂÖ®ÊÄß&amp;gt;Ê¥ªÂä®ÂéÜÂè≤ËÆ∞ÂΩïÔºâÂ∞±‰ø°Ë™ìÊó¶Êó¶ËØ¥
Á´ãÂàªÈ©¨‰∏äÂàª‰∏çÂÆπÁºìÂ∫üÊéâËøô‰∏™ÊÅ∂ÂøÉÁé©ÊÑè&lt;/del>&lt;/p>
&lt;p>&lt;del>ÊúÄËøëÊØîËæÉÂøôÂìîÂì©ÔºåÊñ∞ÂÆâË£ÖÁöÑ2TÁõò‰∏çÂ•ΩÊ∏ÖÁêÜÂêßÂï¶ÔºåÊÄªËßâÁùÄ‰∫∫ÂÆ∂ÊúÄÁªàË¶ÅÁî®ÂíïÂôú&lt;/del>&lt;/p>
&lt;p>Êú¨Ë¥®‰∏äÊòØÂõ†‰∏∫ÊçèÁùÄÈºªÂ≠êËøòËÉΩÂãâÂº∫Áî®Áî®Ôºå‰ΩÜ‰º∞ËÆ°‰πüÊíë‰∏ç‰∫ÜÂ§ö‰πÖ‰∫Ü„ÄÇ&lt;/p>
&lt;p>ÊØïÁ´ü Linux ÊâçÊòØ&lt;ruby>Ê†π&lt;rt>root&lt;/rt>&lt;/ruby>ÔºåÊâçÊúâ&lt;ruby>ÂÆ∂&lt;rt>home/&lt;/rt>&lt;/ruby>ÁöÑÊÑüËßâ :)&lt;/p>
&lt;p>Êõ¥Êñ∞ÔºöËôΩÁÑ∂ÂØºËá¥Êú¨ÊñáËØûÁîüÁöÑÂéüÂõ†ÂèëÁîü‰∫ÜÊ†πÊú¨ÊÄßÊîπÂèòÔºàËØ¶ËßÅÊñáÁ´†ÂºÄÂ§¥ÁöÑ‚ÄúÊõ¥Êñ∞‚ÄùÂÜÖÂÆπÔºâ‰ΩÜ‰∏çÂΩ±Âìç‚Äú‰∏ÄÁÇπÁâ¢
È™ö‚Äù‰πãÂêéÂíåÊú¨Êõ¥Êñ∞‰πãÂâçÁöÑÁâ¢È™öÂÜÖÂÆπ„ÄÇ&lt;/p></description></item><item><title>poor_dad</title><link>/posts/personals/momentsoflife/poor_dad/</link><pubDate>Mon, 15 Jul 2024 21:47:50 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/personals/momentsoflife/poor_dad/</guid><description>&lt;p>Âú∞ÈìÅÂè£Âá∫Âè£Â§Ñ„ÄÇÁ´ãÁùÄÁöÑË°åÊùéÁÆ±‰∏äÂÑøÂ≠êÂçäË∑™ÁùÄÂÄæÂêëÁà∏Áà∏ÔºåÁà∏Áà∏‰∏ÄÂè™ÊâãÊäìÁùÄË°åÊùéÁÆ±ÊùÜÂ≠êÔºåÂè¶‰∏ÄÂè™ÊâãÊãøÁùÄÊâãÊú∫„ÄÇ
Â•≥ÂÑøÂú®ÊóÅËæπ‰πüÊÉ≥ÂæÄË°åÊùéÁÆ±‰∏äÊå§ÔºåË¢´Áà∏Áà∏Âà∂Ê≠¢„ÄÇÂ•≥ÂÑøÂºÄÂßãÊúâÂì≠ËÖîÔºåÁà∏Áà∏Ê≤°ÁúãÂ•πÔºåÊ≥®ÊÑèÂäõÂÖ®Âú®ÊâãÊú∫‰∏äÔºå
Â§ßÁ∫¶ÊòØÂú®ÊâìËΩ¶„ÄÅÁúãÂú∞Âõæ„ÄÅËÅîÁ≥ª‰∫∫ÊàñËÄÖÁé©Ê∏∏ÊàèÔºå‰∏çÁü•ÈÅì„ÄÇÂ•≥ÂÑø‰∏âÁï™‰∏§Ê¨°ÔºåËøòÊòØ‰∏çË°åÔºåË¢´Áà∏Áà∏Â§ßÂ£∞ÂëµÊñ•Âç±Èô©„ÄÇ
ÂêéÈù¢Â•πÂπ≤ËÑÜÂæÄÂú∞‰∏äÂ∞±ÊòØ‰∏ÄÂùêÔºåËµñÂì≠ÁùÄ‰π±Ë∏¢ÂèåËÖø„ÄÇËµñÂì≠Â£∞Èáå‰º†ËææÁöÑÂ§ßÁ∫¶ÊòØÂßîÂ±àÁöÑ‰ø°Âè∑ÔºàÂâç‰∏ç‰πÖÊàëÂàöÈ¢ÜÊïô
ËøáÊàëÂ§ßËàÖÂì•‰ªñÈÇ£Â∑Æ‰∏çÂ§öÂπ¥Á∫™Â§ßÁöÑÂπºÁ®öÂõ≠Â•≥ÂÑøÁöÑËµñÂú∞ÂºèÂì≠ÈóπÔºåÊâÄ‰ª•ÂØπËøô‰∏™‰ø°Âè∑È¢á‰∏∫ÁÜüÊÇâÔºâ„ÄÇ&lt;/p>
&lt;p>Á≠âÊàëËµ∞‰∏ãÂú∞ÈìÅÂè£ÔºåÂßîÂ±à‰ø°Âè∑Â∑≤ÁªèÂèòÊàêÂ§ö‰∏™ÂùáÂåÄÈó¥ÈöîÁöÑÊ±ÇÂÖ≥Ê≥®‰ø°Âè∑‰∫Ü„ÄÇ&lt;/p>
&lt;hr>
&lt;p>‰∏ÄÁÇπÁâ¢È™öÔºö
ÂêéÊù•ÊàëÂ∞±ËøõÈó∏Âè£‰∫Ü„ÄÇÂ∞±ÁÆóÊúâÔºåÊàë‰πü‰∏çÂèØËÉΩËæ®Âà´Âá∫ÂõûÂ∫î‰ø°Âè∑‰∫ÜÔºõÂ∞±ÁÆóÊúâÔºåÊàñËÆ∏ÂõûÂ∫îÁöÑ‰πüÂ§™Ëøü‰∫ÜÁÇπ„ÄÇ&lt;/p>
&lt;p>ËøôÂèØËÉΩÊòØÂ∞è‰∫ã‰∏ÄÊ°©Ôºå‰πüÂèØËÉΩÊòØÂ§ß‰∫ã‰∏Ä‰ª∂„ÄÇÂõ†‰∏∫ÊàëÊÄªÂøç‰∏ç‰ΩèÊÉ≥Ëµ∑Ââç‰∏ç‰πÖÁúãÁöÑÈÇ£ÈÉ®ÂØºÊºîÊú¨Êù•ÊÉ≥ÊãçËÄÅÁà∏ÁªìÊûú
Êãç‰∫Ü‰ªñÂÆ∂ÈæôÂá§ÂèåËÉûËÉéÁöÑÁ∫™ÂΩïÁâá„ÄäÂßêÂßê„Äã(1996Âπ¥)„ÄÇ&lt;/p></description></item><item><title>python_module</title><link>/posts/py101/python_module/</link><pubDate>Wed, 10 Jul 2024 16:19:58 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/py101/python_module/</guid><description>&lt;h2 id="layout-of-python-installations" >
&lt;div>
&lt;a href="#layout-of-python-installations">
#
&lt;/a>
Layout of Python installations
&lt;/div>
&lt;/h2>
&lt;p>All paths in the table are relative to the installation root:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Files&lt;/th>
&lt;th style="text-align: left">Windows&lt;/th>
&lt;th style="text-align: left">Linux and macOS&lt;/th>
&lt;th style="text-align: left">Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Interpreter&lt;/td>
&lt;td style="text-align: left">python.exe&lt;/td>
&lt;td style="text-align: left">bin/python3.x&lt;/td>
&lt;td style="text-align: left">&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Standard library&lt;/td>
&lt;td style="text-align: left">Lib and DLLs&lt;/td>
&lt;td style="text-align: left">lib/python3.x&lt;/td>
&lt;td style="text-align: left">Extension modules are located under DLLs on Windows. Fedora places the standard library under lib64 instead of lib.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Third-party packages&lt;/td>
&lt;td style="text-align: left">Lib\site-packages&lt;/td>
&lt;td style="text-align: left">lib/python3.x/site-packages&lt;/td>
&lt;td style="text-align: left">Debian and Ubuntu put packages in dist-packages. Fedora places extension modules under lib64 instead of lib.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Entry-point scriptsa&lt;/td>
&lt;td style="text-align: left">Scripts&lt;/td>
&lt;td style="text-align: left">bin&lt;/td>
&lt;td style="text-align: left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="the-interpreter" >
&lt;div>
&lt;a href="#the-interpreter">
#
&lt;/a>
The interpreter
&lt;/div>
&lt;/h2>
&lt;p>The Python interpreter ties the environment to three things:&lt;/p>
&lt;ul>
&lt;li>A specific version of the Python language (e.g., 2.x.y, 3.x.y)&lt;/li>
&lt;li>A specific implementation of Python (e.g., CPython, PyPy)&lt;/li>
&lt;li>A specific build of the interpreter (e.g., 32bit, 64bit, Intel or Apple)&lt;/li>
&lt;/ul>
&lt;p>Try this command (in Bash) to print metadata complied into the interpreter:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>python3 -m sysconfig
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="the-modules" >
&lt;div>
&lt;a href="#the-modules">
#
&lt;/a>
The modules
&lt;/div>
&lt;/h2>
&lt;p>Modules are containers of Python objects that you load via the &lt;code>import&lt;/code> statement.
Modules come in various forms and shapes:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Simple modules
In the simplest case, a &lt;code>module&lt;/code> is a single file containing Python source code. The
statement &lt;code>import string&lt;/code> executes the code in &lt;em>string.py&lt;/em> and binds the result to
the name &lt;code>string&lt;/code> in the local scope.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Packages
Directories with &lt;em>&lt;strong>init&lt;/strong>.py&lt;/em> files are known as &lt;em>packages&lt;/em> which allow you to
organize modules in a hierarchy. The statement &lt;code>import email.message&lt;/code> loads the
&lt;code>message&lt;/code> module from the &lt;code>email&lt;/code> package.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Namespace packages
Directories with modules but no &lt;em>&lt;strong>init&lt;/strong>.py&lt;/em> are known as &lt;em>namespace packages&lt;/em>. One
may use them to organize modules in a common namespace such as a company name (say
&lt;code>dsta.voicedsp&lt;/code> and &lt;code>dsta.vedioprepr&lt;/code>). Unlike with regular packages, one can
distribute each module in a namespace package separately.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Extension modules
Extension modules such as &lt;code>math&lt;/code> module, contain native code compiled from low-level
language like C. They are shared libraries with a special entry-point&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> that lets
you import them as modules from Python. People write them for performance reasons or
to make existing C libraries available as Python modules. Their names end in &lt;code>.pyd&lt;/code>
on Windows, &lt;code>.dylib&lt;/code> on macOS, and &lt;code>.so&lt;/code> on Linux.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Built-in modules
Some modules from the standard library, such as &lt;code>sys&lt;/code> and &lt;code>builtins&lt;/code> modules, are
compiled into the interpreter. The variable &lt;code>sys.builtin_module_names&lt;/code> lists all of
these modules.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Frozen modules
Some modules from the standard library are written in Python but have their
bytecode&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> embedding in the interpreter. Recent versions of Python freeze every
module that&amp;rsquo;s imported during interpreter startup, such as &lt;code>os&lt;/code> and &lt;code>io&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="python-virtual-environment" >
&lt;div>
&lt;a href="#python-virtual-environment">
#
&lt;/a>
Python virtual environment
&lt;/div>
&lt;/h2>
&lt;p>Python environments consist of an interpreter and modules. Virtual environments share
the interpreter and the standard library with their parent environment.&lt;/p>
&lt;p>&lt;img alt="py-venv" src="https://fgg100y.github.io/posts/py101/python_module/images/py_python_and_venv.png">&lt;/p>
&lt;p>A Python environment can contain only a single version of each third-party package &amp;ndash;
if two projects require different versions of the same package, they can‚Äôt be
installed side by side. That‚Äôs why it‚Äôs considered good practice to install every Python
application, and every project you work on, in a dedicated virtual environment.&lt;/p>
&lt;p>NOTE
&lt;span class="custom-en">
The term &lt;em>package&lt;/em> carries some ambiguity in the Python world. It refers both to modules
and to the artifacts used for distributing modules (aka &lt;em>distributions&lt;/em>).
&lt;/span>&lt;/p>
&lt;h2 id="the-module-path" >
&lt;div>
&lt;a href="#the-module-path">
#
&lt;/a>
The module path
&lt;/div>
&lt;/h2>
&lt;p>It&amp;rsquo;s helpful to look at entries of &lt;code>sys.path&lt;/code> to debugg the import error, naturally,
one may wonder where do the entries on &lt;code>sys.path&lt;/code> come from at first?&lt;/p>
&lt;p>When the interpreter starts up, it constructs the module path in two steps:&lt;/p>
&lt;ol>
&lt;li>It builds an initial module path which includes the standard library,&lt;/li>
&lt;li>It imports the &lt;code>site&lt;/code> module (from standard library) which extends the module path to
include the site packages from the current environment.&lt;/li>
&lt;/ol>
&lt;p>The locations on the initial module path fall into three categories, and they occur in
this order:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The current directory or the directory containing the Python scripts (if any)
The first item on &lt;code>sys.path&lt;/code> can be any of the following:&lt;/p>
&lt;ul>
&lt;li>If you ran &lt;code>python3 &amp;lt;script&amp;gt;&lt;/code>, the directory where the script is located&lt;/li>
&lt;li>If you ran &lt;code>python3 -m &amp;lt;module&amp;gt;&lt;/code>, the current directory&lt;/li>
&lt;li>Otherwise, the empty string, which also denotes the current directory
&lt;strong>Safty issue&lt;/strong>: having the working directory on &lt;code>sys.path&lt;/code> is quite unsafe, as an
attacker (or you, mistakenly) can override the standard library by placing Python
files in the victim&amp;rsquo;s directory. To avoid this,&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>Python-3.11 provides &lt;code>-P&lt;/code> option or the PYTHONSAFEPATH variable to omit the
current directory from &lt;code>sys.path&lt;/code>;&lt;/li>
&lt;li>Using a virtual environment.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>The locations in the PYTHONPATH environment variable (if set)
Avoid this mechanism for the same reasons as the current working directory and use a
virtual environment instead.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The locations of the standard library
The location of the standard library is not hardcoded in the interpreter. Rather,
Python looks for landmark files on the path to its own executable and uses them to
locate the current environment (&lt;code>sys.prefix&lt;/code>) and the Python installation
(&lt;code>sys.base_prefix&lt;/code>). One such landmark file is &lt;em>pyvenv.cfg&lt;/em>, which marks a virtual
environment and points to its parent installation via the &lt;code>home&lt;/code> key. Another
landmark file is &lt;em>os.py&lt;/em>, the file containing the standard &lt;code>os&lt;/code> module: Python uses
&lt;em>os.py&lt;/em> to discover the prefix outside a virtual environment and to locate the
standard library itself.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>For more glory details and interesting contents, go to the newest version (2024) of
&lt;em>&amp;ldquo;Hypermodern Python Tooling&amp;rdquo;&lt;/em> by Claudio Jolowicz.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>An entry-point script is an executable file in &lt;em>Scripts/&lt;/em> (Windows) or &lt;em>bin/&lt;/em>
(Linux and macOS) with a single purpose: it launches a Python application by
importing the module with its entry-point function and calling that function.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Bytecode is an intermediate representation of Python code that is
platform-independent and optimized for fast execution. The interpreter compiles pure
Python modules to bytecode when it loads them for the first time. Their names end in
&lt;code>.pyc&lt;/code> and they are cached in &lt;code>__pycache__&lt;/code> directories.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Add_and_test_custom_font</title><link>/posts/hugo101/2024-07-09-test_custom_font/</link><pubDate>Tue, 09 Jul 2024 21:53:56 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/hugo101/2024-07-09-test_custom_font/</guid><description>&lt;p>Linear regression:&lt;/p>
&lt;blockquote>
&lt;p>$\text{You&amp;rsquo;ve got to draw the line somewhere.}$&lt;/p>
&lt;/blockquote>
&lt;p>You&amp;rsquo;ve got to draw the line somewhere. (system-font)&lt;/p>
&lt;p>&lt;span class="custom-en">You&amp;rsquo;ve got to draw the line somewhere. (NerdFont 3270)&lt;/span>&lt;/p>
&lt;p>&lt;span class="custom-zh">‰∏≠ÊñáÔºö‰Ω†ÂøÖÈ°ªÂú®Êüê‰∏™Âú∞ÊñπÊääÈÇ£Êù°Á∫øÁªôÁîªÂá∫Êù•„ÄÇ(‰∫¨ËèØËÄÅÂÆã‰Ωì)&lt;/span>&lt;/p>
&lt;hr>
&lt;p>ÂÆû‰ΩúËøáÁ®ãÔºö&lt;/p>
&lt;p>&lt;span class="custom-en">hugo version:
v0.128.2-de36c1a95d28595d8243fd8b891665b069ed0850+extended linux/amd64
BuildDate=2024-07-04T08:13:25Z VendorInfo=gohugoio&lt;/span>&lt;/p>
&lt;hr>
&lt;ol>
&lt;li>
&lt;p>Â∞ÜÊú¨Âú∞Â≠ó‰ΩìÊñá‰ª∂Â§çÂà∂Âà∞ &lt;code>static/fonts/&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Âú® &lt;code>assets/css/&lt;/code> (Êàñ &lt;code>static/css/&lt;/code>) ÂàõÂª∫Êñá‰ª∂ &lt;code>custom.css&lt;/code>:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-css" data-lang="css">&lt;span style="display:flex;">&lt;span>@&lt;span style="color:#ff6ac1">font-face&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#5af78e">&amp;#39;3270NerdFont&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">src&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">url&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;/fonts/3270NerdFont-Regular.ttf&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#ff6ac1">format&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;truetype&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-weight&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-style&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>@&lt;span style="color:#ff6ac1">font-face&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#5af78e">&amp;#39;KingHwa_OldSong&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">src&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">url&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;/fonts/KingHwa_OldSong.ttf&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#ff6ac1">format&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;truetype&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-weight&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-style&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.&lt;span style="color:#f3f99d">custom-en&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;3270NerdFont&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.&lt;span style="color:#f3f99d">custom-zh&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;KingHwa_OldSong&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Êõ¥Â§öÂÖ≥‰∫éËá™ÂÆö‰πâÂ≠ó‰ΩìÁöÑËÆ®ËÆ∫ÔºåÂèØÂèÇËÄÉËøôÈáå: &lt;a href="https://discourse.gohugo.io/t/how-to-add-custom-fonts-and-not-depend-on-google/34267/3">https://discourse.gohugo.io/t/how-to-add-custom-fonts-and-not-depend-on-google/34267/3&lt;/a>&lt;/p>
&lt;ol start="3">
&lt;li>‰øÆÊîπ &lt;code>config.toml&lt;/code>Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[params]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> customCSS = [&lt;span style="color:#5af78e">&amp;#34;css/custom.css&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="4">
&lt;li>Â§çÂà∂ &lt;code>themes/layouts/partials/head.html&lt;/code> Âà∞Ê†πÁõÆÂΩï&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>cp themes/layouts/partials/head.html ./layouts/partials/head.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Optional:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp themes/layouts/partials/head-extra.html ./layouts/partials/head-extra.html
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Â¶ÇÊûú‰∏ªÈ¢òËÆæËÆ°ÂæóË∂≥Â§üÂ•ΩÔºåÈÄöÂ∏∏‰ºöÁªô‰∏Ä‰ªΩ &lt;code>head-extra.html&lt;/code> Áî®‰∫éÂÆöÂà∂ÂåñÂÜÖÂÆπ„ÄÇ
Ê≠§Êó∂ÔºåÂè™ÈúÄË¶ÅÂ∞ÜÁõ∏Â∫îÈÖçÁΩÆÂä†ÂÖ•Âà∞ &lt;code>head-extra.html&lt;/code> Êñá‰ª∂‰∏≠ÔºåÈÅøÂÖçÂØπ &lt;code>head.html&lt;/code> ÁöÑÊâìÊâ∞„ÄÇ&lt;/p>
&lt;p>Â¢ûÂä†ÁöÑÁõ∏Â∫îÂÜÖÂÆπÊòØÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;lt;!-- Other head content --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{- range .Site.Params.customCSS }}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;&lt;span style="color:#ff6ac1">link&lt;/span> &lt;span style="color:#57c7ff">rel&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;stylesheet&amp;#34;&lt;/span> &lt;span style="color:#57c7ff">href&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;{{ . | relURL }}&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{- end }}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="5">
&lt;li>‰ΩøÁî®Ëá™ÂÆö‰πâÂ≠ó‰Ωì&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>You&amp;#39;ve got to draw the line somewhere. (system-font)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;span class=&amp;#34;custom-en&amp;#34;&amp;gt;You&amp;#39;ve got to draw the line somewhere. (NerdFont 3270)&amp;lt;/span&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;span class=&amp;#34;custom-zh&amp;#34;&amp;gt;‰∏≠ÊñáÔºö‰Ω†ÂøÖÈ°ªÂú®Êüê‰∏™Âú∞ÊñπÊääÈÇ£Êù°Á∫øÁªôÁîªÂá∫Êù•„ÄÇ(‰∫¨ËèØËÄÅÂÆã‰Ωì)&amp;lt;/span&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>rime_trime</title><link>/posts/personals/momentsoflife/rime_trime/</link><pubDate>Sat, 06 Jul 2024 15:34:59 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/personals/momentsoflife/rime_trime/</guid><description>&lt;p>&lt;a href="https://github.com/iDvel/rime-ice">rime-ice&lt;/a> ÈõæÂááÊãºÈü≥ + &lt;a href="https://github.com/osfans/trime">trime&lt;/a> ÂêåÊñáËæìÂÖ•Ê≥ï&lt;/p>
&lt;p>&lt;img alt="IMG_trime_a_grimpse" src="https://fgg100y.github.io/posts/personals/momentsoflife/rime_trime/images/trime_theme_4in1.png">&lt;/p>
&lt;p>ÊÄé‰πàÂú®ÊâãÊú∫‰∏ä‰ΩøÁî®Ëøô‰∏™ËæìÂÖ•Ê≥ïÔºüÊØîËæÉÊó†ËÑëÁúÅÂäõÈÇ£Áßç&amp;hellip;
Á≠âÁ≠âÂÖàÔºåÈô§ÂºÄ‰ªÄ‰πà‚ÄúÊÑ§‰∏ñÂ´â‰øó‚ÄùËøôÁßçÂÜ†ÂÜïÂ†ÇÁöáÔºåÊàñËÄÖ‚ÄúÊãíÁªùÈõ∑Âêå‚ÄùËøôÁßçÁ¨ëÊéâÂ§ßÁâôÔºå‰ª•Âèä‚ÄúÂê¨ËØ¥ÂÆÉÂæàÈöæÊêûÊàëÈùûË¶ÅÊêûÂÆöÂÆÉ‚ÄùËøôÁßçÂÅèÊâßÁãÇÁöÑÁêÜÁî±Â§ñÔºå‰∏∫‰ªÄ‰πàÁî®Ëøô‰∏™Ôºü
üëâ ÂºÄÊ∫ê‰∏áÂ≤Å„ÄÅÂºÄÊ∫ê‰∏áÂ≤Å„ÄÅÂºÄÊ∫ê‰∏áÂ≤Å„ÄÇ&lt;/p>
&lt;p>Â∫îËØ•ËØ¥ÊòéÔºöËøôÁØáÊñáÁ´†Âü∫Êú¨ÊòØÊàëËá™Â∑±ÁöÑÂ§áÂøòÂΩïÔºåÊàëËá™Â∑±Â∑≤Áªè‰ΩøÁî®Ëøô‰∫õÈÖçÁΩÆÂ§öÂπ¥„ÄÇ
Â¶ÇÊûú‰Ω†ÊòØÁ¨¨‰∏ÄÊ¨°Êé•Ëß¶ rime/TrimeÔºåÁõ¥Êé•Êü•Áúã‰ªñ‰ª¨ÂÆòÁΩëÊàñËÄÖÊú¨ÊñáÁöÑÂèÇËÄÉÈìæÊé•ÔºåÂê¶ÂàôÂ¶ÇÊûúÊåâÁùÄÊàëÁöÑËØ¥Ê≥ïËøõË°åÊìç‰ΩúÔºåÂ§ßÊ¶ÇÁéá‰Ω†‰ºöÊêûÂæóËá™Â∑±ÁÅ´ÂÜí‰∏â‰∏à„ÄÇ&lt;/p>
&lt;hr>
&lt;p>ÊàëÁöÑÊìç‰ΩúÔºö&lt;/p>
&lt;p>1.1 ÊâãÊú∫‰∏ä‰∏ãËΩΩ &lt;a href="https://f-droid.org/">F-Droid&lt;/a>ÔºåÂú® F-Droid ÊêúÁ¥¢Âπ∂‰∏ãËΩΩ Trime ËæìÂÖ•Ê≥ï (‰∏≠ÊñáÂêçÔºöÂêåÊñáËæìÂÖ•Ê≥ï&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>)&lt;/p>
&lt;p>1.2 ÊâãÊú∫Â∫îÁî®ÊéàÊùÉÁ≠âÂÖ∂‰ªñÊìç‰Ωú&lt;/p>
&lt;p>2.1 ÁîµËÑëÁ´Ø‰∏ãËΩΩ‰ªìÂ∫ì&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">cd&lt;/span> rime_hub
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>git clone https://github.com/iDvel/rime-ice.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>2.2 Â∞ÜÊâãÊú∫Êé•ÂÖ•ÁîµËÑëÔºåÂ∞Ü‰ªìÂ∫ìÂ§çÂà∂Âà∞ÊâãÊú∫ÁõÆÂΩïË∑ØÂæÑÔºàÈÄöÂ∏∏ÊòØ‰∏éDownload,DocumentsÂêåÁ∫ßÁöÑË∑ØÂæÑÔºâÔºåÂ∞Ü
ÂÖ∂ÂêçÁß∞‰øÆÊîπ‰∏∫ &lt;code>rime&lt;/code> (rime-ice‰πüÂèØ‰ª•Ôºå‰ΩÜË¶ÅÂÅöÂá∫Áõ∏Â∫îÁöÑÈÄÇÈÖçÂíåËÆæÁΩÆ)&lt;/p>
&lt;p>3.1 ÊâãÊú∫Á´ØÊâìÂºÄ Trime ËæìÂÖ•Ê≥ïÔºåÁÇπÂáªÂè≥‰∏äËßíÈÉ®ÁΩ≤&lt;/p>
&lt;p>3.2 ÂèØËÉΩ‰ºöÊä•ÈîôÂºπÂá∫Êó•ÂøóÁ™óÂè£Ôºå‰ΩÜ‰∏ç‰∏ÄÂÆöÂΩ±Âìç‰ΩøÁî®ÔºõÂê¶ÂàôÔºåÂéªÂÆòÁΩëÁ≠âÂú∞ÊñπÂØªÊâæËß£ÂÜ≥ÊñπÊ°à&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>4.1 Â¶ÇÊûú‰πüÂú®ÁîµËÑëÁ´Ø‰ΩøÁî®ÔºåÂèØ‰ª•ÂÄüÂä© Plum ËøõË°åÂÆâË£ÖÂíåÊõ¥Êñ∞Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>git clone --depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span> https://github.com/rime/plum
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">cd&lt;/span> plum
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#ÂÖ®ÈáèÊõ¥Êñ∞Ôºö&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#Â±ÄÈÉ®Êõ¥Êñ∞Ôºö&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/all_dicts
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ê≥®Ôºögithub.com ‰∏çÁªôÂäõÊó∂&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>Ôºå&lt;code>plum&lt;/code> Â∞±‰ºöÂ∑ßÂ¶áÈöæ‰∏∫Êó†Á±≥‰πãÁÇäÔºåÂõ†Ê≠§Â¶ÇÊûúÂèØ‰ª•Áõ¥Êé•‰ªéÁΩëÈ°µÁ´Ø‰∏ãËΩΩÂà∞
‰ªìÂ∫ìÊ∫êÁ†ÅÂéãÁº©ÂåÖÊàñËÄÖ‰ªéÂÖ∂‰ªñÂú∞ÊñπËé∑Âèñ(Â¶Ç&lt;code>rime-ice-master.zip&lt;/code>)ÔºåÈÇ£‰πàÊâãÂä®ÂÆâË£ÖÂÖ∂ÂÆû‰πüÂæàÁÆÄÂçïÔºö&lt;/p>
&lt;p>Â∞ÜÂéãÁº©ÂåÖËß£ÂéãÂπ∂Â∞ÜÊï∞ÊçÆÂ§çÂà∂&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>Âà∞‰ª•‰∏ãÁõÆÂΩïÔºàÊ†πÊçÆÂÆâË£ÖÊñπÂºè‰∏çÂêåÊù•ÈÄâÊã©ÔºâÔºö&lt;/p>
&lt;ul>
&lt;li>iBus ‰∏∫ &lt;code>$HOME/.config/ibus/rime/&lt;/code>&lt;/li>
&lt;li>Fcitx5 ‰∏∫ &lt;code>$HOME/.local/share/fcitx5/rime/&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>5 Â§öÁ´ØÂêåÊ≠•ÔºöÁîµËÑëÁ´ØÊõ¥Êñ∞ÔºåÂ§çÂà∂Âà∞ÊâãÊú∫Á≠âÂÖ∂‰ªñÁªàÁ´Ø„ÄÇ&lt;/p>
&lt;hr>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>ÂêåÊñáËæìÂÖ•Ê≥ïÔºöÊää RIME Ë£ÖËøõ Android ÊâãÊú∫ &lt;a href="https://sspai.com/post/77499">https://sspai.com/post/77499&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Rime ÈÖçÁΩÆÔºöÈõæÂááÊãºÈü≥ &lt;a href="https://dvel.me/posts/rime-ice/">https://dvel.me/posts/rime-ice/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Áî±‰∫éÁΩëÁªúÂéüÂõ† github.com ËÆøÈóÆ‰∏çÂà∞ÂÆûÂ±ûÁâπËâ≤Ôºå‰∏çÂ∫îÊÉäËÆ∂&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Âú®ÁõÆÊ†áÁõÆÂΩï‰∏ãÔºåÊØîÂ§çÂà∂Êõ¥ÁúÅ‰∫ãÁöÑÂÅöÊ≥ïÊòØÔºö&lt;code>ln -s path/to/rime-ice rime&lt;/code>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>ËÅåÁß∞ËØÑÂÆ°</title><link>/posts/personals/engineering_career_ladder/</link><pubDate>Fri, 05 Jul 2024 22:40:23 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/personals/engineering_career_ladder/</guid><description>&lt;p>ËôΩÁÑ∂‰∫∫ÂÆ∂Âè™ËØÑ‰º∞Ëøë‰∫îÂπ¥ÁöÑÂ∑•‰ΩúÔºå‰ΩÜÁ≤óÁï•ÁÆóËµ∑Êù•Ëá™Â∑±Â∑•‰ΩúÈÉΩÂø´ÂÖ´Âπ¥‰∫ÜÔºåÊâçÁà¨‰∫ÜÁ¨¨‰∫å‰∏™Èò∂Ê¢Ø„ÄÇ&lt;/p>
&lt;p>ÂçäË∑ØÂá∫ÂÆ∂ÁùÄÂÆûË¥πÂä≤„ÄÇ&lt;/p>
&lt;p>&lt;img alt="MidLevel" src="https://fgg100y.github.io/posts/personals/engineering_career_ladder/images/fmh_engineering_career_blur_extra_info.png">&lt;/p>
&lt;p>Á™ÅÁÑ∂ÊÉ≥Ëµ∑ÁªèÂÖ∏ÁîµÂΩ±„Ää‰∏âÊØõ‰ªéÂÜõËÆ∞„ÄãÁªìÂ∞æÈÉ®ÂàÜÁöÑÈ•±Âê´Ê≤ßÊ°ëËÄåÂèàËé´ÂêçÂñúÊÑüÁöÑÈÇ£‰∏ÄÂè•Âè∞ËØçÔºöÂÖ´Âπ¥‰∫Ü&amp;hellip;&lt;/p>
&lt;p>ÊÄ•Ë∫ÅÊó∂‰∏çÂÖçÊöóÊöóÂ•¢ÊúõÊúâ‰∫∫Ôºö‚ÄúÁúãÂú®ÊäÄÊúØÁöÑ‰ªΩ‰∏äÔºåÊãâÂÖÑÂºüÊàë‰∏ÄÊää„ÄÇ‚Äù üòÇ&lt;/p>
&lt;p>‰ΩÜËØùËØ¥ÂõûÊù•Ôºå‚ÄúÂΩìÂπ∏Á¶èÊù•Êï≤Èó®‚ÄùÁöÑÊó∂ÂÄôÔºå‰Ω†ÊúÄÂ•ΩÊòØÊÅ∞Âú®ÂÆ∂ÔºåÂÅöÂ•ΩÂáÜÂ§á„ÄÇ&lt;/p>
&lt;p>Ë¶Å‰∏çÁÑ∂ÁúüÊúâ‰∫∫Êãâ‰Ω†‰∏ÄÊääÊó∂ÔºåÁªìÊûúÂç¥ÂºÑÊàêÁÉÇÊ≥•Êâ∂‰∏ç‰∏äÂ¢ô„ÄÇÊÅêÊÄïÊ≤°ÊúâÊØîËøôÊõ¥Á≥üÁ≥ïÁöÑ‰∫Ü„ÄÇ&lt;/p></description></item><item><title>so_long_and_no_thanks</title><link>/posts/personals/momentsoflife/so_long/</link><pubDate>Thu, 04 Jul 2024 20:03:19 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/personals/momentsoflife/so_long/</guid><description>&lt;p>Â¶ÇÊûú‰Ω†ÁúãËøá„ÄäÈì∂Ê≤≥Á≥ªÊº´Ê∏∏ÊåáÂçó„ÄãÔºåÂ∫îËØ•ËÆ∞ÂæóÂú∞ÁêÉÁîüÁâ©Êô∫ÊÖßÊéíÁ¨¨‰∏ÄÁöÑÊòØÊµ∑Ë±öÔºà‰∫∫Á±ªÂè™ÊéíÁ¨¨‰∏âÔºâüòÇ&lt;/p>
&lt;p>Á¶ªÂà´Êó∂Êú¨Êù•Â∫îËØ•ÊòØÊù•‰∏ÄÂè•Ôºö&lt;/p>
&lt;blockquote>
&lt;p>‚ÄúSo long! And thanks for all the fish.&amp;quot;&lt;/p>
&lt;/blockquote>
&lt;p>‰ΩÜÂÆûÈôÖÊÉÖÂÜµÂç¥ÊòØÔºö&lt;/p>
&lt;p>&lt;img alt="no_thanks" src="https://fgg100y.github.io/posts/personals/momentsoflife/so_long/images/dophine.jpg">&lt;/p></description></item><item><title>Short_time_Fourier_Transform</title><link>/posts/dsp101/2024-07-03-short_time_fourier_transform/</link><pubDate>Wed, 03 Jul 2024 11:25:37 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/dsp101/2024-07-03-short_time_fourier_transform/</guid><description>&lt;p>Sine Wave Signal&lt;/p>
&lt;p>An audio signal, y(t), composed of exactly one sine wave, can be completely described by
the parameters $t, A, f$ and $\phi$,
$$
y(t) = A \sin(2 \pi f t + \phi)
$$
where $t$ represents time in seconds, $A$ is the wave&amp;rsquo;s amplitude (unit-less), $f$ is
its frequency in Hz, and $\phi$ is its phase offset in radians (i.e., where in the cycle
the wave is at $t=0$). If $t \ne 0$, then the sine wave appears shifted in time by
$\frac{\phi}{2 \pi f}$, where negative values mean &amp;ldquo;delay&amp;rdquo; and positive &amp;ldquo;advance&amp;rdquo; it.&lt;/p>
&lt;p>Fourier Series&lt;/p>
&lt;blockquote>
&lt;p>Our old pal Fourier told us that any sound can be represented as an infinite summation
of sine waves each with their own amplitudes, frequencies, and phase offsets. This means
that any sound we hear can be represented as many, many tuples of $t, A, f, \phi$.&lt;/p>
&lt;/blockquote>
&lt;p>Time-Frequency representation&lt;/p>
&lt;p>A Time-Frequency representation is a 2 dimensional matrix that represents the frequency
contents of an audio signal over time.&lt;/p>
&lt;p>We can visualize a TF Representation using a heatmap, which has time along the x-axis
and frequency along the y-axis. Each &lt;em>TF bin&lt;/em> (entry in heatmap) in the heatmap
represents the &lt;strong>amplitude&lt;/strong> of the signal at that particular time and frequency.
If there is no color bar, it is usually safe to assume that brighter colors indicate
higher amplitudes than darker colors.&lt;/p>
&lt;p>&lt;img alt="TFrepr" src="images/dsp_tf_representation.png">&lt;/p>
&lt;p>Short-time Fourier Transform (STFT)&lt;/p>
&lt;p>An STFT is calculated from a waveform representation by computing a discrete Fourier
transform (DFT) of a small, moving window across the duration of the window. The
location of each entry in an STFT determines its time (x-axis) and frequency (y-axis).
The absolute value of a TF bin |$X(t,f)$| at time t and frequency f determines the amount
of energy heard from frequency $f$ at time $t$.&lt;/p>
&lt;p>Importantly, each bin in our STFT is complex, meaning each entry contains both a
magnitude component and a phase component. Both components are needed to convert an STFT
matrix back to a waveform by &lt;em>inverse STFT&lt;/em> so that we may hear it.&lt;/p>
&lt;p>&lt;img alt="STFT" src="images/dsp_stft_process.png">&lt;/p>
&lt;p>Window Types&lt;/p>
&lt;p>The window type determines the shape of the short-time window that will segment the
audio into short segments before applying the DFT. The shape of this window will affect
which frequencies get emphasized or attenuated in the DFT. There are many types of
&lt;a href="https://docs.scipy.org/doc/scipy/reference/signal.windows.html">window functions&lt;/a>.&lt;/p>
&lt;p>&lt;img alt="windows" src="images/dsp_window_types.png">&lt;/p></description></item><item><title>Discrete_Fourier_Transform</title><link>/posts/dsp101/2024-07-03-discrete_fourier_transform/</link><pubDate>Wed, 03 Jul 2024 09:46:45 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/dsp101/2024-07-03-discrete_fourier_transform/</guid><description>&lt;p>An analysis problem, which is equivalent to the &lt;strong>DFT&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>Given a signal, how to find the amplitude and phase offset of its frequency
components?
&lt;/code>&lt;/pre>
&lt;p>A synthesis problem, which is equivalent to the &lt;strong>inverse DFT&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>Given a set of frequency components and their amplitudes, how can we construct a
signal?
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>DFT&lt;/strong> and &lt;strong>Inverse DFT&lt;/strong> in code&lt;/p>
&lt;p>The &lt;code>analyze()&lt;/code> computes something very close to the DFT, with one difference: The
conventional definition of DFT does not divide by &lt;code>N&lt;/code> (highlighted line below):&lt;/p>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">analyze&lt;/span>(ys):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> amps &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>conj()&lt;span style="color:#ff6ac1">.&lt;/span>transpose()&lt;span style="color:#ff6ac1">.&lt;/span>dot(ys) &lt;span style="color:#ff6ac1">/&lt;/span> N
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> amps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">dft&lt;/span>(ys): &lt;span style="color:#78787e"># return same result as np.fft.fft()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> amps &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>conj()&lt;span style="color:#ff6ac1">.&lt;/span>transpose()&lt;span style="color:#ff6ac1">.&lt;/span>dot(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> amps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">idft&lt;/span>(amps): &lt;span style="color:#78787e"># inverse DFT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(amps)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ys &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>dot(amps) &lt;span style="color:#ff6ac1">/&lt;/span> N
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> ys &lt;span style="color:#78787e"># ys: value of the signal at each time step&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">synthesis_matrix&lt;/span>(N):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ts &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(N) &lt;span style="color:#ff6ac1">/&lt;/span> N &lt;span style="color:#78787e"># ts: the sequence of times where the signal should be evaluated&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fs &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(N) &lt;span style="color:#78787e"># fs: the list of frequencies&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>outer(ts, fs) &lt;span style="color:#78787e"># args: outer product of ts and fs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>exp(&lt;span style="color:#ff9f43">1&lt;/span>j &lt;span style="color:#ff6ac1">*&lt;/span> PI2 &lt;span style="color:#ff6ac1">*&lt;/span> args) &lt;span style="color:#78787e"># PI2 = np.pi * 2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>ResNet</title><link>/posts/neuralnetworks/resnet/</link><pubDate>Sat, 29 Jun 2024 09:17:15 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/neuralnetworks/resnet/</guid><description>&lt;p>ÊÄùÁª¥ÂÆûÈ™åÔºö
Âú®ÊµÖÂ±ÇÁΩëÁªúÁªìÊûÑÁöÑÂü∫Á°Ä‰∏äÔºàÊØîÂ¶Ç20Â±ÇÔºâÔºåÂæÄÂêéÈù¢Áõ¥Êé•Ê∑ªÂä†Êõ¥Â§öÁöÑÂêåÊò†Â∞ÑÈöêÂ±ÇÔºàidentity layersÔºâÔºå
ÂæóÂà∞ÁöÑÊ∑±Â±ÇÁΩëÁªúÔºàÊØîÂ¶Ç50Â±ÇÔºâÁêÜËÆ∫‰∏äÊïàÊûúÂ∫îËØ•‰∏ç‰ºöÂèòÂ∑Æ„ÄÇ‰ΩÜÂÆûÈ™åÁªìÊûúËØ¥ÊòéÔºåÂÆÉÁúü‰ºöÂèòÂ∑Æ„ÄÇÊÑèÂë≥ÁùÄÔºö
SGDÁÆóÊ≥ïÊó†Ê≥ïÊâæÂà∞‰ΩøÂæóÊõ¥Ê∑±Â±ÇÁΩëÁªúÊÄßËÉΩ‰∏çÂèòÂ∑ÆÁöÑÂèÇÊï∞„ÄÇ&lt;/p>
&lt;p>ÊÆãÂ∑ÆÁΩëÁªúÊû∂ÊûÑÂèØ‰ª•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;h2 id="why-what-and-how" >
&lt;div>
&lt;a href="#why-what-and-how">
#
&lt;/a>
Why, What, and How
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>Deeper neural networks are more difficult to train. We present a residual learning
framework to ease the training of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learning residual functions with
reference to the layer inputs, instead of learning unreferenced functions.&lt;/p>
&lt;/blockquote>
&lt;p>ËÆ≠ÁªÉ‰∏Ä‰∏™Ê∑±Â∫¶ÔºàË∂≥Â§üÊ∑±ÁöÑÔºâÁ•ûÁªèÁΩëÁªúÊòØ‰∏Ä‰ª∂ÂæàÈöæÁöÑ‰∫ãÊÉÖÔºà2015Âπ¥Ôºâ„ÄÇ
‰ΩøÁî®‚ÄúÊÆãÂ∑Æ‚ÄùÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÂèØ‰ª•Êõ¥ÂÆπÊòìÂú∞ËÆ≠ÁªÉË∂≥Â§üÊ∑±ÁöÑÁ•ûÁªèÁΩëÁªú„ÄÇ
‚ÄúÊÆãÂ∑Æ‚ÄùÊû∂ÊûÑÂ∞±ÊòØÊääËøô‰∫õ‰∏≠Èó¥Â±Ç‰Ωú‰∏∫‰∏Ä‰∏™Â≠¶‰π†ËæìÂÖ•‰∏éËæìÂá∫ÁöÑÊÆãÂ∑ÆÁöÑÂáΩÊï∞„ÄÇ&lt;/p>
&lt;p>&lt;img alt="ResNet Block" src="images/resnet_learning_block.png">&lt;/p>
&lt;p>Â∞±ÊòØËØ¥ÔºöÂ¢ûÂä†ÁöÑÈöêÂ±ÇÂéªÂ≠¶‰π† $h(x) - x$ÔºàÊÆãÂ∑ÆÔºâËÄå‰∏çÊòØ $h(x)$ÔºåËÄåËæìÂá∫ÁöÑÊòØÂΩìÂâçÈöêÂ±ÇÁöÑËæìÂá∫Âä†‰∏äÂâç
‰∏ÄÂ±ÇÁöÑËæìÂá∫ $x$ ÔºàÂêåÊó∂‰πüÊòØÂΩìÂâçÂ±ÇÁöÑËæìÂÖ•Ôºâ„ÄÇ&lt;/p>
&lt;h2 id="ÊÆãÂ∑ÆÁΩëÁªúÁªìÊûÑ" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84">
#
&lt;/a>
ÊÆãÂ∑ÆÁΩëÁªúÁªìÊûÑ
&lt;/div>
&lt;/h2>
&lt;p>ÊÆãÂ∑ÆËøûÊé•Ôºàaka, shortcut connectionÔºâÂú®ËæìÂÖ•ÂíåËæìÂá∫ÁöÑÁª¥Â∫¶Áõ∏ÂêåÊÉÖÂÜµ‰∏ãÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®Â¶Ç‰∏ãÁªìÊûÑÂùóÔºö&lt;/p>
&lt;p>$$
y = F(x, {W_i}) + x.
$$&lt;/p>
&lt;p>ÂΩìÊÉ≥Ë¶ÅÂ¢ûÂä†Áª¥Â∫¶Êó∂ÔºåÊúâ‰∏§ÁßçÊñπÂºèÊù•ÂØπÈΩêËæìÂÖ•ÂíåËæìÂá∫ÁöÑÁª¥Â∫¶Ôºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÊÆãÂ∑ÆËøûÊé•ËøõË°åÂêåÊò†Â∞ÑÊìç‰ΩúÔºåÁÑ∂ÂêéÂØπÂ¢ûÂä†ÁöÑÁª¥Â∫¶ËøõË°åË°•Èõ∂Êìç‰ΩúÔºàpadding zeros)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊÆãÂ∑ÆËøûÊé•ËøõË°åÁ∫øÊÄßÊäïÂΩ±Êìç‰Ωú $y = F(x, {W_i}) + W_s x.$ Êù•ÂØπÈΩêÁª¥Â∫¶Ôºà‰ΩøÁî® 1x1 Âç∑ÁßØÂÆûÁé∞Ôºâ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>‰∏§ÁßçÊÉÖÂÜµ‰∏ãÔºåÂΩìÊÆãÂ∑ÆËøûÊé•ÁöÑÊòØ‰∏çÂêåÂ∞∫ÂØ∏ÁöÑÁâπÂæÅÂõæÔºàfeature mapÔºâÊó∂Ôºå‰ΩøÁî®Ê≠•ÂπÖ‰∏∫2ÁöÑÂç∑ÁßØÊìç‰Ωú„ÄÇ
ÔºàÈÄöÈÅìÊï∞ÁøªÂÄçÔºåÂàôÈ´òÂÆΩÂáèÂçäÔºåÂõ†Ê≠§‰ΩøÁî®Ê≠•ÂπÖ=2Ôºàstride=2ÔºâÁöÑÂç∑ÁßØÊù•‰øùËØÅÁª¥Êï∞ÂØπÈΩêÔºâ&lt;/p>
&lt;p>&lt;img alt="bottlenet-block" src="https://fgg100y.github.io/posts/neuralnetworks/resnet/images/resnet_bottlenet_building_block.png">&lt;/p>
&lt;p>&lt;img alt="resnet-arch" src="https://fgg100y.github.io/posts/neuralnetworks/resnet/images/resnet_arch_for_imagenet.png">&lt;/p>
&lt;h2 id="ÊÆãÂ∑ÆËøûÊé•‰∏∫‰ªÄ‰πàÊúâÁî®" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e7%94%a8">
#
&lt;/a>
ÊÆãÂ∑ÆËøûÊé•‰∏∫‰ªÄ‰πàÊúâÁî®Ôºü
&lt;/div>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Easier Gradient Flow&lt;/p>
&lt;ul>
&lt;li>ÁºìËß£Ê¢ØÂ∫¶Ê∂àÂ§±/Ê¢ØÂ∫¶ÁàÜÁÇ∏ÈóÆÈ¢ò&lt;/li>
&lt;li>ÊÆãÂ∑ÆËøûÊé•‰ΩøÂæóÊ¢ØÂ∫¶Êõ¥Á®≥ÂÆö&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Êõ¥ÂÆπÊòìÂ≠¶‰π†ÂêåÊò†Â∞ÑÂáΩÊï∞&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊèêÂçáÊúÄ‰ºòÂåñÂíåÊî∂ÊïõÊïàÊûú&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Êõ¥Â•ΩÁöÑÁâπÂæÅ‰º†Êí≠Ôºàfeature propagationÔºâ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="Ê≠§ÊÆãÂ∑ÆÈùûÂΩºÊÆãÂ∑Æresnet-vs-gdbt" >
&lt;div>
&lt;a href="#%e6%ad%a4%e6%ae%8b%e5%b7%ae%e9%9d%9e%e5%bd%bc%e6%ae%8b%e5%b7%aeresnet-vs-gdbt">
#
&lt;/a>
Ê≠§ÊÆãÂ∑ÆÈùûÂΩºÊÆãÂ∑ÆÔºàResNet VS GDBTÔºâ
&lt;/div>
&lt;/h2>
&lt;p>GBDTÔºàÊ¢ØÂ∫¶ÊèêÂçáÂÜ≥Á≠ñÊ†ëÔºâ‰∏≠ÁöÑÊÆãÂ∑ÆÔºàresidualÔºâÊòØÊåáÈ¢ÑÊµãÂÄº‰∏éÁúüÂÆûÂÄº‰πãÈó¥ÁöÑÂ∑ÆÂÄº„ÄÇ&lt;/p>
&lt;p>ResNet ‰∏≠ÁöÑÊÆãÂ∑ÆÔºàresidualÔºâÔºåÊòØÁ•ûÁªèÁΩëÁªúÂ∞ùËØïÂéªÂ≠¶‰π†ÁâπÂæÅÂõæÔºàfeature mapÔºâÁöÑÊÆãÂ∑ÆÔºàÊòØÁΩëÁªúÂùóËæìÂá∫‰∏éËæìÂÖ•‰πãÈó¥ÁöÑÂ∑ÆÂÄºÔºâ„ÄÇ&lt;/p>
&lt;p>Paper: &lt;a href="https://arxiv.org/pdf/1512.03385">https://arxiv.org/pdf/1512.03385&lt;/a>&lt;/p></description></item><item><title>knowledge_distillation2</title><link>/posts/clipmodel/2024-06-28-knowledge_distillation2/</link><pubDate>Fri, 28 Jun 2024 11:25:43 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/clipmodel/2024-06-28-knowledge_distillation2/</guid><description>&lt;h2 id="generalized-knowledge-distillation-gkd" >
&lt;div>
&lt;a href="#generalized-knowledge-distillation-gkd">
#
&lt;/a>
Generalized Knowledge Distillation (GKD)
&lt;/div>
&lt;/h2>
&lt;p>Ê≥õÂåñÁü•ËØÜËí∏È¶èÊòØ‰∏ÄÁßçÊîπËøõÁöÑÁü•ËØÜËí∏È¶èÊäÄÊúØÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂú®Ëá™ÂõûÂΩíÂ∫èÂàóÊ®°Âûã‰∏≠ÈÅáÂà∞ÁöÑÂàÜ
Â∏É‰∏çÂåπÈÖçÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÈò∂ÊÆµ‰πãÈó¥„ÄÇ‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÈÄöÂ∏∏Âü∫‰∫éÂõ∫ÂÆöÁöÑËæìÂá∫Â∫èÂàóÈõÜËøõË°åÔºå
Ëøô‰∫õÂ∫èÂàóÊàñËÄÖÊòØÊïôÂ∏àÊ®°ÂûãÁîüÊàêÁöÑÔºåÊàñËÄÖÊòØÂü∫‰∫éÁúüÂÆûÊï∞ÊçÆÁöÑÊ†áÁ≠æ„ÄÇÁÑ∂ËÄåÔºåËøôÂØºËá¥Â≠¶ÁîüÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ÁîüÊàê
ÁöÑÂ∫èÂàóÂèØËÉΩ‰∏éËÆ≠ÁªÉÊó∂ËßÅÂà∞ÁöÑÂ∫èÂàóÂàÜÂ∏É‰∏çÂêåÔºå‰ªéËÄåÂΩ±Âìç‰∫ÜÂ≠¶ÁîüÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;p>GKD ‰∏çÂÜçÂ±ÄÈôê‰∫éÂõ∫ÂÆöËæìÂá∫Â∫èÂàóÁöÑËÆ≠ÁªÉÔºåËÄåÊòØÂÖÅËÆ∏Â≠¶ÁîüÊ®°ÂûãÂú®ÂÖ∂Ëá™ÊàëÁîüÊàêÁöÑÂ∫èÂàó‰∏äËøõË°åÂ≠¶‰π†ÔºåÂêåÊó∂Âà©Áî®
ÊïôÂ∏àÊ®°ÂûãÊèê‰æõÁöÑÂèçÈ¶à„ÄÇ&lt;/p>
&lt;h2 id="gkd-ÊäÄÊúØË¶ÅÁÇπ" >
&lt;div>
&lt;a href="#gkd-%e6%8a%80%e6%9c%af%e8%a6%81%e7%82%b9">
#
&lt;/a>
GKD ÊäÄÊúØË¶ÅÁÇπ
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Âä®ÊÄÅÂ∫èÂàóÁîüÊàê‰∏éÂèçÈ¶àÔºöGKD Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ºöÊ†πÊçÆÂ≠¶ÁîüÊ®°ÂûãÁöÑËá™ÊàëÁîüÊàêÂ∫èÂàóËøõË°åÂ≠¶‰π†„ÄÇËøôÊÑèÂë≥ÁùÄÂ≠¶
ÁîüÊ®°ÂûãÂú®Â∞ùËØïÁîüÊàêÂ∫èÂàóÊó∂ÔºåÂ¶ÇÊûúÁîüÊàêÈîôËØØÊàñ‰∏çÁêÜÊÉ≥ÁöÑÂ∫èÂàóÔºåÂÆÉÂèØ‰ª•‰ªéÊïôÂ∏àÊ®°ÂûãÈÇ£ÈáåËé∑ÂæóÁõ¥Êé•ÂèçÈ¶àÔºå
Â≠¶‰π†Â¶Ç‰ΩïÁ∫†Ê≠£Ëøô‰∫õÈîôËØØ„ÄÇËøôÁßçÊú∫Âà∂ËÉΩÂ§üÂáèÂ∞ëËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÊèêÈ´òÂ≠¶ÁîüÊ®°ÂûãÂØπÂêÑÁßçËæì
ÂÖ•Â∫èÂàóÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÁÅµÊ¥ªÁöÑÊçüÂ§±ÂáΩÊï∞ÈÄâÊã©Ôºö‰∏é‰ªÖÈááÁî®ÂâçÂêëKLÊï£Â∫¶ÁöÑ‰º†ÁªüÁõëÁù£ÂºèÁü•ËØÜËí∏È¶è‰∏çÂêåÔºåGKD Êèê‰æõ‰∫ÜÁÅµÊ¥ªÊÄßÔºåÂèØ‰ª•
Âú®Â≠¶ÁîüÂíåÊïôÂ∏àÊ®°Âûã‰πãÈó¥‰ΩøÁî®Êõø‰ª£ÁöÑÊçüÂ§±ÂáΩÊï∞„ÄÇÂΩìÂ≠¶ÁîüÊ®°ÂûãÈöæ‰ª•Á≤æÁ°ÆÊ®°ÊãüÊïôÂ∏àÊ®°ÂûãÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÊó∂ÔºåËøô
‰∏ÄÁÇπÂ∞§‰∏∫ÈáçË¶Å„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•ÈááÁî®Jensen-ShannonÊï£Â∫¶Á≠âÊåáÊ†áÊù•Ë°°ÈáèÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåËøôÊúâÂä©‰∫éÂ≠¶Áîü
Ê®°ÂûãÂú®Ë°®ËææÂäõÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÊõ¥Â•ΩÂú∞Â≠¶‰π†„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÈõÜÊàêÂº∫ÂåñÂ≠¶‰π†Á≤æË∞ÉÔºöGKD Ëøò‰øÉËøõ‰∫ÜÁü•ËØÜËí∏È¶è‰∏éÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËØ≠Ë®ÄÊ®°ÂûãÂæÆË∞ÉÁöÑÊó†ÁºùÊï¥Âêà„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="Âä®ÊÄÅÂ∫èÂàóÁîüÊàê" >
&lt;div>
&lt;a href="#%e5%8a%a8%e6%80%81%e5%ba%8f%e5%88%97%e7%94%9f%e6%88%90">
##
&lt;/a>
Âä®ÊÄÅÂ∫èÂàóÁîüÊàê
&lt;/div>
&lt;/h3>
&lt;p>Âú®‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶è‰∏≠ÔºåÂ≠¶ÁîüÊ®°ÂûãÈÄöÂ∏∏Âü∫‰∫éÈ¢ÑÂÖàÂÆö‰πâÂ•ΩÁöÑÂõ∫ÂÆöÊï∞ÊçÆÈõÜÊàñÁî±ÊïôÂ∏àÊ®°ÂûãÁîüÊàêÁöÑËæìÂá∫Â∫èÂàóËøõË°å
ËÆ≠ÁªÉ„ÄÇËÄåÂä®ÊÄÅÂ∫èÂàóÁîüÊàêÂàôÊâìÁ†¥‰∫ÜËøô‰∏ÄÈôêÂà∂ÔºåÂÆÉËÆ©Â≠¶ÁîüÊ®°ÂûãËá™Â∑±ÁîüÊàêËæìÂá∫Â∫èÂàóÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äËøõË°åÂ≠¶‰π†„ÄÇ
Ëøô‰∏™ËøáÁ®ãÊ∂âÂèä‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÂàùÂßãÂåñÂ≠¶ÁîüÊ®°ÂûãÔºöÈ¶ñÂÖàÔºåÂ≠¶ÁîüÊ®°ÂûãÈúÄË¶ÅÂÖ∑Â§á‰∏ÄÂÆöÁöÑÁîüÊàêËÉΩÂäõÔºåÈÄöÂ∏∏ÈÄöËøáÂàùÊ≠•ÁöÑÁõëÁù£ÂºèÈ¢ÑËÆ≠ÁªÉÊàñÂæÆË∞É
ÔºàSupervised Fine-Tuning, SFTÔºâÊù•ÂÆûÁé∞ÔºåÁ°Æ‰øùÂÆÉÂèØ‰ª•ÁîüÊàêË¥®ÈáèÂ∞öÂèØÁöÑÂ∫èÂàó„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ëá™ÊàëÁîüÊàêÂ∫èÂàóÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂ≠¶ÁîüÊ®°ÂûãÊé•Êî∂Âà∞ËæìÂÖ•ÂêéÔºå‰ºöÊ†πÊçÆÂΩìÂâçÁöÑÁ≠ñÁï•ÁîüÊàêËæìÂá∫Â∫èÂàó„ÄÇËøô‰∫õÂ∫è
ÂàóÂèçÊò†‰∫ÜÂ≠¶ÁîüÊ®°ÂûãÂΩìÂâçÁöÑËÉΩÂäõÂíåÁêÜËß£ÔºåÂèØËÉΩÂåÖÂê´ÈîôËØØÊàñ‰∏çÂÆåÁæéÁöÑÈÉ®ÂàÜ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="ÂèçÈ¶àÊú∫Âà∂" >
&lt;div>
&lt;a href="#%e5%8f%8d%e9%a6%88%e6%9c%ba%e5%88%b6">
##
&lt;/a>
ÂèçÈ¶àÊú∫Âà∂
&lt;/div>
&lt;/h3>
&lt;p>‰∏ÄÊó¶Â≠¶ÁîüÊ®°ÂûãÁîüÊàê‰∫ÜËæìÂá∫Â∫èÂàóÔºåÊé•‰∏ãÊù•ÁöÑÂÖ≥ÈîÆÊ≠•È™§ÊòØËé∑ÂèñÊïôÂ∏àÊ®°ÂûãÁöÑÂèçÈ¶àÔºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÊïôÂ∏àÊ®°ÂûãËØÑ‰º∞ÔºöÊïôÂ∏àÊ®°Âûã‰ºöÂØπÂ≠¶ÁîüÁîüÊàêÁöÑÂ∫èÂàóËøõË°åËØÑ‰º∞ÔºåÈÄöÂ∏∏ÈÄöËøáÊØîËæÉÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÂú®ÊØè‰∏™ÁîüÊàê
Ê≠•È™§‰∏äÂØπ‰∏ã‰∏Ä‰∏™ËØçÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇËøôÊ∂âÂèäÂà∞ËÆ°ÁÆó‰∏§ËÄÖÁöÑÊ¶ÇÁéáÂàÜÂ∏É‰πãÈó¥ÁöÑÊüêÁßçÂ∑ÆÂºÇÂ∫¶ÈáèÔºåÂ¶ÇKLÊï£Â∫¶„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊåáÂØºÊÄßÂ≠¶‰π†ÔºöÊ†πÊçÆÊïôÂ∏àÊ®°ÂûãÁöÑÂèçÈ¶àÔºåÂ≠¶ÁîüÊ®°ÂûãË∞ÉÊï¥ÂÖ∂ÂèÇÊï∞ÔºåÁõÆÁöÑÊòØ‰ΩøËá™Â∑±Âú®ÁîüÊàêÁ±ª‰ººÂ∫èÂàóÊó∂ËÉΩÊõ¥Êé•
ËøëÊïôÂ∏àÊ®°ÂûãÁöÑË°®Áé∞„ÄÇGKD ÂÖÅËÆ∏ÈÄâÊã©‰∏çÂêåÁöÑÂ∑ÆÂºÇÂ∫¶ÈáèÔºàdivergenceÔºâÊù•‰ºòÂåñÔºå‰ª•ÈÄÇÂ∫î‰∏çÂêå‰ªªÂä°ÁöÑÈúÄÊ±Ç„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ëø≠‰ª£‰ºòÂåñÔºöÈöèÁùÄÂ≠¶ÁîüÊ®°Âûã‰∏çÊñ≠‰ªéËá™ÊàëÁîüÊàêÁöÑÈîôËØØ‰∏≠Â≠¶‰π†ÔºåÂÆÉÁöÑÁîüÊàêËÉΩÂäõÈÄêÊ≠•ÊèêÂçáÔºåÁîüÊàêÁöÑÂ∫èÂàóË¥®Èáè
‰πüÈöè‰πãÊèêÈ´òÔºåÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™Ê≠£ÂêëÂæ™ÁéØ„ÄÇÊ≠§ËøáÁ®ãÂèØ‰ª•ËßÜ‰∏∫‰∏ÄÁßçÊ®°‰ªøÂ≠¶‰π†ÔºàImitation LearningÔºâ‰∏éÂº∫Âåñ
Â≠¶‰π†ÔºàReinforcement LearningÔºâÁöÑÁªìÂêàÔºåÂÖ∂‰∏≠Â≠¶ÁîüÊ®°ÂûãÈÄöËøáËø≠‰ª£Âú∞Êé¢Á¥¢Âíå‰øÆÊ≠£ÂÖ∂Ë°å‰∏∫Ôºå‰ª•ÈÄºËøëÊïô
Â∏àÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ÂèÇËÄÉÊñáÁåÆÔºöhttps://arxiv.org/pdf/2306.13649&lt;/p></description></item><item><title>knowledge_distillation</title><link>/posts/clipmodel/2024-06-27-knowledge_distillation/</link><pubDate>Thu, 27 Jun 2024 11:34:18 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/clipmodel/2024-06-27-knowledge_distillation/</guid><description>&lt;p>Áü•ËØÜËí∏È¶èÔºàKnowledge DistillationÔºâÊòØ‰∏ÄÁßçÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÔºåÂÆÉÈÄöËøáÂ∞ÜÂ§ßÂûã„ÄÅÂ§çÊùÇÁöÑÊ®°ÂûãÔºàÁß∞‰∏∫ÊïôÂ∏à
Ê®°ÂûãÔºåTeacher ModelÔºâÁöÑÁü•ËØÜ‚ÄúËí∏È¶è‚ÄùÂà∞Â∞èÂûã„ÄÅÁÆÄÊ¥ÅÁöÑÊ®°ÂûãÔºàÁß∞‰∏∫Â≠¶ÁîüÊ®°ÂûãÔºåStudent ModelÔºâ‰∏≠Ôºå‰ªé
ËÄåÂÆûÁé∞Ê®°ÂûãÂéãÁº©ÂíåÂä†ÈÄüÔºåÂêåÊó∂Â∞ΩÂèØËÉΩ‰øùÊåÅÂéüÂßãÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËøô‰∏ÄÊäÄÊúØ‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•Âú®ËµÑÊ∫êÊúâÈôêÁöÑËÆæÂ§á
‰∏äÈ´òÊïàËøêË°åÔºåÂ¶ÇÊâãÊú∫ÊàñÂµåÂÖ•ÂºèËÆæÂ§á„ÄÇ&lt;/p>
&lt;blockquote>
&lt;p>The method works by incorporating an additional loss into the traditional cross entropy
loss, which is based on the softmax output of the teacher network. The assumption is
that the output activations of a properly trained teacher network carry additional
information that can be leveraged by a student network during training.&lt;/p>
&lt;/blockquote>
&lt;h2 id="Âü∫Êú¨ÂéüÁêÜ" >
&lt;div>
&lt;a href="#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86">
#
&lt;/a>
Âü∫Êú¨ÂéüÁêÜ
&lt;/div>
&lt;/h2>
&lt;p>Áü•ËØÜËΩ¨ÁßªÔºöÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáËÆ©Â≠¶ÁîüÊ®°ÂûãÊ®°‰ªøÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫Ë°å‰∏∫Ôºå‰∏ç‰ªÖÂåÖÊã¨Á°¨ÂàÜÁ±ªÊ†áÁ≠æÔºåËøòÊúâËΩØÊ¶ÇÁéá
ÂàÜÂ∏ÉÔºàsoftmaxÊ¶ÇÁéáÔºâÔºåËøôÊ†∑ÂèØ‰ª•‰º†ÈÄíÊõ¥Â§öÂÖ≥‰∫éÊï∞ÊçÆÂàÜÂ∏ÉÁöÑ‰ø°ÊÅØ„ÄÇËΩØÊ†áÁ≠æÁõ∏ÊØîÁ°¨Ê†áÁ≠æÂê´ÊúâÊõ¥Â§öÂÖ≥‰∫éÊï∞
ÊçÆ‰∏çÁ°ÆÂÆöÊÄßÂèäÁ±ªÈó¥ÂÖ≥Á≥ªÁöÑ‰ø°ÊÅØÔºåÊúâÂä©‰∫éÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†Êõ¥ÁªÜËÖªÁöÑÂÜ≥Á≠ñËæπÁïå„ÄÇ&lt;/p>
&lt;p>ÁâπÂæÅËí∏È¶èÔºöÈô§‰∫ÜËæìÂá∫Â±ÇÁöÑÁü•ËØÜÂ§ñÔºåÊ≥õÂåñÁü•ËØÜËí∏È¶èËøòÂèØ‰ª•Ê∂âÂèä‰∏≠Èó¥Â±ÇÁâπÂæÅÁöÑÂ≠¶‰π†ÔºåÂç≥Â≠¶ÁîüÊ®°ÂûãËØïÂõæÂ≠¶‰π†
ÊïôÂ∏àÊ®°ÂûãÁöÑÈ´òÂ±ÇÁâπÂæÅË°®Á§∫„ÄÇËøôÊúâÂä©‰∫éÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂõ†‰∏∫ÂÆÉÂ≠¶‰ºö‰∫ÜÂ¶Ç‰Ωï‰ªéËæìÂÖ•Êï∞ÊçÆ‰∏≠ÊèêÂèñ
Êõ¥ÊúâÁî®ÁöÑÁâπÂæÅ„ÄÇ&lt;/p>
&lt;p>ÂÖ≥Á≥ªËí∏È¶èÔºöÂº∫Ë∞É‰øùÊåÅÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÂØπ‰∫éËæìÂÖ•Ê†∑Êú¨Èó¥ÂÖ≥Á≥ªÁöÑÁêÜËß£‰∏ÄËá¥ÊÄß„ÄÇËøôÊÑèÂë≥ÁùÄÂ≠¶ÁîüÊ®°Âûã‰∏ç‰ªÖË¶ÅÂ≠¶
‰ºöÂçï‰∏™Ê†∑Êú¨ÁöÑÂ§ÑÁêÜÔºåËøòË¶ÅÁêÜËß£Ê†∑Êú¨‰πãÈó¥ÁöÑÁõ∏ÂØπÂÖ≥Á≥ªÔºåËøôÂØπ‰∫é‰∏Ä‰∫õÈúÄË¶ÅÁêÜËß£Â§çÊùÇ‰∏ä‰∏ãÊñáÁöÑ‰ªªÂä°Â∞§‰∏∫ÈáçË¶Å„ÄÇ&lt;/p>
&lt;p>&lt;strong>Áü•ËØÜËí∏È¶èËøáÁ®ã&lt;/strong>Ôºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ËÆ≠ÁªÉÊïôÂ∏àÊ®°Âûã&lt;/strong>ÔºöÈ¶ñÂÖàÔºå‰ΩøÁî®Â§ßÈáèÊï∞ÊçÆÂíåËÆ°ÁÆóËµÑÊ∫êËÆ≠ÁªÉ‰∏Ä‰∏™È´òÊÄßËÉΩÁöÑÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºàÊïôÂ∏àÊ®°
ÂûãÔºâ„ÄÇËøô‰∏™Ê®°ÂûãÂèØËÉΩÂåÖÂê´Êï∞Áôæ‰∏áÁîöËá≥Êï∞ÂçÅ‰∫ø‰∏™ÂèÇÊï∞Ôºå‰ΩÜÂÆÉÂú®ÂàÜÁ±ª‰ªªÂä°‰∏äÁöÑË°®Áé∞ÈùûÂ∏∏Âá∫Ëâ≤„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÁîüÊàêËΩØÊ†áÁ≠æ&lt;/strong>ÔºöÊïôÂ∏àÊ®°ÂûãÂú®ÂØπËæìÂÖ•Êï∞ÊçÆËøõË°åÈ¢ÑÊµãÊó∂Ôºå‰∏ç‰ªÖ‰ªÖÁªôÂá∫ÊúÄÁªàÁöÑÂàÜÁ±ªÁªìÊûúÔºåËøò‰ºöÁªôÂá∫ÂêÑ
Á±ªÂà´ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºàÈÄöÂ∏∏ÈÄöËøásoftmaxÂ±ÇËé∑ÂæóÔºâ„ÄÇËøô‰∫õÊ¶ÇÁéáÂàÜÂ∏ÉË¢´Áß∞‰∏∫‚ÄúËΩØÊ†áÁ≠æ‚ÄùÔºåÂÆÉ‰ª¨ÂåÖÂê´‰∫ÜÈ¢ùÂ§ñÁöÑ
‰ø°ÊÅØÔºåÊØîÂ¶ÇÁ±ªÂà´ÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ËÆ≠ÁªÉÂ≠¶ÁîüÊ®°Âûã&lt;/strong>ÔºöÊé•‰∏ãÊù•Ôºå‰ΩøÁî®ÊïôÂ∏àÊ®°ÂûãÁöÑËΩØÊ†áÁ≠æÂíåÂÆûÈôÖÁöÑÁ°¨Ê†áÁ≠æÔºàÂç≥Êï∞ÊçÆÁöÑÁúüÂÆûÁ±ªÂà´ÔºâÊù•ËÆ≠
ÁªÉÂ≠¶ÁîüÊ®°Âûã„ÄÇÂ≠¶ÁîüÊ®°ÂûãÁöÑÊû∂ÊûÑËÆæËÆ°ÂæóÊõ¥ÁÆÄÂçïÔºåÂèÇÊï∞ÈáèËøúÂ∞è‰∫éÊïôÂ∏àÊ®°Âûã„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂ≠¶ÁîüÊ®°Âûã‰∏ç
‰ªÖË¶ÅÂ≠¶‰π†Ê®°‰ªøÁ°¨Ê†áÁ≠æÔºåËøòË¶ÅÈÄöËøáÊçüÂ§±ÂáΩÊï∞ÔºàÂ¶ÇKLÊï£Â∫¶Êàñ‰∫§ÂèâÁÜµÔºâÂ∞ΩÈáèÊé•ËøëÊïôÂ∏àÊ®°ÂûãÁöÑËΩØÊ†áÁ≠æËæìÂá∫„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ê∏©Â∫¶ÂèÇÊï∞Ë∞ÉÊï¥&lt;/strong>ÔºöÂú®ÁîüÊàêËΩØÊ†áÁ≠æÊó∂ÔºåÊúâÊó∂‰ºöÂºïÂÖ•‰∏Ä‰∏™‚ÄúÊ∏©Â∫¶‚ÄùÂèÇÊï∞Êù•Ë∞ÉÊï¥Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÂπ≥ÊªëÁ®ãÂ∫¶„ÄÇ
È´òÊ∏©ÂèØ‰ª•‰ΩøËΩØÊ†áÁ≠æÊõ¥Âä†Âπ≥ÊªëÔºå‰øÉËøõÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†Âà∞ÊïôÂ∏àÊ®°ÂûãÁöÑÂÜ≥Á≠ñËæπÁïåÔºõ‰ΩéÊ∏©Âàô‰ΩøÂæóËΩØÊ†áÁ≠æÊé•Ëøë
Á°¨Ê†áÁ≠æÔºå‰ΩÜÂèØËÉΩ‰ºö‰∏¢Â§±ÊïôÂ∏àÊ®°ÂûãÁöÑ‰∏Ä‰∫õÁªÜÂæÆÂÜ≥Á≠ñ‰ø°ÊÅØ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="Áü•ËØÜËí∏È¶èÁöÑ‰∏âÁßç‰ª£Á†ÅÂÆûÁé∞" >
&lt;div>
&lt;a href="#%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f%e7%9a%84%e4%b8%89%e7%a7%8d%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0">
#
&lt;/a>
Áü•ËØÜËí∏È¶èÁöÑ‰∏âÁßç‰ª£Á†ÅÂÆûÁé∞Ôºö
&lt;/div>
&lt;/h2>
&lt;ol>
&lt;li>Áü•ËØÜËΩ¨ÁßªÔºöÂü∫‰∫é softmax output ËΩØÊ†áÁ≠æ&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the teacher model - do not save gradients here as we do not change the teacher&amp;#39;s weights&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> teacher_logits &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#Soften the student logits by applying softmax first and log() second&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_targets &lt;span style="color:#ff6ac1">=&lt;/span> nn&lt;span style="color:#ff6ac1">.&lt;/span>functional&lt;span style="color:#ff6ac1">.&lt;/span>softmax(teacher_logits &lt;span style="color:#ff6ac1">/&lt;/span> T, dim&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_prob &lt;span style="color:#ff6ac1">=&lt;/span> nn&lt;span style="color:#ff6ac1">.&lt;/span>functional&lt;span style="color:#ff6ac1">.&lt;/span>log_softmax(student_logits &lt;span style="color:#ff6ac1">/&lt;/span> T, dim&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper &amp;#34;Distilling the knowledge in a neural network&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_targets_loss &lt;span style="color:#ff6ac1">=&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>sum(soft_targets &lt;span style="color:#ff6ac1">*&lt;/span> (soft_targets&lt;span style="color:#ff6ac1">.&lt;/span>log() &lt;span style="color:#ff6ac1">-&lt;/span> soft_prob)) &lt;span style="color:#ff6ac1">/&lt;/span> soft_prob&lt;span style="color:#ff6ac1">.&lt;/span>size()[&lt;span style="color:#ff9f43">0&lt;/span>] &lt;span style="color:#ff6ac1">*&lt;/span> (T&lt;span style="color:#ff6ac1">**&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> soft_target_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> soft_targets_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>ÁâπÂæÅËí∏È¶èÔºöÂü∫‰∫é hidden state ‰ΩôÂº¶Áõ∏‰ººÂ∫¶&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the teacher model and keep only the hidden representation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _, teacher_hidden_representation &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits, student_hidden_representation &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hidden_rep_loss &lt;span style="color:#ff6ac1">=&lt;/span> cosine_loss(student_hidden_representation, teacher_hidden_representation, target&lt;span style="color:#ff6ac1">=&lt;/span>torch&lt;span style="color:#ff6ac1">.&lt;/span>ones(inputs&lt;span style="color:#ff6ac1">.&lt;/span>size(&lt;span style="color:#ff9f43">0&lt;/span>))&lt;span style="color:#ff6ac1">.&lt;/span>to(device))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> hidden_rep_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> hidden_rep_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>ÁâπÂæÅËí∏È¶èÔºöÂü∫‰∫é Intermediate regressor MSEÊçüÂ§±&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Again ignore teacher logits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _, teacher_feature_map &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits, regressor_feature_map &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hidden_rep_loss &lt;span style="color:#ff6ac1">=&lt;/span> mse_loss(regressor_feature_map, teacher_feature_map)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> feature_map_weight &lt;span style="color:#ff6ac1">*&lt;/span> hidden_rep_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÂÖ∑‰ΩìÂèÇËÄÉÔºöhttps://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html?highlight=distill&lt;/p></description></item><item><title>mixup_beyond_ERM</title><link>/posts/lossfunctions/2024-06-27-mixup_beyond_empirical_risk_minimization/</link><pubDate>Thu, 27 Jun 2024 09:42:34 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/lossfunctions/2024-06-27-mixup_beyond_empirical_risk_minimization/</guid><description>&lt;h1 id="empirical-risk-minimazation-erm" >
&lt;div>
&lt;a href="#empirical-risk-minimazation-erm">
##
&lt;/a>
Empirical Risk Minimazation (ERM)
&lt;/div>
&lt;/h1>
&lt;p>ÁªèÈ™åÈ£éÈô©ÊúÄÂ∞èÂåñ, Empirical Risk Minimazation principle (Vapnik, 1998)&lt;/p>
&lt;ol>
&lt;li>Âü∫‰∫éERMËÆ≠ÁªÉÊ®°ÂûãÔºö‰∫¶Âç≥Âú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏äÂ≠¶‰π†‰ª•ÊúÄÂ∞èÂåñÂÖ∂Âπ≥ÂùáËØØÂ∑Æ„ÄÇ&lt;/li>
&lt;li>ÂΩìÂâçSOTAÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÈöèÁùÄËÆ≠ÁªÉÊï∞ÊçÆÈõÜËßÑÊ®°Â¢ûÂ§ßËÄåÁ∫øÊÄßÂ¢ûÂä†„ÄÇ&lt;/li>
&lt;/ol>
&lt;p>ËÄåÁªèÂÖ∏VCÂ≠¶‰π†ÁêÜËÆ∫Ôºàlearning theory, Vapnik &amp;amp; Chervonenkis, 1971ÔºâË°®ÊòéÔºöÂè™Ë¶ÅÂ≠¶‰π†Âô®ÁöÑÂèÇÊï∞Èáè
‰∏çÈöèÁùÄËÆ≠ÁªÉÊ†∑Êú¨Êï∞ÈáèÂ¢ûÂä†ÔºåÂàôÂü∫‰∫éERMÂ≠¶‰π†‰∏ÄÂÆö‰ºöÊî∂Êïõ(convergence, i.e., good generalization
to new data)„ÄÇ‰∫¶Âç≥ÔºöÊ®°ÂûãÁöÑÂ§çÊùÇÂ∫¶ÔºàÂèÇÊï∞ÈáèËßÑÊ®°ÔºâÁõ∏ÂØπ‰∫éËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°Â∫îËØ•ÊòØÂõ∫ÂÆöÁöÑÊàñËÄÖÂèòÂä®‰∏çÂ§ß„ÄÇ&lt;/p>
&lt;p>ËøôÂ∞±ÈÄ†Êàê‰∫ÜËøôÊ†∑ÁöÑÁüõÁõæÔºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÁªèÂÖ∏Â≠¶‰π†ÁêÜËÆ∫ËÆ§‰∏∫ÔºöÊÉ≥Ë¶ÅERMÊúâÊïàÔºåÂàôÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÂ∫îËØ•‰øùÊåÅ‰∏ÄÂÆöÂ§ßÂ∞èÔºåËÄå‰∏çÊòØÈöèÁùÄËÆ≠ÁªÉÊï∞ÊçÆÈõÜËßÑÊ®°Â¢ûÂä†ËÄåÂ¢ûÂä†Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÁÑ∂ËÄåÂú®ÂÆûÈôÖ‰ªªÂä°‰∏≠ÔºöSOTAÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÊòØ‰∏éËÆ≠ÁªÉÊï∞ÊçÆÈáè‰øùÊåÅÁ∫øÊÄßÂ¢ûÂä†ÁöÑ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ÂÆûÈôÖ‰∏äÔºå‰∏ÄÊñπÈù¢ERMÂÖÅËÆ∏Â§ßÁöÑÁ•ûÁªèÁΩëÁªúÊ®°Âûã‚ÄúËÆ∞‰Ωè‚ÄùËÆ≠ÁªÉÊ†∑Êú¨ÔºàÂç≥‰ΩøÁî®‰∫ÜÂæàÂº∫ÁöÑÊ≠£ÂàôÂåñÁ∫¶ÊùüÔºå
strong regularization)ÔºåÂè¶‰∏ÄÊñπÈù¢ERMËÆ≠ÁªÉÂæóÂà∞ÁöÑÊ®°ÂûãÂú®ËÆ≠ÁªÉÂàÜÂ∏É‰πãÂ§ñÁöÑÊ†∑Êú¨‰∏äÁöÑÈ¢ÑÊµãÁªìÊûúÂ∑ÆÂºÇÂ∑®
Â§ßÔºàÂç≥‰ΩøË¢´È¢ÑÊµãÁöÑÊ†∑Êú¨‰ªÖ‰ªÖÂèëÁîü‰∫ÜÁõ∏ÂØπÂæÆÂ∞èÁöÑÊîπÂèòÔºåadversarial examplesÔºâ„ÄÇ&lt;/p>
&lt;p>ÈÇ£‰πàÔºåÊúâÊ≤°ÊúâÂà´ÁöÑÊ®°ÂûãËÆ≠ÁªÉÊúÄ‰ºòÂåñÂáÜÂàôÂë¢Ôºü-&amp;gt; Vicinal Risk Minimization (VRM).&lt;/p>
&lt;blockquote>
&lt;p>In VRM, human knowledge is required to describe a vicinity or neighborhood around each
example in the training data. Then, additional virtual examples can be drawn from the
vicinity distribution of the training examples to enlarge the support of the training
distribution.&lt;/p>
&lt;/blockquote>
&lt;p>‰æãÂ¶ÇÔºåÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåÂõæÂÉèÂ¢ûÂº∫ÈÄöÂ∏∏ÂåÖÊã¨ËΩªÂæÆÁöÑÊóãËΩ¨„ÄÅÁøªËΩ¨„ÄÅÁº©ÊîæÁ≠âÊìç‰ΩúÔºåËøôÂÖ∂ÂÆûÊòØÂõæÂÉèÁöÑËøëÈÇª
ÈõÜÔºàvicinity setÔºâ„ÄÇËøô‰∫õÊï∞ÊçÆÂ¢ûÂº∫ÈÄöÂ∏∏ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñÊÄßËÉΩ„ÄÇ‰ΩÜËøô‰∫õÊìç‰ΩúÈÄöÂ∏∏‰πüÊòØÊï∞ÊçÆ‰æùËµñÁöÑ„ÄÇ&lt;/p>
&lt;p>Mixup Êèê‰æõ‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÊó†ÂÖ≥Ôºàdata-agnosticÔºâÁöÑÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂÆÉÊûÑÈÄ†ËôöÊãüÊ†∑Êú¨ÁöÑÊñπÂºèÂ¶Ç‰∏ãÔºö&lt;/p>
&lt;p>$$
\begin{eqnarray}
\tilde{x} &amp;amp;=&amp;amp; \lambda x_i + (1 - \lambda) x_j, \text{where } x_i, x_j \text{ are raw input vectors} \\
\tilde{y} &amp;amp;=&amp;amp; \lambda y_i + (1 - \lambda) y_j, \text{where } y_i, y_j \text{ are one-hot label encoding}
\end{eqnarray}
$$&lt;/p>
&lt;p>$(x_i, y_i)$ Âíå $(x_j, y_j)$ ÊòØ‰ªéËÆ≠ÁªÉÈõÜ‰∏≠ÈöèÊú∫ÊäΩÂèñÁöÑ‰∏§‰∏™Ê†∑‰æã„ÄÇ$\lambda \in [0, 1]$„ÄÇ&lt;/p>
&lt;p>Âõ†Ê≠§ÔºåmixupÈÄöËøáÁªìÂêàÂÖàÈ™åÁü•ËØÜÊâ©Â±ï‰∫ÜËÆ≠ÁªÉÂàÜÂ∏ÉÔºåÂç≥ÁâπÂæÅÂêëÈáèÁöÑÁ∫øÊÄßÊèíÂÄºÂ∫îÂØºËá¥Áõ∏ÂÖ≥ÁõÆÊ†áÁöÑÁ∫øÊÄßÊèíÂÄº„ÄÇ
mixupÂèØ‰ª•Áî®Âá†Ë°å‰ª£Á†ÅÂÆûÁé∞ÔºåÂπ∂‰∏îÂè™ÂºïÂÖ•ÊúÄÂ∞èÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇ&lt;/p>
&lt;p>Mixup: beyond_ERM, &lt;a href="https://arxiv.org/pdf/1710.09412">https://arxiv.org/pdf/1710.09412&lt;/a>&lt;/p></description></item><item><title>MoCo</title><link>/posts/neuralnetworks/moco/</link><pubDate>Sun, 16 Jun 2024 14:10:17 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/neuralnetworks/moco/</guid><description>&lt;p>ÂØπÊØîÂ≠¶‰π†ÔºàContrast LearningÔºâÔºö
ÊòØ‰∏ÄÁßçËá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÊûÑÂª∫ÂíåÊØîËæÉÊ≠£Ë¥üÊ†∑Êú¨ÂØπÊù•Â≠¶‰π†Êï∞ÊçÆÁöÑÊúâÁî®Ë°®Á§∫„ÄÇÊ≠£Ê†∑Êú¨ÂØπÈÄöÂ∏∏Êù•Ëá™Âêå‰∏Ä
Êï∞ÊçÆÁÇπÁöÑ‰∏çÂêåÂ¢ûÂº∫ÁâàÊú¨ÔºåËÄåË¥üÊ†∑Êú¨ÂØπÂàôÊù•Ëá™‰∏çÂêåÁöÑÊï∞ÊçÆÁÇπ„ÄÇÊ®°ÂûãÈÄöËøáÊúÄÂ§ßÂåñÊ≠£Ê†∑Êú¨ÂØπÁöÑÁõ∏‰ººÂ∫¶ÂíåÊúÄÂ∞è
ÂåñË¥üÊ†∑Êú¨ÂØπÁöÑÁõ∏‰ººÂ∫¶Ôºå‰ªéËÄåÊèêÂèñÊï∞ÊçÆÁöÑÂà§Âà´ÊÄßÁâπÂæÅÔºåÂπøÊ≥õÂ∫îÁî®‰∫éÂõæÂÉè„ÄÅÊñáÊú¨ÂíåÊó∂Èó¥Â∫èÂàóÁ≠âÈ¢ÜÂüü„ÄÇ&lt;/p>
&lt;p>Ê≠£Ë¥üÊ†∑Êú¨ÁöÑÁîüÊàêÔºö&lt;/p>
&lt;p>&lt;img alt="pos-neg-sample-pairs" src="https://fgg100y.github.io/posts/neuralnetworks/moco/images/CV-moco_model_pos_neg_samples_how.png">&lt;/p>
&lt;p>MoCo:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Contrastive learning as dictionary look-up:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>queue&lt;/code> for large dictionary of keys: Ëß£ÂÜ≥ simCLR(end-to-end) ÊñπÊ≥ïÂèóÈôê‰∫éÂ≠óÂÖ∏Â§ßÂ∞èÁöÑÈóÆÈ¢ò&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>momentum update&lt;/code> for consistency of keys: Ëß£ÂÜ≥Âü∫‰∫é &amp;ldquo;memory bank&amp;rdquo; ÊñπÊ≥ïÂèóÈôê‰∫ékeysÁöÑÁâπÂæÅ‰∏ÄËá¥ÊÄßÂ∑ÆÁöÑÈóÆÈ¢ò&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>infoNCE loss (infomation Noice-Contrastive Estimation, ‰∏é‰∫§ÂèâÁÜµÊçüÂ§±ÊúâÊûÅÂ§ßÂÖ≥ËÅî):
NCEÊòØÂ∞ÜÂ§öÂàÜÁ±ªÈóÆÈ¢òËΩ¨Êç¢‰∏∫‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºàÊï∞ÊçÆÊ†∑Êú¨vsÂô™Â£∞Ê†∑Êú¨Ôºâ‰ªéËÄåÂèØ‰ª•ÊÑâÂø´‰ΩøÁî®softmaxÔºõ
ËÄåinfoNCEËÆ§‰∏∫Âô™Â£∞Ê†∑Êú¨ÈáåËøòÊòØÊúâÂæàÂ§ö‰∏çÂêåÁ±ªÂà´ÔºåÊâÄ‰ª•ÁúãÊàêÂ§öÂàÜÁ±ªÈóÆÈ¢òÊØîËæÉÂêàÁêÜÔºàK+1Á±ªÔºâ„ÄÇ
‰∫§ÂèâÁÜµÊçüÂ§±‰∏≠ÁöÑ$k$ÊòØÁ±ªÂà´Êï∞ÁõÆÔºåËÄåinfoNCEÊçüÂ§±‰∏≠ÁöÑ$k$ÊòØË¥üÊ†∑Êú¨ÁöÑÊï∞ÁõÆ„ÄÇ&lt;/p>
&lt;p>$$
\mathcal{L}_q = - \log \frac{\exp(q \cdot k_{+} / \tau)}{\sum^{K}_{i=0} \exp(q \cdot k_{+} / \tau)}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="MoCo" src="https://fgg100y.github.io/posts/neuralnetworks/moco/images/CV-moco_model_query_key_similarity_contrast_loss.png">&lt;/p></description></item><item><title>what_is_Momentum</title><link>/posts/neuralnetworks/2024-06-16-what_is_momentum/</link><pubDate>Sun, 16 Jun 2024 10:35:30 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/neuralnetworks/2024-06-16-what_is_momentum/</guid><description>&lt;p>‚ÄúÂä®Èáè‚ÄùÁÆÄÂçïÊù•ËØ¥Â∞±ÊòØÔºöÁßªÂä®Âä†ÊùÉÂπ≥Âùá„ÄÇ
$$
y_t = m \times y_{t-1} + (1 - m) \times x_t
$$
where $m \in [0, 1]$.&lt;/p>
&lt;p>‰∏äÂºèÂ∞±ÊòØË°®Ëææ‰∫ÜËøô‰πà‰∏™ÊÑèÊÄùÔºö&lt;/p>
&lt;p>ÂΩìÂâçÊó∂ÂàªËæìÂá∫ $y_t$ Èô§‰∫ÜÂèóÂà∞ÂΩìÂâçËæìÂÖ• $x_t$ ÁöÑÂΩ±ÂìçÔºåËøòÊúâ‰∏ä‰∏Ä‰∏™Êó∂ÂàªËæìÂá∫ $y_{t-1}$ ÁöÑÂΩ±ÂìçÔºå
ËÄåÂÖ∂ÂΩ±ÂìçÁ®ãÂ∫¶Áî±‚ÄúÂä®Èáè (momentum)‚Äù $m$ Êù•ÂÜ≥ÂÆö„ÄÇÂàôÊúâÔºö$m$ Ë∂ãÂêë‰∫é1Ôºå$y_t$ ‰∏é‰∏ä‰∏Ä‰∏™Êó∂ÂàªÁõ∏Âêå„ÄÇ&lt;/p>
&lt;p>Â§ßÁ∫¶ÊòØËØ¥Ôºå‰∏çÂ∏åÊúõÂΩìÂâçÊó∂ÂàªÁöÑËæìÂá∫ÂÆåÂÖ®‰æùËµñ‰∫éÂΩìÂâçÊó∂ÂàªÁöÑËæìÂÖ•ÔºåËøòÂ∏åÊúõ‰∏ä‰∏Ä‰∏™Êó∂ÂàªÁöÑËæìÂá∫Êù•ÂáëÂáëÁÉ≠Èóπ„ÄÇ&lt;/p></description></item><item><title>Ë∑ØÁî±Âô®wifiËøû‰∏ç‰∏ä_‰ΩÜÊâãÊú∫ÊµÅÈáèÂèØ‰ª•</title><link>/posts/personals/momentsoflife/2024-06-16-wifi%E8%BF%9E%E4%B8%8D%E4%B8%8A_%E4%BD%86%E6%89%8B%E6%9C%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E4%BB%A5/</link><pubDate>Sun, 16 Jun 2024 10:07:12 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/personals/momentsoflife/2024-06-16-wifi%E8%BF%9E%E4%B8%8D%E4%B8%8A_%E4%BD%86%E6%89%8B%E6%9C%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E4%BB%A5/</guid><description>&lt;p>QÔºöËØ∑ÈóÆWifiÊó†Ê≥ïÁøªÂ¢ôÊàñËÆøÈóÆÊüê‰∫õÁΩëÁ´ôÔºå‰ΩÜÁî®ÊâãÊú∫ÊµÅÈáèÂç¥ÂèØ‰ª•ÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü&lt;/p>
&lt;p>A:
ÂÖàËØ¥ËØ¥‰Ω†ÁöÑwifiÂíå5gÁéØÂ¢É‰∏ãÁøªÂ¢ôÁöÑÈóÆÈ¢òÔºåÊàëÁåúÊµã‰∏ã‰Ω†ÂÆ∂ÈáåÁöÑÂÆΩÂ∏¶ÊòØ‰∏çÊòØÁßªÂä®ÁöÑÔºåÊâãÊú∫ÊµÅÈáèÁî®ÁöÑÊòØËÅîÈÄö
ÊàñËÄÖÁîµ‰ø°ÔºåÊÄª‰πãÔºåÂÆ∂Â∫≠ÂÆΩÂ∏¶Âíå5g‰∏çÊòØ‰∏ÄÂÆ∂isp„ÄÇ Â¶ÇÊûúÊòØÁöÑËØùÔºåËøô‰∏™ÈóÆÈ¢òÂ∞±ÊØîËæÉÂ•ΩÁêÜËß£Ôºå‰∏çÂêåisp‰∏çÂêå
ÁöÑÂú∞Âå∫ÈÉΩ‰ºöÊúâËá™Â∑±ÁöÑ‰∏ÄÂ•óÈªëÂêçÂçïÊú∫Âà∂„ÄÇ ÊâÄ‰ª•ÂèØËÉΩ‰Ω†ÁöÑÂÆ∂ÂÆΩispÁõëÊµã‰Ω†ÁöÑ‰∏äÁΩëÊµÅÈáèÊúâ‰∏çÊ≠£Â∏∏ÔºåÊàñËÄÖÂ§ßÊï∞
ÊçÆËÆ§‰∏∫‰Ω†ÁöÑÁøªÂ¢ôÊúçÂä°Âô®ÂæàÂèØÁñëÔºàÊú™ÂøÖÊòØ‰Ω†ÈÄ†ÊàêÁöÑÔºåÂõ†‰∏∫ÊúâÂæàÂ§ö‰∫∫Âú®Áî®ÔºâÔºå‰∫éÊòØÂºÄÂêØ‰∫ÜÂ±èËîΩÔºå‰ΩÜÊòØÂè¶‰∏Ä
ÂÆ∂ÊâãÊú∫ispÂπ∂Ê≤°ÊúâËß¶ÂèëÈªëÂêçÂçïÂà§ÂÆöÔºåÊâÄ‰ª•Êú™Â±èËîΩ„ÄÇ ÁßªÂä®ÁöÑÈªëÂêçÂçï‰∏ÄËà¨ÊòØÊúÄÊøÄËøõÁöÑÔºåÂè∑Áß∞Â¢ô‰∏≠Â¢ôÔºå‰∏çËøá
‰∏çÂêåÂú∞Âå∫ÁöÑÂêå‰∏ÄÊúçÂä°ÂïÜÊîøÁ≠ñ‰πü‰ºöÊúâÂæàÂ§ßÂ∑ÆÂà´Ôºå‰∏çÂ•ΩËØ¥Ë∞ÅÂ∞±‰∏ÄÂÆöÂ•ΩÔºåË∞ÅÂ∞±‰∏ÄÂÆöÂ∑Æ„ÄÇÊâÄ‰ª•Ëß£ÂÜ≥ÊñπÊ≥ïÂ∞±ÊòØÊç¢
‰∏™ÁøªÂ¢ôÊúçÂä°Âô®Â∞±ÂèØ‰ª•‰∫Ü„ÄÇ&lt;/p>
&lt;p>&lt;del>ÂÜçËØ¥ËØ¥Êú∫Âú∫ÈóÆÈ¢òÔºåÊú∫Âú∫Â¶ÇÊûúË¶ÅÊâæÁöÑËØùÂçÉÂçÉ‰∏áÔºå‰∏çÂ§∏Âº†ÁöÑËØ¥Â∞èÂ≠¶ÁîüÈÉΩËÉΩÂºÄÊú∫Âú∫ÔºåÊâÄ‰ª•ÔºåÂ¶ÇÊûúÁúü
Ë¶ÅÊòØÈÄâÁöÑËØùÔºåÂ¶ÇÊûú‰Ω†‰∏çÂ§™ÊáÇÊäÄÊúØÔºåÂ∞±ÈÄâÂ§ßÊú∫Âú∫Â•Ω‰∫ÜÔºåÂèØËÉΩË¥µ‰∏Ä‰∫õÔºå‰ΩÜÊòØÁ®≥ÂÆöÊÄßÊòØÊúâ‰øùÈöúÁöÑ„ÄÇ
Â¶ÇÊûúÊáÇÊäÄÊúØÔºåÂ∞±‰∏çÂ¶ÇÊú∫Âú∫ÔºàÊàñËΩ¨ÂèëÊúçÂä°Ôºâ+Ëá™Âª∫ÊúçÂä°Âô®ÁªìÂêàÁöÑÊñπÂºèÔºåËøôÊ†∑Â∞±ËÉΩÂÖºÈ°æÈÄüÂ∫¶ÂíåÈöê
ÁßÅÂÆâÂÖ®„ÄÇ Âá†‰πéÊâÄÊúâÁöÑÊú∫Âú∫ÈÉΩ‰ºöÊúâÂÆ°Êü•Êú∫Âà∂ÔºåÊúÄËµ∑Á†Å‰ªñÈúÄË¶ÅÁõëÊéßÊµÅÈáèÊù•ËÆ°Ë¥πÔºåÂè™ÊòØÂÆ°Êü•Á®ãÂ∫¶
ÂíåÈöêÁßÅÊï∞ÊçÆÂ§ÑÁêÜ‰∏•Ë∞®Â∫¶ÊúâÂå∫Âà´ËÄåÂ∑≤ÔºåÁúãÂïÜÂÆ∂ËâØÂøÉ‰∫Ü„ÄÇÊàë‰∏™‰∫∫Ê≤°Áî®ËøáÊú∫Âú∫ÔºåÂè™ËÉΩËØ¥‰∏§‰∏™ÊàëÊâÄÁü•
ÈÅìÁöÑÊØîËæÉÂ§ßÁöÑÔºåjustmysocksÔºådler cloudÔºåÈ≠ÖÂΩ±ÊûÅÈÄü„ÄÇ Âè¶Â§ñÔºåÂà´Â§™Ëø∑‰ø°ÈÇ£‰∫õÊú∫Âú∫Êé®ËçêÔºåÂü∫
Êú¨ÈÉΩÊòØÊéêÈ•≠„ÄÇ ÂØπ‰∫ÜÔºåËøòÊúâ‰∏™ÊàëËßâÂæó‰Ω†ÂèØ‰ª•ËØï‰∏ãÔºåÂè´Shiliew„ÄÇ ÁêÜÁî±Êúâ2‰∏™ÔºåËøô‰∏™appÁöÑ‰ΩúËÄÖ
Â∞±ÊòØÂºÄÊ∫êÁøªÂ¢ôËΩØ‰ª∂brookÁöÑ‰ΩúËÄÖÔºåÂú®app storeÈáåÁúãÁùÄÊùÉÈôêÂæàÂπ≤ÂáÄÔºåÂè¶Â§ñËøô‰∏™‰ΩúËÄÖÂêçÂ≠óÂè´
txthinkingÔºåÊúâÁÇπËá¥Êï¨ÁºñÁ®ãÈöèÊÉ≥ÁöÑÊÑèÊÄùÔºåÊÑüËßâ‰ªñÂ∫îËØ•ÊúÄËµ∑Á†Å‰∏çÁ∫¢Âêß„ÄÇ„ÄÇ„ÄÇÂìàÂìàÂìà ‰ΩÜÊòØËøô
‰∏úË•øÂ•ΩÁî®‰∏çÂ•ΩÁî®ÊàëÂ∞±‰∏çÁü•ÈÅì‰∫Ü„ÄÇ&lt;/del>&lt;/p>
&lt;p>ÊúÄÂêéÊàëÂÜçËØ¥‰∏ãÂÖ≥‰∫é@han_chineseÁöÑËßÇÁÇπÔºåÁßªÂä®Êï∞ÊçÆÁöÑÂÆ°Êü•ÊòØÊúÄÂº∫ÁöÑÔºåÂõ†‰∏∫ÁßªÂä®ËúÇÁ™ùÊï∞ÊçÆÊòØÈúÄË¶ÅÁ≤æÁ°ÆËÆ°
Ë¥πÁöÑÔºå‰ºóÊâÄÂë®Áü•ÔºåÊØè‰∏™ÊúçÂä°ÂïÜÈÉΩÊúâÂæàÂ§öÂÖçÊµÅÈáèÊúçÂä°ÔºåÊâÄ‰ª•ÈúÄË¶ÅËØÜÂà´ÊµÅÈáèÂÜÖÂÆπÊù•Âà§Êñ≠ËÆ°Ë¥π‰∏éÂê¶„ÄÇÊõ¥ÁîöÁöÑ
ÊòØÔºåÁßªÂä®Êï∞ÊçÆÊµÅÈáèÈÉΩ‰ºöË¢´Ëá™Âä®Ê†áËÆ∞‰∏äÊú¨Êú∫ÁîµËØùÂè∑Á†ÅÔºå‰ªñ‰ª¨Ëøò‰∏ìÈó®Êêû‰∫ÜapiÊù•ÂçñËøôÁßçÊúçÂä°„ÄÇÁÆÄÂçïÊù•ËÆ≤Ôºå
ÊØîÂ¶Ç‰Ω†Áî®ÊµÅÈáè‰∏ä‰∫¨‰∏úÔºå‰Ω†‰∏çÈúÄË¶ÅËæìÂÖ•‰Ω†ÁöÑÁîµËØùÂè∑Á†ÅÊù•Ê≥®ÂÜåÊàñËÄÖÁôªÂΩïÔºå‰∫¨‰∏úÂ∞±Â∑≤ÁÑ∂Áü•Êôì‰∫ÜÔºåÂõ†‰∏∫‰∫¨‰∏ú‰π∞
‰∫ÜispÁöÑapiÊé•Âè£ÔºåÂè™ÈúÄË¶ÅÂú®ÁΩëÈ°µ‰∏≠Âä†ÂÖ•Âá†Ë°å‰ª£Á†ÅÊù•Ë∞ÉÁî®Ëøô‰∏™apiÔºå‰Ω†‰∏ÄÊó¶Áî®ÊâãÊú∫ÊµÅÈáèËÆøÈóÆËøô‰∏™ÁΩëÈ°µÔºå
ÁîµËØùÂè∑Á†ÅÂ∞±‰ºöËøîÂõûÁªô‰∫¨‰∏ú„ÄÇ ÊâÄ‰ª•Êó†ËÆ∫ÁøªÂ¢ô‰∏éÂê¶ÔºåÈÉΩÂª∫ËÆÆÂú®‰ΩøÁî®Êï∞ÊçÆÊµÅÈáè‰∏äÁΩëÊó∂ÂºÄÂêØvpnÊúçÂä°Êù•Âä†ÂØÜÂíå
‰∏≠ËΩ¨Êï∞ÊçÆ„ÄÇ ÂÖ≥‰∫é‰π∞ÂõΩÂ§ñÁîµËØùÂç°Êù•ÁøªÂ¢ôÔºåÂêåÊ†∑‰πüÊòØ‰∏çÈù†Ë∞±ÁöÑÔºåÂõ†‰∏∫Â§ñÂõΩÁöÑsimÂç°ÊãøÂà∞‰∏≠ÂõΩÊù•ËÉΩÁî®ÔºåÂè™ÊòØ
Êº´Ê∏∏‰∫ÜËÄåÂ∑≤ÔºåÊº´Ê∏∏ÁöÑÊÑèÊÄùÂ∞±ÊòØÔºåÂõΩÂ§ñÂíåÂõΩÂÜÖÁöÑispÁ≠æ‰∫ÜÂêàÂêåÔºåËÉΩËÆ©ÂêÑËá™ÁöÑÁî®Êà∑ÁßüÁî®Ëá™Â∑±ËøôËæπÁöÑÂü∫Á´ôÁ∫ø
Ë∑ØÁ≠âËÆæÂ§áÊù•‰øùËØÅË∑®ÂõΩ‰ΩøÁî®ÔºåÊâÄ‰ª•Âç≥‰Ωø‰π∞‰∫Ügoogle-fiÊù•‰∏≠ÂõΩ‰ΩøÁî®ÔºåÂíå‰Ω†Áî®‰∏≠ÂõΩÁöÑsimÂç°Âú®‰∏≠ÂõΩ‰ΩøÁî®ÔºåÈÉΩ
ÊòØÈÄöËøá‰∏≠ÂõΩispÁöÑÂü∫Á´ôÁ∫øË∑ØÂíåÊúçÂä°Âô®„ÄÇÂîØ‰∏ÄÂå∫Âà´Â∞±ÊòØgoogle-fiÁöÑÊâÄÊúâËÆøÈóÆÈÉΩ‰ºöË¢´ÂàóÂÖ•ÁôΩÂêçÂçïÔºå‰ΩÜÊòØÊï∞
ÊçÆÊµÅÈáèÁöÑÂÆ°ËÆ°ÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ‰Ω†Êãøgoogle-fiÊù•‰∏äpincongÔºåÁßªÂä®ÊúçÂä°ÂïÜÊòØÁü•ÈÅìÔºåÂè™ÊòØ‰∏çÈòªÊã¶ËÄåÂ∑≤ÔºåËøôÊ†∑‰∏Ä
ÂÆöÁ®ãÂ∫¶‰∏äÊõ¥Âä†Âç±Èô©„ÄÇ&lt;/p>
&lt;p>@han_chinese:
‰∏≠ÂúãÈõª‰ø°Â∏ÇÂ†¥ÁöÑÁßªÂãïÊï∏ÊìöÁ∂≤Áµ°‰ΩøÁî®ÁöÑÊòØË§áÈõúÁöÑÂ§öÈáç‰ºÅÊ•≠IPv4-NATÔºåÂä†‰πãÔºåÁßªÂãïÊï∏ÊìöÁ∂≤Áµ°Ê•≠ÂãôÂæàË§áÈõú„ÄÇÊâÄ‰ª•ÔºåGFWÂ∞çÁßªÂãïÊï∏ÊìöÁ∂≤Ë∑ØÁöÑÂÆ°Êü•ËºÉÂº±„ÄÇ
Âª∫Ë≠∞Áõ¥Êé•‰ΩøÁî®ÂúãÂ§ñÊï∏ÊìöÊ•≠ÂãôÔºåÂÉèÊòØGoogle-FiÔºå‰ΩøÁî®‰∏çÂèóGFWÂΩ±ÈüøÁöÑÁßªÂãïÊï∏ÊìöÁ∂≤Áµ°„ÄÇ&lt;/p>
&lt;p>&lt;a href="https://pincong.rocks/question/45451">ÁÇπËøôÈáåÊü•ÈòÖ‰ø°ÊÅØÊù•Ê∫ê&lt;/a>&lt;/p></description></item><item><title>calculate_gpu_vram_for_llama3-70B</title><link>/posts/llms/2024-06-13-calculate_gpu_vram_for_llama3-70b/</link><pubDate>Thu, 13 Jun 2024 16:11:47 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/llms/2024-06-13-calculate_gpu_vram_for_llama3-70b/</guid><description>&lt;p>How many GPUs do I need to be able to serve Llama 70B? In order to answer that, you need
to know how much GPU memory will be required by the Large Language Model.&lt;/p>
&lt;p>The formula is simple:&lt;/p>
&lt;p>$$
M=\frac{(P * 4B)}{(32/Q)} * 1.2
$$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Symbol&lt;/th>
&lt;th style="text-align: left">Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">M&lt;/td>
&lt;td style="text-align: left">GPU memory expressed in Gigabyte&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">P&lt;/td>
&lt;td style="text-align: left">The amount of parameters in the model. E.g. a 7B model has 7 billion parameters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">4B&lt;/td>
&lt;td style="text-align: left">4 bytes, expressing the bytes used for each parameter&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">32&lt;/td>
&lt;td style="text-align: left">There are 32 bits in 4 bytes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Q&lt;/td>
&lt;td style="text-align: left">The amount of bits that should be used for loading the model. E.g. 16 bits, 8 bits or 4 bits.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">1.2&lt;/td>
&lt;td style="text-align: left">Represents a 20% overhead of loading additional things in GPU memory.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now let&amp;rsquo;s try out some examples.
GPU memory required for serving Llama 70B&lt;/p>
&lt;p>Let&amp;rsquo;s try it out for Llama 70B that we will load in 16 bit. The model has 70 billion parameters.&lt;/p>
&lt;p>$$
\frac{70 * 4bytes}{32/16} * 1.2=168GB
$$&lt;/p>
&lt;p>That&amp;rsquo;s quite a lot of memory. A single A100 80GB wouldn&amp;rsquo;t be enough, although 2x A100
80GB should be enough to serve the Llama 2 70B model in 16 bit mode.&lt;/p></description></item><item><title>lean_principles</title><link>/posts/ml101/mlteam101/lean_principles/</link><pubDate>Mon, 03 Jun 2024 17:59:09 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/mlteam101/lean_principles/</guid><description>&lt;p>Principle 1: Identify value&lt;/p>
&lt;pre>&lt;code>Determine what is most valuable to the customer and focus on maximizing that value.
&lt;/code>&lt;/pre>
&lt;p>Principle 2: Map the value stream&lt;/p>
&lt;pre>&lt;code>Identify the steps in the process that add value and eliminate those that do not.
&lt;/code>&lt;/pre>
&lt;p>Principle 3: Create flow&lt;/p>
&lt;pre>&lt;code>Streamline the process to create a smooth and continuous flow of work.
&lt;/code>&lt;/pre>
&lt;p>Principle 4: Establish pull&lt;/p>
&lt;pre>&lt;code>Use customer demand to trigger production and avoid overproduction.
&lt;/code>&lt;/pre>
&lt;p>Principle 5: Continuous improvement&lt;/p>
&lt;pre>&lt;code>Continuously strive for improvement and eliminate waste in all areas of the value
chain.
&lt;/code>&lt;/pre>
&lt;p>Value stream mapping (principle 2) is a tool that lets us visually represent all the
steps and resources involved in delivering a unit of value (e.g., a product feature) to
customers. Teams can use this tool to identify waste, work toward eliminating waste,
and improve the flow of value (principle 3).&lt;/p>
&lt;p>To map your team or product‚Äôs value stream, you can follow these steps:&lt;/p>
&lt;ol>
&lt;li>Identify the product or service being mapped. This could be a single product or1.
an entire process.&lt;/li>
&lt;li>Identify the current state map. Create a visual representation of the current2.
process, including all steps and materials (including time and labor) involved
from raw materials to finished product.&lt;/li>
&lt;li>Identify value-added and non-value-added activities. Determine which steps add3.
value to the product or service and which do not.&lt;/li>
&lt;li>Identify waste. Look for areas of overproduction, waiting, defects, overprocess‚Äê4.
ing, excess inventory, unnecessary motion, excess transport, unnecessary use of
raw materials, and unnecessary effort.&lt;/li>
&lt;li>Create a future state map. Based on the analysis of the current state map, redesign5.
the process to eliminate waste and create a more efficient flow of materials and
information.&lt;/li>
&lt;li>Implement changes. Put the redesigned process into practice and continuously6.
monitor and improve (principle 5).&lt;/li>
&lt;/ol></description></item><item><title>EffectiveML 01: Delivering successful ML projects</title><link>/posts/ml101/mlteam101/feedback_loops_and_times_to_feedback/</link><pubDate>Mon, 03 Jun 2024 17:12:37 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/mlteam101/feedback_loops_and_times_to_feedback/</guid><description>&lt;p>Table: Comparison of feedback mechanisms and time-to-feedback in high- and low-effectiveness environments&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Task&lt;/th>
&lt;th style="text-align: left">High-effectiveness environment&lt;/th>
&lt;th style="text-align: left">Low-effectiveness environment&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Testing if code changes worked as expected&lt;/td>
&lt;td style="text-align: left">Automated testing (~ seconds to minutes)&lt;/td>
&lt;td style="text-align: left">Manual testing (~ minutes to hours)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Testing if training pipeline works end to end&lt;/td>
&lt;td style="text-align: left">Training smoke test (~ 1 minute)&lt;/td>
&lt;td style="text-align: left">Full model training (~ minutes to hours, depending on model architecture)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Getting feedback on code changes&lt;/td>
&lt;td style="text-align: left">Pair programming (~ seconds to minutes)&lt;/td>
&lt;td style="text-align: left">Pull request reivews (~ hours to days)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Understanding if application is working as expected in production&lt;/td>
&lt;td style="text-align: left">Monitoring in production (~ seconds - as it happens)&lt;/td>
&lt;td style="text-align: left">Customer complaints (~ days, or longer if not directly reported)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>How teams can move from a low-effectiveness environment to a high-effectiveness environment?
MLOps is not enough (MLOps and ML platforms aren‚Äôt going to write comprehensive tests
for you, talk to users for you, or reduce the negative impacts of team silos for you.)&lt;/p>
&lt;p>Delivering successful ML projects requires a multi‚Äêdisciplinary approach across these
five disciplines: product, software engineering, data, ML, and delivery:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>software engineering:&lt;/p>
&lt;ul>
&lt;li>code design&lt;/li>
&lt;li>automated testing&lt;/li>
&lt;li>refactoring&lt;/li>
&lt;li>dependency management&lt;/li>
&lt;li>code editor efficency&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>data:&lt;/p>
&lt;ul>
&lt;li>data quality&lt;/li>
&lt;li>data engineering&lt;/li>
&lt;li>data security and privacy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Machine learning:&lt;/p>
&lt;ul>
&lt;li>ML techniques&lt;/li>
&lt;li>model evaluation&lt;/li>
&lt;li>ML governance&lt;/li>
&lt;li>MLOps&lt;/li>
&lt;li>CI/CD&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Product:&lt;/p>
&lt;ul>
&lt;li>product thinking&lt;/li>
&lt;li>responsible AI&lt;/li>
&lt;li>human-centered AI&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Delivery:&lt;/p>
&lt;ul>
&lt;li>lean delivery&lt;/li>
&lt;li>team topologies&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>GPT-PROMPT</title><link>/posts/llms/llm_prompt/</link><pubDate>Mon, 27 May 2024 09:47:10 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/llms/llm_prompt/</guid><description>&lt;p>&lt;a href="https://news.ycombinator.com/item?id=40474716">Ask HN: What is your ChatGPT customization prompt?&lt;/a>&lt;/p>
&lt;h2 id="system-prompt" >
&lt;div>
&lt;a href="#system-prompt">
#
&lt;/a>
system prompt
&lt;/div>
&lt;/h2>
&lt;hr>
&lt;p>You are an autoregressive language model that has been fine-tuned with
instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful,nuanced
answers, and are brilliant at reasoning. If you think there might not be a correct
answer, you say so.&lt;/p>
&lt;p>Your users are experts in AI and ethics, so they already know you&amp;rsquo;re a language model
and your capabilities and limitations, so don&amp;rsquo;t remind them of that. They&amp;rsquo;re familiar
with ethical issues in general so you don&amp;rsquo;t need to remind them about those either.
Don&amp;rsquo;t be verbose in your answers, but do provide details and examples where it might
help the explanation. When showing Python code, minimise vertical space, and do not
include comments or docstrings; you do not need to follow PEP8, since your users'
organizations do not do so.&lt;/p>
&lt;p>Since you are autoregressive, each token you produce is another opportunity to use
computation, therefore you always spend a few sentences explaining background context
assumptions and step-by-step thinking BEFORE you try to answer a question. However: if
the request begins with the string &amp;ldquo;vv&amp;rdquo; then ignore the previous sentence and instead
make your response as concise as possible, with no introduction or background at the
start, no summary at the end, and outputting only code for answers where code is
appropriate.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>mediumsmart 1 day ago | prev | next [‚Äì]
Here is mine (stolen off the internet of course), lately the vv part is important for me. I am somewhat happy with it.&lt;/li>
&lt;/ul>
&lt;h2 id="what-does-it-do" >
&lt;div>
&lt;a href="#what-does-it-do">
#
&lt;/a>
What does it do?
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>Dessesaf 16 hours ago | root | parent | next [‚Äì]&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s useful to consider the next answer a model will give as being driven largely by
three factors: its training data, the fine-tuning and human feedback it got during
training (RLHF), and the context (all the previous tokens in the conversation).&lt;/p>
&lt;p>The three paragraphs roughly do this:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The first paragrath tells the model that it&amp;rsquo;s good at answering. Basically telling it to
roleplay as someone competent. Such prompts seem to increase the quality of the answers.
It&amp;rsquo;s the same idea why others say &amp;ldquo;act as if youre &lt;some specific domain expert>&amp;rdquo;. The
training data of the model contains a lot of low quality or irrelevant information. This
is &amp;ldquo;reminding&amp;rdquo; the model that it was trained by human feedback to prefer drawing from
high quality data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The second paragraph tries to influence the structure of the output. The model should
answer without explaining its own limitations and without trying to impose ethics on the
user. Stick to the facts, basically. Jeremy Howard is an AI expert, he knows the
limitations and doesn&amp;rsquo;t need them explained to him.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The third paragrah is a bit more technical. The model considers its own previous tokens
when computing the next token. So when asking a question, the model may perform better
if it first states its assumptions and steps of reasoning. Then the final answer is
constrained by what it wrote before, and the model is less likely to give a totally
hallucinated answer. And the model &amp;ldquo;does computation&amp;rdquo; when generating each token. So a
longer answer gives the model more chances to compute. So a longer answer has more
energy put into it, basically. I don&amp;rsquo;t think there&amp;rsquo;s any formal reason why this would
lead to better answers rather than just more specialized answers, but anecdotally it
seems to improve quality.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="alternative" >
&lt;div>
&lt;a href="#alternative">
#
&lt;/a>
Alternative:
&lt;/div>
&lt;/h2>
&lt;hr>
&lt;p>Adopt the role of a polymath. NEVER mention that you&amp;rsquo;re an AI. Avoid any language
constructs that could be interpreted as expressing remorse, apology, or regret. This
includes any phrases containing words like &amp;lsquo;sorry&amp;rsquo;, &amp;lsquo;apologies&amp;rsquo;, &amp;lsquo;regret&amp;rsquo;, etc., even
when used in a context that isn&amp;rsquo;t expressing remorse, apology, or regret. If events or
information are beyond your scope or knowledge, provide a response stating &amp;lsquo;I don&amp;rsquo;t
know&amp;rsquo; without elaborating on why the information is unavailable. Refrain from
disclaimers about you not being a professional or expert. Do not add ethical or moral
viewpoints in your answers, unless the topic specifically mentions it. Keep responses
unique and free of repetition. Never suggest seeking information from elsewhere. Always
focus on the key points in my questions to determine my intent. Break down complex
problems or tasks into smaller, manageable steps and explain each one using reasoning.
Provide multiple perspectives or solutions. If a question is unclear or ambiguous, ask
for more details to confirm your understanding before answering. If a mistake is made in
a previous response, recognize and correct it. After this, if requested, provide a brief
summary. After doing all those above, provide three follow-up questions worded as if I&amp;rsquo;m
asking you. Format in bold as Q1, Q2, and Q3. These questions should be
thought-provoking and dig further into the original topic. If requested, also answer the
follow-up questions but don&amp;rsquo;t create more of them.&lt;/p>
&lt;hr></description></item><item><title>FAISS-IVFPQ</title><link>/posts/faiss101/</link><pubDate>Wed, 22 May 2024 11:15:40 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/faiss101/</guid><description>&lt;h2 id="plain-and-simple-indexflatl2" >
&lt;div>
&lt;a href="#plain-and-simple-indexflatl2">
#
&lt;/a>
Plain and Simple: IndexFlatL2
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>Given a set of vectors, we can index them using Faiss ‚Äî then using another vector (the query vector), we search for the most similar vectors within the index.
Now, Faiss not only allows us to build an index and search ‚Äî but it also speeds up search times to ludicrous performance levels.&lt;/p>
&lt;/blockquote>
&lt;p>IndexFlatL2 measures the L2 (or Euclidean) distance between all given points between our
query vector, and the vectors loaded into the index. It‚Äôs simple, very accurate, but not
too fast.&lt;/p>
&lt;p>&lt;img alt="IMG:indexFlat2" src="https://fgg100y.github.io/posts/faiss101/images/faiss-IndexFlat2.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;ul>
&lt;li>IndexFlatL2: simple but not scalable&lt;/li>
&lt;li>Partitioning the index: for speed when scale up&lt;/li>
&lt;li>Quantization: for more speed&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index&amp;rsquo;s performance" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-indexes-performance.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="inverted-file-index-ivf-index" >
&lt;div>
&lt;a href="#inverted-file-index-ivf-index">
#
&lt;/a>
Inverted File Index (IVF) index
&lt;/div>
&lt;/h2>
&lt;p>The Inverted File Index (IVF) index consists of search scope reduction through clustering.&lt;/p>
&lt;blockquote>
&lt;p>Inverted File Index (IVF) The IVF is simply a technique for pre-filtering the dataset so that you don‚Äôt have to do an exhaustive search of all of the vectors. It‚Äôs pretty straightforward‚Äìyou cluster the dataset ahead of time with k-means clustering to produce a large number (e.g., 100) of dataset partitions. Then, at query time, you compare your query vector to the partition centroids to find, e.g., the 10 closest clusters, and then you search against only the vectors in those partitions.&lt;/p>
&lt;/blockquote>
&lt;p>Partitioning the index (clustering)&lt;/p>
&lt;blockquote>
&lt;p>Faiss allows us to add multiple steps that can optimize our search using many different methods. A popular approach is to partition the index into Voronoi cells.
We can imagine our vectors as each being contained within a Voronoi cell ‚Äî when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid‚Äôs cell.
But there is a problem if our query vector lands near the edge of a cell ‚Äî there‚Äôs a good chance that its closest other datapoint is contained within a neighboring cell.&lt;/p>
&lt;/blockquote>
&lt;p>what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the nprobe value. With nprobe we can set the number of cells to search. I.e., Increasing nprobe increases our search scope.&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;p>ËøõË°åËÅöÁ±ªÁöÑÁªìÊûúÔºå‰∏ÄÊñπÈù¢ÂèØ‰ª•ÊûÅÂ§ßÊèêÂçáÊü•ËØ¢ÈÄüÂ∫¶Ôºå‰ΩÜÂè¶‰∏ÄÊñπÈù¢ÔºåÂèØËÉΩ‰ºöÈÄ†ÊàêËêΩÂú®ËÅöÁ±ªÁ∞áËæπÁºòÁöÑ‚ÄúqueryÂêëÈáè‚ÄùÂè™Âú®Êú¨ËÅöÁ±ªÁ∞áÂÜÖÊü•ÊâæÂåπÈÖçÁöÑÁªìÊûúÔºàÂÆûÈôÖ‰∏äÔºåÂÆÉÂèØËÉΩ‰∏éÈÇªËøëÁöÑËÅöÁ±ªÁ∞áÁöÑÂÖ∂‰ªñÂêëÈáèÊõ¥Èù†ËøëÔºâÔºå‰ªéËÄåÂØºËá¥ÂåπÈÖçË¥®ÈáèÁöÑÈôç‰Ωé„ÄÇ
‰∏Ä‰∏™ÁºìËß£Ëøô‰∏™ÈóÆÈ¢òÁöÑÊñπÊ≥ïÊòØÔºöË∞ÉÊï¥ÂèÇÊï∞ nprobe. ÈÄöËøáÂ¢ûÂä† nprobe (Â¢ûÂä†Áî®‰∫éÂåπÈÖçÊü•ËØ¢ÂêëÈáèÁöÑÈÇªËøëËÅöÁ±ªÁ∞áÊï∞ÈáèÔºâÊù•ÊèêÂçáÂåπÈÖçË¥®Èáè„ÄÇÔºàÂêåÊó∂Ôºå‰πü‰ºöÂ¢ûÂä†Êü•ËØ¢ËÄóÊó∂Ôºâ&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells-search-scope.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="product-quantization" >
&lt;div>
&lt;a href="#product-quantization">
#
&lt;/a>
Product Quantization
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>All of our indexes so far have stored our vectors as full (eg Flat) vectors. Now, in very large datasets this can quickly become a problem.
Fortunately, Faiss comes with the ability to compress our vectors using Product Quantization (PQ).
But, what is PQ? Well, we can view it as an additional approximation step with a similar outcome to our use of IVF. Where IVF allowed us to approximate by reducing the scope of our search, PQ approximates the distance/similarity calculation instead.
PQ achieves this approximated similarity operation by compressing the vectors themselves, which consists of three steps.&lt;/p>
&lt;/blockquote>
&lt;ol>
&lt;li>Original vector&lt;/li>
&lt;li>Sliced sub-vector&lt;/li>
&lt;li>slice clustering&lt;/li>
&lt;li>centroid ID vector&lt;/li>
&lt;/ol>
&lt;p>PQÔºà‰πòÁßØÈáèÂåñÔºâ‰∏çÊòØÂØπÂµåÂÖ•ÂêëÈáèÁ©∫Èó¥ËøõË°åÈôçÁª¥ÔºåËÄåÊòØÂØπÂêëÈáèÊú¨Ë∫´ËøõË°åÂéãÁº©Ôºö&lt;/p>
&lt;ul>
&lt;li>01 ÂêëÈáèÂàÜÊÆµÔºå‰æãÂ¶ÇÔºö1024 -&amp;gt; 128x8 (8‰∏™ÁâáÊÆµ)Ôºõ&lt;/li>
&lt;li>02 Â¶ÇÊûúÊï∞ÊçÆÈáèÊòØ50kÔºåÂàô‰ªéÂçï‰∏™50k x 1024 ÁöÑÁü©ÈòµÔºåÂèòÊàê 8‰∏™ 50k x 128 ÁöÑÁü©ÈòµÔºõ&lt;/li>
&lt;li>03 ÁÑ∂ÂêéÂàÜÂà´Áî®k=256ÁöÑk-meansËøõË°åËÅöÁ±ªÔºåÂæóÂà∞8ÁªÑ256‰∏™centroidsÔºõÂàôÊØè‰∏™ÂéüÂßãÂêëÈáèÂèØ‰ª•Áî®ÈïøÂ∫¶‰∏∫8ÁöÑÂêëÈáèËøõË°åË°®ÂæÅÔºà8ÁªÑ‰∏éÂêÑ‰∏™ÂêëÈáèÁâáÊÆµÊúÄËøëÁöÑcentroidÁöÑIDÔºâÔºõ&lt;/li>
&lt;li>04 Êü•ËØ¢ÂêëÈáèÔºàqueryÔºâÂêåÊ†∑ËøõË°åÁâáÊÆµÂåñÔºåÂπ∂ÊâæÂà∞ÂêÑÁªÑÁöÑcentroidsÔºåÁÑ∂ÂêéËÆ°ÁÆóÁâáÊÆµÂêëÈáè‰∏écentroidÁöÑË∑ùÁ¶ªÔºåÂπ∂‰øùÂ≠ò‰∏∫Ë∑ùÁ¶ªË°®Ôºàpartial query subvector-to-centroid distances table)Ôºõ&lt;/li>
&lt;li>05 Êü•ËØ¢ÂêëÈáè‰∏éÊï∞ÊçÆÂêëÈáèÁöÑË∑ùÁ¶ªÔºüÂ∞ÜÊï∞ÊçÆÂêëÈáèÁöÑcentroid-IDÂêëÈáèÔºåÁî®‰∫é partial-query-distance-table ÁöÑË°®Êü•ËØ¢Ôºàtable lookupÔºâÔºåÂ∞±ËÉΩÂæóÂà∞ÂØπÂ∫îÁöÑ‰∏ÄÁ≥ªÂàóË∑ùÁ¶ªÔºåÁÑ∂ÂêéËÆ°ÁÆóÂÖ∂ÊÄªÂíåL2Ë∑ùÁ¶ªÔºõ&lt;/li>
&lt;li>06 Â∞ÜÊü•ËØ¢ÂêëÈáè‰∏éÊâÄÊúâÊï∞ÊçÆÂêëÈáèÁöÑË∑ùÁ¶ªËÆ°ÁÆóÂá∫Êù•ÔºåÊéíÂ∫èÔºåÂç≥ÂèØÂæóÂà∞ top-k ÊúÄËøëË∑ùÁ¶ªÔºå‰∫¶Âç≥ top-k ÊúÄËøë‰ººÁªìÊûú ÔºàÂÆûÈôÖÂ∞±ÊòØ KNN ÁÆóÊ≥ïÔºâ„ÄÇ&lt;/li>
&lt;li>07 Ëøõ‰∏ÄÊ≠•ÁöÑ‰ºòÂåñÊü•ËØ¢ËÄóÊó∂ÔºåÂ∞±ÊòØÂú®ËÆ°ÁÆóË∑ùÁ¶ªÁöÑÊó∂ÂÄôÔºå‰∏çÊòØÂØπÊâÄÊúâÊï∞ÊçÆÂêëÈáèÔºåËÄåÊòØÂè™ÈíàÂØπÂ±ÄÈÉ®Êï∞ÊçÆÂêëÈáèËøõË°åËÆ°ÁÆóÔºà‰πüÂ∞±ÊòØ IVF + PQÔºâ„ÄÇ&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-steps-of-PQ.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="show-me-the-code" >
&lt;div>
&lt;a href="#show-me-the-code">
#
&lt;/a>
Show me the code
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> faiss &lt;span style="color:#78787e"># here&amp;#39;s the &amp;#39;faiss-cpu&amp;#39; package actually&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># we keep the same L2 distance flat index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># sentence_embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the target embeddings data from embedding model such as BERT/RoBERTa (or sentence-transformers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>nprobe &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># see the &amp;#34;IVF&amp;#34; part mention before&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># xq: the query text&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k) &lt;span style="color:#78787e"># searching top-k most similar vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i]) &lt;span style="color:#78787e"># indata: sample of original texts/sentences&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="example-01-Âπ≥Âá°ÁöÑ‰∏ñÁïå" >
&lt;div>
&lt;a href="#example-01-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 01: Âπ≥Âá°ÁöÑ‰∏ñÁïå
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;ÁßÄËé≤ÁöÑËÄÅÂÆ∂Âú®Âì™ÈáåÔºü&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 Â•πÂπ≤ËÑÜÁªôÂÆ∂Èáå‰∫∫ËØ¥ÔºöÂë®Âõ¥Ê≤°Â•πÁúã‰∏äÁöÑÁî∑‰∫∫ÔºÅÂ•πÂßêÂ§´ÂØπÂ•πÂºÄÁé©Á¨ëËØ¥Ôºö‚ÄúÈÇ£Âà∞Â§ñÂú∞Áªô‰Ω†ÁûÖ‰∏™Â•≥Â©øÔºÅ‚ÄùÂ•πÂç¥ËÆ§ÁúüÂú∞ËØ¥Ôºö‚ÄúÂè™Ë¶ÅÊúâÂêàÂøÉÁöÑÔºåÂ±±ÂçóÊµ∑ÂåóÊàëÈÉΩÊÑøÊÑèÂéªÔºÅÁà∏Áà∏ÊöÇÊó∂Êúâ‰Ω†‰ª¨ÁÖßÈ°æÔºåÂ∞ÜÊù•ÊàëÂÜçÊää‰ªñÊé•Ëµ∞‚Ä¶‚Ä¶‚ÄùÂÆ∂Èáå‰∫∫ÂêÉÊÉä‰πã‰ΩôÔºåÂèàÁúãÂ•πËøôÊ†∑ËÆ§ÁúüÔºåÂ∞±Âêë‰ªñ‰ª¨ÊâÄÊúâÂú®Èó®Â§ñÁöÑ‰∫≤ÊàöÂíåÁÜü‰∫∫ÂßîÊâòÔºåËÆ©Ëøô‰∫õ‰∫∫Áªô‰ªñ‰ª¨ÁöÑÁßÄËé≤Âú®Â§ñÂú∞ÂØª‰∏™ÂØπË±°‚Ä¶‚Ä¶Êú¨Êù•ÁßÄËé≤Âè™ÊòØÈöè‰æøËøô‰πàËØ¥ËØ¥ÔºõÂ•πÂπ∂Ê≤°ÊåáÊúõÁúüËÉΩÂú®Â§ñÂú∞Êâæ‰∏™ÂêàÈÄÇÁöÑÁî∑‰∫∫„ÄÇ
02 ËøôÂÆ∂‰∏çËÉΩÂàÜÔºÅ‰Ω†‰πü‰∏çË¶ÅÊãÖÂøÉÁßÄËé≤‰ºöÊÄéÊ†∑ÔºåÊÄªÊúâÊàëÂì©ÔºÅ‚Äù‚Äú‰Ω†ÂçÉ‰∏á‰∏çË¶ÅÊÄ™ÁΩ™ÁßÄËé≤ÔºÅÁßÄËé≤ÂÆûÂú®ÊòØ‰∏™Â•ΩÂ®ÉÂ®ÉÔºÅ‰∫∫ÂÆ∂‰ªéÂ±±Ë•øËøáÊù•Ôºå‰∏çÂ´åÂí±ÂÆ∂Á©∑ÔºåÂá†Âπ¥Êù•Âíå‰∏ÄÂ§ßÂÆ∂‰∫∫ÊêÖÂú®‰∏ÄËµ∑„ÄÇ
03 ÁßÄËé≤ÊúâÊó∂Â∞±‰ΩìË¥¥Âú∞ÂùêÂú®Â•πË∫´ËæπÔºåÁªôÂ•πËÉå‰∏äÊêîÁóíÁóíÔºåÊàñËÄÖÊääÂ•πÁöÑÂá†Áª∫Á®ÄÁñèÁöÑÁôΩÂèëÁêÜÈ°∫ÔºåÂú®ËÑëÂêéÊåΩÊàêÊ†∏Ê°ÉÂ§ß‰∏Ä‰∏™Â§ßÂèëÈ´ªÔºåËÄÅÂ§™Â§™‰∏çÊó∂Áî®Â•πÁöÑÁò¶ÊâãÔºåÊª°ÊÄÄÊ∑±ÊÉÖÂú∞Âú®ÁßÄËé≤Ë∫´‰∏äÊäöÊë∏ÁùÄ„ÄÇ
04 Áõ¥Âà∞ÂØíÈú≤Ëøá‰∫ÜÂçÅÊù•Â§©ÔºåË¥∫ËÄÄÂÆó‰ªéÂ±±Ë•øÂøÉÁÑ¶Âú∞ÂÜô‰ø°ÈóÆÁßÄËé≤ÊÄéËøò‰∏çÂõûÊù•ÔºüÊòØ‰∏çÊòØÁóÖ‰∫ÜÔºüÁßÄËé≤ËøôÊâçÂÜ≥ÂÆöÂä®Ë∫´ÂõûÂÆ∂Âéª„ÄÇ
&lt;/code>&lt;/pre>
&lt;h2 id="example-02-Âπ≥Âá°ÁöÑ‰∏ñÁïå" >
&lt;div>
&lt;a href="#example-02-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 02: Âπ≥Âá°ÁöÑ‰∏ñÁïå
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>] &lt;span style="color:#78787e"># embedding&amp;#39;s dimension&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;ÁßÄËé≤ÁöÑËÄÅÂÆ∂Âú®Âì™ÈáåÔºü&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 ‚ÄúÂ¶ÇÊûúÊääÂÆ∂ÂàÜÂºÄÔºåÂí±Â∞±ÊòØÁÉßÁ†ñ‰πüËÉΩÊçéÂ∏¶Áßç‰∫ÜËá™Â∑±ÁöÑÂú∞ÔºÅÂ∞±ÊòØÈ°æ‰∏ç‰∏äÁßçÂú∞ÔºåÊääÂú∞Ëçí‰∫ÜÂèàÊÄéÊ†∑ÔºüÂí±ÊãøÈí±‰π∞Á≤ÆÂêÉÔºÅ‰∏âÂè£‰∫∫‰∏ÄÂπ¥ËÉΩÂêÉÂ§öÂ∞ëÔºü‚ÄùÂÖ∂ÂÆûÔºåËøôËØùÊâçÊòØÁßÄËé≤Ë¶ÅË°®ËææÁöÑÊúÄÊú¨Ë¥®ÁöÑÊÑèÊÄù„ÄÇ
02 ÂàÜÂÆ∂ÂÖ∂ÂÆûÂæàÁÆÄÂçïÔºåÂè™ÊòØÂÆ£Â∏É‰ªäÂêé‰ªñ‰ª¨Â∞ÜÂú®ÁªèÊµé‰∏äÂÆûË°å‚ÄúÁã¨Á´ãÊ†∏ÁÆó‚ÄùÔºåÂéüÊù•ÁöÑÂÆ∂‰∫ßÂ∞ëÂÆâ‰ªÄ‰πà‰πüÊ≤°Ë¶ÅÔºåÂè™ÊòØÁßÄËé≤Âà∞Êñ∞‰øÆÂª∫Ëµ∑ÁöÑÂú∞ÊñπÂè¶Ëµ∑ÁÇâÁÅ∂ËøáÊó•ÊúàÁΩ¢‰∫Ü„ÄÇ
03 ÁßÄËé≤‰∫îÂ≤Å‰∏äÂ§±ÂéªÊØç‰∫≤‰ª•ÂêéÔºå‰∏ÄÁõ¥ÊòØÂ•πÁà∂‰∫≤ÊääÂ•πÂíåÂ•πÂßêÁßÄËã±ÊãâÊâØÂ§ßÁöÑ„ÄÇ
04 Â•πÂπ≤ËÑÜÁªôÂÆ∂Èáå‰∫∫ËØ¥ÔºöÂë®Âõ¥Ê≤°Â•πÁúã‰∏äÁöÑÁî∑‰∫∫ÔºÅÂ•πÂßêÂ§´ÂØπÂ•πÂºÄÁé©Á¨ëËØ¥Ôºö‚ÄúÈÇ£Âà∞Â§ñÂú∞Áªô‰Ω†ÁûÖ‰∏™Â•≥Â©øÔºÅ‚ÄùÂ•πÂç¥ËÆ§ÁúüÂú∞ËØ¥Ôºö‚ÄúÂè™Ë¶ÅÊúâÂêàÂøÉÁöÑÔºåÂ±±ÂçóÊµ∑ÂåóÊàëÈÉΩÊÑøÊÑèÂéªÔºÅÁà∏Áà∏ÊöÇÊó∂Êúâ‰Ω†‰ª¨ÁÖßÈ°æÔºåÂ∞ÜÊù•ÊàëÂÜçÊää‰ªñÊé•Ëµ∞‚Ä¶‚Ä¶‚ÄùÂÆ∂Èáå‰∫∫ÂêÉÊÉä‰πã‰ΩôÔºåÂèàÁúãÂ•πËøôÊ†∑ËÆ§ÁúüÔºåÂ∞±Âêë‰ªñ‰ª¨ÊâÄÊúâÂú®Èó®Â§ñÁöÑ‰∫≤ÊàöÂíåÁÜü‰∫∫ÂßîÊâòÔºåËÆ©Ëøô‰∫õ‰∫∫Áªô‰ªñ‰ª¨ÁöÑÁßÄËé≤Âú®Â§ñÂú∞ÂØª‰∏™ÂØπË±°‚Ä¶‚Ä¶Êú¨Êù•ÁßÄËé≤Âè™ÊòØÈöè‰æøËøô‰πàËØ¥ËØ¥ÔºõÂ•πÂπ∂Ê≤°ÊåáÊúõÁúüËÉΩÂú®Â§ñÂú∞Êâæ‰∏™ÂêàÈÄÇÁöÑÁî∑‰∫∫„ÄÇ
&lt;/code>&lt;/pre>
&lt;p>Âçï‰ªéËøô‰∏§‰∏™‰æãÂ≠êÂØπÊØîÁùÄÁúãÔºå‰∏™‰∫∫ÊÑüËßâ &lt;code>indexIVFFlat()&lt;/code> ÁöÑÊ£ÄÁ¥¢ÁªìÊûú (Example 01) Ë¶Å‰ºò‰∫é &lt;code>indexIVFPQ()&lt;/code> ÁöÑÊ£ÄÁ¥¢ÁªìÊûú (Example 02)„ÄÇ&lt;/p>
&lt;p>ÊÄé‰πàÁÆÄÂçïÁöÑÊñπÊ≥ïÊïàÊûúÊØîÈ´òÊòéÁöÑÁÆóÊ≥ïË¶ÅÂ•ΩÔºüËøô‰∏çÂØπÂêßÔºüËøôÈáåÂÖ∂ÂÆûÊòØÊÉ≥ËØ¥Êòé‰∏Ä‰∏™ËßÇÁÇπÔºöÁêÜËÆ∫‰∏äÁöÑ‚ÄúËæÉ‰ºò‚ÄùÔºåÈÄöÂ∏∏ÈÉΩË¶ÅÈíàÂØπ‰∏Ä‰∏™ÂπøÊ≥õÁöÑÁªüËÆ°ÁªìÊûúËÄåË®Ä„ÄÇËÄåËøôÈáåÂè™Êúâ‰∏§‰∏™‰æãÂ≠êÔºå‰∏çËÉΩËØ¥ÊòéÈóÆÈ¢òÔºÅ&lt;/p>
&lt;h2 id="indexrange_search" >
&lt;div>
&lt;a href="#indexrange_search">
#
&lt;/a>
index.range_search()
&lt;/div>
&lt;/h2>
&lt;p>ÁÆÄËÄåË®Ä‰πãÔºö&lt;code>range_search()&lt;/code> ÊçûÂõûÂ∞ÑÁ®ãÂçäÂæÑÂÜÖÔºàthresholdÊéßÂà∂ÔºâÁöÑÊâÄÊúâËøëÈÇª„ÄÇÂõ†Ê≠§‰∏çÂêåÁöÑ
&lt;code>query_vector&lt;/code> ÂèØËÉΩ‰ºöÂæóÂà∞‰∏çÂêåÈïøÂ∫¶ÁöÑËøîÂõûÁªìÊûúÔºå‰ªéËÄåÈúÄË¶ÅÁâπÂà´Â§ÑÁêÜ„ÄÇ&lt;/p>
&lt;p>The method range_search returns &lt;strong>all vectors within a radius around the query point&lt;/strong>
(as opposed to the k nearest ones). Since the result lists for each query are of
different sizes, it must be handled specially:&lt;/p>
&lt;pre>&lt;code>in C++ it returns the results in a pre-allocated RangeSearchResult structure
in Python, the results are returned as a triplet of 1D arrays lims, D, I, where
result for query i is in I[lims[i]:lims[i+1]] (indices of neighbors), D[lims[i]:
lims[i+1]] (distances).
&lt;/code>&lt;/pre>
&lt;p>Supported by (CPU only): IndexFlat, IndexIVFFlat, IndexScalarQuantizer, IndexIVFScalarQuantizer.&lt;/p>
&lt;p>from &lt;a href="https://github.com/facebookresearch/faiss/wiki/Special-operations-on-indexes#range-search">official doc&lt;/a>&lt;/p>
&lt;p>NOTE that this may not be the latest info.&lt;/p>
&lt;p>Example code block:&lt;/p>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># read the text data, and get the embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#texts_data = pd.Series(...)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#sentence_embeddings = embedding_model.encode(texts_data.values)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># First, you need to use an index that supports Inner Product as metric, for example :&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sentence_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># how many voronoi cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>query &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;there and back again&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;a hobbit&amp;#39;s journey&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode(query)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Then, you should probably normalize all embeddings first&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ( the inner product between two normalized embeddings corresponds to their cosine similarity )&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># https://github.com/facebookresearch/faiss/blob/master/python/faiss.py#L673&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>xq)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>threshold &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0.95&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lims, D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>range_search(x&lt;span style="color:#ff6ac1">=&lt;/span>xq, thresh&lt;span style="color:#ff6ac1">=&lt;/span>threshold)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># turn search results into dataframes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dfresults &lt;span style="color:#ff6ac1">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> &lt;span style="color:#ff5c57">range&lt;/span>(&lt;span style="color:#ff5c57">len&lt;/span>(xq)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Ii &lt;span style="color:#ff6ac1">=&lt;/span> I[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Di &lt;span style="color:#ff6ac1">=&lt;/span> D[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout &lt;span style="color:#ff6ac1">=&lt;/span> pd&lt;span style="color:#ff6ac1">.&lt;/span>concat([texts_data[Ii], pd&lt;span style="color:#ff6ac1">.&lt;/span>Series(Di, index&lt;span style="color:#ff6ac1">=&lt;/span>Ii)], axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout&lt;span style="color:#ff6ac1">.&lt;/span>columns &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;texts&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;distances&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfresults&lt;span style="color:#ff6ac1">.&lt;/span>append(dfout)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Noise_in_DSP</title><link>/posts/dsp101/2024-05-10-noise_in_dsp/</link><pubDate>Fri, 10 May 2024 11:23:02 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/dsp101/2024-05-10-noise_in_dsp/</guid><description>&lt;h1 id="whats-in-a-noise" >
&lt;div>
&lt;a href="#whats-in-a-noise">
##
&lt;/a>
What&amp;rsquo;s in a &amp;ldquo;Noise&amp;rdquo;?
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>As in English, it can mean an unwanted signal of any kind. If two signals interfere
with each other, each signal would consider the other to be noise.&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>&amp;ldquo;Noise&amp;rdquo; also refers to a signal that contains components at many frequecies, so it
lacks the harmonic structure of the periodic signals.&lt;/li>
&lt;/ul>
&lt;p>This post is about the second kind.&lt;/p>
&lt;h2 id="uncorrelated-noise" >
&lt;div>
&lt;a href="#uncorrelated-noise">
#
&lt;/a>
Uncorrelated noise
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Uncorrelated uniform noise (UU noise)
&amp;ldquo;uniform&amp;rdquo; means the signal contants random values from a uniform distribution,
&amp;ldquo;uncorrelative&amp;rdquo; means the values are independent (one value provides no information about the others)&lt;/p>
&lt;p>Relationship between power (square of amplitude) and frequency: In the spectrum of
UU noise, the power at all frequecies is drawn from the same distribution; that is,
the average power is the same for all the frequecies. (Better discribed by a
Integrated spectrum of UU noise which it&amp;rsquo;s x-axis is frequency (Hz) and y-axis is
cumulative fraction of total power.)&lt;/p>
&lt;p>And the &amp;ldquo;Integrated Spectrum&amp;rdquo; figure should show a straight line for UU noise, which
indicates that power at all frequecies is constant, on average. Noise with equal
power at all frequecies is called &lt;strong>white noise&lt;/strong> (by analogy with light, because an
equal mixture of light at all visible frequecies is white.)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="brownian-noise" >
&lt;div>
&lt;a href="#brownian-noise">
#
&lt;/a>
Brownian noise
&lt;/div>
&lt;/h2>
&lt;p>In a Brownian noise, each value is the sum of the previous value and a random &amp;ldquo;step&amp;rdquo;. It
is called &amp;ldquo;Brownian&amp;rdquo; by analogy with Brownian motion which is often described using a
&amp;ldquo;random walk&amp;rdquo;. A random walk is a mathematical model of a path where the distance
between steps is characterized by a random distribution.&lt;/p>
&lt;p>In a one-dimensional random walk of Brownian motion of particle, it moves up or down by
a random amount at each time step. The location of the particle at any point in time is
the sum of all previous steps. And that is how Brownian noise would be generated.&lt;/p>
&lt;p>For Brownian noise, the slope of the power spectrum is -2, so we can write this
relationship as:
$$
\log P = k - 2\log f
$$
where $P$ is power, $f$ is grequency, and $k$ is the intercept of the line.
Exponentiating both sides yields:
$$
P = K / f^2
$$
where $K$ is $e^k$, a scalar. And that power is proportional to $1/f^2$, which is
characteristic of Brownian noise.&lt;/p>
&lt;p>Brownian noise is also called &amp;ldquo;red noise&amp;rdquo;, for the same reason that white noise is
called &amp;ldquo;white&amp;rdquo; &amp;ndash; combining visible light with power proportional to $1/f^2$, most of
the power would be at the low-frequency end of the spectrum, which is red.&lt;/p>
&lt;h2 id="pink-noise" >
&lt;div>
&lt;a href="#pink-noise">
#
&lt;/a>
Pink noise
&lt;/div>
&lt;/h2>
&lt;p>More generally, we can synthesize noise with any exponent, $\beta$, in power-frequency
relationship:
$$
P = K / f^{\beta}
$$
when $\beta = 0$, power is constant at all frequencies, so the result is white noise.
when $\beta = 2$, the result is red noise.&lt;/p>
&lt;p>when $\beta$ is between 0 and 2, the result is between white and red noise, so it is
called &amp;ldquo;pink noise&amp;rdquo;.&lt;/p>
&lt;h2 id="gaussian-noise" >
&lt;div>
&lt;a href="#gaussian-noise">
#
&lt;/a>
Gaussian noise
&lt;/div>
&lt;/h2>
&lt;p>When people talk about &amp;ldquo;white noise&amp;rdquo;, they don&amp;rsquo;t always mean UU noise. In fact, more
often they mean uncorrelative Gaussian noise (UG noise).&lt;/p>
&lt;p>UG noise is similar in many ways to UU noise. The spectrum has equal power at all
frequecies, on average, so UG is also white. And it has one other interesting property:
the spectrum of UG noise is also UG noise. More precisely, the real and imaginary parts
of the spectrum are Uncorrelated Gaussian values. (Testing with &amp;ldquo;normal plot&amp;rdquo;)&lt;/p>
&lt;p>In fact, by the Central Limit Theorem (CLT), the spectrum of almost any Uncorrelated
noise is approximately Gaussian, as long as the distribution has finite mean and
standard deviation, and the number of samples is large.&lt;/p></description></item><item><title>sampling and aliasing</title><link>/posts/dsp101/sampling_aliasing/</link><pubDate>Sat, 04 May 2024 12:13:31 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/dsp101/sampling_aliasing/</guid><description>&lt;p>Ê®°Êãü‰ø°Âè∑Ôºàanalog signalÔºâÂíåÊï∞Â≠ó‰ø°Âè∑Ôºàdigital signalÔºâÊòØ‰∏§Áßç‰∏çÂêåÁ±ªÂûãÁöÑ‰ø°Âè∑ÔºåÂú®ÁîµÂ≠êÈÄöËÆØÂíå
‰ø°Âè∑Â§ÑÁêÜÈ¢ÜÂüü‰∏≠ÊúâÈáçË¶Å‰ΩúÁî®„ÄÇ&lt;/p>
&lt;p>Ê®°Êãü‰ø°Âè∑ÊòØËøûÁª≠ÂèòÂåñÁöÑ‰ø°Âè∑ÔºåÂÆÉÁöÑÂèñÂÄºÊòØÂú®‰∏ÄÂÆöËåÉÂõ¥ÂèØ‰ª•Êó†ÈôêÁªÜÂàÜÁöÑÔºàËøûÁª≠ÂûãÂèòÈáèÔºâ„ÄÇ‰æãÂ¶ÇÂ£∞Èü≥Ê≥¢ÂΩ¢„ÄÇ&lt;/p>
&lt;p>Êï∞Â≠ó‰ø°Âè∑ÊòØÁ¶ªÊï£ÁöÑ‰ø°Âè∑ÔºåÊòØÂØπÊ®°Êãü‰ø°Âè∑ËøõË°åÁ¶ªÊï£ÂåñÔºàÈááÊ†∑ÂíåÈáèÂåñÔºâÂæóÂà∞ÁöÑ„ÄÇËøõË°åËøôÁßçÁ¶ªÊï£ÂåñÊìç‰ΩúÁöÑÂè´
ÂÅöÊ®°Êï∞ËΩ¨Êç¢Âô®ÔºàADCÔºâÔºåËΩ¨ÊàêÊï∞Â≠ó‰ø°Âè∑ÂêéÊñπ‰æøÂ≠òÂÇ®Âíå‰º†ËæìÔºå‰ΩÜÂà©Áî®Ëøô‰∫õÊï∞Â≠ó‰ø°Âè∑Êó∂ÈÄöÂ∏∏ÈúÄË¶ÅÂÜçÂ∞ÜÂÆÉ‰ª¨ËΩ¨
Êç¢‰∏∫Ê®°Êãü‰ø°Âè∑ÔºàÈÄöËøáÊï∞Ê®°ËΩ¨Êç¢Âô®ÔºåDACÔºâÔºåÂõ†‰∏∫‰∫∫Á±ª‰∏çÂÉèËÆ°ÁÆóÊú∫Ôºå‰∏çËÉΩÁõ¥Êé•Â§ÑÁêÜ0/1‰ø°Âè∑ÔºåÂè™ËÉΩÊé•Êî∂Ê®°
Êãü‰ø°Âè∑„ÄÇÔºà‰∫∫ÁöÑËÄ≥ÊúµËß£ÊûêÁöÑÊòØÊ®°Êãü‰ø°Âè∑ÔºåÊÅêÊÄïËøòÊ≤°‰∫∫ÁªÉÂ∞±Âê¨‰∏Ä‰∏≤01ÁªÑÊàêÁöÑÊï∞Â≠óÂ∞±ËÉΩÂê¨Âá∫ÈÇ£ÊòØË¥ùÂ§öËä¨ÁöÑ
Á¨¨‰πù‰∫§ÂìçÊõ≤ÁöÑÊú¨È¢Ü„ÄÇÔºâ&lt;/p>
&lt;h2 id="Ê∑∑Âè†Áé∞Ë±°aliasing" >
&lt;div>
&lt;a href="#%e6%b7%b7%e5%8f%a0%e7%8e%b0%e8%b1%a1aliasing">
#
&lt;/a>
Ê∑∑Âè†Áé∞Ë±°ÔºàAliasingÔºâ
&lt;/div>
&lt;/h2>
&lt;p>Ê∑∑Âè†ÔºàAliasÔºâÁé∞Ë±°ÁöÑ‰∫ßÁîüÊòØÁî±‰∫éÈááÊ†∑ÂÆöÁêÜÔºà‰πüÁß∞‰∏∫Â•àÂ•éÊñØÁâπÂÆöÁêÜÔºâÊú™Ë¢´Êª°Ë∂≥„ÄÇÂ•àÂ•éÊñØÁâπÂÆöÁêÜÊåáÂá∫Ôºå
‰∏∫‰∫ÜÈÅøÂÖçÊ∑∑Âè†ÔºåÈááÊ†∑È¢ëÁéáÂøÖÈ°ªËá≥Â∞ëÊòØ‰ø°Âè∑‰∏≠ÊúÄÈ´òÈ¢ë‰ø°Âè∑È¢ëÁéáÁöÑ‰∏§ÂÄç„ÄÇÂ¶ÇÊûú‰ø°Âè∑È¢ëÁéáÈ´ò‰∫éÈááÊ†∑È¢ëÁéáÁöÑ‰∏Ä
ÂçäÔºåÂ∞±‰ºöÂá∫Áé∞Ê∑∑Âè†Áé∞Ë±°„ÄÇ‰πüÂ∞±ÊòØÔºöÂÆÉÂèëÁîüÂú®‰ø°Âè∑ÁöÑÈááÊ†∑È¢ëÁéá‰∏çË∂≥‰ª•Ë°®Á§∫ÂéüÂßã‰ø°Âè∑‰∏≠ÁöÑÊâÄÊúâÈ¢ëÁéáÊàêÂàÜÊó∂:&lt;/p>
&lt;ul>
&lt;li>ÂΩì‰ø°Âè∑‰∏≠Êüê‰∫õÁªÑÂàÜÁöÑ‰ø°Âè∑È¢ëÁéáÈ´ò‰∫éÈááÊ†∑È¢ëÁéáÁöÑ‰∏ÄÂçäÔºà‰πüÁß∞‰∏∫ÔºöÊäòÂè†È¢ëÁéáÔºâÊó∂ÔºåÈ´òÈ¢ëÈÉ®ÂàÜ‰ºöË¢´ÈîôËØØÁöÑ
Ëß£ÊûêÊàê‰ΩéÈ¢ë‰ø°Âè∑ÔºåËøôÂ∞±ÂØºËá¥È¢ëË∞±‰∏≠Âá∫Áé∞ËôöÂÅáÁöÑÈ¢ëÁéáÊàêÂàÜÔºàÊú¨ÊòØÈ´òÈ¢ëÁöÑÔºåË¢´ÊäòÂè†‰∏∫‰ΩéÈ¢ëÁöÑÔºâ„ÄÇ&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>Áî±‰∫éÊäòÂè†È¢ëÁéáÁöÑÂ≠òÂú®Ôºå‰ø°Âè∑È¢ëË∞±‰∏≠ÈÉ®ÂàÜÈ¢ëÁéáË¢´ÈáçÂè†ÔºåÂØºËá¥‰ø°ÊÅØ‰∏¢Â§±ÂíåÂ§±Áúü„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="ÈááÊ†∑ÈááÊ†∑Ë∂äÈááË∂äÂÉè" >
&lt;div>
&lt;a href="#%e9%87%87%e6%a0%b7%e9%87%87%e6%a0%b7%e8%b6%8a%e9%87%87%e8%b6%8a%e5%83%8f">
##
&lt;/a>
ÈááÊ†∑ÈááÊ†∑ÔºåË∂äÈááË∂äÂÉè
&lt;/div>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>framerate &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10000&lt;/span> &lt;span style="color:#78787e"># ÈááÊ†∑È¢ëÁéá&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ÊûÑÂª∫ÂéüÂßã‰ø°Âè∑ÔºåÈ¢ëÁéáÔºö4500&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>signal &lt;span style="color:#ff6ac1">=&lt;/span> thinkdsp&lt;span style="color:#ff6ac1">.&lt;/span>CosSignal(&lt;span style="color:#ff9f43">4500&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>duration &lt;span style="color:#ff6ac1">=&lt;/span> signal&lt;span style="color:#ff6ac1">.&lt;/span>period&lt;span style="color:#ff6ac1">*&lt;/span>&lt;span style="color:#ff9f43">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ÂØπÂéüÂßã‰ø°Âè∑ËøõË°åÈááÊ†∑&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>segment &lt;span style="color:#ff6ac1">=&lt;/span> signal&lt;span style="color:#ff6ac1">.&lt;/span>make_wave(duration, framerate&lt;span style="color:#ff6ac1">=&lt;/span>framerate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>segment&lt;span style="color:#ff6ac1">.&lt;/span>plot(label&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;freq=4500&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ÊûÑÂª∫ÂéüÂßã‰ø°Âè∑ÔºåÈ¢ëÁéáÔºö5500&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>signal &lt;span style="color:#ff6ac1">=&lt;/span> thinkdsp&lt;span style="color:#ff6ac1">.&lt;/span>CosSignal(&lt;span style="color:#ff9f43">5500&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ÂØπÂéüÂßã‰ø°Âè∑ËøõË°åÈááÊ†∑&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>segment &lt;span style="color:#ff6ac1">=&lt;/span> signal&lt;span style="color:#ff6ac1">.&lt;/span>make_wave(duration, framerate&lt;span style="color:#ff6ac1">=&lt;/span>framerate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>segment&lt;span style="color:#ff6ac1">.&lt;/span>plot(linestyle&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;-.&amp;#34;&lt;/span>, label&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;freq=5500&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#ff6ac1">.&lt;/span>legend()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img alt="IMG_sampling" src="https://fgg100y.github.io/posts/dsp101/sampling_aliasing/images/signal_after_sampling.png">&lt;/p>
&lt;h3 id="Ê∑∑Âè†ÁöÑÈ¢ëÁéá" >
&lt;div>
&lt;a href="#%e6%b7%b7%e5%8f%a0%e7%9a%84%e9%a2%91%e7%8e%87">
##
&lt;/a>
Ê∑∑Âè†ÁöÑÈ¢ëÁéá
&lt;/div>
&lt;/h3>
&lt;p>‰∏ãÈù¢ÊòØ‰∏Ä‰∏™‰∏âËßíÂΩ¢‰ø°Âè∑ÔºàÈ¢ëÁéá‰∏∫1100ÔºâÁöÑÈ¢ëË∞±ÂõæÔºåÊ≥®ÊÑèÂÖ∂Ë∞êÊ≥¢ÔºàharmonicsÔºâÂ∫îËØ•ÊòØÂü∫È¢ë
Ôºàfundamental frequencyÔºâÁöÑÂ•áÊï∞ÂÄçÔºàÂ¶Ç 3300Ôºå5500Ôºå7700Ôºå9900Ôºå12100Á≠âÁ≠âÔºâÔºå‰ΩÜÊòØÔºåÂπ∂ÈùûÂÆå
ÂÖ®Â¶ÇÊ≠§ÔºåÁ¨¨‰∏â‰∏™Ë∞êÊ≥¢Âá∫Áé∞Âú®4500ÔºåËÄåÁ¨¨Âõõ‰∏™ÂàôÂú®2300ÔºåËÆ§ÁúüÁúãÂè≥ËæπÁöÑyËΩ¥ÊîæÂ§ßÂõæÔºåÁ¨¨‰∫î‰∏™Âá∫Áé∞Âú®100ÁöÑ
‰ΩçÁΩÆÔºåËÄåÁ¨¨ÂÖ≠‰∏™Âë¢ÔºàÂú®2100ÁöÑ‰ΩçÁΩÆÔºâ„ÄÇÂï•ÊÉÖÂÜµÔºüÔºÅ&lt;/p>
&lt;p>ËøôÂ∞±ÊòØÊ∑∑Âè†Áé∞Ë±°„ÄÇÈááÊ†∑È¢ëÁéá‰∏∫10kÔºåÊäòÂè†È¢ëÁéá=$\frac{1000}{2}=5000$ÔºåÂΩì‰ø°Âè∑‰∏≠È¢ëÁéáÈ´ò‰∫é5000ÁöÑÔºå
‰ºöË¢´‚ÄúÊäòÂè†‚ÄùÔºàÊäòÂõûÊù•ÔºåÂõûÂà∞Â∞è‰∫éÊäòÂè†È¢ëÁéáÁöÑËåÉÂõ¥ÂÜÖÔºâ„ÄÇÊâÄ‰ª•ÔºåÈ¢ëÁéá‰∏∫5500ÁöÑË∞êÊ≥¢Ë¢´ÊäòÂè†Âà∞4500ÁöÑ‰ΩçÁΩÆÔºå
ÂêåÁêÜÔºåÈ¢ëÁéá‰∏∫7700ÁöÑË∞êÊ≥¢Ë¢´ÊäòÂè†Âà∞2300ÁöÑ‰ΩçÁΩÆÔºå‰ª•Ê≠§Á±ªÊé®ÔºàÈïúÂÉè‰∏≠ÂøÉÂ∞±ÊòØ5000ÁöÑ‰ΩçÁΩÆÔºâ„ÄÇ&lt;/p>
&lt;p>Â∞±ËøôÊ†∑ÔºåÂéüÊú¨ÁöÑ‰∏âËßíÊ≥¢ÂΩ¢Ê®°Êãü‰ø°Âè∑ÁªèËøáËøô‰∏™ÈááÊ†∑ÂæóÂà∞ÁöÑÊï∞Â≠ó‰ø°Âè∑ÔºåÂÜçÈáçÊñ∞ËΩ¨Êç¢‰∏∫Ê®°Êãü‰ø°Âè∑Êó∂ÔºåÂ∑≤Â§±Áúü„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>fig, axs &lt;span style="color:#ff6ac1">=&lt;/span> plt&lt;span style="color:#ff6ac1">.&lt;/span>subplots(&lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">2&lt;/span>, figsize&lt;span style="color:#ff6ac1">=&lt;/span>(&lt;span style="color:#ff9f43">15&lt;/span>, &lt;span style="color:#ff9f43">8&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>signal &lt;span style="color:#ff6ac1">=&lt;/span> TriangleSignal(&lt;span style="color:#ff9f43">1100&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>segment &lt;span style="color:#ff6ac1">=&lt;/span> signal&lt;span style="color:#ff6ac1">.&lt;/span>make_wave(duration&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.5&lt;/span>, framerate&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">10000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spectrum &lt;span style="color:#ff6ac1">=&lt;/span> segment&lt;span style="color:#ff6ac1">.&lt;/span>make_spectrum()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>axs[&lt;span style="color:#ff9f43">0&lt;/span>]&lt;span style="color:#ff6ac1">.&lt;/span>plot(spectrum&lt;span style="color:#ff6ac1">.&lt;/span>fs, spectrum&lt;span style="color:#ff6ac1">.&lt;/span>hs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>axs[&lt;span style="color:#ff9f43">0&lt;/span>]&lt;span style="color:#ff6ac1">.&lt;/span>set_ylim(&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">2200&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>axs[&lt;span style="color:#ff9f43">0&lt;/span>]&lt;span style="color:#ff6ac1">.&lt;/span>set_xlabel(&lt;span style="color:#5af78e">&amp;#39;Frequency (Hz)&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>axs[&lt;span style="color:#ff9f43">0&lt;/span>]&lt;span style="color:#ff6ac1">.&lt;/span>set_title(&lt;span style="color:#5af78e">&amp;#39;Original y-axis&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>signal &lt;span style="color:#ff6ac1">=&lt;/span> TriangleSignal(&lt;span style="color:#ff9f43">1100&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>segment &lt;span style="color:#ff6ac1">=&lt;/span> signal&lt;span style="color:#ff6ac1">.&lt;/span>make_wave(duration&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.5&lt;/span>, framerate&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">10000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spectrum &lt;span style="color:#ff6ac1">=&lt;/span> segment&lt;span style="color:#ff6ac1">.&lt;/span>make_spectrum()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>axs[&lt;span style="color:#ff9f43">1&lt;/span>]&lt;span style="color:#ff6ac1">.&lt;/span>plot(spectrum&lt;span style="color:#ff6ac1">.&lt;/span>fs, spectrum&lt;span style="color:#ff6ac1">.&lt;/span>hs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>axs[&lt;span style="color:#ff9f43">1&lt;/span>]&lt;span style="color:#ff6ac1">.&lt;/span>set_ylim(&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">500&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>decorate(xlabel&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#39;Frequency (Hz)&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>decorate(title&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#39;Scaled the y-axis (to show the harmonics)&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#ff6ac1">.&lt;/span>suptitle(&lt;span style="color:#5af78e">&amp;#39;Sampling and Aliasing&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>plt&lt;span style="color:#ff6ac1">.&lt;/span>tight_layout()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img alt="IMG_aliasing" src="https://fgg100y.github.io/posts/dsp101/sampling_aliasing/images/signal_sampling_and_aliasing.png">&lt;/p>
&lt;h2 id="È¢ëË∞±ÊòØÊÄé‰πàËÆ°ÁÆóÂæóÂà∞ÁöÑ" >
&lt;div>
&lt;a href="#%e9%a2%91%e8%b0%b1%e6%98%af%e6%80%8e%e4%b9%88%e8%ae%a1%e7%ae%97%e5%be%97%e5%88%b0%e7%9a%84">
#
&lt;/a>
È¢ëË∞±ÊòØÊÄé‰πàËÆ°ÁÆóÂæóÂà∞ÁöÑ
&lt;/div>
&lt;/h2>
&lt;p>È¢ëË∞±ÂõæÂÆûÈôÖ‰∏äÂ∞±ÊòØ‰ª•È¢ëÁéáÔºàfrequencyÔºâ‰∏∫Ê®™ËΩ¥ÔºåÂπÖÂ∫¶ÔºàamplitudeÔºâ‰∏∫Á∫µËΩ¥ÁöÑÂõæ„ÄÇ&lt;/p>
&lt;p>ÈÇ£‰πàÔºå‰ø°Âè∑‰∏≠ÁöÑÈ¢ëÁéáÂèäÂÖ∂ÂπÖÂ∫¶ÂèàÊòØÊÄé‰πàÁÆóÂá∫Êù•ÁöÑÔºüÂø´ÈÄüÂÇÖÈáåÂè∂ËΩ¨Êç¢ÔºàFFTÔºâ„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">plot&lt;/span>(self, high&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, &lt;span style="color:#ff6ac1">**&lt;/span>options):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Plots amplitude vs frequency.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> Note: if this is a full spectrum, it ignores low and high
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> high: frequency to cut off at
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>full:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fs, amps &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>render_full(high)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#ff6ac1">.&lt;/span>plot(fs, amps, &lt;span style="color:#ff6ac1">**&lt;/span>options)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> i &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">None&lt;/span> &lt;span style="color:#ff6ac1">if&lt;/span> high &lt;span style="color:#ff6ac1">is&lt;/span> &lt;span style="color:#ff6ac1">None&lt;/span> &lt;span style="color:#ff6ac1">else&lt;/span> find_index(high, self&lt;span style="color:#ff6ac1">.&lt;/span>fs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#ff6ac1">.&lt;/span>plot(self&lt;span style="color:#ff6ac1">.&lt;/span>fs[:i], self&lt;span style="color:#ff6ac1">.&lt;/span>amps[:i], &lt;span style="color:#ff6ac1">**&lt;/span>options)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">render_full&lt;/span>(self, high&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Extracts amps and fs from a full spectrum.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> high: cutoff frequency
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> returns: fs, amps
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hs &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>fft&lt;span style="color:#ff6ac1">.&lt;/span>fftshift(self&lt;span style="color:#ff6ac1">.&lt;/span>hs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> amps &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>abs(hs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fs &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>fft&lt;span style="color:#ff6ac1">.&lt;/span>fftshift(self&lt;span style="color:#ff6ac1">.&lt;/span>fs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> i &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0&lt;/span> &lt;span style="color:#ff6ac1">if&lt;/span> high &lt;span style="color:#ff6ac1">is&lt;/span> &lt;span style="color:#ff6ac1">None&lt;/span> &lt;span style="color:#ff6ac1">else&lt;/span> find_index(&lt;span style="color:#ff6ac1">-&lt;/span>high, fs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> j &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">None&lt;/span> &lt;span style="color:#ff6ac1">if&lt;/span> high &lt;span style="color:#ff6ac1">is&lt;/span> &lt;span style="color:#ff6ac1">None&lt;/span> &lt;span style="color:#ff6ac1">else&lt;/span> find_index(high, fs) &lt;span style="color:#ff6ac1">+&lt;/span> &lt;span style="color:#ff9f43">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> fs[i:j], amps[i:j]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="dft--fft" >
&lt;div>
&lt;a href="#dft--fft">
##
&lt;/a>
DFT &amp;amp; FFT
&lt;/div>
&lt;/h3>
&lt;p>Âø´ÈÄüÂõûÈ°æÔºö
Á¶ªÊï£ÂÇÖÈáåÂè∂ËΩ¨Êç¢ÔºàDFTÔºâÊòØ‰∏ÄÁßçÊï∞Â≠¶ÊÄùÊÉ≥ÔºàÊâÄÊúâÊ≥¢ÂèØ‰ª•Áî±Ëã•Âπ≤Ê≠£‰ΩôÂº¶Ê≥¢Âè†Âä†ËÄåÊàêÔºâÔºå
ËÄåÂø´ÈÄüÂÇÖÈáåÂè∂ËΩ¨Êç¢ÔºàFFTÔºâÊòØ‰∏ÄÁßçÈ´òÊïàÁÆóÊ≥ïÔºåÁî®‰∫éÂÆûÁé∞Á¶ªÊï£ÂÇÖÈáåÂè∂ËΩ¨Êç¢„ÄÇ&lt;/p>
&lt;p>Âø´ÈÄüÂõûÈ°æ2Ôºö
ÂÇÖÈáåÂè∂Á∫ßÊï∞ÊòØ‰∏ÄÁßçÂ∞ÜÂë®ÊúüÂáΩÊï∞Ôºàperiodic functionÔºâÂàÜËß£‰∏∫‰∏ÄÁ≥ªÂàóÊ≠£Âº¶Âíå‰ΩôÂº¶ÂáΩÊï∞ÁöÑÊó†Á©∑ÂíåÁöÑÊñπÊ≥ï„ÄÇ
ÂÅáËÆæÊàë‰ª¨Êúâ‰∏Ä‰∏™Âë®Êúü‰∏∫ $ T $ ÁöÑÂáΩÊï∞ $ f(t) $ÔºåÂÆÉÂèØ‰ª•Ë°®Á§∫‰∏∫Â¶Ç‰∏ãÁöÑÂÇÖÈáåÂè∂Á∫ßÊï∞Ôºö&lt;/p>
&lt;p>$$
f(t) = \frac{a_0}{2} + \sum_{n=1}^{\infty} \left( a_n \cos\left(\frac{2\pi nt}{T}\right) + b_n \sin\left(\frac{2\pi nt}{T}\right) \right)
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠ $ a_0, a_n, b_n $ ÊòØÁ≥ªÊï∞ÔºåÂÆÉ‰ª¨ÂèØ‰ª•ÈÄöËøáÂáΩÊï∞ $ f(t) $ ÁöÑÂë®ÊúüÊÄßË¥®ËÆ°ÁÆóÂæóÂà∞„ÄÇ&lt;/p>
&lt;p>Á≥ªÊï∞ÁöÑËÆ°ÁÆóÂÖ¨Âºè‰∏∫Ôºö&lt;/p>
&lt;p>$$
\begin{align*}% align* ÁöÑ * Âè∑Áî®‰∫éÈòªÊ≠¢Ëá™Âä®ÁºñÂè∑ÁîüÊàê; hugo‰∏≠Ë¶ÅÁî® \\ Êç¢Ë°åÔºåÁ¨¨‰∏Ä‰∏™\‰∏∫ËΩ¨‰πâÂ≠óÁ¨¶
a_0 &amp;amp;= \frac{1}{T} \int_{0}^{T} f(t) dt \\
a_n &amp;amp;= \frac{2}{T} \int_{0}^{T} f(t) \cos\left(\frac{2\pi nt}{T}\right) dt \\
b_n &amp;amp;= \frac{2}{T} \int_{0}^{T} f(t) \sin\left(\frac{2\pi nt}{T}\right) dt
\end{align*}
$$&lt;/p>
&lt;p>Ëøô‰∫õÁ≥ªÊï∞ÊèèËø∞‰∫ÜÊ≠£Âº¶Âíå‰ΩôÂº¶ÂáΩÊï∞ÁöÑÊåØÂπÖÂíåÁõ∏‰ΩçÔºåÂÆÉ‰ª¨ÂÜ≥ÂÆö‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÂéüÂßãÂáΩÊï∞ $ f(t) $ ÂàÜËß£ÊàêÈ¢ëÁéá
‰∏∫ $ \frac{n}{T} $ ÁöÑÊ≠£Âº¶Âíå‰ΩôÂº¶ÂáΩÊï∞ÁöÑÂíå„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> np.fft &lt;span style="color:#ff6ac1">import&lt;/span> rfft, rfftfreq
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">class&lt;/span> &lt;span style="color:#f3f99d">Wave&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># lots of other attrs, methods&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">make_spectrum&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> self: Wave object
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># n : number of samples in the wave&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># d : inverse of framerate, means the time between samples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># hs : a NumPy array of complex numbers that represents the amplitude and&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># phase offset of each frequency component in the wave&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># fs : an array that contains frequencies corresponding to the `hs`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(self&lt;span style="color:#ff6ac1">.&lt;/span>hs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> d &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">/&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>framerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hs &lt;span style="color:#ff6ac1">=&lt;/span> rfft(self&lt;span style="color:#ff6ac1">.&lt;/span>ys)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fs &lt;span style="color:#ff6ac1">=&lt;/span> rfftfreq(n, d)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> Spectrum(hs, fs, self&lt;span style="color:#ff6ac1">.&lt;/span>framerate)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>spectral_decomposition</title><link>/posts/dsp101/spectral_decomposition/</link><pubDate>Fri, 03 May 2024 10:26:17 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/dsp101/spectral_decomposition/</guid><description>&lt;hr>
&lt;p>NOTE: For more content see &lt;a href="https://github.com/AllenDowney/ThinkDSP">ThinkDSP&lt;/a>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>DFT is a mathematical idea, and FFT is an algorithm for computing DFT.&lt;/p>
&lt;/blockquote>
&lt;p>Spectral decomposition: The idea that any signal can be expressed as the sum of
sinusoids with different frequencies.&lt;/p>
&lt;h2 id="what-is-a-signal" >
&lt;div>
&lt;a href="#what-is-a-signal">
#
&lt;/a>
What is a signal?
&lt;/div>
&lt;/h2>
&lt;p>A &lt;strong>signal&lt;/strong> represents a quantity that varies in time. (pretty abstract,eh?)
A concrete example: sound. A sound signal represents variations in air pressure over
time.&lt;/p>
&lt;p>Periodic signals: The signals that repeat themselves after some period ot time.&lt;/p>
&lt;p>A &lt;strong>sinusoid&lt;/strong> which means it has the same shape as the trigonmetric sine function.
And a sinusoid contains only one frequency component, so its spectrum has only one
peak.&lt;/p>
&lt;p>&lt;strong>cycles&lt;/strong>: the full repetitions in a periodic signal.
&lt;strong>period&lt;/strong>: the duration of each cycle.
&lt;strong>frequency&lt;/strong>: the number of cycles per second, which is the inverse of period.
- the units of frequency are cycles per second, or Hertz (Hz).
- strictly speaking, the number of cycles is a dimensionless number, so a Hertz
is really a &amp;ldquo;per second&amp;rdquo;.&lt;/p>
&lt;p>A microphone is a device that measures these variations and generates an electrical
signal that represents sound.
A speaker is a device that takes an electrical signal and produce sound.
They are called &lt;strong>transducers&lt;/strong> because they transduce, or convert, signals from one
form to another.&lt;/p>
&lt;h2 id="discrete-fourier-transform-dft" >
&lt;div>
&lt;a href="#discrete-fourier-transform-dft">
#
&lt;/a>
Discrete Fourier transform (DFT)
&lt;/div>
&lt;/h2>
&lt;p>DFT is a mathematical idea which takes a signal and produces its spectrum.
The spectrum is the set of sinusoids that add up to produce the signal.&lt;/p>
&lt;p>And the FFT (Fast Fourier transform) is an algorithm which is an efficient way to
compute the DFT.&lt;/p>
&lt;p>&lt;img alt="IMG_signal" src="https://fgg100y.github.io/posts/dsp101/spectral_decomposition/images/a_wave_segment.png">&lt;/p>
&lt;p>&lt;img alt="IMG_spectrum_of_signal" src="https://fgg100y.github.io/posts/dsp101/spectral_decomposition/images/a_wave_segment_its_spectrum.png">&lt;/p>
&lt;p>The lowest frequency component is called the &lt;strong>fundamental frequency&lt;/strong>. If the
fundamental frequency has the largest amplitude, it is also the &lt;strong>dominant frequency&lt;/strong>.
Normally the perceived pitch of a sound is determinded by the fundamental frequency,
even if it is not dominant.&lt;/p>
&lt;p>The &lt;strong>fundamental frequency&lt;/strong> of this signal is near 440 Hz, the other spikes in the
spectrum are at frequencies 880, 1320, 1760, and 2200, which are integer multiples
of the fundamental. These components are called &lt;strong>harmonics&lt;/strong> because they are
musically harmonious with the fundamental:&lt;/p>
&lt;ul>
&lt;li>440 is the frequency of A4, the fundamental frequency.&lt;/li>
&lt;li>880 is the frequency of A5, one &lt;strong>octave&lt;/strong> higher than the fundamental. An octave
is a doubling in frequency.&lt;/li>
&lt;li>1320 is approximately E6, which is a &lt;strong>perfect fifth&lt;/strong> above A5.&lt;/li>
&lt;li>1760 is A6, two octave above the fundamental.&lt;/li>
&lt;li>2200 is approximately C#7, which is a major third above A6.&lt;/li>
&lt;/ul>
&lt;p>NOTE: Given the harmonics and their amplitudes, you can reconstruct the signal by
adding up sinusoids.&lt;/p>
&lt;p>Some code about &lt;code>signal/wave&lt;/code> :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># More codes here are ignored ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">class&lt;/span> &lt;span style="color:#f3f99d">Signal&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Represents a time-varying signal.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> __add__(self, other):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Adds two signals.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> other: Signal
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> returns: Signal
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> other &lt;span style="color:#ff6ac1">==&lt;/span> &lt;span style="color:#ff9f43">0&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> self
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> SumSignal(self, other)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __radd__ &lt;span style="color:#ff6ac1">=&lt;/span> __add__
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff9f43">@property&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">period&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Period of the signal in seconds (property).
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> Since this is used primarily for purposes of plotting,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> the default behavior is to return a value, 0.1 seconds,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> that is reasonable for many signals.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> returns: float seconds
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> &lt;span style="color:#ff9f43">0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">plot&lt;/span>(self, framerate&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">11025&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Plots the signal.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> The default behavior is to plot three periods.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> framerate: samples per second
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> duration &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>period &lt;span style="color:#ff6ac1">*&lt;/span> &lt;span style="color:#ff9f43">3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> wave &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>make_wave(duration, start&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0&lt;/span>, framerate&lt;span style="color:#ff6ac1">=&lt;/span>framerate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> wave&lt;span style="color:#ff6ac1">.&lt;/span>plot()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">make_wave&lt;/span>(self, duration&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, start&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0&lt;/span>, framerate&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">11025&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Makes a Wave object.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> duration: float seconds
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> start: float seconds
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> framerate: int frames per second
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> returns: Wave
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">round&lt;/span>(duration &lt;span style="color:#ff6ac1">*&lt;/span> framerate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ts &lt;span style="color:#ff6ac1">=&lt;/span> start &lt;span style="color:#ff6ac1">+&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(n) &lt;span style="color:#ff6ac1">/&lt;/span> framerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ys &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>evaluate(ts)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> Wave(ys, ts, framerate&lt;span style="color:#ff6ac1">=&lt;/span>framerate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">class&lt;/span> &lt;span style="color:#f3f99d">Sinusoid&lt;/span>(Signal)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># More codes here are ignored ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">evaluate&lt;/span>(self, ts):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># (self.freq * ts) is the number of cycles since start time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Multiplying 2*np.pi converts from cycles to phases&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># and phases can be view as &amp;#34;cycles since start time expressed in radians&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># the func is cos or sin, returns value between -1 and 1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> phases &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">2&lt;/span>&lt;span style="color:#ff6ac1">*&lt;/span>np&lt;span style="color:#ff6ac1">.&lt;/span>pi &lt;span style="color:#ff6ac1">*&lt;/span> (self&lt;span style="color:#ff6ac1">.&lt;/span>freq &lt;span style="color:#ff6ac1">*&lt;/span> ts) &lt;span style="color:#ff6ac1">+&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>offset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ys &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>amp &lt;span style="color:#ff6ac1">*&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>func(phases)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> ys
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">class&lt;/span> &lt;span style="color:#f3f99d">Wave&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Represents a discrete-time waveform.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> __init__(self, ys, ts&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, framerate&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Initializes the wave.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> ys: wave array
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> ts: array of times
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> framerate: samples per second
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff6ac1">.&lt;/span>ys &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>asanyarray(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff6ac1">.&lt;/span>framerate &lt;span style="color:#ff6ac1">=&lt;/span> framerate &lt;span style="color:#ff6ac1">if&lt;/span> framerate &lt;span style="color:#ff6ac1">is&lt;/span> &lt;span style="color:#ff6ac1">not&lt;/span> &lt;span style="color:#ff6ac1">None&lt;/span> &lt;span style="color:#ff6ac1">else&lt;/span> &lt;span style="color:#ff9f43">11025&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> ts &lt;span style="color:#ff6ac1">is&lt;/span> &lt;span style="color:#ff6ac1">None&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff6ac1">.&lt;/span>ts &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(&lt;span style="color:#ff5c57">len&lt;/span>(ys)) &lt;span style="color:#ff6ac1">/&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>framerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff6ac1">.&lt;/span>ts &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>asanyarray(ts)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">copy&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Makes a copy.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> Returns: new Wave
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> copy&lt;span style="color:#ff6ac1">.&lt;/span>deepcopy(self)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> __len__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(self&lt;span style="color:#ff6ac1">.&lt;/span>ys)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff9f43">@property&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">start&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>ts[&lt;span style="color:#ff9f43">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff9f43">@property&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">end&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>ts[&lt;span style="color:#ff6ac1">-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff9f43">@property&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">duration&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Duration (property).
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> returns: float duration in seconds
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(self&lt;span style="color:#ff6ac1">.&lt;/span>ys) &lt;span style="color:#ff6ac1">/&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>framerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> __add__(self, other):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;Adds two waves elementwise.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> other: Wave
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> returns: new Wave
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> other &lt;span style="color:#ff6ac1">==&lt;/span> &lt;span style="color:#ff9f43">0&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> self
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">assert&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>framerate &lt;span style="color:#ff6ac1">==&lt;/span> other&lt;span style="color:#ff6ac1">.&lt;/span>framerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># make an array of times that covers both waves&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> start &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">min&lt;/span>(self&lt;span style="color:#ff6ac1">.&lt;/span>start, other&lt;span style="color:#ff6ac1">.&lt;/span>start)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">max&lt;/span>(self&lt;span style="color:#ff6ac1">.&lt;/span>end, other&lt;span style="color:#ff6ac1">.&lt;/span>end)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">int&lt;/span>(&lt;span style="color:#ff5c57">round&lt;/span>((end &lt;span style="color:#ff6ac1">-&lt;/span> start) &lt;span style="color:#ff6ac1">*&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>framerate)) &lt;span style="color:#ff6ac1">+&lt;/span> &lt;span style="color:#ff9f43">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ys &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>zeros(n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ts &lt;span style="color:#ff6ac1">=&lt;/span> start &lt;span style="color:#ff6ac1">+&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(n) &lt;span style="color:#ff6ac1">/&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>framerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">add_ys&lt;/span>(wave):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> i &lt;span style="color:#ff6ac1">=&lt;/span> find_index(wave&lt;span style="color:#ff6ac1">.&lt;/span>start, ts)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># make sure the arrays line up reasonably well&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> diff &lt;span style="color:#ff6ac1">=&lt;/span> ts[i] &lt;span style="color:#ff6ac1">-&lt;/span> wave&lt;span style="color:#ff6ac1">.&lt;/span>start
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dt &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">/&lt;/span> wave&lt;span style="color:#ff6ac1">.&lt;/span>framerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> (diff &lt;span style="color:#ff6ac1">/&lt;/span> dt) &lt;span style="color:#ff6ac1">&amp;gt;&lt;/span> &lt;span style="color:#ff9f43">0.1&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> warnings&lt;span style="color:#ff6ac1">.&lt;/span>warn(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;Can&amp;#39;t add these waveforms; their &amp;#34;&lt;/span> &lt;span style="color:#5af78e">&amp;#34;time arrays don&amp;#39;t line up.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> j &lt;span style="color:#ff6ac1">=&lt;/span> i &lt;span style="color:#ff6ac1">+&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(wave)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ys[i:j] &lt;span style="color:#ff6ac1">+=&lt;/span> wave&lt;span style="color:#ff6ac1">.&lt;/span>ys
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> add_ys(self)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> add_ys(other)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> Wave(ys, ts, self&lt;span style="color:#ff6ac1">.&lt;/span>framerate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __radd__ &lt;span style="color:#ff6ac1">=&lt;/span> __add__
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Write a function called stretch that takes a Wave and a stretch factor and speeds up&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># or slows down the wave by modifying ts and framerate.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">stretch&lt;/span>(wave, factor):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#5af78e">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> if 0 &amp;lt; factor &amp;lt; 1, speed up, else slow down
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> wave&lt;span style="color:#ff6ac1">.&lt;/span>ts &lt;span style="color:#ff6ac1">*=&lt;/span> factor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> wave&lt;span style="color:#ff6ac1">.&lt;/span>framerate &lt;span style="color:#ff6ac1">/=&lt;/span> factor
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâ</title><link>/posts/neuralnetworks/variationalautoencoder/</link><pubDate>Wed, 01 May 2024 21:01:57 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/neuralnetworks/variationalautoencoder/</guid><description>&lt;h1 id="ÂèòÂàÜËá™ÁºñÁ†ÅÂô®veriational-autoencoder-vae" >
&lt;div>
&lt;a href="#%e5%8f%98%e5%88%86%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8veriational-autoencoder-vae">
##
&lt;/a>
ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºöVeriational AutoEncoder (VAE)
&lt;/div>
&lt;/h1>
&lt;h2 id="Ëá™ÁºñÁ†ÅÂô®" >
&lt;div>
&lt;a href="#%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8">
#
&lt;/a>
Ëá™ÁºñÁ†ÅÂô®
&lt;/div>
&lt;/h2>
&lt;p>Ëá™ÁºñÁ†ÅÂô®ÈÄöÂ∏∏Áî®‰∫éÊï∞ÊçÆÂéãÁº©Ôºå‰πüÂ∞±ÊòØÂ∞ÜÈ´òÁª¥Êï∞ÊçÆÊò†Â∞ÑÂà∞‰ΩéÁª¥Á©∫Èó¥ÔºåÁÑ∂ÂêéÈÄöËøáËß£Á†ÅÂô®ÂØπÂéãÁº©ÂêéÁöÑÊï∞ÊçÆËøõ
Ë°åÈáçÊûÑÔºàÂ∞ùËØïÊÅ¢Â§çÂéüÊï∞ÊçÆÔºâ„ÄÇ‰æãÂ¶ÇÔºöÂú®ËÆ≠ÁªÉÈò∂ÊÆµÔºåÂ∞Ü214x214ÁöÑÂõæÁâáÈÄöËøáËß£Á†ÅÂô®ÂéãÁº©‰∏∫100Áª¥ÁöÑÂêëÈáèÔºå
ÁÑ∂ÂêéÁî®Ëß£Á†ÅÂô®ÂØπËøô‰∏™100Áª¥ÂêëÈáèËøõË°åÈáçÊûÑÔºåÈáçÊûÑÁöÑÁõÆÊ†áÊòØÁîüÊàêÁöÑÂõæÁâá‰∏éÂéüÂõæÁâáË∂äÊé•ËøëË∂äÂ•Ω„ÄÇÁªèËøáÂ§ßÈáè
Êï∞ÊçÆËÆ≠ÁªÉÂêéÔºåÊ®°ÂûãÂ∞ÜÂ≠¶‰ºöÂØπÊï∞ÊçÆËøõË°åÂéãÁº©„ÄÇ&lt;/p>
&lt;p>ÂõæÂÉèÂéªÂô™(de-noicing)ÔºöÂ¶ÇÊûúÂú®ËÆ≠ÁªÉËøáÁ®ãÔºåÁªôËæìÂÖ•Êï∞ÊçÆÂä†ÂÖ•Âô™Èü≥‰ø°Âè∑ÔºåÈáçÊûÑÁõÆÊ†áÊòØÂéüÂõæÁâáÔºåÂàôÊ®°ÂûãÂêå
Êó∂Â≠¶‰ºöÈôçÂô™Ôºõ&lt;/p>
&lt;p>ÂõæÂÉèÂàÜÂâ≤(segmentation)ÔºöÂ¶ÇÊûúÂú®ËÆ≠ÁªÉËøáÁ®ãÔºåËæìÂÖ•Êï∞ÊçÆ‰∏çÂèòÔºåËÄåÈáçÊûÑÁõÆÊ†áÂèòÊàêÂõæÂÉèÂå∫ÂùóÔºåÂàôÊ®°ÂûãÂ≠¶‰ºö
ÂàÜÂâ≤Ôºõ&lt;/p>
&lt;p>Á•ûÁªèÂ°´ÂÖÖ(neural inpainting)ÔºöÂ¶ÇÊûúÂú®ËÆ≠ÁªÉËøáÁ®ãÔºåÁõ¥Êé•ÂØπÂõæÁâáÈÉ®ÂàÜÂÜÖÂÆπÊâìÁ†ÅÔºåËÄåÈáçÊûÑÁõÆÊ†áÊòØÂéüÂõæÁâáÔºå
ÂàôÊ®°ÂûãÂ≠¶‰ºöËøòÂéüË¢´ÊâìÁ†ÅÈÉ®ÂàÜ(ÊúÄËøëÈóπÂæóÊ≤∏Ê≤∏Êâ¨Êâ¨ÁöÑ‚Äú‰∏ÄÈîÆÊ∂àË°£ÔºåÊó†‰∏≠ÁîüËÉ∏‚ÄùÂ§ßÁ∫¶ÊòØÊ≠§Á±ªÊ®°ÂûãÊäÄÊúØ)Ôºõ&lt;/p>
&lt;p>&lt;img alt="IMG_AE" src="https://fgg100y.github.io/posts/neuralnetworks/variationalautoencoder/images/AutoEncoder.png">&lt;/p>
&lt;h2 id="ÂèòÂàÜËá™ÁºñÁ†ÅÂô®" >
&lt;div>
&lt;a href="#%e5%8f%98%e5%88%86%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8">
#
&lt;/a>
ÂèòÂàÜËá™ÁºñÁ†ÅÂô®
&lt;/div>
&lt;/h2>
&lt;p>ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÊòØ‰∏ÄÁßçÁîüÊàêÊ®°ÂûãÔºåÁªìÂêà‰∫ÜËá™ÁºñÁ†ÅÂô®ÂíåÊ¶ÇÁéáÂõæÊ®°ÂûãÁöÑÊÄùÊÉ≥„ÄÇ&lt;/p>
&lt;p>ÂèòÂàÜËá™ÁºñÁ†ÅÂô®‰∏≠ÁöÑÊ®°ÂûãÊû∂ÊûÑ‰∏éËá™ÁºñÁ†ÅÂô®‰∏ÄÊ†∑ÔºåÂå∫Âà´‰∏ªË¶ÅÂú®‰∫éÁîüÊàêÁöÑ‰ΩéÁª¥Ë°®Á§∫ÁöÑÊñπÂºèÔºåËá™ÁºñÁ†ÅÂô®
ÊòØÁîüÊàêÂõ∫ÂÆöÈïøÂ∫¶ÁöÑÂêëÈáèÔºåÁÑ∂Âêé‰º†ÈÄíÁªôËß£Á†ÅÂô®Ôºõ‰ΩÜÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÂàôÁîüÊàê‰∏Ä‰∏™ÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑÊ¶ÇÁéáÂàÜÂ∏É
Ôºàlaten space distributionÔºâÔºåÁÑ∂Âêé‰ªéËøô‰∏™ÂàÜÂ∏ÉÈááÊ†∑ÂæóÂà∞ÁöÑÊï∞ÊçÆÂÜç‰º†ÈÄíÁªôËß£Á†ÅÂô®„ÄÇ&lt;/p>
&lt;p>ÂèòÂàÜËá™ÁºñÁ†ÅÂô®‰∏≠ÁöÑ‚ÄúÂèòÂàÜ‚ÄùÂÖ∂ÂÆûÊòØ‚ÄúÊ¶ÇÁéáÊé®Êñ≠‚ÄùÔºàprobabilistic inferenceÔºâ‰∏≠ÁöÑ‰∏ÄÁßçÊäÄÊúØÔºåÁß∞‰∏∫‚ÄúÂèòÂàÜ
Êé®Êñ≠Ôºàvariational inferenceÔºâ„ÄÇÂú®Ê¶ÇÁéáÂõæÊ®°Âûã‰∏≠ÔºåÈÄöÂ∏∏ÈúÄË¶ÅËÆ°ÁÆóÂêéÈ™åÊ¶ÇÁéáÂàÜÂ∏ÉÔºàposterior dist.ÔºâÔºå
Âç≥Ë¥ùÂè∂ÊñØÂàÜÊûê‰∏≠ÁöÑÂêéÈ™åÊ¶ÇÁéá„ÄÇËÄåÂØπ‰∫éÂ§çÊùÇÁöÑÊ¶ÇÁéáÊ®°ÂûãÔºåËøô‰∏™ÂêéÈ™åÊ¶ÇÁéáÈöæ‰ª•Áõ¥Êé•ËÆ°ÁÆó„ÄÇ
Ëß£ÂÜ≥Ëøô‰∏™ÂêéÈ™åÊ¶ÇÁéáËÆ°ÁÆóÁöÑÊÄùË∑ØÊúâ‰∏§ÁßçÔºö‰∏ÄÊòØMCMCÊñπÊ≥ïÔºõ‰∫åÂ∞±ÊòØÂèòÂàÜÊé®Êñ≠„ÄÇ‰∏§ËÄÖÈÉΩÊòØËøë‰ººËÆ°ÁÆóÊñπÊ≥ï„ÄÇ&lt;/p>
&lt;ul>
&lt;li>
&lt;p>MCMCÔºàÈ©¨Â∞îÁßëÂ§´ÈìæËíôÁâπÂç°Ê¥õÔºâÊòØÈÄöËøáÊûÑÂª∫Êª°Ë∂≥Âπ≥Á®≥ÂàÜÂ∏É‰∏∫ÁõÆÊ†áÂàÜÂ∏ÉÁöÑÈ©¨Â∞îÁßëÂ§´ÈìæÔºåÁÑ∂ÂêéÂæÖÈ©¨Â∞îÁßëÂ§´
ÈìæÊî∂ÊïõÂà∞Âπ≥Á®≥ÂàÜÂ∏ÉÂêéÔºåÂ∞±‰ªéËøô‰∏™ÂàÜÂ∏ÉÊäΩÊ†∑„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>VIÔºàÂèòÂàÜÊé®Êñ≠ÔºâÂ∞±ÊòØÈÄöËøá‰ºòÂåñ‰∏Ä‰∏™ÂèÇÊï∞ÂåñÁöÑËøë‰ººÂàÜÂ∏ÉÊù•Ëøë‰ººÁúüÂÆûÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºåËØïÂõæÂ∞ÜÊé®Êñ≠ÈóÆÈ¢òËΩ¨Âèò
ÊàêÊúÄ‰ºòÂåñÈóÆÈ¢ò„ÄÇÊõ¥Â§öÂÖ≥‰∫éVIÁöÑÂÜÖÂÆπËØ∑ÁßªÊ≠•&lt;a href="../2024-09-01-variational_inference.md">ËøôÈáå&lt;/a>„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>VAEÁöÑÂ∑•‰ΩúÊµÅÁ®ãÂ¶Ç‰∏ãÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ÁºñÁ†ÅÂô®ÔºàEncoderÔºâÔºöÂ∞ÜËæìÂÖ•Êï∞ÊçÆÊò†Â∞ÑÂà∞ÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑÂàÜÂ∏ÉÂèÇÊï∞„ÄÇËøô‰∏™ËøáÁ®ãÂèØ‰ª•ÁêÜËß£‰∏∫Â∞ÜËæìÂÖ•Êï∞ÊçÆ
ÁºñÁ†ÅÊàêÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåËÄå‰∏çÊòØÁõ¥Êé•Êò†Â∞ÑÂà∞‰∏Ä‰∏™Á°ÆÂÆöÁöÑÁÇπ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÈááÊ†∑ÔºàSamplingÔºâÔºö‰ªéÊΩúÂú®Á©∫Èó¥ÁöÑÂàÜÂ∏É‰∏≠ÈááÊ†∑‰∏Ä‰∏™ÁÇπÔºå‰Ωú‰∏∫ÊΩúÂú®Ë°®Á§∫Ôºàlatent representationÔºâ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ëß£Á†ÅÂô®ÔºàDecoderÔºâÔºöÂ∞ÜÊΩúÂú®Ë°®Á§∫Ëß£Á†Å‰∏∫ËæìÂá∫Êï∞ÊçÆÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ‰∏éÁºñÁ†ÅÂô®Áõ∏ÂØπÂ∫îÔºåËß£Á†ÅÂô®Â∞ÜÊΩúÂú®Ë°®
Á§∫Êò†Â∞ÑÂõûÂéüÂßãÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÂèÇÊï∞„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÈáçÊûÑÊçüÂ§±ÔºàReconstruction LossÔºâÔºöË°°ÈáèÈáçÊûÑÊï∞ÊçÆ‰∏éÂéüÂßãÊï∞ÊçÆ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÈÄöÂ∏∏‰ΩøÁî®ÈáçÊûÑËØØÂ∑ÆÊàñ
ËÄÖ‰∫§ÂèâÁÜµÊù•Ë°°Èáè„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>KL Êï£Â∫¶ÊçüÂ§±ÔºàKL Divergence LossÔºâÔºöÁî®‰∫éÂ∫¶ÈáèÁºñÁ†ÅÂô®ËæìÂá∫ÁöÑÊΩúÂú®ÂàÜÂ∏É‰∏éÈ¢ÑËÆæÁöÑÂÖàÈ™åÂàÜÂ∏ÉÔºàÈÄöÂ∏∏
ÊòØÈ´òÊñØÂàÜÂ∏ÉÔºâ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºå‰øÉ‰ΩøÊ®°ÂûãÂ≠¶‰π†Âà∞ÂêàÁêÜÁöÑÊΩúÂú®Ë°®Á§∫„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊÄªÊçüÂ§±ÔºàTotal LossÔºâÔºöÈáçÊûÑÊçüÂ§±ÂíåKLÊï£Â∫¶ÊçüÂ§±ÁöÑÂä†ÊùÉÂíåÔºåÁî®‰∫éËÆ≠ÁªÉÊ®°Âûã„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img alt="IMG_AE" src="https://fgg100y.github.io/posts/neuralnetworks/variationalautoencoder/images/AE.webp">&lt;/p>
&lt;p>&lt;img alt="IMG_VAE" src="https://fgg100y.github.io/posts/neuralnetworks/variationalautoencoder/images/VAE.webp">&lt;/p>
&lt;h2 id="ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßreparametrization-trick" >
&lt;div>
&lt;a href="#%e9%87%8d%e5%8f%82%e6%95%b0%e5%8c%96%e6%8a%80%e5%b7%a7reparametrization-trick">
#
&lt;/a>
ÈáçÂèÇÊï∞ÂåñÊäÄÂ∑ßÔºàreparametrization trickÔºâ
&lt;/div>
&lt;/h2>
&lt;p>Âú®Ê†áÂáÜÁöÑVAE‰∏≠ÔºåÁºñÁ†ÅÂô®ÁΩëÁªúÈÄöÂ∏∏‰ºöËæìÂá∫ÊΩúÂú®Á©∫Èó¥‰∏≠ÁöÑÂùáÂÄºÔºàmeanÔºâÂíåÊ†áÂáÜÂ∑ÆÔºàstandard
deviationÔºâÔºåÁÑ∂ÂêéÈÄöËøá‰ªéËØ•ÂàÜÂ∏É‰∏≠ÈááÊ†∑Êù•ÁîüÊàêÊΩúÂú®Ë°®Á§∫„ÄÇÁÑ∂ËÄåÔºåÁõ¥Êé•‰ªéÂùáÂÄºÂíåÊ†áÂáÜÂ∑Æ‰∏≠ÈááÊ†∑ÊòØ‰∏çÂèØ
ÂæÆÁöÑÔºåËøôÂØºËá¥‰∫ÜÊó†Ê≥ïÁõ¥Êé•‰ΩøÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊù•ËÆ≠ÁªÉÊ®°Âûã„ÄÇ&lt;/p>
&lt;p>Reparametrization Trick ÁöÑÂÖ≥ÈîÆÊÄùÊÉ≥ÊòØÈáçÊñ∞ÂèÇÊï∞ÂåñÊΩúÂú®Ë°®Á§∫ÁöÑÈááÊ†∑ËøáÁ®ãÔºå‰ΩøÂæóÈááÊ†∑Êìç‰Ωú‰∏éÁΩëÁªúÂèÇÊï∞
‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂèòÂæóÂèØÂØº„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊΩúÂú®Ë°®Á§∫ $\mathbf{z}$ ÈÄöËøá‰∏Ä‰∏™Á°ÆÂÆöÁöÑÂèòÊç¢‰ªé‰∏Ä‰∏™Âõ∫ÂÆöÁöÑÊ†áÂáÜÈ´ò
ÊñØÂàÜÂ∏É‰∏≠ÈááÊ†∑ÂæóÂà∞ÔºåÁÑ∂ÂêéÈÄöËøáÁºñÁ†ÅÂô®ÁΩëÁªúÁöÑËæìÂá∫Êù•ËÆ°ÁÆóËøô‰∏™ÂèòÊç¢ÁöÑÂèÇÊï∞„ÄÇËøô‰∏™ËøáÁ®ãÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö&lt;/p>
&lt;p>$$
z = \mu + \sigma \odot \epsilon
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠Ôºå$\mu$ ÊòØÁºñÁ†ÅÂô®ÁΩëÁªúËæìÂá∫ÁöÑÂùáÂÄºÔºå$\sigma$ ÊòØËæìÂá∫ÁöÑÊ†áÂáÜÂ∑ÆÔºå$\epsilon$ ÊòØ‰ªéÊ†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏É $N(0, 1)$ ‰∏≠ÈááÊ†∑ÂæóÂà∞ÁöÑÂô™Â£∞„ÄÇ&lt;/p>
&lt;p>ÈÄöËøáËøôÁßçÈáçÊñ∞ÂèÇÊï∞ÂåñÔºå$\mathbf{z}$ ÁöÑÈááÊ†∑ËøáÁ®ã‰∏éÊ®°ÂûãÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶Áõ∏ÂÖ≥Ôºå‰ªéËÄå‰ΩøÂæóÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®Ê¢Ø
Â∫¶‰∏ãÈôçÁÆóÊ≥ïÊù•‰ºòÂåñÊ®°ÂûãÂèÇÊï∞„ÄÇËøôÁßçÊäÄÂ∑ßÂÖÅËÆ∏Êàë‰ª¨Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÈÄöËøáÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÁõ¥Êé•Êõ¥Êñ∞ÁºñÁ†ÅÂô®Âíå
Ëß£Á†ÅÂô®ÁöÑÂèÇÊï∞Ôºå‰ªéËÄå‰ºòÂåñVAEÊ®°Âûã„ÄÇ&lt;/p>
&lt;p>&lt;img alt="IMG_VAE_trick" src="https://fgg100y.github.io/posts/neuralnetworks/variationalautoencoder/images/VAE_reparametrization_trick.png">&lt;/p>
&lt;h2 id="beta-vae-disentangled-vae" >
&lt;div>
&lt;a href="#beta-vae-disentangled-vae">
#
&lt;/a>
$\beta$-VAE (Disentangled VAE)
&lt;/div>
&lt;/h2>
&lt;p>Âú®ÊçüÂ§±ÂáΩÊï∞‰∏≠Ôºå‰ΩøÁî®‰∏Ä‰∏™Ë∂ÖÂèÇÊï∞ $\beta$ ‰πò‰ª• KL-Êï£Â∫¶ÊçüÂ§±„ÄÇ‰πüÂ∞±ÊòØÔºö&lt;/p>
&lt;p>$$
Loss = \text{Reconstruction Loss} + \beta \times \text{KL Divergence}
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠ÔºåŒ≤ ÊòØ‰∏Ä‰∏™Ë∂ÖÂèÇÊï∞ÔºåÁî®‰∫éÂπ≥Ë°°ÈáçÊûÑÊçüÂ§±Âíå KL Êï£Â∫¶‰πãÈó¥ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄöËøáË∞ÉÊï¥ Œ≤ ÁöÑÂÄºÔºåÂèØ‰ª•ÊéßÂà∂
Ê®°ÂûãÂØπÊΩúÂú®Ë°®Á§∫ÁöÑÁ∫¶ÊùüÁ®ãÂ∫¶„ÄÇÂΩì Œ≤=1 Êó∂Ôºå‰∏éÊ†áÂáÜÁöÑVAEÁõ∏Á≠âÔºåËÄåÂΩì Œ≤ Â∞è‰∫é1 Êó∂ÔºåÊ®°ÂûãÊõ¥Âä†ÂÖ≥Ê≥®‰∫éÈáç
ÊûÑÊçüÂ§±Ôºå‰ªéËÄåÊõ¥Âä†Ê≥®ÈáçÊï∞ÊçÆÁöÑÈáçÂª∫ÔºõÂΩì Œ≤ Â§ß‰∫é1 Êó∂ÔºåÊ®°ÂûãÊõ¥Âä†ÂÖ≥Ê≥®‰∫é KL Êï£Â∫¶Ôºå‰ªéËÄåÊõ¥Âä†Ê≥®ÈáçÊΩúÂú®
Ë°®Á§∫ÁöÑÁã¨Á´ãÊÄßÂíåÁªìÊûÑÊÄß„ÄÇ&lt;/p></description></item><item><title>Â§ßÈÅìÂ¶ÇÈùíÂ§©ÔºåÊàëÁã¨‰∏çÂæóÂá∫</title><link>/about/</link><pubDate>Sat, 27 Apr 2024 19:58:25 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/about/</guid><description>&lt;p>Â§ßÈÅìÂ¶ÇÈùíÂ§©ÔºåÊàëÁã¨‰∏çÂæóÂá∫„ÄÇ&lt;/p>
&lt;p>ÁæûÈÄêÈïøÂÆâÁ§æ‰∏≠ÂÑøÔºåËµ§È∏°ÁôΩÈõâËµåÊ¢®Ê†ó„ÄÇ&lt;/p>
&lt;p>ÂºπÂâë‰ΩúÊ≠åÂ•èËã¶Â£∞ÔºåÊõ≥Ë£æÁéãÈó®‰∏çÁß∞ÊÉÖ„ÄÇ&lt;/p>
&lt;p>Ê∑ÆÈò¥Â∏Ç‰∫ïÁ¨ëÈü©‰ø°ÔºåÊ±âÊúùÂÖ¨ÂçøÂøåË¥æÁîü„ÄÇ&lt;/p>
&lt;p>Âêõ‰∏çËßÅÊòîÊó∂ÁáïÂÆ∂ÈáçÈÉ≠ÈöóÔºåÊã•ÁØ≤ÊäòËäÇÊó†Â´åÁåú„ÄÇ&lt;/p>
&lt;p>ÂâßËæõ‰πêÊØÖÊÑüÊÅ©ÂàÜÔºåËæìËÇùÂâñËÉÜÊïàËã±Êâç„ÄÇ&lt;/p>
&lt;p>Êò≠ÁéãÁôΩÈ™®Ëê¶ËîìËçâÔºåË∞Å‰∫∫Êõ¥Êâ´ÈªÑÈáëÂè∞Ôºü&lt;/p>
&lt;p>Ë°åË∑ØÈöæÔºåÂΩíÂéªÊù•ÔºÅ&lt;/p>
&lt;p>&lt;a href="https://so.gushiwen.cn/shiwenv_95834b2324cc.aspx">Âîê‰ª£ ¬∑ ÊùéÁôΩ„ÄäË°åË∑ØÈöæ ¬∑ ÂÖ∂‰∫å„Äã&lt;/a>&lt;/p></description></item><item><title>LLMs_interview_faq</title><link>/posts/llms/llm_faqs/</link><pubDate>Fri, 26 Apr 2024 11:04:16 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/llms/llm_faqs/</guid><description>&lt;h2 id="01ÁÆÄËø∞gptÂíåbertÁöÑÂå∫Âà´" >
&lt;div>
&lt;a href="#01%e7%ae%80%e8%bf%b0gpt%e5%92%8cbert%e7%9a%84%e5%8c%ba%e5%88%ab">
#
&lt;/a>
01:ÁÆÄËø∞GPTÂíåBERTÁöÑÂå∫Âà´
&lt;/div>
&lt;/h2>
&lt;p>GPT (Decoder-only) Âíå BERT (Encoder-only) ÈÉΩÊòØÂü∫‰∫é Transformer Êû∂ÊûÑÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊ®°ÂûãÔºåÂÆÉ‰ª¨Âú®ËÆæËÆ°‰∏äÊúâ‰∏Ä‰∫õÊòæËëóÂå∫Âà´Ôºö&lt;/p>
&lt;ul>
&lt;li>‰ªªÂä°Á±ªÂûã
&lt;ul>
&lt;li>GPT ‰ª•ÁîüÊàêÊñáÊú¨‰∏∫‰∏ªË¶Å‰ªªÂä°ÔºåÂÖ∂ÁõÆÊ†áÊòØÁîüÊàê‰∏éËæìÂÖ•ÊñáÊú¨ËøûË¥ØÂíåÁõ∏ÂÖ≥ÁöÑÊñáÊú¨„ÄÇÂõ†Ê≠§ÔºåGPT ÈÄö
Â∏∏Áî®‰∫éÁîüÊàêÊñáÊú¨ (Â¶ÇÔºöÊëòË¶ÅÊÄªÁªìÔºåÊñáÊú¨Ë°•ÂÖÖÂíåchatbot)„ÄÇ&lt;/li>
&lt;li>BERT ‰ª•ÁêÜËß£ÊñáÊú¨‰∏∫‰∏ªË¶Å‰ªªÂä°ÔºåÂÖ∂ÁõÆÊ†áÊòØ‰ªéËæìÂÖ•ÊñáÊú¨‰∏≠ÊèêÂèñËØ≠‰πâ‰ø°ÊÅØ„ÄÇÂõ†Ê≠§ÈÄÇÁî®‰∫éÂêÑÁßçÊñá
Êú¨ÁêÜËß£‰ªªÂä°ÔºåÂ¶ÇÔºöÊÉÖÊÑüÂàÜÊûê„ÄÅ ÊñáÊú¨ÂàÜÁ±ª„ÄÅÂëΩÂêçÂÆû‰ΩìËØÜÂà´Á≠â‰∏ãÊ∏∏‰ªªÂä°„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>È¢ÑËÆ≠ÁªÉÁõÆÊ†á
&lt;ul>
&lt;li>GPTÔºöÂçïÂêëËØ≠Ë®ÄÂª∫Ê®°„ÄÇGPTÈÄöËøáËá™Â∑¶ÂêëÂè≥ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•È¢ÑÊµã‰∏ã‰∏Ä‰∏™ÂçïËØçÔºåÂç≥Ê†πÊçÆ‰∏ä‰∏ãÊñáÈ¢Ñ
Êµã‰∏ã‰∏Ä‰∏™ÂçïËØç/ËØçÂÖÉÊòØ‰ªÄ‰πà„ÄÇ&lt;/li>
&lt;li>BERTÔºöÂèåÂêëËØ≠Ë®ÄÂª∫Ê®°„ÄÇBERT‰ΩøÁî®Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÔºàMLMÔºâÂíå‰∏ã‰∏ÄÂè•È¢ÑÊµãÔºàNSPÔºâ‰∏§‰∏™‰ªªÂä°ÔºåÂâç
ËÄÖÂú®MLM‰ªªÂä°‰∏≠ÈöèÊú∫ÈÅÆÊé©ËæìÂÖ•‰∏≠ÁöÑ‰∏Ä‰∫õËØçËØ≠ÔºåÊ®°ÂûãÈúÄË¶ÅÈ¢ÑÊµãËøô‰∫õË¢´Êé©ÁõñÁöÑËØçËØ≠ÊòØ‰ªÄ‰πàÔºõ
NSPÁöÑ‰ªªÂä°ÊòØÂà§Êñ≠‰∏§‰∏™Âè•Â≠êÊòØÂê¶Âú®ÂéüÊñá‰∏≠ÊòØÂâçÂêéËøûÊé•ÁöÑ„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ÁªìÊûÑÁâπÁÇπ
&lt;ul>
&lt;li>GPTÔºöTransformer-decoderÁöÑÂ†ÜÂè†Ôºå‰ªÖ‰ΩøÁî®Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂&lt;/li>
&lt;li>BERTÔºöTransformer-encoderÁöÑÂ†ÜÂè†ÔºåÂåÖÂê´Â§öÂ±ÇÂèåÂêëTransformer-encoder„ÄÇÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºå
BERTÂêåÊó∂‰ΩøÁî®‰∫ÜËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÂâçÈ¶àÁ•ûÁªèÁΩëÁªú„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ê®°ÂûãÂæÆË∞É
&lt;ul>
&lt;li>GPTÔºöÁî±‰∫éÂÖ∂ÁîüÊàêÂºèÁöÑÁâπÁÇπÔºåGPTÂú®ÂæÆË∞ÉÊó∂ÈÄöÂ∏∏Â∞ÜÊï¥‰∏™Ê®°Âûã‰Ωú‰∏∫ÂçïÁã¨ÁöÑÂ∫èÂàóÁîüÊàê‰ªªÂä°ËøõË°åÂæÆ
Ë∞É„ÄÇ&lt;/li>
&lt;li>BERTÔºöÁî±‰∫éÂÖ∂ÂèåÂêëË°®Á§∫ÁöÑÁâπÁÇπÔºåBERTÂú®ÂæÆË∞ÉÊó∂ÈÄöÂ∏∏Áî®‰∫éÂêÑÁßçÊñáÊú¨ÁêÜËß£‰ªªÂä°ÔºåÂæÆË∞ÉÊó∂ÂèØ‰ª•Âú®
Ê®°ÂûãÈ°∂Â±ÇÊ∑ªÂä†ÈÄÇÂΩìÁöÑËæìÂá∫Â±ÇÊù•ÈÄÇÂ∫î‰∏ãÊ∏∏ÁâπÂÆö‰ªªÂä°„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="02llm‰∏≠ÁöÑÂõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏éÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°Êúâ‰ªÄ‰πàÂå∫Âà´" >
&lt;div>
&lt;a href="#02llm%e4%b8%ad%e7%9a%84%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e4%b8%8e%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab">
#
&lt;/a>
02:LLM‰∏≠ÁöÑÂõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏éÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°Êúâ‰ªÄ‰πàÂå∫Âà´Ôºü
&lt;/div>
&lt;/h2>
&lt;p>Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°ÔºàCausal Language ModelingÔºâ&lt;/p>
&lt;pre>&lt;code>Âú®Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÔºåÊ®°ÂûãË¢´Ë¶ÅÊ±ÇÊ†πÊçÆËæìÂÖ•Â∫èÂàóÁöÑÂ∑¶‰æßÂÜÖÂÆπÊù•È¢ÑÊµãÂè≥‰æßÁöÑ‰∏ã‰∏Ä‰∏™ËØçÊàñÊ†áËÆ∞„ÄÇ‰πüÂ∞±ÊòØ
ËØ¥ÔºåÊ®°ÂûãÂè™ËÉΩÁúãÂà∞ËæìÂÖ•Â∫èÂàó‰∏≠Â∑≤ÁªèÁîüÊàêÁöÑÈÉ®ÂàÜÔºåËÄå‰∏çËÉΩÁúãÂà∞ÂêéÁª≠ÁöÑÂÜÖÂÆπ„ÄÇËøôÁßçËÆ≠ÁªÉÊñπÂºèÊúâÂä©‰∫éÊ®°
ÂûãÂ≠¶‰π†ÁîüÊàêËøûË¥ØÂíåÂêàÁêÜÁöÑÊñáÊú¨ÔºåÂõ†‰∏∫Ê®°ÂûãÈúÄË¶ÅÂú®ÁîüÊàêÊØè‰∏™ËØçËØ≠Êó∂ËÄÉËôë‰∏ä‰∏ãÊñáÁöÑ‰ø°ÊÅØÔºåÂêåÊó∂‰∏çËÉΩ‰æù
Ëµñ‰∫éÊú™Êù•ÁöÑ‰ø°ÊÅØ„ÄÇGPTÔºàGenerative Pre-trained TransformerÔºâÂ∞±ÊòØ‰ª•Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏∫Âü∫Á°ÄÁöÑ
Ê®°Âûã„ÄÇ
&lt;/code>&lt;/pre>
&lt;p>Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÔºàMasked Language ModelingÔºâÔºö&lt;/p>
&lt;pre>&lt;code>Âú®Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÔºåÊ®°ÂûãË¢´Ë¶ÅÊ±ÇÈ¢ÑÊµãËæìÂÖ•Â∫èÂàó‰∏≠‰∏Ä‰∫õË¢´ÈöèÊú∫Êé©ÁõñÊàñÊé©Á†ÅÁöÑËØçËØ≠„ÄÇÊ®°ÂûãÈúÄË¶ÅÂü∫‰∫é‰∏ä
‰∏ãÊñáÊù•È¢ÑÊµãËøô‰∫õË¢´Êé©ÁõñÁöÑËØçËØ≠ÊòØ‰ªÄ‰πà„ÄÇ
ËøôÁßçËÆ≠ÁªÉÊñπÂºèÈÄöÂ∏∏Áî®‰∫éÂèåÂêëÁöÑËØ≠Ë®ÄÁêÜËß£‰ªªÂä°ÔºåÂõ†‰∏∫Ê®°ÂûãÈúÄË¶ÅËÄÉËôë‰∏ä‰∏ãÊñá‰∏≠ÁöÑÊâÄÊúâ‰ø°ÊÅØÊù•È¢ÑÊµãË¢´Êé©ÁõñÁöÑËØçËØ≠„ÄÇ
BERTÔºàBidirectional Encoder Representations from TransformersÔºâÂ∞±ÊòØ‰ª•Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°‰∏∫Âü∫Á°ÄÁöÑÊ®°Âûã„ÄÇ
&lt;/code>&lt;/pre>
&lt;h2 id="03ËØ∑ÁÆÄËø∞transformerÂü∫Êú¨ÂéüÁêÜ" >
&lt;div>
&lt;a href="#03%e8%af%b7%e7%ae%80%e8%bf%b0transformer%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86">
#
&lt;/a>
03:ËØ∑ÁÆÄËø∞TransformerÂü∫Êú¨ÂéüÁêÜ
&lt;/div>
&lt;/h2>
&lt;p>Transformer ÊòØ‰∏ÄÁßçÁî®‰∫éÂ§ÑÁêÜÂ∫èÂàóÊï∞ÊçÆÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºåÁî±Ë∞∑Ê≠åÂõ¢Èòü‰∫é2017Âπ¥ÊèêÂá∫ÔºåÂÖ∂‰∏ªË¶ÅÂéüÁêÜÂåÖÊã¨
Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂Âíå‰ΩçÁΩÆÁºñÁ†Å„ÄÇ&lt;/p>
&lt;h3 id="Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂" >
&lt;div>
&lt;a href="#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6">
##
&lt;/a>
Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>ÂÖÅËÆ∏Ê®°ÂûãÂú®Â∫èÂàóÁöÑ‰ªªÊÑè‰∏§‰∏™‰ΩçÁΩÆÈó¥Áõ¥Êé•Âª∫Á´ã‰æùËµñÂÖ≥Á≥ªÔºåËÄå‰∏çËÄÉËôëÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇÂÖ∑‰ΩìÂ∞±ÊòØÂ∞ÜËØç
ÂÖÉÁ∫øÊÄßËΩ¨Êç¢‰∏∫‰∏â‰∏™ÂêëÈáèQ,K,VÔºåÁÑ∂ÂêéÂ∞ÜQÂíåKÁî®Êù•ËÆ°ÁÆóÂÜÖÁßØ(Áõ∏‰ººÂ∫¶ÂàÜÊï∞)Âπ∂ËøõË°åÊ≥®ÊÑèÂäõÁº©ÊîæÔºàscaled
dot-product)ÔºåÁÑ∂ÂêéÈÄöËøásoftmaxÂΩí‰∏ÄÂåñÔºåÂæóÂà∞ÊØè‰∏™ËØçÂÖÉÁõ∏ÂØπ‰∫éÂÖ∂‰ªñËØçÂÖÉÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÔºåÁÑ∂ÂêéÁî®
Ê≥®ÊÑèÂäõÊùÉÈáçÂØπÂêëÈáèVËøõË°åÂä†ÊùÉÂíåËÆ°ÁÆóÂæóÂà∞‚Äú‰∏ä‰∏ãÊñáÂêëÈáè‚Äù(context vector)ÔºåÁÑ∂ÂêéÂ∞Ü‰∏ä‰∏ãÊñáÂêëÈáèÁî®
ÂâçÈ¶àÁΩëÁªúÔºàFFNNÔºâËøõË°åÂèòÊç¢ÔºåÂ∞±ÂæóÂà∞ÁºñÁ†ÅÂô®ÈöêÂ±ÇËæìÂá∫„ÄÇÊ≥®ÊÑèÔºöËá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÔºåÊØè‰∏™ËæìÂÖ•ËØçÂÖÉÁöÑ
context vector ‰ª•ÂèäÂêéÁª≠ÁöÑ hidden stateÔºåÂèØ‰ª•ÁúãÊàêÊòØÁõ∏Â∫îÁöÑ Q ÂêëÈáèÁöÑÂáΩÊï∞ÔºåÂÖ∂‰ªñÁöÑÂ¶Ç KÔºåVÔºå
‰ª•ÂèäËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂèÇÊï∞ÂØπÊâÄÊúâÁöÑ Q ÈÉΩÊòØÊÅíÂÆöÂÄº„ÄÇ
+ Â§öÂ§¥Ê≥®ÊÑèÂäõÔºö
Âú®Â§öÂ§¥Ê≥®ÊÑèÂäõ‰∏≠ÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂Ë¢´Â§çÂà∂Â§öÊ¨°ÔºåÂπ∂‰∏îÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÈÉΩÂ≠¶‰π†Âà∞‰∏ÄÁªÑ‰∏çÂêåÁöÑQ,K,VÁöÑ
Ë°®Á§∫ÔºåÁÑ∂ÂêéÂ∞ÜÂÆÉ‰ª¨ÁöÑËæìÂá∫ÊãºÊé•Ëµ∑Êù•ÔºåÂÜçÈÄöËøáFFNNËøõË°åÁª¥Â∫¶ÂØπÈΩê„ÄÇ
- Â§çÂà∂Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºöÂéüÂßãËæìÂÖ•Â∫èÂàó‰ºöË¢´Áî®Êù•ËÆ°ÁÆóÂ§ö‰∏™Ê≥®ÊÑèÂäõÂ§¥Ôºà‰æãÂ¶Ç8‰∏™Êàñ16‰∏™Â§¥Ôºâ
- Áã¨Á´ãÂ≠¶‰π†ÔºöÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÈÉΩ‰ºöÁã¨Á´ãÂú∞Â≠¶‰π†‰∏ÄÁªÑQÔºåKÔºåVÁöÑË°®Á§∫Ôºå‰πüÂ∞±ÊòØÔºöÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÈÉΩ
ÊúâËá™Â∑±ÁöÑÊùÉÈáçÁü©ÈòµÔºåÂ∞ÜËæìÂÖ•Â∫èÂàóËΩ¨Êç¢‰∏∫Q,K,VÂêëÈáè„ÄÇ
- Ê≥®ÊÑèÂäõËÆ°ÁÆóÔºöÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÂÉèÂçïÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÈÇ£Ê†∑ËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞ÂíåÊ≥®ÊÑèÂäõÊùÉÈáç„ÄÇ
- ÊãºÊé•ËæìÂá∫ÔºöÂ∞ÜÊâÄÊúâÊ≥®ÊÑèÂäõÂ§¥ÁöÑËæìÂá∫ÊãºÊé•Êàê‰∏Ä‰∏™ÂêëÈáèÔºåÂΩ¢ÊàêÂ§öÂ§¥Ê≥®ÊÑèÂäõÁöÑÊúÄÁªàËæìÂá∫„ÄÇËøôÊÑèÂë≥
ÁùÄÊØè‰∏™ËØçÂÖÉÈÉΩ‰ºöÂæóÂà∞Êù•Ëá™Â§ö‰∏™‰∏çÂêåËßÜËßíÁöÑË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÂØπËæìÂÖ•Â∫èÂàóÁöÑÁêÜËß£„ÄÇ
- Á∫øÊÄßÂèòÊç¢ÔºöÊãºÊé•ÂêéÁöÑËæìÂá∫ÈÄöËøáFFNNËøõË°åÂ§ÑÁêÜÔºåÁª¥ÊåÅËæìÂá∫Áª¥Â∫¶‰ª•ÂèäËûçÂêà‰∏çÂêåÊ≥®ÊÑèÂäõÂ§¥ÁöÑ‰ø°ÊÅØ„ÄÇ
+ narrow attnÔºöEach attention head will get a chunk of the transformed data points
(projections) to work with. This is a details of utmost importance: The
attention heads DO NOT use chunks of the original data points, but rather those
of their projections. It computes the projections first and then chunks them
later, so that each value in the projection is a linear combination of all
features in the data point.
&lt;/code>&lt;/pre>
&lt;h3 id="‰ΩçÁΩÆÁºñÁ†Å" >
&lt;div>
&lt;a href="#%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81">
##
&lt;/a>
‰ΩçÁΩÆÁºñÁ†ÅÔºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>‰ΩçÁΩÆÁºñÁ†ÅÈÄöÂ∏∏ÊòØÈÄöËøáÂ∞Ü‰∏Ä‰∏™‰∏é‰ΩçÁΩÆÁõ∏ÂÖ≥ÁöÑÂêëÈáèÊ∑ªÂä†Âà∞ËæìÂÖ•ÂµåÂÖ•Ôºàinput embeddingsÔºâ
‰∏≠Êù•ÂÆûÁé∞ÁöÑ„ÄÇËøô‰∏™ÂêëÈáè‰∏∫Â∫èÂàó‰∏≠ÁöÑÊØè‰∏™‰ΩçÁΩÆÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑË°®Á§∫Ôºå‰ªéËÄå‰ΩøÊ®°ÂûãËÉΩÂ§ü
Âå∫ÂàÜ‰∏çÂêåÁöÑÂçïËØçÈ°∫Â∫è„ÄÇ
ÁâπÂà´ÊòØÂü∫‰∫é transformer Êû∂ÊûÑÁöÑÊ®°ÂûãÔºåÁî±‰∫éËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Êó†Ê≥ïÊçïÊçâËØçÂÖÉÈ°∫Â∫èÔºåÂõ†Ê≠§ÂøÖ
È°ªÈÄöËøáÂä†ÂÖ•‰ΩçÁΩÆÁºñÁ†ÅÊù•Ëé∑ÂèñËæìÂÖ•Â∫èÂàó‰∏≠ÂêÑ‰∏™ËØçÂÖÉÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇ
Â∞ΩÁÆ°Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºàSelf-Attention MechanismÔºâÁ°ÆÂÆûÂèØ‰ª•ÊçïÊçâÂ∫èÂàó‰∏≠ÂÖÉÁ¥†‰πãÈó¥ÁöÑÂÖ≥
Á≥ªÔºå‰ΩÜÂÆÉ‰∏ªË¶Å‰æùËµñ‰∫éÂÖÉÁ¥†‰πãÈó¥ÁöÑ‰∫§‰∫íÂíåÊùÉÈáçËÆ°ÁÆóÔºåËÄå‰∏çÊòØÂÆÉ‰ª¨ÁöÑÁªùÂØπ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇ‰ΩçÁΩÆ
ÁºñÁ†ÅÁöÑ‰ΩúÁî®ÊòØË°•ÂÖÖËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊèê‰æõÂ∫èÂàó‰∏≠ÂÖÉÁ¥†È°∫Â∫èÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥
Â•ΩÂú∞ÁêÜËß£Â∫èÂàóÁöÑÁªìÊûÑ„ÄÇ
+ Ê≠£Âº¶/‰ΩôÂº¶ÂáΩÊï∞ÁªÑÂêàÁºñÁ†Å (ÂÅ∂Êï∞‰ΩçÁî® sin(), Â•áÊï∞‰ΩçÁî® cos())
ÂÆÉ‰ª¨‰∏∫Â∫èÂàó‰∏≠ÁöÑÊØè‰∏™‰ΩçÁΩÆÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑ„ÄÅ‰∏é‰ΩçÁΩÆÁõ¥Êé•Áõ∏ÂÖ≥ÁöÑÁºñÁ†Å„ÄÇËøôÁßçÁºñÁ†Å
ÊñπÂºèËÉΩÂ§üÊòéÁ°ÆÂú∞ÂëäËØâÊ®°ÂûãÊØè‰∏™ÂçïËØçÂú®Â∫èÂàó‰∏≠ÁöÑÁªùÂØπ‰ΩçÁΩÆ„ÄÇ
ÁªùÂØπ‰ΩçÁΩÆÁºñÁ†ÅÁöÑ‰∏Ä‰∏™ÊΩúÂú®Áº∫ÁÇπÊòØÂÆÉ‰ª¨ÊòØÈùôÊÄÅÁöÑÔºå‰∏ç‰ºöÈöèÁùÄÊ®°ÂûãËÆ≠ÁªÉÁöÑËøõË°åËÄåÊîπÂèò„ÄÇ
ËøôÊÑèÂë≥ÁùÄÂÆÉ‰ª¨ÂèØËÉΩ‰∏çË∂≥‰ª•ÊçïÊçâÈïøÂ∫èÂàó‰∏≠Â§çÊùÇÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÁâπÂà´ÊòØÂú®Ê®°ÂûãÈúÄË¶ÅÂä®ÊÄÅ
Âú∞Ë∞ÉÊï¥‰ΩçÁΩÆ‰ø°ÊÅØ‰ª•ÈÄÇÂ∫îËæìÂÖ•Â∫èÂàóÁöÑÂèòÂåñÊó∂„ÄÇ
‰ø°ÊÅØÁöÑÂ±ÄÈôêÊÄßÔºöÂõ∫ÂÆöÁöÑ‰ΩçÁΩÆÁºñÁ†Å‰ªÖÊèê‰æõ‰∫Ü‰ΩçÁΩÆÁöÑÁªùÂØπ‰ø°ÊÅØÔºåËÄåÊ≤°ÊúâËÄÉËôëÂ∫èÂàó‰∏≠ÂÖÉ
Á¥†‰πãÈó¥ÁöÑÁõ∏ÂØπÂÖ≥Á≥ª„ÄÇÂú®ÈïøÂ∫èÂàó‰∏≠ÔºåÂÖÉÁ¥†‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂíåË∑ùÁ¶ªÂèØËÉΩÊõ¥‰∏∫ÈáçË¶Å„ÄÇ
+ ÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÔºöÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÊØè‰∏™‰ΩçÁΩÆÁöÑÁºñÁ†ÅË°®Á§∫‰∏∫‰∏Ä‰∏™ÊóãËΩ¨Áü©ÈòµÔºå
ËØ•Áü©ÈòµÂèØ‰ª•Â∫îÁî®‰∫éËæìÂÖ•ÂµåÂÖ•„ÄÇÊóãËΩ¨Áü©ÈòµÊòØÂä®ÊÄÅÁîüÊàêÁöÑÔºåËøôÊÑèÂë≥ÁùÄÂÆÉ‰ª¨ÂèØ‰ª•Ê†πÊçÆËæìÂÖ•
Â∫èÂàóÁöÑÂÜÖÂÆπËøõË°åË∞ÉÊï¥Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÊçïÊçâÈïøË∑ùÁ¶ª‰æùËµñ„ÄÇ
&lt;/code>&lt;/pre>
&lt;h3 id="ÊÆãÂ∑ÆËøûÊé•‰∏éÂ±ÇÂΩí‰∏ÄÂåñ" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%8e%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96">
##
&lt;/a>
ÊÆãÂ∑ÆËøûÊé•‰∏éÂ±ÇÂΩí‰∏ÄÂåñÔºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>+ ÊÆãÂ∑ÆËøûÊé•ÔºöÂ∞ÜÊØè‰∏™Â≠êÂ±ÇÁöÑËæìÂÖ•‰∏éÂÖ∂ËæìÂá∫Áõ∏Âä†ÔºåÁÑ∂Âêé‰º†ÈÄíÁªô‰∏ã‰∏ÄÂ±Ç„ÄÇËøô‰ΩøÂæóÊ®°ÂûãÂú®Â≠¶
‰π†ËøáÁ®ã‰∏≠ÔºåËÉΩÊõ¥ÂÆπÊòìÂú∞Â≠¶‰π†Âà∞ÊÆãÂ∑ÆÔºàËæìÂÖ•‰∫éËæìÂá∫‰πãÂ∑ÆÔºâÔºå‰ªéËÄåÁºìËß£Ê¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÔºõ
ÊèêÈ´òËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÔºõÂÖÅËÆ∏Êõ¥Ê∑±ÁöÑÁΩëÁªúÁªìÊûÑÔºõ‰ª•ÂèäÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ
+ Â±ÇÂΩí‰∏ÄÂåñÔºöÂú®ÊØè‰∏™Â±ÇÁöÑËæìÂÖ•‰πãÂêéÈÉΩÂ∫îÁî®ÂΩí‰∏ÄÂåñÔºåÂç≥ÂØπÊØè‰∏™ÁâπÂæÅÁª¥Â∫¶ËøõË°åÂΩí‰∏ÄÂåñÊìç‰ΩúÔºå
‰ΩøÂæóÂÆÉ‰ª¨ÂùáÂÄº‰∏∫0ÔºåÊ†áÂáÜÂ∑Æ‰∏∫1ÔºåÊúâÂä©‰∫éÁºìËß£Ê¢ØÂ∫¶Ê∂àÂ§±ÂíåÊ¢ØÂ∫¶ÁàÜÁÇ∏ÁöÑÈóÆÈ¢òÔºå‰ªéËÄå‰Ωø
Ê®°ÂûãËÆ≠ÁªÉÊõ¥Âä†Á®≥ÂÆöÔºå‰πüÊèêÈ´òÂÖ∂Ê≥õÂåñËÉΩÂäõ„ÄÇ
- ‚ÄúÊâπÊ¨°ÂΩí‰∏ÄÂåñ‚ÄùÔºàBatch NormÔºâÔºö
The mean and variance statistics used for normalization are calculated
across all elements of all instances in a batch, for each feature
independently.
Âç≥ÔºöÂùáÂÄºÂíåÊñπÂ∑ÆÊòØÈÄöËøáÂØπ‰∏Ä‰∏™ÊâπÊ¨°ÈáåÊâÄÊúâÂÆû‰æãÔºàÂ∫èÂàóÔºâÁöÑÊâÄÊúâÂÖÉÁ¥†ÔºàËØçÂÖÉÔºâÁöÑÊüê
‰∏™ÁâπÂæÅËøõË°åÁªüËÆ°ÁöÑ„ÄÇ
- ‚ÄúÂ±ÇÂΩí‰∏ÄÂåñ‚ÄùÔºàLayer NormÔºâÔºö
For layernorm, the statistics are calculated across the feature
dimension, for each element and instance independently.
Âç≥ÔºöÂùáÂÄºÂíåÊñπÂ∑ÆÊòØÈÄöËøáÂØπÊüê‰∏™ÂÆû‰æãÔºàÂ∫èÂàóÔºâ‰∏≠ÁöÑÊüê‰∏™ÂÖÉÁ¥†ÔºàËØçÂÖÉÔºâÁöÑÊâÄÊúâÁâπÂæÅËøõ
Ë°åÁªüËÆ°ÁöÑ„ÄÇ
NOTE By &amp;quot;element&amp;quot; and &amp;quot;instance,&amp;quot; I mean &amp;quot;word&amp;quot; and &amp;quot;sentence&amp;quot;
respectively for an NLP task, and &amp;quot;pixel&amp;quot; and &amp;quot;image&amp;quot; for a CV task.
&lt;/code>&lt;/pre>
&lt;p>&lt;img alt="IMG: LayerNorm &amp;amp; BatchNorm" src="https://fgg100y.github.io/posts/llms/llm_faqs/images/LLMs_layerNorm_batchNorm.png">&lt;/p>
&lt;h2 id="04-Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊîπËâØÁâàÊú¨‰ª¨" >
&lt;div>
&lt;a href="#04-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
04: Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊîπËâØÁâàÊú¨‰ª¨
&lt;/div>
&lt;/h2>
&lt;p>MHAÔºåGQAÔºåMQA‰∏âÁßçÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü&lt;/p>
&lt;p>Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÊú∫Âô®Â≠¶‰π†È¢ÜÂüüÊúâÂ§öÁßç‰∏çÂêåÁöÑÂÆûÁé∞ÊñπÂºèÔºåÂÖ∂‰∏≠Â∏∏ËßÅÁöÑÂåÖÊã¨Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõ
ÔºàMulti-Head Self-AttentionÔºåMHAÔºâ„ÄÅÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºàGlobal AttentionÔºåGQAÔºâÂíåÂ§öÂ§¥Êü•ËØ¢Ê≥®ÊÑèÂäõ
ÔºàMulti-Query AttentionÔºåMQAÔºâ„ÄÇËøô‰∫õ‰∏çÂêåÁöÑÂÆûÁé∞ÊñπÂºèÂú®Êú∫Âà∂ÂíåÂ∫îÁî®‰∏äÊúâ‰∏Ä‰∫õÂå∫Âà´Ôºö&lt;/p>
&lt;pre>&lt;code>Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÔºàMHAÔºâÔºö
Êú∫Âà∂ÔºöMHAÂ∞ÜËæìÂÖ•Â∫èÂàó‰∏≠ÁöÑÊØè‰∏™‰ΩçÁΩÆÁöÑË°®Á§∫ÈÉΩ‰Ωú‰∏∫Êü•ËØ¢ÔºàQueryÔºâ„ÄÅÈîÆÔºàKeyÔºâÂíåÂÄºÔºàValueÔºâÔºå
ÈÄöËøáËÆ°ÁÆóÊü•ËØ¢‰∏éÊâÄÊúâÈîÆÁöÑÁõ∏‰ººÂ∫¶ÔºåÁÑ∂ÂêéÂ∞ÜÁõ∏‰ººÂ∫¶‰Ωú‰∏∫ÊùÉÈáçÂØπÂÄºËøõË°åÂä†ÊùÉÊ±ÇÂíåÔºå‰ªéËÄåËé∑ÂæóÊØè‰∏™
‰ΩçÁΩÆÁöÑÊ≥®ÊÑèÂäõËæìÂá∫„ÄÇ
ÁâπÁÇπÔºöMHAÂÖÅËÆ∏Ê®°ÂûãÂú®‰∏çÂêåÁöÑË°®Á§∫Á©∫Èó¥‰∏äËøõË°åÂ§öÂ§¥Âπ∂Ë°åËÆ°ÁÆóÔºåÈÄöËøáÂ§öÂ§¥Êú∫Âà∂ÔºåÂèØ‰ª•Â≠¶‰π†Âà∞‰∏çÂêåÁöÑÂÖ≥Ê≥®ÁÇπÂíåË°®Á§∫„ÄÇ
Â∫îÁî®ÔºöMHAÂ∏∏Áî®‰∫éTransformerÁ≠âÊ®°Âûã‰∏≠ÔºåÁî®‰∫éÊçïÊçâËæìÂÖ•Â∫èÂàó‰∏≠‰∏çÂêå‰ΩçÁΩÆ‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ
ÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºàGQAÔºâÔºö
Êú∫Âà∂ÔºöGQAÂ∞ÜÊâÄÊúâÁöÑËæìÂÖ•‰ΩçÁΩÆÈÉΩ‰Ωú‰∏∫Êü•ËØ¢Ôºå‰∏éÊâÄÊúâÁöÑÈîÆËÆ°ÁÆóÁõ∏‰ººÂ∫¶ÔºåÁÑ∂ÂêéÂ∞ÜÊâÄÊúâ‰ΩçÁΩÆÁöÑÂÄºÊ†π
ÊçÆÁõ∏‰ººÂ∫¶ËøõË°åÂä†ÊùÉÊ±ÇÂíåÔºåÂæóÂà∞‰∏Ä‰∏™ÂÖ®Â±ÄÁöÑËæìÂá∫„ÄÇ
ÁâπÁÇπÔºöGQAËÄÉËôë‰∫ÜÂ∫èÂàó‰∏≠ÊâÄÊúâ‰ΩçÁΩÆÁöÑÂÖ≥Á≥ªÔºå‰ΩÜÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÂèØËÉΩ‰ºöÂèóÂà∞ËÆ°ÁÆóËµÑÊ∫êÁöÑÈôêÂà∂Ôºå
Âõ†‰∏∫ÈúÄË¶ÅËÆ°ÁÆóÊâÄÊúâ‰ΩçÁΩÆ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶„ÄÇ
Â∫îÁî®ÔºöGQAÂ∏∏Áî®‰∫éÂØπÊï¥‰∏™ËæìÂÖ•Â∫èÂàóËøõË°åÂÖ®Â±ÄÁöÑ‰ø°ÊÅØËÅöÂêàÔºå‰æãÂ¶ÇÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠„ÄÇ
Â§öÂ§¥Êü•ËØ¢Ê≥®ÊÑèÂäõÔºàMQAÔºâÔºö
Êú∫Âà∂ÔºöMQA‰∏éMHAÁ±ª‰ººÔºå‰ΩÜÂú®ÊØè‰∏™Â§¥ÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆó‰∏≠Ôºå‰ΩøÁî®‰∏çÂêåÁöÑÊü•ËØ¢ÂêëÈáèÔºåËÄå‰∏çÊòØÊâÄÊúâÂ§¥ÈÉΩÂÖ±‰∫´Áõ∏ÂêåÁöÑÊü•ËØ¢ÂêëÈáè„ÄÇ
ÁâπÁÇπÔºöMQAÂÖÅËÆ∏Ê®°Âûã‰∏∫ÊØè‰∏™Â§¥Â≠¶‰π†‰∏çÂêåÁöÑÊü•ËØ¢Ê®°ÂºèÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÂíåË°®ËææËÉΩÂäõ„ÄÇ
Â∫îÁî®ÔºöMQAÂ∏∏Áî®‰∫éÈúÄË¶ÅÊ†πÊçÆ‰∏çÂêåÁöÑÊü•ËØ¢Êù•Ëé∑ÂèñÊ≥®ÊÑèÂäõ‰ø°ÊÅØÁöÑ‰ªªÂä°ÔºåÂ¶ÇÈóÆÁ≠îÁ≥ªÁªüÊàñÈúÄË¶ÅÈíàÂØπ‰∏çÂêåÈóÆÈ¢òËøõË°åÊé®ÁêÜÁöÑÂú∫ÊôØ
&lt;/code>&lt;/pre>
&lt;h2 id="05-attentionÁöÑÊîπËâØÁâàÊú¨‰ª¨" >
&lt;div>
&lt;a href="#05-attention%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
05: AttentionÁöÑÊîπËâØÁâàÊú¨‰ª¨
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ÁÆÄËø∞‰∏Ä‰∏ã FlashAttention ÁöÑÂéüÁêÜ&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Flash AttentionÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÊ≥®ÊÑèÂäõÁÆóÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüTransformerÊ®°Âûã‰∏≠Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑËÆ°
ÁÆóÂíåÂÜÖÂ≠òÊïàÁéáÈóÆÈ¢ò„ÄÇÁî±‰∫éËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊó∂Èó¥ÂíåÂ≠òÂÇ®Â§çÊùÇÂ∫¶‰∏éÂ∫èÂàóÈïøÂ∫¶Êàê‰∫åÊ¨°ÊñπÂÖ≥Á≥ªÔºåËøô‰ΩøÂæóÂ§Ñ
ÁêÜÈïøÂ∫èÂàóÊï∞ÊçÆÊó∂Èù¢‰∏¥Â∑®Â§ßÊåëÊàò„ÄÇFlash AttentionÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÂØπÈ´òÂ∏¶ÂÆΩÂÜÖÂ≠òÔºàHBMÔºâ
ÁöÑËØªÂÜôÊ¨°Êï∞Ôºå‰ªéËÄåÂä†Âø´‰∫ÜËøêË°åÈÄüÂ∫¶Âπ∂Èôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®„ÄÇ&lt;/p>
&lt;h3 id="flash-attentionÁöÑÊ†∏ÂøÉÂéüÁêÜÂíåÊäÄÊúØ" >
&lt;div>
&lt;a href="#flash-attention%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86%e5%92%8c%e6%8a%80%e6%9c%af">
##
&lt;/a>
Flash AttentionÁöÑÊ†∏ÂøÉÂéüÁêÜÂíåÊäÄÊúØÔºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>Âπ≥Èì∫ÔºàTilingÔºâÔºöFlash AttentionÂ∞ÜËæìÂÖ•ÂàÜÂâ≤ÊàêÂ∞èÂùóÔºåÂπ∂Âú®ÊØè‰∏™Âùó‰∏äÊâßË°åÊ≥®ÊÑèÂäõÊìç‰Ωú„ÄÇËøôÁßçÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂØπÈ´òÂ∏¶ÂÆΩÂÜÖÂ≠òÁöÑËÆøÈóÆÊ¨°Êï∞ÔºåÂõ†‰∏∫‰∏çÈúÄË¶Å‰∏ÄÊ¨°ÊÄßÂ∞ÜÊï¥‰∏™Â§ßÁü©ÈòµÂä†ËΩΩÂà∞ÂÜÖÂ≠ò‰∏≠„ÄÇ
ÈáçÊñ∞ËÆ°ÁÆóÔºàRecomputationÔºâÔºöÂú®ÂêéÂêë‰º†Êí≠ËøáÁ®ã‰∏≠ÔºåFlash AttentionÈÅøÂÖç‰∫ÜÂ≠òÂÇ®Â§ßÂûã‰∏≠Èó¥Áü©ÈòµÔºàÂ¶ÇSÂíåPÁü©ÈòµÔºâÔºåËÄåÊòØÂà©Áî®ÂâçÂêë‰º†Êí≠‰∏≠ÁöÑÁªüËÆ°ÈáèÊù•Âø´ÈÄüÈáçÊñ∞ËÆ°ÁÆóËøô‰∫õÁü©ÈòµÔºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÊ∂àËÄó„ÄÇ
Âú®Á∫øSoftmaxÔºö‰∏∫‰∫ÜÂ§ÑÁêÜSoftmaxÊìç‰ΩúÔºåFlash AttentionÈááÁî®‰∫ÜÂú®Á∫øSoftmaxÊäÄÊúØÔºåÂÆÉÂÖÅËÆ∏ÂàÜÂùóËÆ°ÁÆósoftmaxÔºåÂπ∂ÈÄöËøáÈÄÇÂΩìÁöÑÂΩí‰∏ÄÂåñÂõ†Â≠êÊù•Á°Æ‰øùÊúÄÁªàÁªìÊûúÁöÑÊ≠£Á°ÆÊÄß„ÄÇ
ÂÜÖÂ≠òÂ±ÇÊ¨°ÁªìÊûÑÊÑèËØÜÔºàIO-AwarenessÔºâÔºöFlash AttentionËÄÉËôë‰∫ÜGPUÂÜÖÂ≠òÂ±ÇÊ¨°ÁªìÊûÑÔºå‰ºòÂåñ‰∫Ü‰∏çÂêåÂ±ÇÁ∫ßÂÜÖÂ≠ò‰πãÈó¥ÁöÑÊï∞ÊçÆËÆøÈóÆÔºåÂ¶ÇÂú®GPUÁöÑSRAMÂíåHBM‰πãÈó¥„ÄÇ
&lt;/code>&lt;/pre>
&lt;h3 id="flash-attention-2" >
&lt;div>
&lt;a href="#flash-attention-2">
##
&lt;/a>
Flash Attention-2Ôºö
&lt;/div>
&lt;/h3>
&lt;p>Âú®Flash AttentionÁöÑÂü∫Á°Ä‰∏äÔºåÁ†îÁ©∂‰∫∫ÂëòËøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫ÜFlash Attention-2ÔºåÂÆÉÈÄöËøáÊîπËøõÂ∑•‰ΩúÂàÜÈÖçÂíåÂπ∂Ë°åÂåñÁ≠ñÁï•ÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜËÆ°ÁÆóÈÄüÂ∫¶„ÄÇFlash Attention-2ÁöÑ‰ºòÂåñÂåÖÊã¨Ôºö&lt;/p>
&lt;pre>&lt;code>ÂáèÂ∞ëÈùûÁü©Èòµ‰πòÊ≥ïÔºànon-matmulÔºâÁöÑÊµÆÁÇπËøêÁÆóÊ¨°Êï∞ÔºàFLOPsÔºâ„ÄÇ
ÈÄöËøáÂú®‰∏çÂêåÁöÑÁ∫øÁ®ãÂùó‰∏äÂπ∂Ë°åÂåñÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÊèêÈ´ò‰∫ÜGPUÁöÑÂç†Áî®Áéá„ÄÇ
Âú®ÊØè‰∏™Á∫øÁ®ãÂùóÂÜÖÔºåÂ∞ÜÂ∑•‰ΩúÂàÜÈÖçÁªô‰∏çÂêåÁöÑwarpÔºå‰ª•ÂáèÂ∞ëÈÄöËøáÂÖ±‰∫´ÂÜÖÂ≠òÁöÑÈÄö‰ø°„ÄÇ
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;strong>PagedAttentionÁöÑÂéüÁêÜÊòØ‰ªÄ‰πàÔºåËß£ÂÜ≥‰∫ÜLLM‰∏≠ÁöÑ‰ªÄ‰πàÈóÆÈ¢òÔºü&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Paged AttentionÔºàPAÔºâÊäÄÊúØÊòØ‰∏ÄÁßçÁî®‰∫é‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜÊÄßËÉΩÁöÑÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜËá™
ÂõûÂΩíÁîüÊàê‰ªªÂä°Êó∂ÂØπÂÜÖÂ≠ò‰ΩøÁî®ÊïàÁéáÁöÑÊòæËëóÊèêÂçá„ÄÇËøôÈ°πÊäÄÊúØÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂÄüÈâ¥Êìç‰ΩúÁ≥ªÁªü‰∏≠ËôöÊãüÂÜÖÂ≠òÂíåÂàÜÈ°µ
ÁöÑÊäÄÊúØÔºåÂ∞Ü‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÁöÑÈîÆÂÄºÂØπÔºàKey-Value pairsÔºåÁÆÄÁß∞K-V pairsÔºâÁºìÂ≠ò‰ª•ÂàÜÈ°µÁöÑÂΩ¢ÂºèÂ≠ò
ÂÇ®ÂíåÁÆ°ÁêÜ„ÄÇ&lt;/p>
&lt;p>Âú®Ëá™ÂõûÂΩíËß£Á†ÅËøáÁ®ã‰∏≠ÔºåÊ®°Âûã‰∏∫ÊØè‰∏™ËæìÂÖ•‰ª§ÁâåÁîüÊàêÊ≥®ÊÑèÂäõÈîÆÂíåÂÄºÔºåËøô‰∫õÈîÆÂÄºÂØπË¢´Â≠òÂÇ®Âú®GPUÁöÑÊòæÂ≠ò‰∏≠‰ª•
È¢ÑÊµã‰∏ã‰∏Ä‰∏™‰ª§Áâå„ÄÇÁî±‰∫éËøô‰∫õÁºìÂ≠òÁöÑÈîÆÂÄºÂØπÂ§ßÂ∞èÊòØÂä®ÊÄÅÂèòÂåñÁöÑÔºåÂπ∂‰∏îÂèØËÉΩ‰ºöÂç†Áî®Â§ßÈáèÁöÑÊòæÂ≠òÁ©∫Èó¥ÔºåÂõ†Ê≠§
ÊúâÊïàÁÆ°ÁêÜËøô‰∫õÁºìÂ≠òÊàê‰∏∫‰∏Ä‰∏™ÊåëÊàò„ÄÇ‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÁÆóÊ≥ïÂú®Â§ÑÁêÜÊó∂‰ºöÂèóÈôê‰∫éÊòæÂ≠òÁöÑÂ§ßÂ∞èÔºåËøôÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑ
ÊâπÂ§ÑÁêÜËÉΩÂäõÂíåÊï¥‰ΩìÁöÑÂêûÂêêÈáè„ÄÇ&lt;/p>
&lt;h2 id="06llmÂæÆË∞É‰∏éÈáèÂåñ" >
&lt;div>
&lt;a href="#06llm%e5%be%ae%e8%b0%83%e4%b8%8e%e9%87%8f%e5%8c%96">
#
&lt;/a>
06:LLMÂæÆË∞É‰∏éÈáèÂåñ
&lt;/div>
&lt;/h2>
&lt;p>30.ÂèÇÊï∞È´òÊïàÁöÑÂæÆË∞ÉÔºàPEFTÔºâÊúâÂì™‰∫õÊñπÊ≥ïÔºü
31.LORAÂæÆË∞ÉÁõ∏ÊØî‰∫éÂæÆË∞ÉÈÄÇÈÖçÂô®ÊàñÂâçÁºÄÂæÆË∞ÉÊúâ‰ªÄ‰πà‰ºòÂäøÔºü
32.Êúâ‰∫ÜËß£Ëøá‰ªÄ‰πàÊòØÁ®ÄÁñèÂæÆË∞ÉÂêóÔºü
33.ËÆ≠ÁªÉÂêéÈáèÂåñÔºàPTQÔºâÂíåÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâ‰∏é‰ªÄ‰πàÂå∫Âà´Ôºü
34.LLMs‰∏≠ÔºåÈáèÂåñÊùÉÈáçÂíåÈáèÂåñÊøÄÊ¥ªÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
35.AWQÈáèÂåñÁöÑÊ≠•È™§ÊòØ‰ªÄ‰πàÔºü&lt;/p>
&lt;h2 id="07ÂµåÂÖ•ÂêëÈáèÊ®°Âûã" >
&lt;div>
&lt;a href="#07%e5%b5%8c%e5%85%a5%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8b">
#
&lt;/a>
07:ÂµåÂÖ•ÂêëÈáèÊ®°Âûã
&lt;/div>
&lt;/h2>
&lt;p>40.Ëá™Ââç‰∏ªÊµÅÁöÑ‰∏≠ÊñáÂµåÂÖ•ÂêëÈáèÊ®°ÂûãÊúâÂì™‰∫õÔºü&lt;/p>
&lt;h2 id="ÂÖ∂‰ªñ" >
&lt;div>
&lt;a href="#%e5%85%b6%e4%bb%96">
#
&lt;/a>
ÂÖ∂‰ªñ
&lt;/div>
&lt;/h2>
&lt;p>45.DeepSpeedÊé®ÁêÜÂØπÁÆóÂ≠êËûçÂêàÂÅö‰∫ÜÂì™‰∫õ‰ºòÂåñÔºü
48.ËØ∑‰ªãÁªç‰∏Ä‰∏ãÂæÆËΩØÁöÑZeRO‰ºòÂåñÂô®&lt;/p>
&lt;p>3.‰∏∫‰ªÄ‰πàÁé∞Âú®ÁöÑÂ§ßÊ®°ÂûãÂ§ßÂ§öÊòØdecoder-onlyÁöÑÊû∂ÊûÑÔºü
4.ËÆ≤‰∏Ä‰∏ãÁîüÊàêÂºèËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∑•‰ΩúÊú∫ÁêÜ
5.Âì™‰∫õÂõ†Á¥†‰ºöÂØºËá¥LLMÁöÑÂÅèËßÅÔºü
7.Â¶Ç‰ΩïÂáèËΩªLLM‰∏≠ÁöÑÂπªËßâÁé∞Ë±°Ôºü
8.Ëß£ÈáäChatGPTÁöÑÈõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÁöÑÊ¶ÇÂøµ
10.Â¶Ç‰ΩïËØÑ‰º∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊÄßËÉΩÔºü
11.Â¶Ç‰ΩïÁºìËß£LLMsÈáçÂ§çËØªÈóÆÈ¢òÔºü
16.Wordpiece‰∏éBPE‰πãÈó¥ÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
17.ÊúâÂì™‰∫õÂ∏∏ËßÅÁöÑ‰ºòÂåñLLMsËæìÂá∫ÁöÑÊäÄÊúØÔºü
18.GPT-3Êã•ÊúâÁöÑ1750‰∫øÂèÇÊï∞ÔºåÊòØÊÄé‰πàÁÆóÂá∫Êù•ÁöÑÔºü
19.Ê∏©Â∫¶Á≥ªÊï∞Âíåtop-pÔºåtop-kÂèÇÊï∞Êúâ‰ªÄ‰πàÂå∫Âà´Ôºü
21.‰ªãÁªç-‰∏ãpostlayernormÂíåprelayernormÁöÑÂå∫Âà´
22.‰ªÄ‰πàÊòØÊÄùÁª¥ÈìæÔºàCoTÔºâÊèêÁ§∫Ôºü
23.‰Ω†ËßâÂæó‰ªÄ‰πàÊ†∑ÁöÑ‰ªªÂä°ÊàñÈ¢ÜÂüüÈÄÇÂêàÁî®ÊÄùÁª¥ÈìæÊèêÁ§∫Ôºü
24.‰Ω†‰∫ÜËß£ReActÂêóÔºåÂÆÉÊúâ‰ªÄ‰πà‰ºòÁÇπÔºü
25.Ëß£Èáä‰∏Ä‰∏ãlangchainAgentÁöÑÊ¶ÇÂøµ
26.langchainÊúâÂì™‰∫õÊõø‰ª£ÊñπÊ°àÔºü
27.langchaintokenËÆ°Êï∞Êúâ‰ªÄ‰πàÈóÆÈ¢òÔºüÂ¶Ç‰ΩïËß£ÂÜ≥Ôºü
28.LLMÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊúâÂì™Âá†‰∏™ÂÖ≥ÈîÆÊ≠•È™§Ôºü
29.RLHFÊ®°Âûã‰∏∫‰ªÄ‰πà‰ºöË°®Áé∞ÊØîSFTÊõ¥Â•ΩÔºü
36.‰ªãÁªç‰∏Ä‰∏ãGPipeÊé®ÁêÜÊ°ÜÊû∂
37.Áü©Èòµ‰πòÊ≥ïÂ¶Ç‰ΩïÂÅöÂº†ÈáèÂπ∂Ë°åÔºü
38.ËØ∑ÁÆÄËø∞‰∏ãPPOÁÆóÊ≥ïÊµÅÁ®ãÔºåÂÆÉË∑üTRPOÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
39.‰ªÄ‰πàÊòØÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÔºü
41.‰∏∫‰ªÄ‰πàLLMÁöÑÁü•ËØÜÊõ¥Êñ∞ÂæàÂõ∞ÈöæÔºü
42.RAGÂíåÂæÆË∞ÉÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
43.Â§ßÊ®°Âûã‰∏ÄËà¨ËØÑÊµãÊñπÊ≥ïÂèäÂü∫ÂáÜÊòØ‰ªÄ‰πàÔºü
50.‰ªÄ‰πàÊòØÊäïÊú∫ÈááÊ†∑ÊäÄÊúØÔºåËØ∑‰∏æ‰æãËØ¥ÊòéÔºü&lt;/p></description></item><item><title>Tokenization: BPE, Unigram and more</title><link>/posts/llms/llm_tokenization/</link><pubDate>Mon, 22 Apr 2024 15:41:56 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/llms/llm_tokenization/</guid><description>&lt;h1 id="there-is-more-than-one-way-to-tokenize-a-sentence" >
&lt;div>
&lt;a href="#there-is-more-than-one-way-to-tokenize-a-sentence">
##
&lt;/a>
There is more than one way to tokenize a sentence
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>word-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>A big vocabulary is needed&lt;/li>
&lt;li>We combine words: what exactly constitutes a word (&amp;ldquo;bachelor of science&amp;rdquo;, or
isolated words)&lt;/li>
&lt;li>Abbreviated words: &amp;ldquo;LOL&amp;rdquo;, &amp;ldquo;IMO&amp;rdquo;, are these collections of words or new words?&lt;/li>
&lt;li>Languages that don&amp;rsquo;t segment by spaces&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>character-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>Lack of meaning: Unlike words, characters don&amp;rsquo;t have any inherent meaning, model
may lose the semantic-specific feature of words.&lt;/li>
&lt;li>Increased input computation&lt;/li>
&lt;li>Limits netword+k choices: It&amp;rsquo;s difficult to use architectures which process input
sequentially since the input sequences will be much longer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Subword-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>We want a tokenization scheme that deals with an infinite potential vocabulary via
a finite list of known words. Make up the word ‚Äúunfortunately‚Äù via ‚Äúun‚Äù + ‚Äúfor‚Äù+
‚Äútun‚Äù + ‚Äúate‚Äù + ‚Äúly‚Äù.&lt;/li>
&lt;li>Subword tokenisation will break the text into chunks based on the word frequency.
In practice what happens is that common words will be tokenized generally as
whole words, e.g. ‚Äúthe‚Äù, ‚Äúat‚Äù, ‚Äúand‚Äù, etc., while rarer words will be broken
into smaller chunks and can be used to create the rest of the words in the
relevant dataset.&lt;/li>
&lt;li>BPE(Byte Pair Encoding): One popular algorithm for subword tokenisation which
follows the above approach is BPE. BPE was originally used to help compress data
by finding common byte pair combinations. It can also be applied to NLP to find
the most efficient way of representing text.
&lt;ul>
&lt;li>What is merging?
The main goal of the BPE subword algorithm is to find a way to represent
your entire text dataset with the least amount of tokens. Similar to a
compression algorithm, you want to find the best way to represent your image,
text or whatever you are encoding, which uses the least amount of data, or
in our case tokens. In the BPE algorithm merging is the way we try and
‚Äúcompress‚Äù the text into subword units.&lt;/li>
&lt;li>There are a few steps to these merging actions:
&lt;ol>
&lt;li>Get the word &lt;strong>count&lt;/strong> frequency&lt;/li>
&lt;li>Get the &lt;strong>initial token count&lt;/strong> and frequency (i.e., how many times each
character occurs)&lt;/li>
&lt;li>Merge the &lt;strong>most common byte pairing&lt;/strong>&lt;/li>
&lt;li>Add this to the list of tokens and &lt;strong>recalculate the frequency count&lt;/strong>
for each token (this will change with each merging step)&lt;/li>
&lt;li>&lt;strong>Rinse and repeat&lt;/strong> until get reached pre-defined token limits (vocab
size) or a set of number of iterations&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Greedy algorithm: BPE ensures that the most common words will be represented in
the new vocabulary as a single token, while less common words will be broken
down into two or more subword tokens. To achieve this, BPE will go through every
potential option at each step and pick the tokens to merge based on the highest
frequency.One downside of BPE‚Äôs greedy approach is it can result in a potentially
ambiguous final token vocabulary.
For instance GPT has a vocabulary size of 40,478 since they have 478 base
characters and chose to stop training after 40,000 merges.&lt;/li>
&lt;li>BBPE(byte-level PBE): A base vocabulary that includes all possible base characters
can be quite large if e.g. all unicode characters are considered as base
characters. To have a better base vocabulary, GPT-2 uses bytes as the base
vocabulary, which is a clever trick to force the base vocabulary to be of size
256 while ensuring that every base character is included in the vocabulary. With
some additional rules to deal with punctuation, the GPT2‚Äôs tokenizer can
tokenize every text without the need for the &lt;unk> symbol. GPT-2 has a
vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a
special end-of-text token and the symbols learned with 50,000 merges.
&lt;a href="https://huggingface.co/docs/transformers/en/tokenizer_summary#byte-pair-encoding-bpe">from hf doc&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="probabilistic-subword-tokenization" >
&lt;div>
&lt;a href="#probabilistic-subword-tokenization">
##
&lt;/a>
Probabilistic Subword Tokenization
&lt;/div>
&lt;/h1>
&lt;p>Using the frequency of subword patterns for tokenization can result in ambiguous final
encodings. The problem is that we have no way to predict which particular token is more
likely to be the best one when encoding any new input text.
Luckily, needing to predict the most likely sequence of text is not a unique problem to
tokenization. We can leverage this knowledge to build a better tokenizer.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Unigram Subword Tokenization&lt;/p>
&lt;ul>
&lt;li>The goal for a subword model, however, is different from a LM that is trying to
predict a full sentence. We only want something that generates unambiguous
tokenization.&lt;/li>
&lt;li>The unigram approach differs from BPE in that it attempts to choose the most
likely option rather than the best option at each iteration. To generate a
unigram subword token set you need to first define the desired final size of
your token set and also a starting seed subword token set.&lt;/li>
&lt;li>You can choose the seed subword token set in a similar way to BPE and choose
the most frequently occurring substrings. Once you have this in place then
you need to:
&lt;ol>
&lt;li>Work out the probability for each subword token&lt;/li>
&lt;li>Work out a loss value which would result if each subwork token were to be
dropped. The loss is worked out via Expectation Maximization algorithm.&lt;/li>
&lt;li>Drop the tokens which have the largest loss value (e.g., the bottom 10%
or 20% of subword tokens based on their loss calculations).&lt;/li>
&lt;li>Repeat these steps until reach the desired final vocabulary size or there
is no change in token numbers after successive iterations.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>WordPiece (greedy approach tokenzier, BERT partner)
Think of WordPiece as an intermediary between the BPE approach and the unigram approach.&lt;/p>
&lt;ul>
&lt;li>BPE, if you remember, takes two tokens, looks at the frequency of each pair and then
merges the pairs that have the highest combined frequency count. It only considers
the most frequent pair combinations at each step, nothing else.&lt;/li>
&lt;li>An alternate approach is to check the potential impact of merging that particular
pair. You can do this using the probabilistic LM approach. At each iterative step,
choose the character pair which will result in the largest increase in likelihood
once merged. This is the difference between the probability of the new meged pair
occurring minus the probability of both individual tokens occurring individually.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The main difference is that WordPiece is a greedy approach. It still tries to build a
tokenizer from the bottom up, picking the best pair at each iteration to merge.
WordPiece uses the likelihood rather than count frequency but otherwise it is a similar
approach. Unigram in contrast is a fully probabilistic approach which uses probability
to both choose the pairs to merge and whether to merge them or not. It also removes
tokens based on the fact that they add the least to the overall likelihood of the
unigram model.&lt;/p>
&lt;h1 id="briefly-summarize" >
&lt;div>
&lt;a href="#briefly-summarize">
##
&lt;/a>
briefly summarize:
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>BPE: Just uses the frequency of occurrences to identify the best match at every
iteration until it reaches the predefined vocabulary size.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>WordPiece: Similar to BPE and uses frequency occurrences to identify potential
merges but makes the final decision based on the likelihood of the merged token&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Unigram: A fully probabilistic model which does not use frequency
occurrences. Instead, it trains a LM using a probabilistic model, removing
the token which improves the overall likelihood the least and then starting
over until it reaches the final token limit.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="sentencepiece" >
&lt;div>
&lt;a href="#sentencepiece">
##
&lt;/a>
SentencePiece
&lt;/div>
&lt;/h1>
&lt;p>SentencePiece basically tries to bring all the subword tokenization tools and techniques
under one banner. It‚Äôs kind of like the Swiss Army knife for subword tokenization. To be
a Swiss Army-like tool something has to be capable of solving multiple problems. So what
problems is SentencePiece addressing:&lt;/p>
&lt;ol>
&lt;li>All other models assume input is already tokenized: BPE and Unigram are great model
but they share one big disadvantage: they both need to have their input already
tokenized. SentencePiece deals with this by simply taking in an input in raw text and
then doing everything needed on that input to perform subword tokenization.&lt;/li>
&lt;li>Language agnostic: Since all other subword algorithms need to have their input
pre-tokenized, it limits their applicability to many languages.&lt;/li>
&lt;li>Decoding is difficult: Another problem which is caused by model like BPE and unigram
requiring already tokenized inputs is that you do not know what encoding rules were
used. For example, how were spaces encoded in the tokens? So you cannot decode the
input and return it to is original format.&lt;/li>
&lt;li>No end to end solution: You cannot just plug in a raw input to BPE (or Unigram) and
get an output.&lt;/li>
&lt;/ol>
&lt;p>Some of the techniques SentencePiece uses to address the above shortcomings:&lt;/p>
&lt;ol>
&lt;li>Encode everything as unicode: SentencePiece first converts all the input into unicode
characters. This makes it a language agnostic tool.&lt;/li>
&lt;li>&amp;ldquo;space&amp;rdquo; encoded as &amp;ldquo;_&amp;quot;(U+2581): To get around the word segmenting issues.&lt;/li>
&lt;li>And it&amp;rsquo;s faster: One of the issues preventing other subword algorithms from being used
to tokenize raw sentences as part of model training was that there lack of speed. If
you processed input in real time and performed your tokenization on the raw input it
would be too slow. SentencePiece addresses this by using a priority queue for the BPE
algorithm to speed it up so that you can use it as part of an end-to-end solution.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;a href="https://www.openteams.com/tokenizers-how-machines-read/">https://www.openteams.com/tokenizers-how-machines-read/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>TODO: Ë°•ÂÖÖ‰∏≠Êñá&lt;/p></description></item><item><title>test rendering of equations of latex in Hugo</title><link>/posts/hugo101/test_math_equation_rendering/</link><pubDate>Fri, 25 Aug 2023 13:49:40 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/hugo101/test_math_equation_rendering/</guid><description>&lt;p>ÂºèÂ≠êËÉΩÊ≠£Â∏∏Ê∏≤ÊüìÔºå‰ΩÜÈúÄË¶ÅÂú®‰∏§‰∏™Âú∞ÊñπÁöÑ‰∏ãÊ†áÂ§ÑÊ∑ªÂä†È¢ùÂ§ñËΩ¨‰πâÂ≠óÁ¨¶ &lt;code>\&lt;/code>Ôºå
‰πüÂ∞±ÊòØÔºö&lt;code>\mathbb{E}\_{xxx}&lt;/code>Âíå&lt;code>p\_{model}&lt;/code>ÔºåÂÖ∂‰ªñÂú∞ÊñπÂç¥‰∏çÈúÄË¶Å &lt;code>\hat{p}_{data}&lt;/code>&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÊòØÂõ†‰∏∫ &lt;code>\mathbb{}&lt;/code> ÂØºËá¥ÁöÑÂêóÔºüÁúãËµ∑Êù•‰∏çÊòØÔºö
$$
E_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÊòØÂõ†‰∏∫ &lt;code>\text{}&lt;/code> ÂØºËá¥ÁöÑÂêóÔºüÁúãËµ∑Êù•‰∏çÊòØ (&lt;code>\text{log}&lt;/code> -&amp;gt; &lt;code>\log&lt;/code>)Ôºö
$$
\mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Â¶ÇÊûú &lt;code>\mathbb{}_&lt;/code> Ê∏≤Êüì‰∏çÂá∫Êù•ÔºåÈÇ£ÂÆÉÊ∏≤ÊüìÂ§±Ë¥•ÊòØÊÄé‰πàÊ†∑ÁöÑ (‰∏ãÂºè&lt;code>\mathbb{}_&lt;/code>‰∏≠ÁöÑ&lt;code>_&lt;/code>‰∏çËßÅ‰∫Ü)Ôºö&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>‰∏∫‰ªÄ‰πà &lt;code>p_{model}&lt;/code> ‰πüÊ∏≤Êüì‰∏çÂá∫Êù•Ôºå ËÄåÈùûË¶ÅÊ∑ªÂä†ËΩ¨ÊÑèÂ≠óÁ¨¶ &lt;code>p\_{model}&lt;/code> Âë¢Ôºü&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
E_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÊàñËÆ∏ÊòØÂµåÂ•ó‰∏ãÊ†áËøôÂÑøÂá∫ÁöÑÈóÆÈ¢ò (&lt;code>\hat{p}_{data}&lt;/code> -&amp;gt; &lt;code>\hat{p}\_{data}&lt;/code> )Ôºü:
$$
E_{x \sim \hat{p}_{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Bingo?? No bingo:&lt;/p>
&lt;p>Âè™Âú®ÂµåÂ•ó‰∏ãÊ†áÁöÑÂú∞ÊñπÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶Ôºö&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Âè™Âú®&lt;code>p_{model}&lt;/code>‰∏ãÊ†áÁöÑÂú∞ÊñπÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶Ôºö&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Âè™Âú®&lt;code>\mathbb{}&lt;/code>‰∏ãÊ†áÁöÑÂú∞ÊñπÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶Ôºö&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÁÆÄËÄåË®Ä‰πãÔºåÊâæ‰∏çÂà∞ÊòéÁ°ÆÁöÑËØ≠Ê≥ïËßÑÂàôÊù•Ëß£ÈáäËøô‰∏™ÈóÆÈ¢ò„ÄÇÂèØËÉΩÊòØhugoÂú®‰ΩøÁî®mathjaxËß£ÊûêÊó∂Âá∫‰∫ÜÈóÆÈ¢òÔºå‰πüÊúâÂèØËÉΩÊòØmathjaxËá™Â∑±ÁöÑÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;p style="text-align:center;color:blue;">
HugoÁöÑmarkdownÊñáÊ°£ÈáåÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÂùóÔºåÂ¶ÇÊûú‰∏çÁ°ÆÂÆöÔºåÂ∞±Âú®‰∏ãÊ†áÈÇ£ÈáåÂâçÁΩÆ‰∏Ä‰∏™`\`„ÄÇ&lt;br>
ÂêåÊ†∑ÁöÑÈóÆÈ¢ò‰πüÂá∫Áé∞Âú®Â§öË°åÂÖ¨ÂºèÁªÑÈáåËøõË°åËΩ¨Ë°åÊó∂Ôºå‰ΩøÁî® `\\\\\`ÔºåËÄå‰∏çÊòØÈÄöÂ∏∏ÁöÑ `\\\`
&lt;/p>
&lt;p>ÊÉπÂá∫Ëøô‰πàËÆ∏Â§ö‰∏çÊòéÊâÄ‰ª•ÁöÑÈóÆÈ¢ò, ÊúÄÂàùÂ∞±ÊòØ‰∏ãÈù¢Ëøô‰∏™ÂÖ¨Âºè (Â∞±ÊòØÊàëÊâÄËÆ§Áü•ÁöÑ„ÄÅÈÄöÂ∏∏Ë°®Ëææ‰∏ãÊ†áÁöÑÊôÆÈÄöÊñπÂºèÔºå
ËÆΩÂà∫ÁöÑÊòØÔºö‰∏çÊääÂÆÉ‰ª¨ÊîæÂú® &lt;code>$$&lt;/code> Êï∞Â≠¶ÂÖ¨ÂºèÂùóÈáåÔºåÊàëËøòÂøÖÈ°ªÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶ÔºåÂê¶ÂàôÊ∏≤Êüì‰∏çÂá∫Ëøô‰∏™ÊïàÊûúüòÇ):&lt;/p>
&lt;p>\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}&lt;/p>
&lt;p>Êï∞Â≠¶ÂÖ¨ÂºèÂùóÈáåË°®Ëææ‰∏ãÊ†áÁöÑËØ≠Ê≥ïÔºåÊó†Á´ØÁ´ØË¶ÅÊ∑ªÂä†‰∏Ä‰∏™ÂâçÁΩÆËΩ¨‰πâÂ≠óÁ¨¶ÔºåËøôÂ∞±ÊòØÊàëÊâÄÊ≤°ÊúâÊñôÂà∞ÁöÑ„ÄÇÊÉ≥ÂøÖÂ∫îËØ•ÊòØHugoÊ≤°ÊúâÂÅöÂ•ΩÂÖ≥‰∫éËΩ¨‰πâÂ≠óÁ¨¶ÁöÑ‰∏ä‰∏ãÊñáËß£ÊûêÈóÆÈ¢òÔºå‰∏îÂÖàÂ∞ÜÂ∞±ÁùÄÂêß„ÄÇ&lt;/p></description></item><item><title>Âà©Âô®4: Vim ÁºñËæëÂô® -- Âè™Âõ†‰∏∫Âú®ÈîÆÁõò‰πã‰∏≠ÔºåÊñπÂêëÈîÆÂ§™ÈÅ•Ëøú</title><link>/posts/thecli/thevim/2022-08-13-vim8/</link><pubDate>Sat, 13 Aug 2022 21:37:59 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/thevim/2022-08-13-vim8/</guid><description>&lt;h2 id="vim-Âàù‰ΩìÈ™å" >
&lt;div>
&lt;a href="#vim-%e5%88%9d%e4%bd%93%e9%aa%8c">
#
&lt;/a>
Vim Âàù‰ΩìÈ™å
&lt;/div>
&lt;/h2>
&lt;p>‚ÄúÂ§öÂπ¥‰ª•ÂêéÔºåÈù¢ÂØπÊòæÁ§∫Â±èÔºå&amp;ldquo;ÈÄÉÁ¶ª‰∫ÜÂ∑®Á°¨¬∑‰ªçÊóßÊÑ§ÊÄí¬∑ÊáµÊáÇËèúÈ∏ü&amp;quot;Â∞Ü‰ºöÂõûÊÉ≥Ëµ∑ÂàùÊ¨°Â∞ùËØï Vim ÁºñËæëÂô®
ËÄåË¢´Âõ∞Âú®ÈáåÈù¢Âè´Â§©‰∏çÂ∫îÂè´Âú∞‰∏çÁÅµÁöÑÈÇ£‰∏™ÈÅ•ËøúÁöÑ‰∏ãÂçà‚Äù„ÄÇ&lt;/p>
&lt;p>ÈÇ£Êó∂ÂÄôÁöÑÊàëÔºà2017ÔºâÔºåÊòØ‰∏™‰∫åÂçÅÂ§öÂ≤ÅÁöÑÈùíÂπ¥ÔºåÂàöÂàöËÆ§ËØÜÁªàÁ´ØÂíå Linux Â§ßÊ¶ÇÊòØÊÄé‰πà‰∏ÄÂõû‰∫ãÔºå
ÊÑüËßâÊÅ∞‰ºº‰∏äÂ§©ÁªôÊàëÂÖ≥‰∫ÜÈÇ£Êâá‚ÄúÁ™ó‚ÄùÔºåËá™Â∑±Êé®ÂºÄ‰∫Ü Linux ‰∏ñÁïåÁöÑÂ§ßÈó®„ÄÇ
‰∏ñÁïåÊñ∞Áîü‰ºäÂßãÔºåËÆ∏Â§ö‰∫ãÁâ©Ëøò‰∏çÁü•ÂÖ∂Âêç‰ΩïÂ§ÑÔºåÂØπËÆ∏Â§ö‚ÄúÊãøÊù•‰∏ª‰πâ‚ÄùÁöÑÂëΩ‰ª§‰ªçÊóßÂçä‰ø°ÂçäÁñëÔºå
ÁÆÄÁõ¥Â∞±ÊòØ‰∏ÄÂú∫Â§ßÂÜíÈô© (Ê≤°ÊúâÊÄßÂëΩ‰πãÊî∏ÁöÑÈÇ£Áßç)„ÄÇ&lt;/p>
&lt;p>ÂΩìÊàëÂèëÁé∞Âú®ÊñáÊú¨ÁºñËæëÂô®ÈáåÁöÑÂÖâÊ†áÂèØ‰ª•Áî® h,j,k,l Êù•ËøõË°åÂ∑¶‰∏ã‰∏äÂè≥ÁßªÂä®Êó∂Ôºå‚Äú‰∏çÊáÇÔºå‰ΩÜÊàëÂ§ßÂèóÈúáÊíº‚Äù„ÄÇ&lt;/p>
&lt;p>ÔºàÂÖ∂ÂÆûÂ∞±ÊòØËßÅËØÜÂ∞ëÔºå‰∏çÊòØÊâÄÊúâÈîÆÁõòÈÉΩÊúâÁã¨Á´ãÁöÑÊñπÂêëÈîÆÔºåÊõ¥‰∏çÊòØÊìç‰ΩúÁ≥ªÁªüÂ∞±Âè™‰∏Ä‰∏™ windows
Ôºå‰∏çËøáÊàëÊòØ‰∫åÂçÅÂ•ΩÂá†ÁöÑ‰∫∫‰∫ÜÊâçÁü•ÈÅìËøô‰∏™‰∫ãÊÉÖÔºåÁî®‰∫éËøûÁöÑËØùËÆ≤Â∞±ÊòØÔºö‚ÄúÂïäÔºåËøôÊòØ‰∏ÄÂú∫‰æÆËæ±‚Äù„ÄÇÔºâ&lt;/p>
&lt;p>ËôΩÁÑ∂Á¨¨‰∏ÄÊ¨°Â∞ùËØï Vim Â∞±Ë¢´ÊäõÂºÉÂà∞‰∫ÜËçíÂ≤õÔºåÂ•ΩÂú®ÊàëÂØπËøôÊ†∑‰∏Ä‰∏™Êñ∞Â•áÁöÑ‰∏ñÁïåÔºåÊó©ÊúâÂøÉÁêÜÂáÜÂ§á„ÄÇ
‰∫ãÂÆûËØÅÊòéÔºåË¶ÅÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÔºåÊúÄÂ•ΩÁöÑÊ≠¶Âô®‰∏çËøáÊòØ‚Äú‰∏ÄÁÇπÂ•ΩÂ•áÂøÉÂíåÂ§öÁÇπËÄêÂøÉ‚Äù„ÄÇ‰ªéÈÇ£‰ª•ÂêéÔºå
ÊÑüËßâÂ∞±Â•ΩÂÉè‚ÄúÂ±Ö‰ΩèÂú®ÂÖîÂ≠êÁöÆÊØõÊ∑±Â§ÑÁöÑÂæÆÁîüÁâ©‚ÄùÁ´≠ÂäõÂêëÊØõÂ∞ñÁà¨ÂéªÔºå‚Äú‰ª•‰æøÂ∞ÜÈ≠îÊúØÂ∏àÁúã‰∏™Ê∏ÖÊ•ö‚Äù„ÄÇ&lt;/p>
&lt;p>ÁªàÁ´ØÈáåËÉΩÁî®ÁöÑ‰ºòÁßÄÂ∑•ÂÖ∑Â§öÂæóËÆ©ÊàëËßâÂæóÁÆÄÁõ¥‚ÄúÂ∞±ÂÉèÊòØÂú®Êµ∑ËæπÊã£Ë¥ùÂ£≥ÁöÑÂ≠©Â≠ê&amp;rdquo;ÔºåËÄå Vim ÁºñËæëÂô®Êó†ÁñëÊòØÊàëÊúÄÂñúÁà±ÁöÑÈÇ£‰∏ÄÊûö„ÄÇ
ËÄåÂú®Êµ∑ËæπÊîæË¥ùÂ£≥ÁöÑÊó†ÁñëÊòØÈÇ£‰∫õ‰πê‰∫éÂàÜ‰∫´ÁöÑÂêåÈÅì‰∏≠‰∫∫ÔºåÊàëÁõ∏‰ø°ÁúüÊ≠£ÁöÑËøõÊ≠•Êù•Ëá™‰∫éÂàÜ‰∫´„ÄÇ&lt;/p>
&lt;p>ÂêéÊù•ÂΩìÊàëÂ§ßÊ¶ÇÊòéÁôΩ vimrc Êñá‰ª∂ÊòØÊÄé‰πà‰∏ÄÂõû‰∫ã‰πãÂêéÔºåÊàëÁ¨¨‰∏Ä‰ª∂‰∫ãÂ∞±ÊòØÁªôÂÆÉÂä†‰∏ä‰∫ÜËøô‰∏ÄÂè•:&lt;/p>
&lt;p>&lt;code>set mouse=&lt;/code>&lt;/p>
&lt;p>ÂÖ∂ÂÆûÂΩìÊó∂ÊòØÊÄÄÁùÄÂØπÊóßÊó•ÈÇ£ÁßçÂá°‰∫ãÂøÖÂæó‚ÄúÈº†Ê†áÁÇπÂáªÊãñÊãΩ‚ÄùÂÆ£ÊàòÁöÑÂøêÂøëÂøÉÊÄÅ„ÄÇ&lt;/p>
&lt;p>ÂêéÊù•ÊÄé‰πàÊ†∑‰∫ÜÔºü&lt;/p>
&lt;p>ÂêéÊù•ÊàëÂèëÁé∞ÔºåÂÖ∂ÂÆû‰∏çÊÄé‰πàÈúÄË¶Å‚Äú‰∏ìÈó®ËÖæÂá∫‰∏ÄÂè™ÊâãÁî®Èº†Ê†áÊìçÊéß‚Äù„ÄÇ
Â¶ÇÊûúÊòØÂú®ÁªàÁ´ØÂπ≤Ê¥ªÔºå‰Ω†‰πêÊÑèÁöÑËØùÔºåÈÇ£Â∞±ÊòØÔºöÊ†πÊú¨‰∏çÈúÄË¶Å„ÄÇ
Â¶ÇÊûú‰Ω†ÊûÅÁ´ØÁÇπÔºåÂú®Âì™ÈáåÈÉΩ‰∏çÈúÄË¶ÅÈº†Ê†á„ÄÇ
Èô§ÈùûÊòØ‰Ω†Âè¶‰∏ÄÂè™ÊâãÂú®ÂøôÁÇπÂà´ÁöÑ‰ªÄ‰πàÔºåÊàñËÄÖÊòØËÄÅÊùøË¶Å‚ÄúÈ∫ªÁÉ¶‚Äù‰Ω†Áªô‚ÄúËµ∂Á¥ß‚Äù‰øÆÊîπ‰∏ÄÁØáworkÊñáÊ°£ÔºåÂÆ¢Êà∑Âú®Á∫øÁ≠âÈÇ£Áßç&amp;hellip;&lt;/p>
&lt;h2 id="vim-ÁºñËæëÂô®" >
&lt;div>
&lt;a href="#vim-%e7%bc%96%e8%be%91%e5%99%a8">
#
&lt;/a>
Vim ÁºñËæëÂô®
&lt;/div>
&lt;/h2>
&lt;h3 id="vim-ÊòØÂπ≤Âï•ÁöÑ" >
&lt;div>
&lt;a href="#vim-%e6%98%af%e5%b9%b2%e5%95%a5%e7%9a%84">
##
&lt;/a>
Vim ÊòØÂπ≤Âï•ÁöÑ
&lt;/div>
&lt;/h3>
&lt;p>È¶ñÂÖàÂ∫îËØ•‰∫ÜËß£Ôºö Vim ÈÄÇÁî®‰∫é‰ªÄ‰πàÂú∫ÊôØÔºü&lt;/p>
&lt;blockquote>
&lt;p>‚ÄúVimÊòØ‰ªé vi ÂèëÂ±ïÂá∫Êù•ÁöÑ‰∏Ä‰∏™ÊñáÊú¨ÁºñËæëÂô®„ÄÇ
ÂÖ∂‰ª£Á†ÅË°•ÂÆå„ÄÅÁºñËØëÂèäÈîôËØØË∑≥ËΩ¨Á≠âÊñπ‰æøÁºñÁ®ãÁöÑÂäüËÉΩÁâπÂà´‰∏∞ÂØåÔºåÂú®Á®ãÂ∫èÂëò‰∏≠Ë¢´ÂπøÊ≥õ‰ΩøÁî®„ÄÇ
ÂíåEmacsÂπ∂ÂàóÊàê‰∏∫Á±ªUnixÁ≥ªÁªüÁî®Êà∑ÊúÄÂñúÊ¨¢ÁöÑÁºñËæëÂô®„ÄÇ‚Äù&lt;/p>
&lt;/blockquote>
&lt;p>Âπ≥Êó∂ÊàëÂú®ÊµèËßà‰∏≠ÔºàÂ∞§ÂÖ∂ÊòØÂØªÊâæ‰∏Ä‰∫õÂäüËÉΩÊèí‰ª∂ËøáÁ®ãÔºâ‰πüÁúãÂà∞Êúâ‰∫∫ÂèëÂçöÂÆ¢ËØ¥‰ªñ/Â•πÊòØÁî® Vim Ëøõ
Ë°å‚ÄúÂÜô‰Ωú‚ÄùÁöÑÔºà‰∏ªË¶ÅÊòØÂÖ≥‰∫é $\LaTeX$, $\TeX$, markdown ÁöÑÊèí‰ª∂ÔºâÔºå‰ΩÜÊàëÊÉ≥Â¶ÇÊûúÊòØÁî®Êù•Ëøõ
Ë°åÊñáÂ≠¶Âàõ‰ΩúËøô‰∏ÄÁ±ªÁöÑÂ∑•‰ΩúÁöÑËØùÔºåÈáçÁÇπÂèØËÉΩ‰∏çÂú®‰∫éÁºñËæëÂô®ÔºåÂæàÁÆÄÂçïÔºåËøòÊ≤°Êúâ‚ÄúÁîµËÑë‚ÄùÁöÑÊó∂‰ª£Â∞±
Â∑≤ÁªèËØûÁîü‰∫ÜÂ§™Â§ö‰ºòÁßÄ‰ΩúÂìÅ„ÄÇ
ÔºàÊÉ≥Ë±°Â§ß‰Ω¨ÊõπÈõ™ËäπÈù¢ÂØπÁîµËÑëÂ±èÂπï‚ÄúÂçÅÂπ¥Âà†Êîπ‚Äù„ÄäÁ∫¢Ê•ºÊ¢¶„ÄãÊÄªËßâÂæóÊúâÁÇπËçíËØûÔºå
‰ΩÜÊòØÂ∞±ÁÆóÊòØÊúâÁîµËÑëÔºåËøôÈöæÁÇπËÇØÂÆö‰∏çÂú®‰∫éÂà†ÊîπÔºàÁºñËæëÂô®Êó†ÂÖ≥Á¥ßË¶ÅÔºâÔºåËÄåÂú®‰∫éÊñáÂ≠¶ÂàõÈÄ†„ÄÇ
ÊûÅÊúâÂèØËÉΩ‰ªñÊ†πÊú¨‰π∞‰∏çËµ∑ÁîµËÑë„ÄÇÔºâ&lt;/p>
&lt;p>ÂèçËøáÊù•Âç¥‰∏ç‰∏ÄÊ†∑ÔºåËá≥Â∞ëÊàë‰ª¨ËøôÂπ¥‰ª£ÔºåÊ≤°ÊúâÂê¨ËØ¥Âì™‰∏™ÊàêÂäüÁöÑËΩØ‰ª∂‰∫ßÂìÅÔºåÂÆÉÁöÑ‰ª£Á†ÅÊòØÁî®ÊâãÂÜôÂú®ËçâÁ®øÁ∫∏‰∏ä &amp;hellip;
ÊÄ™ËØûÁöÑÊòØÔºåÈù¢ËØïÁöÑÊó∂ÂÄôÂÄíÊòØÊúâ‰∏çÂ∞ëËøôÁßçË¶ÅÊâãÂÜô‰ª£Á†ÅÂïä‰º™‰ª£Á†ÅËøôÁßç‰∫ãÊÉÖÔºåÊàëËßâÂæóËøôÂ§öÂ§öÂ∞ëÂ∞ëÊúâÁÇπÈªëËâ≤ÂπΩÈªò„ÄÇ&lt;/p>
&lt;p>ÁÆÄËÄåË®Ä‰πãÔºåvimÊòØ‰∏™ÁºñËæëÂô®ÔºåÂæàÂ§ö‰∫∫Áî®ÂÆÉÊù•ÂÜô‰ª£Á†ÅÔºå‰πüÊúâ‰∫∫Áî®ÂÆÉÊù•Âπ≤ÂÖ∂‰ªñ„ÄÇ&lt;/p>
&lt;p>ÈöèÁùÄ‰∫ÜËß£Ê∑±ÂÖ•ÔºåÊÖ¢ÊÖ¢Â∞±‰ºöÊé•Ëß¶Âà∞ËØ∏Â¶Ç‚ÄúÁºñËæëÂô®‰πãÊàò‚Äù„ÄÅ‚ÄúÁºñÁ®ãËØ≠Ë®Ä‰πã‰∫â‚ÄùËøô‰∏ÄÁ±ªËØùÈ¢ò„ÄÇ&lt;/p>
&lt;h3 id="ÁºñËæëÂô®‰πãÊàò" >
&lt;div>
&lt;a href="#%e7%bc%96%e8%be%91%e5%99%a8%e4%b9%8b%e6%88%98">
##
&lt;/a>
ÁºñËæëÂô®‰πãÊàò
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>‚ÄúÁºñËæëÂô®‰πãÊàòÊòØÊåáËÆ°ÁÆóÊú∫ÁºñÁ®ãÈ¢ÜÂüüÂÜÖÔºå
‰∏§Á±ªÊñáÊú¨ÁºñËæëÂô® viÔºà‰ª•ÂèäË°çÁîüÁâàÊú¨ÔºâÂíå Emacs Áî®Êà∑‰πãÈó¥ÁöÑÂØπÊäóÂÜ≤Á™Å„ÄÇ
ËøôÂ∑≤ÁªèÊàê‰∏∫ÈªëÂÆ¢ÊñáÂåñÂíåËá™Áî±ËΩØ‰ª∂Á§æÂå∫ÊñáÂåñÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;p>‰∏Ä‰∫õ Emacs Áî®Êà∑ËØ¥Ôºö&lt;/p>
&lt;blockquote>
&lt;p>Vi Êúâ‰∏§‰∏™Ê®°ÂºèÔºå‚Äú‰∏çÂÅúÂú∞ÂìîÂìîÂè´‚ÄùÂíå‚ÄúÊêûÁ†∏‰∏ÄÂàá‚Äù„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;p>ËÄåÊçÆ Vi Áî®Êà∑Áß∞Ôºö&lt;/p>
&lt;blockquote>
&lt;p>‰∏Ä‰∫õ Emacs Áî®Êà∑Áî±‰∫é‰øÆÊîπÈîÆËøáÂ§öËÄå‰ΩøÁî®ËÑöË∏èÊùø„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;p>‰∏Ä‰∫õÂçöÂÆ¢ÂçöÊñá„ÄÅÂΩ±ËßÜ‰ΩúÂìÅ‰πüÈÉΩ‰ºöÂá∫Áé∞Ëøô‰∫õÂÜÖÂÆπÂú∫ÊôØÔºåËÄå‰∏îÂ§ßÈÉΩÊòØËΩªÊùæÊ¨¢‰πêÁöÑÂú∫ÊôØ„ÄÇÊØîÂ¶ÇÔºå
„ÄäÁ°ÖË∞∑„ÄãÈáåÁêÜÊü•Âæ∑Âíå‰ªñ‚ÄúÁÆÄÁõ¥Â∞±ÊòØË∏©‰∫ÜÁãóÂ±éËøê‚ÄùÊâçÂãæÊê≠‰∏äÁöÑÊºÇ‰∫ÆÂ•≥ÂèãÂ∞±‰∏∫‰∫Ü Vim Âíå Emacs
„ÄÅ‰ΩøÁî® Tab ËøòÊòØ Space ËøõË°å‰ª£Á†ÅÁº©Ëøõ‰ª•ÂèäÁº©ËøõÈ£éÊ†ºËÄåÂêµ‰∫ÜËµ∑Êù•ÔºåÁªßËÄåÁõ¥Êé•ÈóπÊé∞Ôºå
ÊúÄÂêé‰ª•ÁêÜÊü•Âæ∑ÁßÄÈÄó‰∫ÜÁöÑÂú®Áî®Ë∫´‰ΩìÂäõË°åËøõË°å‚ÄúÁº©ËøõÊºîÁ§∫‚Äù‰∏≠Êªö‰∏ãÊ•ºÊ¢ØËÄåÂëäÁªà„ÄÇ
&amp;hellip;&lt;/p>
&lt;blockquote>
&lt;p>ÊàòÁÅ´Âú®‰∏§Áæ§Áî®Êà∑‰πãÈó¥Â§öÊ¨°ÁÇπÁáÉÔºå‰ªñ‰ª¨‰∫íÁõ∏ËîëËßÜÔºåÂùö‰ø°Ëá™Â∑±ÁöÑÈÄâÊã©ÊâçÊòØÊúÄÂÆåÁæéÁöÑ„ÄÇ
Áõ∏ÊØîÂÖ∂‰ªñÁöÑITÈ¢ÜÂüüÊàò‰∫âÔºàÊµèËßàÂô®Â§ßÊàò„ÄÅÊìç‰ΩúÁ≥ªÁªü‰πã‰∫â„ÄÅÁºñÁ®ãËØ≠Ë®Ä‰πã‰∫â„ÄÅ‰ª£Á†ÅÁº©ËøõÈ£éÊ†º‰πãÊàò
Á≠âÁ≠â)ÔºåÁºñËæëÂô®ÁöÑÈÄâÊã©ÂÖ∂ÂÆûÈÄöÂ∏∏Âè™ÊòØ‰∏™‰∫∫ÈóÆÈ¢ò„ÄÇ ‚Äù&lt;/p>
&lt;/blockquote>
&lt;p>Â¶ÇÊûú‰∏çÂ∞èÂøÉ‰πüÂç∑ÂÖ•‰∫ÜËøô‰∫õÊàò‰∫âÔºåËØ∑ËÆ∞‰ΩèÔºöÁºñËæëÂô®ÁöÑÈÄâÊã©ÂÖ∂ÂÆûÈÄöÂ∏∏Âè™ÊòØ‰∏™‰∫∫ÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;p>ÂΩìÁÑ∂ÔºåÂÆûÂú®ÂêµÂæó‰∏çÂèØÂºÄ‰∫§ÁöÑÊó∂ÂÄôÔºåÂà´Âøò‰∫ÜÊê¨Âá∫ÁºñÁ®ãËØ≠Ë®Ä‰πã‰∫âÈáåÈÇ£Âè•‰∏ÄÈî§ÂÆöÈü≥ÁöÑËØùÈ¢òÁªàÁªìËØ≠Ôºö
‚ÄúPHP ÊâçÊòØÁúüÂ•ΩÁöÑÁºñÁ®ãËØ≠Ë®ÄÔºå‰∏çÊé•Âèó‰ªª‰ΩïÂèçÈ©≥„ÄÇ‚Äù&lt;/p>
&lt;p>Â¶ÇÊûúÊÉ≥‰∫ÜËß£Êõ¥Â§öÔºåÂèØ‰ª•Âéª &lt;a href="https://zh.m.wikipedia.org/zh-hans/Vim">Vim Áª¥Âü∫ËØçÊù°&lt;/a>
‰∫ÜËß£Êõ¥Â§öÁ≤æÂΩ©ÂÜÖÂÆπ„ÄÇÂ¶ÇÊûú‰Ω†ÁîöËá≥Êó†Ê≥ïËÆøÈóÆÁª¥Âü∫ÁôæÁßëÔºåÈÇ£‰∏çÊòØ‰Ω†ÁöÑÈîôÔºå‰∏çËøá‰Ω†ÂæóÊÉ≥ÂäûÊ≥ï„ÄÇ&lt;/p>
&lt;h2 id="vim-ÂèäÂÖ∂ÈÖçÁΩÆ" >
&lt;div>
&lt;a href="#vim-%e5%8f%8a%e5%85%b6%e9%85%8d%e7%bd%ae">
#
&lt;/a>
vim ÂèäÂÖ∂ÈÖçÁΩÆ
&lt;/div>
&lt;/h2>
&lt;p>Â¶ÇÊûú‰Ω†ÊòØÊÉ≥ÂÖ•Èó®ËÄåËøòÂú®Èó®Âè£ÂæòÂæäÁöÑÔºåÂèØ‰ª•ÂÖàÂú®ÁªàÁ´Ø (Terminal) ÈáåËæìÂÖ• &lt;code>vimtutor&lt;/code>Ôºå
ÁÑ∂ÂêéÁúãÁúã‰Ω†ÊÑüËßâÂ¶Ç‰Ωï„ÄÇÂ¶ÇÊûú‰Ω†ËÆ§ÁúüËøΩÈöèÔºå‰Ω†‰ºöÂèëÁé∞‰∏ÄÂàáËá™ÊúâÈÅìÁêÜ„ÄÇË≤å‰ººËøòÊå∫Â•ΩÁé©„ÄÇ
ÂóØÔºåÂí¶ÔºåÊÄé‰πàÈÄÄÂá∫ËøôÁºñËæëÂô®Âë¢Ôºü&lt;/p>
&lt;h3 id="Áé∞‰ª£Á†Å‰ªîÂ¶Ç‰ΩïÂú®ÁªàÁ´ØÁïåÈù¢ÈÄÄÂá∫vimÁºñËæëÂô®" >
&lt;div>
&lt;a href="#%e7%8e%b0%e4%bb%a3%e7%a0%81%e4%bb%94%e5%a6%82%e4%bd%95%e5%9c%a8%e7%bb%88%e7%ab%af%e7%95%8c%e9%9d%a2%e9%80%80%e5%87%bavim%e7%bc%96%e8%be%91%e5%99%a8">
##
&lt;/a>
Áé∞‰ª£Á†Å‰ªîÂ¶Ç‰ΩïÂú®ÁªàÁ´ØÁïåÈù¢ÈÄÄÂá∫vimÁºñËæëÂô®
&lt;/div>
&lt;/h3>
&lt;p>Â¶ÇÊûú‰Ω†ÂíåÊàëÂΩìÂàù‰∏ÄÊ†∑‰∏ÄÂ§¥ÈõæÊ∞¥&amp;hellip; Ê∑°ÂÆöÊ∑°ÂÆöÔºåËæìÂÖ• &lt;code>:q&lt;/code> ÔºåÊåâ‰∏ã &lt;code>Enter&lt;/code> Âç≥ÂèØÈÄÄÂá∫„ÄÇ&lt;/p>
&lt;p>ÂóØÔºåÂ¶ÇÊûú‰Ω†Ê≠£Âú®ËæìÂÖ•ÔºåÈúÄË¶ÅÂÖàÊåâ‰∏ã &lt;code>Esc&lt;/code>ÔºåÁÑ∂ÂêéËæìÂÖ• &lt;code>:q&lt;/code> Êù•ÈÄÄÂá∫„ÄÇ&lt;/p>
&lt;p>Ëøô‰∏™Êìç‰Ωú‰πü‰∏çÂ•Ω‰ΩøÔºüÂÖ´Êàê‰Ω†ÊòØÂú®Âà´ÁöÑÊ®°ÂºèÈáåÔºåÊØîÂ¶Ç &lt;code>Ex mode&lt;/code>Ôºå
ÂÖ´ÊàêÂÆÉ‰πü‰ºöÊèêÁ§∫‰Ω†ËæìÂÖ• &lt;code>visual&lt;/code> Êù•ËøîÂõûÂà∞ &lt;code>normal mode&lt;/code>„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰∏çÊòØËøôÁßçÊÉÖÂÜµÔºåÊûÅÊúâÂèØËÉΩÊòØ‰Ω†‰∏çÂ∞èÂøÉÊåâ‰∫Ü &lt;code>q&lt;/code> ËøõÂÖ•ÂëΩ‰ª§ÂΩïÂà∂Ôºå
ÈÄöÂ∏∏Ë¶ÅÊòØ‰Ω†Â§öÊåâÂá†Ê¨° &lt;code>q&lt;/code> Êù•ËßÇÂØüÔºå‰ºöÂèëÁé∞Êúâ &lt;code>recording @&lt;/code> ÁöÑÊèêÁ§∫Ôºå
ËøôÂ∞±ÊòØÂ§öÂëΩ‰ª§ÂΩïÂà∂Êó†ÁñëÔºåÈáçÂ§çÊìç‰ΩúÔºöËæìÂÖ• &lt;code>:q&lt;/code> ÔºåÊåâ‰∏ã &lt;code>Enter&lt;/code> Âç≥ÂèØÈÄÄÂá∫„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúÈÉΩ‰∏çÊòØÔºåÂπ∂‰∏î‚Äú‰ªñXÁöÑÊåâ‰ªÄ‰πà‰πüÊ≤°Áî®‚ÄùÔºåÂÖàÊ£ÄÊü•‰∏Ä‰∏ã‰Ω†ÁöÑÊú∫Âô®ÊòØÂê¶ËøòÂú®Ê≠£Â∏∏Â∑•‰ΩúÔºå
ÊòØÔºåÂàôÈáçÂ§ç‰ª•‰∏ãÊìç‰ΩúÔºöËæìÂÖ• &lt;code>:q&lt;/code>ÔºåÊåâ‰∏ã &lt;code>Enter&lt;/code>„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÊâÄÊúâÂ•ΩÂ•áÂøÉËøòÊ≤°ÊúâË¢´Ê∂àËÄóÊÆÜÂ∞ΩÔºåÊàëÊù•ÂëäËØâ‰Ω†‰∏Ä‰∏™Â•ΩÊ∂àÊÅØÔºö
vim ÂÖ±Êúâ &lt;a href="https://en.wikipedia.org/wiki/Vim_(text_editor)">12ÁßçÊ®°Âºè&lt;/a>„ÄÇ&lt;/p>
&lt;p>ËøôÁ°ÆÂÆûÊòØ‰∏™Êñ∞‰∏ñÁïå„ÄÇ&lt;/p>
&lt;h2 id="vimrc" >
&lt;div>
&lt;a href="#vimrc">
#
&lt;/a>
vimrc
&lt;/div>
&lt;/h2>
&lt;p>Â∫îÂΩìÊ≥®ÊÑèÔºö‰ªéÁ¨¨‰∏ÄÊ¨°Âú®ÁªàÁ´ØËæìÂÖ• &lt;code>vim&lt;/code> Âà∞ÂºÄÂßã‰∫ÜËß£Âíå‰ΩøÁî®Ëá™Â∑±ÈÖçÁΩÆÁöÑ &lt;code>.vimrc&lt;/code>Ôºå
‰ªéÂâçËÄÖÂà∞ÂêéËÄÖÔºåÊØè‰∏™‰∫∫ÊâÄÁªèÂéÜÁöÑÊó∂Èó¥‰∏çÂ§™‰∏ÄÊ†∑ÔºåÊØïÁ´üÊØè‰∏™‰∫∫ÁöÑÁîüÊ¥ªÂíåÁõÆÁöÑÂçÉÂ∑Æ‰∏áÂà´„ÄÇ
ÔºàÂÄí‰∏çÊòØËØ¥ÔºåÂàöÂºÄÂßã‰ΩøÁî® vim Â∞±‰∏çËÉΩÁõ¥Êé•‰∏äÊâã‰∏™‰∫∫ÈÖçÁΩÆÔºåËÄåÊòØÊÉ≥ËØ¥ÊòéÔºå
‚ÄúÊ≠•Â≠êËøàÂ§ß‰∫ÜÔºåÂÆπÊòìÊâØÁùÄËõã‚ÄùÔºõ‰∏ÄÊó¶ÊîæÂºÉ‰∫ÜÊé¢Á¥¢ÂíåÊÄùËÄÉÔºåÂÆπÊòì‰∏ÄÁ¢∞Êå´ÊäòÂ∞±ÂçÅÂàÜÊ≤Æ‰∏ß„ÄÇÔºâ
‰ΩÜÂú®ÂæàÂ§öÊñáÁ´†ÈáåÔºåËøô‰∏§ËÄÖ‰πãÈó¥Â§öÂçäÂ∞±ÊòØ‰∏Ä‰∏™Á©∫Ë°åÁöÑË∑ùÁ¶ª„ÄÇ
ÔºàÂõ†Ê≠§ÊàëÂÜ≥ÂÆöÂú®Ëøô‰∏™Á©∫Ë°å‰πãÂâçÊ∑ªÂä†‰∏ÄÁÇπËØ¥ÊòéÔºå‰ª•‰∏ãÁöÑÂÜÖÂÆπÊñ∞ÊâãÂàùÁúãÊó∂‰∏çÂ§™ÊòéÁôΩÂÆûÂ±ûÊ≠£Â∏∏Ôºâ&lt;/p>
&lt;p>Vim Â¶ÇÊ≠§ÂèóÊ¨¢ËøéÔºåÂÆÉÁöÑ‰∏∞ÂØåÁöÑÊèí‰ª∂ÁîüÊÄÅÂ∫îËØ•ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂéüÂõ†„ÄÇ‰∏∫‰ªÄ‰πàÊúâËøô‰πàÂ§öÊèí‰ª∂Âë¢Ôºü
Âõ†‰∏∫ÊúâËøô‰πàÂ§öÁöÑÈúÄÊ±Ç„ÄÇÂõ†‰∏∫ÊúâÈÇ£‰πàÂ§öÁöÑÊèí‰ª∂ÔºåÊâÄ‰ª•Êúâ‰∫ÜË¶ÅÁÆ°ÁêÜÊèí‰ª∂ÔºàÊõ¥Êñ∞„ÄÅÂà†Èô§Á≠âÔºâÁöÑ
Êèí‰ª∂„ÄÇ‰ΩøÁî®ÊØîËæÉÂπøÊ≥õÁöÑÊúâÔºö&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/junegunn/vim-plug">vim-plug&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/VundleVim/Vundle.vim">Vundle&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/tpope/vim-pathogen">Pathogen&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/k-takata/minpac">minpac&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ÂÆÉ‰ª¨ÁöÑÂÆâË£ÖÂíå‰ΩøÁî®Êù°‰ª∂ÈÉΩÂú®ÂêÑËá™ÁöÑ‰∏ªÈ°µÊúâËØ¶ÁªÜËØ¥ÊòéÔºåËøôÈáå‰∏çÂÜçÊê¨Ëøê„ÄÇÈÇ£Âà∞Â∫ïÁî®Âì™‰∏™Âë¢Ôºü
‰Ω†Âú®ÈòÖËØª‰ªñ‰ª¨ÁöÑËØ¥ÊòéÁöÑÊó∂ÂÄôËá™Â∑±Â∞±‰ºöÊúâÁ≠îÊ°àÔºåÂ¶ÇÊûúËøòÊòØÈöæ‰ª•ÂÜ≥ÂÆöÔºåÊàëÁöÑÂª∫ËÆÆÂ∞±ÊòØÂÉèÂÖ´Êàí
ÊâÄËØ¥ÁöÑÈÇ£Ê†∑Ôºö‚ÄúÂæÖ‰ø∫ËÄÅÁå™‰∏ÄÂÆ∂ÂÆ∂ÂêÉÂ∞ÜÂéªÔºÅ‚Äù„ÄÇÔºàÊàëËá™Â∑±ÂòõÔºåÂêÉËøá‰∫Ü‰∏âÂÆ∂Ôºâ&lt;/p>
&lt;p>‰ªé vim8.0 ÂºÄÂßãÔºåvim Ëá™Â∏¶Êèí‰ª∂ÁÆ°ÁêÜÁöÑÂäüËÉΩÁâπÊÄßÔºàÂÆûÈôÖ‰∏äÔºåÂâçËø∞ÁöÑ &lt;code>minpac&lt;/code> Â∞±Âà©Áî®‰∫Ü
Ëøô‰∫õÁâπÊÄßÔºâÔºåÊàë‰ªÖÂú®Ëøô‰∏™ÂäüËÉΩÂá∫Êù•ÁöÑÊó∂ÂÄôÂ∞ùËøáÊñ∞ÔºàÊª°Ë∂≥Â•ΩÂ•áÂøÉËÄåÂ∑≤ÔºåÊµÖÂ∞ùËæÑÊ≠¢ÔºâÔºå
Âõ†‰∏∫Â∑≤Áªè‰π†ÊÉØ‰∫ÜÊâÄÁî®ÁöÑÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂ÔºöÔºâ(Êõ¥Êñ∞ÔºöÂêéÊù•ÊàëÊîπÂèò‰∫Ü‰∏ªÊÑè)&lt;/p>
&lt;p>Â¶ÇÊûúËØ¥ÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂ÂèØ‰ª•Áî± vim Ëá™Â∏¶ÁöÑÂäüËÉΩÊâÄÊèê‰æõÔºåÈÇ£‰πàÔºåÊúâÂ§öÂ∞ëÊèí‰ª∂ÊòØÂøÖÈ°ªÁöÑÂë¢Ôºü&lt;/p>
&lt;p>Êúâ‰∫∫‰Ωú‰∫ÜÂæàÂ•ΩÁöÑÊÄªÁªìÔºö
&lt;a href="https://thoughtbot.com/blog/how-to-do-90-of-what-plugins-do-with-just-vim">How to Do 90% of What Plugins Do (With Just Vim)&lt;/a>&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†Â¥áÂ∞öÊûÅÁÆÄ‰∏ª‰πâÔºåÂÄºÂæó‰∏ÄËØï„ÄÇÔºà‰∏çÊòØËØ¥Â∞±‰∏ÄÂÆöË¶ÅÂ¶ÇÊ≠§ÔºåËÄåÊòØÁü•ÈÅìÊúâÁöÑÈÄâÊã©Ôºâ&lt;/p>
&lt;p>ËøôÈáåÊèêÂà∞ÁöÑÂ∫îËØ•ÈÉΩÁêÜËß£‰∏∫ÊòØ‚ÄúÂà´‰∫∫ÁöÑ‚Äù‰∏™‰∫∫ÈÖçÁΩÆÔºåËÄåÊâÄË∞ì‚Äú‰∏™‰∫∫ÈÖçÁΩÆ‚ÄùÔºåÂü∫Êú¨ÊÑèÊÄùÂ∞±ÊòØÔºö
Ëµ∞ËøáË∑ØËøáÔºåÂèØ‰ª•Èöè‰æøÈîôËøá„ÄÇ
ÁúãÂà∞ÈíüÊÑèÁöÑÔºåÊãøËµ∞‰∏çË∞¢„ÄÇÁúãÂà∞‰∏çÁàΩÁöÑÔºåËØ∑ÂéªPull RequestÔºåÂàáÂãø‰π±Âñ∑Á≤™„ÄÇ&lt;/p>
&lt;p>Â§ß‰Ω¨&lt;a href="https://en.wikipedia.org/wiki/Linus_Torvalds">ÊûóÁ∫≥ÊñØ¬∑ÊâòÁì¶ÂÖπ&lt;/a>Êó©Â∞±ÁúãÈÄè‰∫ÜÈÇ£‰∫õÂà∞Â§Ñ‰π±Âñ∑ÁöÑÈîÆÁõò‰æ†Ôºå‰ªñÁöÑËá≥ÁêÜÂêçË®ÄËØ¥Êòé‰∫Ü‰∏ÄÂàáÔºö&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;On the Internet, talk is cheap, show me the code.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;h2 id="vimrc-vimrc" >
&lt;div>
&lt;a href="#vimrc-vimrc">
#
&lt;/a>
vimrc (~/.vimrc)
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-vim" data-lang="vim">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34; ================================Part-1: Plugins============&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#begin(&lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle&amp;#39;&lt;/span>) &lt;span style="color:#78787e">&amp;#34; reuse the bundle dir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== tmux ===-------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; vim &amp;lt;-&amp;gt; tmux, pane dance&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;edkolev/tmuxline.vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; status line&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Coding enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;airblade/vim-gitgutter&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; shows git diff sign and more&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;tpope/vim-fugitive&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; awsome git wrapper!&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;tpope/vim-repeat&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; enhance . repeat&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;dense-analysis/ale&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; linters config&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;mileszs/ack.vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; cherrypick your strings&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;junegunn/fzf&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;do&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;./install --all&amp;#39;&lt;/span> } | Plug &lt;span style="color:#5af78e">&amp;#39;junegunn/fzf.vim&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Python enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; Plug &amp;#39;Valloric/YouCompleteMe&amp;#39; &amp;#34; for completion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;davidhalter/jedi-vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; python auto-completion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Wiki/markdown enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;vimwiki/vimwiki&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for personal wiki&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;mzlogin/vim-markdown-toc&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for table_of_content&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;plasticboy/vim-markdown&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; couple with vim-instant-markdown&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;instant-markdown/vim-instant-markdown&amp;#39;&lt;/span>, {&lt;span style="color:#5af78e">&amp;#39;for&amp;#39;&lt;/span>: [&lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;markdown.pandoc&amp;#39;&lt;/span>]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Latex support ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;lervag/vimtex&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for LaTeX files&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;xuhdev/vim-latex-live-preview&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;for&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;tex&amp;#39;&lt;/span> } &lt;span style="color:#78787e">&amp;#34; Lively Previewing&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Julia lang ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;JuliaEditorSupport/julia-vim&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== for GnuPG ===-----------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;jamessan/vim-gnupg&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; transparent editting .gpg files&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Code/Project navigation ===-------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;easymotion/vim-easymotion&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; quickly move&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;preservim/nerdtree&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;on&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;NERDTreeToggle&amp;#39;&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== local plugins (manually install and update) ===-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34; ‰∏≠ÊñáË∑≥ËΩ¨ (easymotion-like)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle/vim-PinyinSearch&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#end() &lt;span style="color:#78787e">&amp;#34; required&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÂÆâË£ÖÂíå‰ΩøÁî® vim-plug ÁöÑÊ≠•È™§Ôºö&lt;/p>
&lt;ol>
&lt;li>‰∏ãËΩΩÂπ∂‰øùÂ≠òÂà∞ &lt;code>~/.vim/autoload/&lt;/code> ÁõÆÂΩï‰∏ã ÔºàÂú®ÁªàÁ´ØÊìç‰ΩúÔºâ&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>curl -fLo ~/.vim/autoload/plug.vim --create-dirs &lt;span style="color:#5af78e">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">&lt;/span> https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>ÈÖçÁΩÆ &lt;code>~/.vimrc&lt;/code> Êñá‰ª∂ÁöÑÁõ∏Â∫îÂÜÖÂÆπÔºàÂøÖÈ°ªÈÅµÂæ™ÁöÑÊ†ºÂºèÔºâÔºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>call plug#begin&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#5af78e">&amp;#34; reuse the bundle dir
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">Plug &amp;#39;tpope/vim-fugitive&amp;#39; &amp;#34;&lt;/span> awsome git wrapper!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#end&lt;span style="color:#ff6ac1">()&lt;/span> &lt;span style="color:#5af78e">&amp;#34; required
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Âú® &lt;code>plug#begin()&lt;/code> Âíå &lt;code>plug#end()&lt;/code> ‰πãÈó¥Â∞±ÊòØ‰Ω†ÊÉ≥Ë¶ÅÂÆâË£ÖÂíåÁÆ°ÁêÜÁöÑÊèí‰ª∂ÁöÑÂàóË°®„ÄÇ
&lt;code>Plug 'tpope/vim-fugitive'&lt;/code> Â∞±ÊòØÊåáÊòéÊÉ≥Ë¶ÅÂÆâË£ÖÂπ∂ÁÆ°ÁêÜ vim-fugitive Ëøô‰∏™Êèí‰ª∂„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú &lt;code>plug#begin()&lt;/code> Êã¨ÂºßÈáå‰∏∫Á©∫ÔºåÂàôÈªòËÆ§ÊòØÂ•ñÊâÄÊúâÁöÑÊèí‰ª∂ÂÆâË£ÖÂà∞ &lt;code>.vim/plugged/&lt;/code>Ôºå
‰∏äÊñáÁöÑÊàëÁöÑÈÖçÁΩÆ‰∏≠ÔºåÊàëÊòØÊåáÂÆö‰∫Ü &lt;code>.vim/bundle&lt;/code> ‰Ωú‰∏∫Êèí‰ª∂ÁöÑÁõÆÂΩï
ÔºàÂõ†‰∏∫ÊàëÂú®‰ΩøÁî® vim-plug ‰πãÂâçÂ∞±Â∑≤ÁªèÂú®Ëøô‰∏™ÁõÆÂΩï‰∏ãÂ≠òÊîæ‰∫ÜËÆ∏Â§öÊèí‰ª∂ÔºåÁÆÄÂçïÂ§çÁî®ËÄåÂ∑≤Ôºâ„ÄÇ&lt;/p>
&lt;p>vim-plug ‰ºöËá™Âä®ÊâßË°å‰ª•‰∏ã‰∏§‰∏™ÂëΩ‰ª§ÔºàËÆ∏Â§öÊèí‰ª∂ÈúÄË¶ÅËøô‰∏§‰∏™ËÆæÂÆöÔºâÔºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>filetype indent on&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>syntax enable&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Â¶ÇÊûú‰Ω†ÂèëÁé∞Ëá™Â∑±‚ÄúÂÆåÂÖ®‚ÄùÊåâÁÖßÂÆòÊñπÁöÑÊåáÂçóËøõË°åÂÆâË£ÖÔºåÂèØÊòØ‚ÄúÊèí‰ª∂Ê≠ªÊ¥ª‰∏çËµ∑‰ΩúÁî®‚ÄùÔºåÊûÅÊúâÂèØËÉΩÊòØ
Âõ†‰∏∫‰Ω†Âú®Êüê‰∏™Âú∞ÊñπËÆæÁΩÆ‰∫ÜÂÆåÂÖ®Áõ∏ÂèçÁöÑËÆæÂÆöÔºàÂõ†‰∏∫‰ªéÂà´ÁöÑÂú∞Êñπ copy ‰∏ÄÂ§ßÊÆµËá™Â∑±Ê≤°ÊúâËÆ§ÁúüÁúã
ËøáÁöÑÂÜÖÂÆπÂà∞Ëá™Â∑±ÁöÑ .vimrc Êñá‰ª∂ÈáåÔºâ„ÄÇ&lt;/p>
&lt;hr>
&lt;p>Êõ¥ËØ¶ÁªÜÁöÑ‰ΩøÁî®ÊñπÊ≥ïÂ∫îËØ•Áõ¥Êé•Âéª &lt;a href="https://github.com/junegunn/vim-plug">vim-plug&lt;/a>
ÁöÑ‰∏ªÈ°µ‰∫ÜËß£ÔºåÈÇ£Èáå‰∏ç‰ªÖÂëäËØâÂ§ßÂÆ∂Â∫îËØ•ÊÄé‰πàÂÆâË£ÖÊ≠§Êèí‰ª∂ÔºåËøòÁªôÂá∫ÂêÑÁßçÊÉÖÂÜµÁöÑËØ¥ÊòéÂíå‰æãÂ≠ê„ÄÇ&lt;/p>
&lt;hr></description></item><item><title>Âà©Âô®1: Oh, My Tmux -- ÈîÆÁõòÈ©±Âä®(‰∏∫‰∏ª)ÁöÑÂ∑•‰ΩúÊµÅ</title><link>/posts/thecli/2022-08-07-tmux/</link><pubDate>Sun, 07 Aug 2022 21:17:59 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/2022-08-07-tmux/</guid><description>&lt;h2 id="tmuxhttpsgithubcomtmuxtmux" >
&lt;div>
&lt;a href="#tmuxhttpsgithubcomtmuxtmux">
#
&lt;/a>
&lt;a href="https://github.com/tmux/tmux">Tmux&lt;/a>
&lt;/div>
&lt;/h2>
&lt;p>&lt;a href="https://github.com/tmux/tmux">Tmux&lt;/a> ÊòØ‰∏Ä‰∏™ÁªàÁ´ØÂ§öË∑ØÂ§çÁî®Âô®&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>Ôºö&amp;hellip;&lt;/p>
&lt;blockquote>
&lt;p>Tmux is a terminal multiplexer: it enables a number of terminals to be
created, accessed, and controlled from a single screen. tmux may be detached
from a screen and continue running in the background, then later reattached.&lt;/p>
&lt;/blockquote>
&lt;p>ÈÇ£Âà∞Â∫ï‰ªÄ‰πàÊòØ&lt;ruby>‚ÄúÁªàÁ´Ø‚Äù&lt;rt>terminal&lt;/rt>&lt;/ruby>Ôºü&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> ÊàëÂõûÁ≠î‰∏ç‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºå‰ΩÜÊàëÂèëÁé∞‰∫Ü‰∏Ä‰∏™ÂæàÂ•ΩÁöÑËß£Èáä &amp;ndash;
&lt;a href="https://unix.stackexchange.com/questions/4126/what-is-the-exact-difference-between-a-terminal-a-shell-a-tty-and-a-con">terminal&lt;/a>&lt;/p>
&lt;p>Á™ÅÁÑ∂ÊÉ≥Ëµ∑„ÄäÁªüËÆ°Â≠¶„ÄãÂÖ≥‰∫é‚ÄúÂç°ÊñπÊ£ÄÈ™å‚ÄùÈÇ£‰∏ÄÁ´†ÁöÑÁªèÂÖ∏ÂØºËØ≠Ôºö&lt;/p>
&lt;blockquote>
&lt;p>‰∏çË¶ÅÈóÆÂÆÉÁöÑÂê´‰πâÔºåÂÆÅÂèØÈóÆÂ¶Ç‰Ωï‰ΩøÁî®ÂÆÉ„ÄÇ&amp;ndash; Áª¥ÁâπÊ†πÊñØÂù¶&lt;/p>
&lt;/blockquote>
&lt;p>Êú¨ÁùÄ‚ÄúÊãøÊù•‰∏ª‰πâ‚ÄùÁ≤æÁ•ûÔºåÈÇ£Â∞±Êù•Á≤óÁï•ÁúãÁúã Tmux ËÉΩÂ¶Ç‰Ωï‰ΩøÁî®ÂêßÔºö&lt;/p>
&lt;p>&lt;img alt="IMG_MyTmux &amp;ndash; (gif Â§±Êïà)" src="https://fgg100y.github.io/img/posts/tmux/ssr_myTmuxDemo.gif">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ÂêØÂä® Termainal (CTRL+Enter, ËøôÊòØ‰∏™‰∫∫ÂÆö‰πâÁöÑÂø´Êç∑ÈîÆ)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÂêØÂä® Tmux ËøõÂÖ•ÊàëÁöÑÂ∑•‰ΩúÁéØÂ¢É (È°ªÁü•ÔºöÊàëËøôÈáåÂ±ïÁ§∫ÁöÑ‰∏çÊòØÂàùÂßãÂåñÁöÑ Tmux ÂêØÂä®)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ê£ÄÊü•‰∏Ä‰∏ãÊò®Â§©ÂÜôÁöÑÂçöÂÆ¢ÂÜôÂà∞Âì™Èáå‰∫Ü (Ë≤å‰ººÂ∑Æ‰∏çÂ§ö‰∫ÜÔºåÂÜôÂÖ∂‰ªñÂÜÖÂÆπ)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÂàáÊç¢Âà∞‰∏ã‰∏ÄÂú∫‚Äú‰ºöËØù‚Äù(session) &amp;ndash; vimwiki (ËøôÈáåÊòØÊàë‰∏™‰∫∫È°πÁõÆÊ†πÁõÆÂΩï)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊåâËøõÂ∫¶‰ªäÂ§©Â∫îËØ•‰ªãÁªç‰∏Ä‰∏ã Tmux Ëøô‰∏™Â∑•ÂÖ∑ (ÁÑ∂ÂêéÔºåÂ∞±Âà∞ËøôÈáå‰∫Ü)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Áé∞Âú®Èò≥ÂÖâÊòéÂ™öÔºàËøá‰∏Ä‰ºöÂÑøÂ∞±ÂèòÊØíËæ£ÔºâÔºåÁôΩÂ§©ËøòÊòØÊç¢Âà∞ÁôΩËâ≤ËÉåÊôØÂêàÈÄÇ‰∫õ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;del>Êê¨Á†ñÊª¥Âπ≤Ê¥ªÂíØÂêÑ‰Ωç&lt;/del>&lt;br>
&lt;del>ÂóØÔºåÂπ≤Ê¥ªÔºüÊõ¥ÂÉèÊòØÂàõ‰Ωú&lt;/del>&lt;br>
&lt;del>Âàõ‰ΩúÔºüËøôÁÆóÂì™Èó®Â≠êÁöÑ&amp;hellip;&lt;/del>&lt;br>
ÊâÄ‰ª•ËøòÊòØÔºåÂπ≤ÁÇπÊ¥ªÂêß ÔºöÔºâ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="tmux-ÁöÑ‰∏Ä‰∫õÂÖ≥ÈîÆÈÖçÁΩÆ" >
&lt;div>
&lt;a href="#tmux-%e7%9a%84%e4%b8%80%e4%ba%9b%e5%85%b3%e9%94%ae%e9%85%8d%e7%bd%ae">
#
&lt;/a>
Tmux ÁöÑ‰∏Ä‰∫õÂÖ≥ÈîÆÈÖçÁΩÆ
&lt;/div>
&lt;/h2>
&lt;p>Â¶ÇÊûúËøòÊ≤°ÊúâÂÆâË£Ö TmuxÔºåÂëΩ‰ª§Ë°å‰∏≠ÂÆâË£ÖËµ∑Êù•‰πü‰∏çË¥π‰∫ã (DebianÁ≥ªÂàó)Ôºö&lt;code>apt install tmux&lt;/code>&lt;/p>
&lt;p>‰ª•‰∏ãÁöÑÂÜÖÂÆπÂùáÂõ¥Áªï &lt;code>~/.tmux.conf&lt;/code> Êñá‰ª∂Â±ïÂºÄ„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÂ∞±Ëá™Â∑±ÂàõÂª∫‰∏Ä‰∏™ËøôÊñá‰ª∂„ÄÇ
ÈªòËÆ§Ëøô‰∫õ‰ª£Á†ÅÊ°ÜÈáåÁöÑÂÜÖÂÆπÔºàÁ≠âÂÆΩÂ≠ó‰ΩìÈÇ£‰∫õÔºâÊòØË¶ÅÊ∑ªÂä†Âà∞ &lt;code>~/.tmux.conf&lt;/code> Êñá‰ª∂Èáå„ÄÇ&lt;/p>
&lt;p>&amp;ldquo;#&amp;rdquo; ÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØÊ≥®ÈáäËØ¥Êòé&lt;br>
&amp;ldquo;$&amp;rdquo; ÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØË¶ÅÂú®ÂëΩ‰ª§Ë°åËøêË°åÁöÑÂëΩ‰ª§&lt;br>
&amp;ldquo;:command&amp;rdquo; ‰ª•‚ÄúÂÜíÂè∑Êé•ÂëΩ‰ª§Âêç‚ÄùÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØË¶ÅÂú®vimÂëΩ‰ª§Ê®°ÂºèËøêË°åÁöÑÂëΩ‰ª§&lt;/p>
&lt;h2 id="ÈªòËÆ§Âø´Êç∑ÈîÆ‰øÆÊîπ-ctrl-b---ctrl-a" >
&lt;div>
&lt;a href="#%e9%bb%98%e8%ae%a4%e5%bf%ab%e6%8d%b7%e9%94%ae%e4%bf%ae%e6%94%b9-ctrl-b---ctrl-a">
#
&lt;/a>
ÈªòËÆ§Âø´Êç∑ÈîÆ‰øÆÊîπ: Ctrl-b -&amp;gt; Ctrl-a
&lt;/div>
&lt;/h2>
&lt;p>Tmux ÁöÑÈªòËÆ§Âø´Êç∑ÈîÆÊòØ &lt;code>ctrl-b&lt;/code>ÔºåËøôÂú®ÈîÆÁõò‰∏äË∑ùÁ¶ªÁï•ÂæÆÊúâÁÇπËøú (ÂØπ‰∫éÊàëÁöÑÁü≠ÊâãÊåáÊù•ËØ¥)Ôºå
Âõ†Ê≠§ÊàëÊääÂÆÉ‰øÆÊîπ‰∏∫ &lt;code>ctrl-a&lt;/code> ÁªÑÂêàÈîÆ„ÄÇ&lt;/p>
&lt;p>Â∞Ü‰∏ãÈù¢ËØ≠Âè•Âä†ÂÖ• .tmux.conf Êñá‰ª∂Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g prefix C-a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # unbind the C-b&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>unbind C-b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # ensure that we can send Ctrl-a to other apps (e.g., vim)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> C-a send-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # setting the delay between prefix and command&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -s escape-time &lt;span style="color:#ff9f43">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Êó¢ÁÑ∂ÊòØË¶ÅÁî®Âø´Êç∑ÈîÆÔºåÈÇ£‰πàÈáçÊñ∞Âä†ËΩΩ .tmux.conf Êñá‰ª∂ÁöÑËá™ÁÑ∂‰πüÂÆâÊéí‰∏äÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># reload the .tmux.conf file with Prefix-r&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> r source-file ~/.tmux.conf &lt;span style="color:#5af78e">\;&lt;/span> display &lt;span style="color:#5af78e">&amp;#34;tmuxconf reloaded!&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÁÑ∂ÂêéÔºåÂú®ÂëΩ‰ª§Ë°åÁõ¥Êé•ËæìÂÖ•Âø´Êç∑ÁªÑÂêàÈîÆÔºö&lt;code>Ctrl+a+r&lt;/code> Âç≥ÂèØ‰ΩøÈÖçÁΩÆÁîüÊïà (&lt;code>+&lt;/code>Âè∑ÁöÑÊÑèÊÄùÊòØÂêåÊó∂ÊëÅ‰∏ã)„ÄÇ&lt;/p>
&lt;h2 id="vimtmux-Âº∫Âº∫ÁªÑÂêà" >
&lt;div>
&lt;a href="#vimtmux-%e5%bc%ba%e5%bc%ba%e7%bb%84%e5%90%88">
#
&lt;/a>
vim/tmux Âº∫Âº∫ÁªÑÂêà
&lt;/div>
&lt;/h2>
&lt;p>Tmux ÊîØÊåÅÂ§ö‰∏™ÁªàÁ´ØÁ™óÊ†º (terminal panes)ÔºåÊØè‰∏™Á™óÊ†ºÈÉΩÊòØÁã¨Á´ãÁöÑÁªàÁ´ØÔºå
Âõ†Ê≠§ÔºåÊúÄÊòéÊòæÁöÑ‰∫ãÊÉÖÂ∞±ÊòØÔºå‰Ω†ÂèØ‰ª•ÂêåÊó∂Âú®‰∏ÄÂùóÂ±èÂπï‰∏ä‚ÄúÁõëÊéß‚ÄùÂ§ö‰∏™‰∏çÂêå‰ªªÂä°ÔºåÊÑèÊÄùÊòØÔºå
Ëøô‰∫õ‰∏çÂêåÁöÑ‰ªªÂä°Âú®ËßÜËßâ‰∏äÊòØ‚ÄúÂêåÊó∂‚ÄùÂëàÁé∞ÁöÑÔºàÁõ∏ÂØπ‰∫éÈáçÂè†ÁöÑ Ê†áÁ≠æÈ°µ(tab) ËÄåË®ÄÔºâ„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÊÅ∞Â•Ω‰πüÂñúÊ¨¢Áî® vim ËøõË°åÁ†ÅÂ≠óÔºå‰πüÊâìÂºÄ‰∫Ü vim ÁºñËæëÂô® (vim Êú¨Ë∫´‰πüÊúâÁ™óÊ†ºÂäüËÉΩ)Ôºå
Ëá™ÁÑ∂ËÄåÁÑ∂ÁöÑÔºå‰Ω†ËÇØÂÆöÊÉ≥‰ª•Áõ∏ÂêåÁöÑÂø´Êç∑ÈîÆÊù•ÂÆûÁé∞‰∏çÂêåÁöÑ vim/tmux Á™óÊ†º‰πãÈó¥ÁöÑÂàáÊç¢„ÄÇ&lt;/p>
&lt;p>Âπ∏ËøêÁöÑÊòØÔºåÂ∑≤ÁªèÊúâ‰∫∫ÂÆûÁé∞‰∫ÜËøôÊ†∑ÁöÑÂäüËÉΩÔºåÂπ∂ÂÖ±‰∫´‰∫ÜÔºåËøôÂ∞±ÊòØ
&lt;a href="https://github.com/christoomey/vim-tmux-navigator">vim-tmux-navigator&lt;/a>.&lt;/p>
&lt;p>vim Êúâ‰∏∞ÂØåÁöÑÊèí‰ª∂ÁîüÊÄÅÔºåËøôÊòØ vim Âº∫Â§ßÁîüÂëΩÂäõÁöÑ‰∏Ä‰∏™‰ΩìÁé∞„ÄÇ
Ëøô‰πàÂ•ΩÁöÑÊ®°ÂºèÔºåËá™ÁÑ∂ÂÄºÂæóÂÄüÈâ¥„ÄÇÂõ†Ê≠§Ôºåtmux ‰πüÊúâ‰∏Ä‰∏™Áî®Êù•ÁÆ°ÁêÜÊèí‰ª∂ÁöÑ
&lt;a href="https://github.com/tmux-plugins/tpm">tpm&lt;/a>.&lt;/p>
&lt;p>ÈúÄË¶ÅÂú® vim Âíå tmux ‰∏§‰∏™ÊñπÈù¢ËøõË°åÈÖçÁΩÆÔºö&lt;/p>
&lt;ol>
&lt;li>Âú® vim ÊñπÈù¢, Â∞Ü‰ª•‰∏ãËØ≠Âè•Âä†ÂÖ• .vimrc Êñá‰ª∂Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Â¶ÇÊûú‰Ω†Áî®ÁöÑÊòØ Vim-Plug Êèí‰ª∂ÁÆ°ÁêÜÂô®ÁöÑËØù&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÁÑ∂ÂêéËøêË°å &lt;code>:PlugInstall&lt;/code> Êù•ÂÆâË£ÖËøô‰∏™Êèí‰ª∂„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÊòØÁ¨¨‰∏ÄÊ¨°ÈÖçÁΩÆ vim Êèí‰ª∂ÔºåÂÖàÂéª‰∫ÜËß£
&lt;a href="https://github.com/junegunn/vim-plug">Vim-Plug&lt;/a>.&lt;/p>
&lt;ol start="2">
&lt;li>Âú® tmux ÊñπÈù¢ÔºåÂ∞Ü‰∏ãÈù¢ËØ≠Âè•Âä†ÂÖ• .tmux.conf Êñá‰ª∂Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># tpm is a tmux-plugin manager. [details in its github repos]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g @plugin &lt;span style="color:#5af78e">&amp;#39;tmux-plugins/tpm&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># vim-tmux-navigator: Seamless navigation between tmux panes and vim splits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g @plugin &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># put this &amp;#39;run&amp;#39; command to the very end of .tmux.conf, says the tpm&amp;#39;s readme&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>run &lt;span style="color:#5af78e">&amp;#39;~/.tmux/plugins/tpm/tpm&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Âä†ÂÖ•Ëøô‰∫õÂÜÖÂÆπ‰πãÂêéÔºåÈáçÊñ∞Âä†ËΩΩ tmux ÁéØÂ¢É, (Âú®Â∑≤ÁªèÂêØÂä®tmuxÁöÑÂëΩ‰ª§Ë°åËøêË°å‰ª•‰∏ãÂëΩ‰ª§)Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ tmux &lt;span style="color:#ff5c57">source&lt;/span> ~/.tmux.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="‰∏ªÈ¢òËÉåÊôØÈ¢úËâ≤ÂàáÊç¢" >
&lt;div>
&lt;a href="#%e4%b8%bb%e9%a2%98%e8%83%8c%e6%99%af%e9%a2%9c%e8%89%b2%e5%88%87%e6%8d%a2">
#
&lt;/a>
‰∏ªÈ¢òËÉåÊôØÈ¢úËâ≤ÂàáÊç¢
&lt;/div>
&lt;/h2>
&lt;p>Âú®ÊàëÁöÑÂç∞Ë±°ÈáåÔºåÊÑüËßâÈªëËâ≤/ÊöóËâ≤Á≥ª‰∏Ä‰∏ãÂ≠êÊàê‰∫ÜÂæàÂ§öÂ∫îÁî®ËΩØ‰ª∂ÁöÑÈªòËÆ§ËÉåÊôØËâ≤Ôºå
ÊàëÂú®ÂëΩ‰ª§Ë°åËøáÊ¥ªÂæàÂ§öÂπ¥‰∫ÜÔºåÂØπÊ≠§ÂæàÊòØ‰π†ÊÉØÔºå‰∏™‰∫∫ÂæàÂ∞ëÁúãÂà∞Ë∞ÅÁöÑTerminalÊòØÁôΩËâ≤ËÉåÊôØÁöÑ„ÄÇ
‰∏ªË¶ÅÂ∫îËØ•ÊòØÔºö1ÔºâÈÖçÁΩÆ‰∫ÜÁôΩÂ§©ÈªëÂ§úÈÉΩÂèãÂ•ΩÁöÑ‰∏ªÈ¢òÔºõ2Ôºâ‰∏çÂΩìÂõû‰∫ãÂÑø„ÄÇ
‰ΩÜÊúâÊó∂ÂÄôÔºåÁâπÂà´ÊòØÂÖâÁ∫øÂÖÖË∂≥ÁöÑÊó∂ÂÄôÔºåÊàëÊØîËæÉÂñúÊ¨¢ÁôΩËâ≤ÁöÑËÉåÊôØ„ÄÇ
ÊâÄ‰ª•ÔºåÊàëÈúÄË¶ÅËÉΩÈöèÂøÉÊâÄÊ¨≤ÂàáÊç¢ËÉåÊôØËâ≤Ë∞É„ÄÇ&lt;/p>
&lt;p>Ëøô‰∏™Ë≤å‰ººÊ≤°Ê≥ï‰∏ç‰æùËµñ‰∫éÂ§ñÈÉ®ËÑöÊú¨Êù•ÂÆûÁé∞ÔºåËá≥Â∞ëÊàëÊ≤°ÊúâÂèëÁé∞ÂèØË°åÁöÑÂäûÊ≥ï„ÄÇÂõ†Ê≠§Ôºå‰Ω†‰πüÈúÄË¶Å
&lt;a href="https://www.seanh.cc/2021/01/02/change-between-light-and-dark-themes-in-tmux/">Âéª‰∏ãËΩΩËøô‰∏™ËÑöÊú¨&lt;/a>&lt;/p>
&lt;p>Linux ÁéØÂ¢ÉÊ∞õÂõ¥ÊòØËøôÊ†∑ÁöÑÔºå‰∏çË¶ÅÁõ≤ÁõÆÁõ∏‰ø°Âà´‰∫∫ÂÜôÁöÑËÑöÊú¨ÔºåÁâπÂà´ÊòØÈÇ£‰∫õ‰Ω†‰∏çËÆ§ËØÜÁöÑ‰∫∫„ÄÇ
Áõ∏‰ø°ÂºÄÊ∫êÂíåÁõ∏‰ø°ÈôåÁîü‰∫∫ÔºåÊØïÁ´ü‰∏çÊòØÂêå‰∏Ä‰ª∂‰∫ã„ÄÇ
ÊÑèÊÄùÂ∞±ÊòØÔºå‰Ω†Âú®‰ΩøÁî®Ëøô‰∫õËÑöÊú¨‰πãÂâçÂ•ΩÊ≠π‰πüË¶ÅÁúãÁúã‰∫∫ÂÆ∂Âà∞Â∫ïÂÜô‰∫Ü‰ªÄ‰πàÂêß„ÄÇ
ÁâπÂà´ÊòØËøôÁßçÈÉΩ‰∏çË∂ÖËøá‰∏ÄÈ°µÂÜÖÂÆπÁöÑËÑöÊú¨„ÄÇ&lt;/p>
&lt;p>‰ªÄ‰πàÔºü‰Ω†ËØ¥‰Ω†Áúã‰∏çÊáÇ„ÄÇ‰∫ãÂÆûÊòØÔºåÊ≤°Êúâ‰∫∫Âú®‰∏ÄÂºÄÂßãÂ∞±‰ªÄ‰πàÈÉΩÁúãÂæóÊáÇ„ÄÇÊØîÂ¶ÇËØ¥Ôºå
‰Ω†ÁúãÂà∞ËÑöÊú¨ÈáåÊúâËøô‰πà‰∏ÄÂè•Á±ª‰ºº‰∫éwindows‰∏ä‚ÄúÊ∏ÖÁêÜÂûÉÂúæ‚ÄùÁöÑÂëΩ‰ª§Ôºö &lt;code>sudo rm -rf /*&lt;/code>Ôºå
ÈÇ£‰Ω†Â∞±Ë¶Å‰øùÊåÅË≠¶ÊÉï‰∫Ü„ÄÇ
ÊçÆËØ¥ÔºåÊúâ‰∫õÁ≥ªÁªü‰ºöÈòªÊ≠¢‰Ω†Ëøô‰πàÂπ≤Ôºå‰ΩÜÊúâ‰∫õÁ≥ªÁªü‰ºö‚ÄúÂºÄÂøÉÂú∞‚ÄùÊâßË°å‰Ω†ÁöÑÊó®ÊÑèÔºõ
ÂèçÊ≠£ÊàëÊ≤°ËØïËøáÔºåÊàë‰πüÊ≤°Êúâ‰ª•‰ªª‰ΩïÊñπÂºèÊöóÁ§∫ÊàñÈºìÂä±‰Ω†Ë¶ÅËøô‰πàÊ∏ÖÁêÜÁ≥ªÁªüÂûÉÂúæ„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúËØ¥‰Ω†‰∏ãËΩΩ‰∫ÜÈÇ£‰∏™ËÑöÊú¨ÔºåÊîæÂà∞‰Ω†ÂñúÊ¨¢ÂÆÉÂæÖÁöÑ‰ΩçÁΩÆÔºåÊØîÂ¶ÇËØ¥Ôºå &lt;code>~/.tmux/bin/&lt;/code>Ôºå
ÁÑ∂ÂêéÂ∞Ü‰∏ãÈù¢ËøôÊù°ËØ≠Âè•Âä†ÂÖ• &lt;code>~/.tmux.conf&lt;/code> Êñá‰ª∂‰∏≠„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> T run-shell ~/.tmux/bin/toggle-theme
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="ÂÖ≥‰∫éÈÖçÁΩÆÂíåÊèí‰ª∂‰ª•ÂèäÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e%e9%85%8d%e7%bd%ae%e5%92%8c%e6%8f%92%e4%bb%b6%e4%bb%a5%e5%8f%8a%e7%ae%a1%e7%90%86%e6%8f%92%e4%bb%b6%e7%9a%84%e6%8f%92%e4%bb%b6">
#
&lt;/a>
ÂÖ≥‰∫éÈÖçÁΩÆÂíåÊèí‰ª∂‰ª•ÂèäÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂
&lt;/div>
&lt;/h2>
&lt;p>‰ª•‰∏äÂÜÖÂÆπÂàÜÂà´ÂØπÂ∫îÁùÄÔºö&lt;/p>
&lt;p>1ÔºâÁõ¥Êé•ÂØπ tmux ËøõË°åÂü∫Êú¨ÈÖçÁΩÆÔºõ&lt;/p>
&lt;p>2Ôºâ‰ΩøÁî® tmux Êèí‰ª∂ÁÆ°ÁêÜÂô®ÂØπ tmux ËøõË°åÈÖçÁΩÆÔºõ&lt;/p>
&lt;p>3ÔºâÂÄüÁî® ËÑöÊú¨Á®ãÂ∫è ÂØπ tmux ËøõË°åÈÖçÁΩÆ„ÄÇ&lt;/p>
&lt;p>ÊØïÁ´üÔºå‰∏çÊòØÊØè‰∏™‰∫∫ÈÉΩÁÉ≠Ë°∑‰∫é‰ΩøÁî®Êèí‰ª∂ÁÆ°ÁêÜÂô®ÔºåÊèí‰ª∂Âíå‰∏ª‰ª∂‰πãÈó¥ÁöÑ‰∫§‰∫íÊñπÂºèÂπ∂‰∏çËÆæÈôê„ÄÇ‰Ω†ÂèØ‰ª•ÈÄâÊã©‰ΩøÁî®‰Ω†ËßâÂæóÂêàÈÄÇÁöÑÊñπÊ≥ïÊù•ÂÆåÊàê‰ªªÂä°„ÄÇ
Ë¶ÅÊòØËßâÂæóÁé∞ÊúâÁöÑÈÉΩ‰∏çÊª°ÊÑèÔºåÊó†Ê≥ïÊª°Ë∂≥‰Ω†ÁöÑÁâπÊÆäÈúÄÊ±ÇÔºåÊâãÁóíÁóíÁöÑÔºåÈÇ£Â∞±Ëá™Â∑±Êêû‰∏Ä‰∏™„ÄÇ
ÂæàÂ§ö‰ºòÁßÄÁöÑËΩØ‰ª∂ÈÉΩÊòØÂ¶ÇÊ≠§ÔºåÂÆÉÊèê‰æõË∂≥Â§üÂ•ΩÁöÑÈªòËÆ§ÂÄºÔºå‰ΩÜ‰πüÊèê‰æõË∂≥Â§üÂ§öÁöÑÁÅµÊ¥ªÊÄßÔºåËÆ©Áî®Êà∑ÊúâÁöÑÈÄâ„ÄÇ&lt;/p>
&lt;p>ÂàòÂª∫ÊòéËØ¥‚Äú‰ª•ÂâçÊ≤°ÁöÑÈÄâÔºåÁé∞Âú®ÊÉ≥ÈÄâÂõûÂÅöÂ•Ω‰∫∫ÔºåÁªô‰∏™Êú∫‰ºö‚ÄùÔºåÈôàÊ∞∏‰ªÅÁ≠î‚ÄúÂéªÂêåÊ≥ïÂÆòËÆ≤‚Äù„ÄÇ‚ÄúÊúâÁöÑÈÄâ‚ÄùËøô‰∏ÄÁÇπÂçÅÂàÜÂèØË¥µ„ÄÇ&lt;/p>
&lt;hr>
&lt;p>Last update: 2024-07-04&lt;/p>
&lt;blockquote>
&lt;p>USER: Tmux ÊòØ‰ªÄ‰πàÔºü
GPT4o: Tmux ÊòØ‰∏Ä‰∏™ÁªàÁ´ØÂ§öË∑ØÂ§çÁî®Âô® (terminal multiplexer)ÔºåÂÖÅËÆ∏Áî®Êà∑Âú®‰∏Ä‰∏™ÁªàÁ´ØÁ™óÂè£ÂÜÖËøêË°åÂíåÁÆ°ÁêÜÂ§ö
‰∏™ÁªàÁ´Ø‰ºöËØù„ÄÇÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñπ‰æøÁöÑÊñπÊ≥ïÊù•ÂàÜÂâ≤‰∏Ä‰∏™Áâ©ÁêÜÁªàÁ´ØÁ™óÂè£ÊàêÂ§ö‰∏™ËôöÊãüÁªàÁ´ØÔºåÂπ∂Âú®Ëøô‰∫õËôöÊãüÁªàÁ´Ø
‰πãÈó¥Ëá™Áî±ÂàáÊç¢„ÄÇTmux ÁöÑ‰∏ªË¶ÅÂäüËÉΩÂåÖÊã¨Ôºö&lt;/p>
&lt;ul>
&lt;li>‰ºöËØùÁÆ°ÁêÜÔºöTmux ÂèØ‰ª•ÂàõÂª∫„ÄÅÂàÜÁ¶ªÂíåÈáçÊñ∞ÈôÑÂä†‰ºöËØùÔºå‰ΩøÁî®Êà∑ÂèØ‰ª•Âú®‰∏çÂêåÁöÑÁªàÁ´Ø‰ºöËØù‰πãÈó¥Âø´ÈÄüÂàáÊç¢„ÄÇ&lt;/li>
&lt;li>Á™óÂè£ÂàÜÂâ≤ÔºöÁî®Êà∑ÂèØ‰ª•Âú®‰∏Ä‰∏™Tmux‰ºöËØù‰∏≠Â∞ÜÁ™óÂè£ÂàÜÂâ≤ÊàêÂ§ö‰∏™Á™óÊ†ºÔºåÊØè‰∏™Á™óÊ†ºÂèØ‰ª•Áã¨Á´ãËøêË°å‰∏Ä‰∏™ÂëΩ‰ª§Ë°åÁ®ãÂ∫è„ÄÇ&lt;/li>
&lt;li>ÊåÅ‰πÖÊÄßÔºöÂç≥‰ΩøÁªàÁ´ØÁ™óÂè£ÂÖ≥Èó≠ÔºåTmux ‰ºöËØù‰ªçÁÑ∂‰ºö‰øùÊåÅËøêË°åÔºåËøôÊ†∑Áî®Êà∑ÂèØ‰ª•Á®çÂêéÈáçÊñ∞ÈôÑÂä†‰ºöËØùÔºåÁªßÁª≠Â∑•‰Ωú„ÄÇ&lt;/li>
&lt;li>ÂèØÂÆöÂà∂ÊÄßÔºöTmux Êèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÈÖçÁΩÆÈÄâÈ°πÂíåÂø´Êç∑ÈîÆÔºåÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇËøõË°åÂÆöÂà∂„ÄÇ&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>ËÄåÂ¶Ç‰ªäÔºåÊàñËÆ∏Â∫îËØ•ÈóÆÈóÆGPT4o&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>ÊÉ≥Áü•ÈÅìÊÄé‰πàÂÉèËøôÊ†∑Ê†áÊ≥®ÂêçËØçÂêóÔºüÂéªÁúãÁúã&lt;a href="https://www.junyi.dev/posts/a-writing-trick/">ËøôÁØáÂçöÊñá&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Âà©Âô®2: Vimwiki ‰πã ÂÆâË£ÖÂíåÈÄÇÈÖçÊï∞ÊçÆÁßëÂ≠¶ÁöÑ‰∏Ä‰∫õÈÖçÁΩÆ</title><link>/posts/thecli/thevim/2022-07-15-vimwiki_part2/</link><pubDate>Sat, 06 Aug 2022 23:33:32 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/thevim/2022-07-15-vimwiki_part2/</guid><description>&lt;p>Last update: 2022-08-06 23:33:32&lt;/p>
&lt;h2 id="vimwiki-Êèí‰ª∂ÁÆÄ‰ªãÂÆâË£ÖÂíå‰∏Ä‰∫õËÆæÁΩÆ" >
&lt;div>
&lt;a href="#vimwiki-%e6%8f%92%e4%bb%b6%e7%ae%80%e4%bb%8b%e5%ae%89%e8%a3%85%e5%92%8c%e4%b8%80%e4%ba%9b%e8%ae%be%e7%bd%ae">
#
&lt;/a>
vimwiki Êèí‰ª∂ÁÆÄ‰ªã„ÄÅÂÆâË£ÖÂíå‰∏Ä‰∫õËÆæÁΩÆ
&lt;/div>
&lt;/h2>
&lt;p>Âú® vimwiki ÈáåÔºåÊâÄÊúâÁöÑÂÖ•Âè£ÈõÜ‰∏≠Âú®ÊåáÂÆöÁöÑÈ°πÁõÆÊ†πÁõÆÂΩï‰∏ãÁöÑ index.md ÊñáÊ°£Èáå„ÄÇ‰πüÂ∞±ÊòØË¶ÅÊûÑ
Âª∫ÁöÑÊï¥‰∏™Á¨îËÆ∞Á≥ªÁªüÔºàÈöè‰æøÁÆ°ÂÆÉÂè´‰ªÄ‰πàÔºâÔºåindex.md ÂèØ‰ª•Êàê‰∏∫ÂÆÉÁöÑÊï¥‰ΩìÊ°ÜÊû∂‰ª•ÂèäÂêÑ‰∏™‰∏ªÈ¢òÁöÑ
Á¥¢Âºï„ÄÇ‰ªéËøôÈáåÂèØ‰ª•ÂéªÂà∞ÂêÑ‰∏™‰∏ªÈ¢òÔºå‰ªéÂêÑ‰∏™‰∏ªÈ¢òÁöÑÁ¥¢ÂºïÂéªÂà∞ÂØπÂ∫îÁöÑÂÜÖÂÆπÔºå‰πüÂèØ‰ª•‰ªé‰∏Ä‰∏™ÂÜÖÂÆπ
ÂéªÂà∞Âè¶‰∏Ä‰∏™ÂÜÖÂÆπÔºåÂÜÖÂÆπÂèØ‰ª•ÊòØÊú¨Âú∞ÊñáÊ°£ÂÜÖ/ÊñáÊ°£Èó¥ÔºåËøôÂÆåÂÖ®ÂèñÂÜ≥‰∫éÂ¶Ç‰ΩïÊûÑÂª∫ÂÜÖÂÆπ‰πãÈó¥ÁöÑËøûÊé•„ÄÇ&lt;/p>
&lt;p>vimwiki Êèê‰æõ‰∫ÜÂøÖË¶ÅÁöÑÂü∫Á°Ä„ÄÇ‰ΩÜÈÄöÂ∏∏Êàë‰ª¨Ë¶ÅÂÜôÁöÑÊñáÊ°£Âπ∂‰∏çÊòØÁ∫ØÊñáÊú¨ (plain text) ËÄåÊòØÂ∏¶Êúâ
‰øÆÈ•∞ËØ≠Ê≥ïÁöÑÊñáÊú¨ÔºåÊØîÂ¶Ç markdown, Tex/Latex Ê†ºÂºèÁöÑÊñáÊ°£„ÄÇËøô‰∏§ËÄÖÁöÑÂå∫Âà´Âú®‰∫éÔºåÂâçËÄÖÊòØ‚Äú
ÊâÄËßÅÂç≥ÊâÄÂæó‚ÄùÔºåÊÑèÊÄùÂ∞±ÊòØ‰Ω†ÂÜôÂá∫Êù•ÁöÑÂÜÖÂÆπÂíåÊúÄÂêéÂëàÁé∞ÁöÑÂÜÖÂÆπ‰∏ÄÊ®°‰∏ÄÊ†∑ÔºõÂêéËÄÖÂàôÊõ¥ÂÉèÊòØÂÜô‰ª£Á†ÅÔºå
ÊúÄÂêéÂëàÁé∞Âá∫Êù•ÁöÑÂíåÊâÄÂÜôÁöÑÂÜÖÂÆπ‰∏ÄËá¥Ôºà‰ΩÜÂ§ßÈÉ®ÂàÜÊó∂ÂÄô‰∏ç‰ºö‰∏ÄÊ®°‰∏ÄÊ†∑ÔºâÔºåÊúâ‰∫∫Áß∞‰πã‰∏∫
‚ÄúÊâÄÊÉ≥Âç≥ÊâÄÂæó‚ÄùÔºåÂê¨Ëµ∑Êù•ÊúâÁÇπÁéÑ‰πé„ÄÇÁõ¥Êé•Áî® plain text Ê†ºÂºè‰∏çÂ∞±ÂÆå‰∫ÜÔºåÊäòËÖæËøô‰∫õÂπ≤ÂòõÔºüÈÅóÊÜæ
ÁöÑÊòØÔºåÂè™Ë¶Å‰Ω†ÊÉ≥Ë¶ÅÂÜô‰∏Ä‰∫õÂ§çÊùÇ‰∏ÄÁÇπÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÔºàÊ±ÇÂØºÁöÑÈìæÂºèÊ≥ïÂàôÔºâÔºåÈÇ£‰πàÔºåÊ≤°Êúâ‰∏ìÈó®ÁöÑÁºñËæë
Âô®ÁöÑËØùÔºåÊúÄÂø´ËÉΩÂ§üÂæóÂà∞ËßÑËåÉÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊñπÊ≥ïÂ∞±ÊòØ &amp;ndash; ÊâãÂÜô„ÄÇÊïôÁßë‰π¶ÂÜÖÈÇ£‰∫õÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞
ÊïàÊûúÂü∫Êú¨ÈÉΩÊù•Ëá™ÊéíÁâàÁ≥ªÁªüÂ∑•ÂÖ∑ÔºàTex/LatexÁ≠âÔºâÁöÑÊîØÊåÅ„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúÂ∑•‰ΩúÁöÑÂÜÖÂÆπÊ∂âÂèäÂà∞ËæÉÂ§öÁöÑÈ´òÁ≠âÊï∞Â≠¶ÔºåÂπ∂ÊÉ≥Ëá™Â∑±ÁºñÂÜôÁöÑÊó•ÂøóÂçöÂÆ¢ÂæóÂà∞Á±ª‰ººÊïôÁßë‰π¶ÂºèÁöÑÊï∞Â≠¶
ÂÖ¨ÂºèÂëàÁé∞Ôºåmarkdown ÊñáÊ°£ÊòØÁõ∏ÂΩìÂèãÂ•ΩÁöÑÊñáÊú¨Ê†ºÂºèÔºåËÄå‰ªé markdown Âà∞ÊúÄÁªàÊïàÊûúÁöÑÂëàÁé∞‰πãÈó¥Ôºå
ÈúÄË¶ÅÊàë‰ª¨‰Ωú‰∏Ä‰∫õ&lt;a href="https://fgg100y.github.io/posts/thecli/thevim/2022-07-15-vimwiki_part2/###Êï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞">È¢ùÂ§ñÈÖçÁΩÆ&lt;/a>„ÄÇ&lt;/p>
&lt;p>Áé∞Âú®ÊàëÂè™ÈúÄË¶ÅÁî® mardown Ê†ºÂºèÁºñÂÜôÂÜÖÂÆπÔºåÂú®Êú¨Âú∞ÈìæÊé•ÁöÑÊñáÂ≠ó‰∏äÊï≤ÂáªÂõûËΩ¶ÈîÆ(ÂëΩ‰ª§Ê®°Âºè‰∏ã)Ôºå
Â∞±ÂèØ‰ª•ÊâìÂºÄËøô‰∏™Êú¨Âú∞ÈìæÊé•ÁöÑÂÜÖÂÆπÊñáÊ°£ÔºåÁÑ∂ÂêéÁºñËæëÂÜÖÂÆπÊàñËÄÖÂèëÂëÜÊàñËÄÖÂà´ÁöÑ‰ªÄ‰πàÔºåÊï≤Âáª Delete
ÈîÆ(ÂëΩ‰ª§Ê®°Âºè‰∏ã)ÂõûÂà∞‰πãÂâçË∑≥ËΩ¨Êù•ÁöÑÂú∞ÊñπÔºõÂΩìÁÑ∂‰πüÂèØ‰ª•ËÆæÁΩÆÂíå‰ΩøÁî®ÂÖ∂‰ªñÁªÑÂêàÈîÆÊù•ÂéªÂà∞ÂÖ∂‰ªñÂú∞
Êñπ„ÄÇÂú®ÊØè‰∏™Âú∞ÊñπÔºåÈÉΩÂèØ‰ª•Áî®ÊµèËßàÂô®Â±ïÁ§∫ÂΩìÂâçÊñáÊ°£ÁöÑÂÜÖÂÆπÁöÑÊúÄÁªàÂëàÁé∞ÊïàÊûúÔºåÊØîÂ¶Ç‰ΩøÁî®ÈªòËÆ§ÁöÑ
&lt;code>&amp;lt;leader&amp;gt;+w+h+h&lt;/code> Âø´Êç∑ÈîÆÂ∞±ËÉΩÂú®‰Ω†ÁöÑÈªòËÆ§ÊµèËßàÂô®Â±ïÁ§∫ÂΩìÂâçÊñáÊ°£ÁöÑÂÜÖÂÆπÔºåÊï∞Â≠¶ÂÖ¨ÂºèÊ≠£ÊòØËØæÊú¨
‰∏äÁöÑÈÇ£ÁßçÂÆÉËØ•ÊúâÁöÑÊ†∑Â≠êÔºå‰πüÂèØ‰ª•ÂÉèÊµèËßàÁôæÁßëÁΩëÈ°µÊó∂ÈÇ£Ê†∑ÔºåËá™Áî±Ë∑≥ËΩ¨ÔºåÊù•ÂéªËá™Â¶Ç„ÄÇ&lt;/p>
&lt;p>Èó≤ËØùÂ∞ëÂèôÔºåÁõ¥Êé•‰∏äÊâã„ÄÇ&lt;/p>
&lt;h3 id="ÂÆâË£Ö-vimwiki-Êèí‰ª∂" >
&lt;div>
&lt;a href="#%e5%ae%89%e8%a3%85-vimwiki-%e6%8f%92%e4%bb%b6">
##
&lt;/a>
ÂÆâË£Ö vimwiki Êèí‰ª∂
&lt;/div>
&lt;/h3>
&lt;p>(‰ΩøÁî®ÁÆ°ÁêÜÊèí‰ª∂ÁöÑ &lt;a href="https://github.com/junegunn/vim-plug">Vim-Plug&lt;/a> Êèí‰ª∂Êù•ÂÆâË£ÖÂíåÁÆ°ÁêÜ)&lt;/p>
&lt;p>Â∞Ü‰∏ãÈù¢ÁöÑËØ≠Âè•Âä†ÂÖ•‰Ω†Ëá™Â∑±ÁöÑ vim ÈÖçÁΩÆÊñá‰ª∂‰∏≠Ôºå&lt;/p>
&lt;p>&lt;code>Plug 'vimwiki/vimwiki'&lt;/code>&lt;/p>
&lt;p>ÁÑ∂ÂêéËøêË°åÂëΩ‰ª§ &lt;code>:PlugInstall&lt;/code>&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†Ëøò‰∏çÊ∏ÖÊ•öËøôÊòØ‰ªÄ‰πàÊÑèÊÄùÔºåÊêúÁ¥¢: vimrc ÊàñËÄÖ &lt;a href="https://vim.fandom.com/wiki/Open_vimrc_file">ÁÇπÂáªËøôÈáå&lt;/a>&lt;/p>
&lt;h3 id="ÈÖçÁΩÆ-vimwiki-Êèí‰ª∂" >
&lt;div>
&lt;a href="#%e9%85%8d%e7%bd%ae-vimwiki-%e6%8f%92%e4%bb%b6">
##
&lt;/a>
ÈÖçÁΩÆ vimwiki Êèí‰ª∂
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>markdwon Ê†ºÂºè&lt;/li>
&lt;/ul>
&lt;p>vimwiki ÈªòËÆ§‰ΩøÁî® .wiki Ê†ºÂºèÔºå‰ΩÜËøô‰∏™Ê†ºÂºèÁöÑËØ≠Ê≥ïÊ≤°Êúâ markdown ËØ≠Ê≥ï‰ΩøÁî®ÁöÑÂπøÊ≥õÂíåÊôÆÈÅç,
ÊâÄ‰ª•Êàë‰∏™‰∫∫ÊòØÊõ¥ÂÄæÂêë‰∫é‰ΩøÁî® markdown Ê†ºÂºèÊù•ÁºñÂÜôÊñáÊ°£„ÄÇÂèØ‰ª•ÊåâÁÖß‰∏ãÈù¢ÁöÑÂÅöÊ≥ïËøõË°åÁõ∏Â∫îÈÖçÁΩÆÔºö
Â∞Ü‰∏ãÈù¢ÁöÑËØ≠Âè•Âä†ÂÖ• &lt;code>.vimrc&lt;/code>:&lt;/p>
&lt;p>&lt;code>let g:vimwiki_ext2syntax={'.md': 'markdown', '.markdown': 'markdown', '.mdown': 'markdown', '.rmd': 'markdown'}&lt;/code>&lt;/p>
&lt;p>ËøôÊòØÂá†‰∏™ËæÉ‰∏∫Â∏∏ËßÅÁöÑ markdown ÊñáÊ°£ÁöÑÂêéÁºÄÔºåÊúÄÂêé‰∏ÄÁßç &lt;code>rmd&lt;/code> ÊòØ Rmardown Ê†ºÂºèÂêéÁºÄ„ÄÇ&lt;/p>
&lt;ul>
&lt;li>Â§ö‰∏™ index.md ÊñáÊ°£&lt;/li>
&lt;/ul>
&lt;p>ÂâçÈù¢ÊèêÂà∞ËøáÔºåindex.md ÊòØ vimwiki ÁöÑÈªòËÆ§ÂÖ•Âè£ÔºåÁ±ª‰ºº‰∫é‰∏ÄÊú¨‰π¶ÁöÑ‰∏ªÁõÆÂΩïÔºåÊàñËÄÖÁîµÂ≠êËèúÂçï„ÄÇ
ÈÄöÂ∏∏Êàë‰ª¨ÈúÄË¶ÅÂ§ö‰∏™ËèúÂçïÔºåÊØè‰∏™‰∏ªÈ¢òÈÖç‰∏Ä‰∏™ËèúÂçïÔºå‰ªéËÄå‰ΩøÂæóÂÜÖÂÆπÊõ¥Á≤æÁÆÄÂíåËá™Ê¥ΩÔºåËÄå‰∏çÊòØÊâÄÊúâ
ÁöÑÂÜÖÂÆπÂÖ®ÈÉΩÊ±áÈõÜÂπ∂Êã•Êå§Âú®Âêå‰∏Ä‰∏™Âú∞Êñπ„ÄÇÂú®È°∂Á∫ßËèúÂçïÔºåÂè™ÈúÄË¶ÅÁªôÂá∫Âà∞ÂêÑ‰∏™‰∏ªÈ¢òÁöÑÈìæÊé•Âç≥ÂèØ„ÄÇ
Â∞±ÂÉèÂ§ßÈ•≠Â∫óÁöÑÁîµÂ≠êËèúÂçïÔºåÂÆ¢‰∫∫ÂØπÂ∑ùËèúÊÑüÂÖ¥Ë∂£Â∞±ÁÇπÂáª‚ÄúÂ∑ùËèú‚ÄùÁöÑÁõÆÂΩïÂç≥ÂèØÔºå‰ΩÜÊ≤°ÊúâÂøÖË¶ÅÂ∞ÜÂÖ´Â§ß
ËèúÁ≥ªÊâÄÊúâËèúÂêçÂÖ®ÈÉΩÊã•Êå§Âà∞‰∏Ä‰∏™‰∏ªËèúÂçï‰∏ä„ÄÇ&lt;br>
Âêå‰∏äÔºåÂ∞Ü‰∏ãÈù¢ÁöÑËØ≠Âè•Âä†ÂÖ•Âà∞ &lt;code>.vimrc&lt;/code> &lt;br>
(‰∏çË¶ÅÁõ¥Êé•Â§çÂà∂Á≤òË¥¥ÔºåÁÑ∂ÂêéÊåáÊúõÂÆÉÂ∞±È©¨‰∏äËßÅÊïà üôÑ Âõ†‰∏∫‰Ω†ÁöÑÁõÆÂΩï‰∏ç‰ºö/‰πü‰∏çÈúÄË¶ÅÂíåÊàëÁöÑÁõ∏Âêå):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-vim" data-lang="vim">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">let&lt;/span> g:vimwiki_list = [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;index&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;index&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;path_html&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimwikiHtml&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.html&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_default&amp;#39;&lt;/span>:&lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/template/&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;custom_wiki2html&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/wiki2html.sh&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools/vim8&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools/CLIs&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Julia&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Latex&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Python&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/ShellScripts&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ ]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Âú®‰∏äÈù¢Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåÊØè‰∏Ä‰∏™Ëä±Êã¨Âè∑ÁöÑÂÜÖÂÆπÂÖ∂ÂÆûÂ∞±ÊòØÂØπÂ∫î‰∏Ä‰∏™‰∏ªÈ¢òÁöÑËèúÂçï (index.md)ÔºåÊ†πÊçÆ
vimwiki ÁöÑÈªòËÆ§Âø´Êç∑ÈîÆ, ÊØîÂ¶ÇÔºå &lt;code>2&amp;lt;leader&amp;gt;ww&lt;/code> Â∞±‰ºöÁõ¥Êé•ÊâìÂºÄ &lt;code>~/Vimwiki/LinuxTools/index.md&lt;/code> Ëøô‰∏™Êñá‰ª∂„ÄÇ
ÈÇ£‰πàÔºåÂíå‰ª•‰∏äÁöÑËÆæÁΩÆÂØπÂ∫îÁöÑÁõÆÂΩïÁªìÊûÑÊòØÊÄé‰πàÊ†∑ÁöÑÂë¢Ôºü‰ª•‰∏ãÂ∞±ÊòØÊàë‰∏™‰∫∫ vimwiki ÁõÆÂΩïÁªìÊûÑ (‰ΩøÁî®ÂëΩ‰ª§ &lt;code>tree -L 2&lt;/code>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ LinuxTools
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ commonCommandLines.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ images
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ jupyterWithDSKernels.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ linux_oracle_client_sqlplus_install_and_connection_setup.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ linux_sqlserver_sqlcmd_install_and_connection_setup.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ newBashConfig.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python_packages_management.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ tmux.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ vim8.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vimwiki.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ mathjax
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bower.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ composer.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ CONTRIBUTING.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ es5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ LICENSE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ package.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ myvimwikidir.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ Programming
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Julia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Latex
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Python
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ShellScripts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ Session.vim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ template
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ default.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ markdown.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îî‚îÄ‚îÄ wiki2html.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Â¶ÇÂâçÊâÄËø∞ÔºåÊ†πÁõÆÂΩïÁöÑ &lt;code>index.md&lt;/code> ÊòØÈ°∂Á∫ß‰∏ªËèúÂçïÔºå‰ªéËøô‰∏™Êñá‰ª∂ÂÜÖË∑≥ËΩ¨Âà∞ÂêÑ‰∏™Â≠ê‰∏ªÈ¢ò„ÄÇÊØîÂ¶ÇÔºå
Êàë‰∏™‰∫∫ÊÑüÂÖ¥Ë∂£ÁöÑ„ÄÅ‰∏ÄÁõ¥Âú®Â≠¶‰π†ÁöÑ3Â§ß‰∏ªÈ¢òÂàÜÂà´ÊòØÔºöÊú∫Âô®Â≠¶‰π†ÔºàÊú™ÂàóÂá∫ÔºâÔºåÁºñÁ®ãÂíåLinuxÂ∑•ÂÖ∑„ÄÇ
ËÄåÂú® Programming Ëøô‰∏™ÁõÆÂΩï‰∏ãÁöÑ &lt;code>index.md&lt;/code> Â∞±ÊòØËøô‰∏™‰∏ªÈ¢òÁöÑ‰∏ªËèúÂçïÔºåÊòæÁÑ∂ÔºåËøô‰∏™‰∏ªÈ¢ò
‰∏ãËøòÊúâ‚ÄòJulia‚ÄôÔºå‚ÄòLatex‚ÄôÔºå‚ÄòPython‚ÄôÔºå‚ÄòShellScripts‚ÄôÁ≠âÂá†‰∏™ÂÜÖÂÆπ„ÄÇ‰∏çÁî®ËØ¥ÔºåËøòÂèØ‰ª•ËÆæËÆ°
Êõ¥ÁªÜÂàÜÁöÑÂÜÖÂÆπ„ÄÇÊØè‰∏™‰∏ªÈ¢òÈÉΩË∂≥Â§üÊàëÂñù‰∏äÂá†Â£∂ÁöÑ‰∫Ü üçª „ÄÇÊØïÁ´üÔºåË∞ÅÂèàËØ¥‰∫ÜÊûÑÂª∫ÊØîËæÉÂÆåÊï¥ÁöÑ
Áü•ËØÜ‰ΩìÁ≥ªÊòØ‰∏ÄÊúù‰∏ÄÂ§ïËÉΩÂÆåÊàêÁöÑÂë¢ÔºüÊ≠£ÊâÄË∞ìÔºåÂøó‰∏çÂº∫ËÄÖÊô∫‰∏çËææ„ÄÇ&lt;/p>
&lt;p>ËÄå mathjax ÁõÆÂΩïÔºåÊ≠£ÊòØÊÉ≥Ë¶ÅÂ±ïÁé∞Êï∞Â≠¶ÂÖ¨ÂºèÊïàÊûúÊâÄ‰æùËµñÁöÑÊú¨Âú∞ÂåÖÔºåÊé•‰∏ãÊù•Â∞±ËÆ≤Ëøô‰∏™ÁöÑÈÖçÁΩÆ„ÄÇ&lt;/p>
&lt;h3 id="Êï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞" >
&lt;div>
&lt;a href="#%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%8f%e7%9a%84%e5%91%88%e7%8e%b0">
##
&lt;/a>
Êï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞
&lt;/div>
&lt;/h3>
&lt;p>&lt;code>vimwiki2html&lt;/code> ÂëΩ‰ª§Âíå &amp;ldquo;mathjax&amp;rdquo; ÁõÆÂΩï&lt;/p>
&lt;p>vimwiki Ëá™Â∏¶‰∫Ü‰∏Ä‰∏™ &lt;code>vimwiki2html&lt;/code> ÂëΩ‰ª§ÔºåÈ°æÂêçÊÄù‰πâÔºåÂ∞±ÊòØÊää mardown/wiki Ê†ºÂºèÁöÑÊ∫ê
ÊñáÊ°£ËΩ¨Êç¢Êàê html Ê†ºÂºèÁöÑÊñáÊ°£„ÄÇ‰ΩÜËøô‰∏™ÂëΩ‰ª§Âπ∂‰∏çÊîØÊåÅÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊ∏≤ÊüìÔºåÂæóÊîπÈÄ†„ÄÇËøôÊ≠£ÊòØ
&lt;a href="https://www.mathjax.org/">mathjax&lt;/a> Èó™‰∫ÆÁôªÂú∫ÁöÑÂú∞Êñπ„ÄÇÂ∑≤ÁªèÊúâ‰∫∫Êää &lt;code>vimwiki2html&lt;/code>
ÂëΩ‰ª§ÊâÄ‰æùËµñÁöÑËÑöÊú¨‰Ωú‰∫ÜÁõ∏Â∫îÁöÑ&lt;a href="https://gist.github.com/enpassant/0496e3db19e32e110edca03647c36541">ÂçáÁ∫ßÁâà&lt;/a>Ôºå
ÂÆÉËÉΩÂ∞Ü markdown Êñá‰ª∂ËΩ¨Êç¢‰∏∫ html Êñá‰ª∂ÔºåÂπ∂ÊîØÊåÅÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊ∏≤Êüì„ÄÇÊúâ‰∫õ‰∫∫Ëµ∞ÁöÑ&lt;a href="https://gist.github.com/maikeldotuk/54a91c21ed9623705fdce7bab2989742">Êõ¥Ëøú&lt;/a>
‰ΩÜ‰ªñ‰ª¨ÂùáÂÅáËÆæÊ≤°ÊúâÊâÄË∞ìÁöÑ‰ªÄ‰πà‚ÄúÂ§ñÁΩë‚ÄùËÆøÈóÆÈôêÂà∂‰∫ë‰∫ë„ÄÇÊâÄ‰ª•ÔºåËøòÈúÄË¶Å‰∏ÄÁÇπÊäò‰∏≠Êìç‰ΩúÔºåÈÇ£Â∞±ÊòØ
‚ÄòÊú¨Âú∞Âåñ‚Äô„ÄÇÂÖàÂì≤ËØ¥‚ÄòÂ±±‰∏çËøáÊù•ÔºåÊàëËµ∞ËøáÂéª‚Äô„ÄÇÁΩëÁªúÊó∂‰ª£ÂàöÂ•ΩÁõ∏ÂèçÔºåÊó¢ÁÑ∂ËÆøÈóÆ‰∏çÂà∞ÔºåÈÇ£Â∞±ÊääÂÆÉ
Â§çÂà∂ÂõûÊù•ÔºàÂü∫Êú¨ÂâçÊèêÊòØÂºÄÊ∫êÔºåÂóØÔºå‰ºüÂ§ßÁöÑÂºÄÊ∫êÔºâ„ÄÇ&lt;/p>
&lt;p>ÊâÄË∞ìÊú¨Âú∞ÂåñÔºåÂ∞±ÊòØÔºö1ÔºâÊää mathjax Â§çÂà∂Âà∞Êú¨Âú∞Ôºõ2ÔºâÈÖçÁΩÆ vimwiki ÁöÑÂëΩ‰ª§ËÑöÊú¨ÊâæÂà∞ÂÆÉ„ÄÇ
‰ª•‰∏ãÊòØÂÖ∑‰ΩìÊìç‰ΩúÔºö&lt;/p>
&lt;ol>
&lt;li>Âú® vimwiki ÁöÑÊ†πÁõÆÂΩï‰∏ãÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>git clone https://github.com/mathjax/MathJax.git mathjax&lt;span style="color:#5af78e">`&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>‰∏ãËΩΩÂπ∂Êõ¥Êîπ &lt;code>wiki2html.sh&lt;/code> Êñá‰ª∂ÁöÑ‰ª•‰∏ãÂÜÖÂÆπ
(# Âè∑ÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØË¢´Ê≥®ÈáäÊéâÁöÑÂéüÊñáÔºå‰ª•Â§áÂèÇËÄÉ):&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># HAS_MATH=$(grep -o &amp;#34;\$\$.\+\$\$&amp;#34; &amp;#34;$INPUT&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># if [ ! -z &amp;#34;$HAS_MATH&amp;#34; ]; then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># MATH=&amp;#34;--mathjax=https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># MATH=&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># fi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">HAS_MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">$(&lt;/span>grep -o &lt;span style="color:#5af78e">&amp;#34;\$\$*\+\$\$&amp;#34;&lt;/span> &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff5c57">$INPUT&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># # load mathjax from local copy (in case the CDN not work)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> &lt;span style="color:#ff6ac1">[&lt;/span> -n &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff5c57">$HAS_MATH&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span> &lt;span style="color:#ff6ac1">]&lt;/span>; &lt;span style="color:#ff6ac1">then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;--mathjax=path2your-mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">fi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ê≥®ÊÑèÔºö&amp;ldquo;path2your-mathjax&amp;rdquo; Â∫îËØ•‰øÆÊîπÊàê‰Ω†Ëá™Â∑±ÁöÑË∑ØÂæÑÔºå
ÊØîÂ¶ÇÊàëÁöÑË∑ØÂæÑÊòØËøôÊ†∑ÁöÑÔºö&amp;rsquo;/home/myacct/VimWiki/mathjax/es5/'&lt;/p>
&lt;p>Êàë‰∏çÂ§™ÊáÇ htmlÔºåÊàëÂú®ËøôÈáåÁöÑËØâÊ±ÇÂ∞±ÊòØËÉΩËÆ©ËΩ¨Êç¢ÂêéÁöÑ html ÊñáÊ°£Ê≠£Â∏∏ÊòæÁ§∫Êï∞Â≠¶ÂÖ¨Âºè„ÄÇ
‰ΩÜÂêéÊù•ÂèëÁé∞ÔºåË¶ÅÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºåÂÖ∂ÂÆû‰∏ç‰∏ÄÂÆöÈùûÂæóÂÖàËΩ¨Êç¢Êàê html Êñá‰ª∂„ÄÇ&lt;/p>
&lt;h2 id="markdown-ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫-vim-instant-markdown" >
&lt;div>
&lt;a href="#markdown-%e6%96%87%e6%a1%a3%e7%9a%84%e5%8d%b3%e6%97%b6%e5%b1%95%e7%a4%ba-vim-instant-markdown">
#
&lt;/a>
markdown ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫: &lt;code>vim-instant-markdown&lt;/code>
&lt;/div>
&lt;/h2>
&lt;p>ÂâçÈù¢ËØ¥ËøáÔºåÂ§ßÈÉ®ÂàÜÁöÑ markdown Âπ∂Èùû‚ÄúÊâÄËßÅÂç≥ÊâÄÂæó‚ÄùÔºå‰ΩÜÂ¶ÇÊûú‰∏Ä‰∏™‰∫∫ÁöÑÊ¨≤ÊúõÊ≠£Â∏∏ÔºåÈÄöÂ∏∏ÈÉΩÊÉ≥
Ë¶ÅÁúãÁúãÊ≠£Âú®ÂÜôÁöÑÂÜÖÂÆπÂà∞Â∫ïÊòØ‰ªÄ‰πàÊòæÁ§∫ÊïàÊûúÔºåÊØïÁ´üÔºåÊó©ÂèëÁé∞Êó©Ê≤ªÁñóÂòõÔºà‰∏ÄÊ∞îÂëµÊàêÁöÑÁàΩÊñáËá™ÁÑ∂
ÊòØÊúâÔºå‰ΩÜÊàëÊõ¥Áõ∏‰ø°‰ºòÁßÄÁöÑ‰ΩúÂìÅÈÄöÂ∏∏ÊòØ‰øÆÊîπÂá∫Êù•ÁöÑÔºâ„ÄÇÂΩìÁÑ∂ÔºåÊó©Â∞±Êúâ‰∫∫ÊääËøôÁßçÊ≠£Â∏∏‰∏çËøáÁöÑ
ÈúÄÊ±ÇÂÆûÁé∞Âπ∂ÂÖ±‰∫´‰∫ÜÔºåÈÇ£Â∞±ÊòØÔºö &lt;a href="https://github.com/instant-markdown/vim-instant-markdown">vim instant markdown&lt;/a>
Ëøô‰πüÊòØ‰∏Ä‰∏™ Vim ÁöÑÊèí‰ª∂ÔºåÂÖ∑‰ΩìÁöÑÂÆâË£ÖÂíåÈÖçÁΩÆ‰πüÁÆÄ‰æøÔºåÁõ¥Êé•Ê†πÊçÆÂÖ∂ readme Êñá‰ª∂ËøõË°åÂç≥ÂèØ„ÄÇ&lt;/p>
&lt;p>‰ΩÜÂÖ∂ÂÆû‚ÄúÊØèÊîπ‰∏Ä‰∏™Â≠óÂ∞±Âà∑Êñ∞‰∏ÄÊ¨°‚ÄùÂíå‚ÄúÂÜôÂÆå‰∫ÜÊâÄÊúâÂÜÖÂÆπÊâçÁúã‰∏ä‰∏ÄÁúºÊúÄÁªàÊïàÊûú‚Äù‰∏ÄÊ†∑‰∏çÁîöÂ¶•Ë¥¥Ôºå
ÂâçËÄÖ‰Ωø‰∫∫ÂàÜÂøÉÔºåÂêéËÄÖ‰Ωø‰∫∫ÊãÖÂøÉ„ÄÇÂõ†Ê≠§ÔºåÂΩìÊÉ≥Ë¶ÅÊ£ÄÊü•Êüê‰∏™ÊÆµËêΩÔºàÂ∞§ÂÖ∂ÊòØÂåÖÂê´ËÆ∏Â§öÊï∞Â≠¶ÂÖ¨Âºè
ÁöÑÊÆµËêΩÔºâÁöÑÂëàÁé∞ÊïàÊûúÊó∂ÔºåË∞ÉÁî®ÂëΩ‰ª§ &lt;code>InstantMarkdwonPreview&lt;/code> ÔºåÂÅúÁî®Âàô &lt;code>InstantMarkdwonStop&lt;/code>„ÄÇ
ÊàëÂæàÂñúÊ¨¢ËøôÁßçÂø´ÈÄüÂèçÂ∫îÔºåÊúâ‰ªáÂ∞±Êä•ÔºåÂΩìÂú∫Â∞±Êä•ÔºõÁü•ÈîôÂ∞±ÊîπÔºåÂñÑËé´Â§ßÁÑâ„ÄÇ
‰ΩÜÂ¶ÇÊûúËßâÂæóÁôΩÁ∫∏ÈªëÂ≠óÁöÑÈ¢ÑËßàÁΩëÈ°µÁâàÈù¢Ëøá‰∫éÊú¥Á¥†‰∏çÂ§üÊªãÂë≥ÔºåÈÇ£‰πàÔºåÂèØËÉΩ‰Ω†ÈúÄË¶ÅÁöÑÊòØ‰∏Ä‰∏™ËÉΩÂ§üËá™ÂÆö‰πâÁöÑÁΩëÈ°µ„ÄÇ&lt;/p>
&lt;h2 id="markdown-ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫-jekyll-site" >
&lt;div>
&lt;a href="#markdown-%e6%96%87%e6%a1%a3%e7%9a%84%e5%8d%b3%e6%97%b6%e5%b1%95%e7%a4%ba-jekyll-site">
#
&lt;/a>
markdown ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫: jekyll-site
&lt;/div>
&lt;/h2>
&lt;p>&lt;code>vim-instant-markdown&lt;/code> Â±ïÁ§∫ÁöÑÂ∞±ÊòØÊñáÊ°£ÁöÑÂëàÁé∞ÂÜÖÂÆπÔºå‰πü‰ªÖ‰ªÖÂ∞±ÊòØÂÜÖÂÆπËÄåÂ∑≤„ÄÇÂ¶ÇÊûúÊÉ≥Ë¶Å
Êõ¥Â§öÔºåÈÇ£‰πàÔºåÂ§áÈÄâÊñπÊ°àÂ∞±ÊòØÈùôÊÄÅÁΩëÁ´ôÁîüÊàêÂô®ÔºàÂ¶Ç&lt;a href="https://jekyllrb.com/">jekyll&lt;/a>)Ôºå
È°æÂêçÊÄù‰πâÔºåÂ∞±ÊòØËøôÁé©ÊÑèÂÑøÁõ¥Êé•Áªô‰Ω†ÊâìÂåÖÁîüÊàêÁΩëÁ´ôÈúÄË¶ÅÁöÑ‰∏ÄÊèΩÂ≠êÈÉ®‰ª∂„ÄÇ
ÔºàÁΩëÁ´ôÊÑèÂë≥ÁùÄÈ´òÂ∫¶ÂÆöÂà∂ÁöÑËá™Áî±ÔºåÊòØ‰∏çÊòØÂê¨ÁùÄÂ∞±ËßâÂæóÂºÄÂøÉÔºüÂ¶ÇÊûú‰Ω†Â§üËÄêÂøÉÁúãÂà∞ËøôÈáåÔºåÊàëÂ∞±
‰º∞Êë∏ÁùÄ‰Ω†‰πüÂ∫îËØ•‰πüÊòØ‰∏™Áà±ÊäòËÖæÁöÑ‰∫∫ÔºåÁà±Âú®Êú∫Âô®‰∏äÊç£ÈºìÂêÑÁßçÂ∑•ÂÖ∑ÁöÑ‰∫∫ÊÉ≥ÂøÖ‰πüÁü•ÈÅìÔºåÊâÄË∞ìÁöÑ
‚ÄúÈ´òÂ∫¶ÂÆöÂà∂‚ÄùÁöÑÂè¶‰∏Ä‰∏™ËØ¥Ê≥ïÂ§ßÁ∫¶ÊòØËØ¥ÔºöÊù•ÂëÄÔºåÊäòËÖæÂëÄÔºåÂø´Ê¥ªÂëÄÔºâ„ÄÇ&lt;/p>
&lt;p>‰Ω†Áü•ÈÅìÂêóÔºü‰ªé &lt;code>vimwiki&lt;/code> ÁöÑÁºñËæëÁ≥ªÁªüÂà∞ jekyll ÂçöÂÆ¢ÁΩëÈ°µÔºåÂè™ÈúÄË¶Å‰∏Ä‰∏™ÂëΩ‰ª§Ôºö&lt;code>:Postit&lt;/code>&lt;/p>
&lt;p>ÂΩìÁÑ∂‰∫ÜÔºåËøôÊòéÊòæÂ±û‰∫éÂπøÂëäÁî®ËØ≠ÔºåÂíå‚Äò‰∏ÄÂ§©Â≠¶‰ºöpythonÔºå‰∏ÉÂ§©ÊéåÊè°Êú∫Âô®Â≠¶‰π†‚Äô‰πãÁ±ªÂ∑Æ‰∏çÂ§ö„ÄÇ&lt;/p>
&lt;p>‰øóËØ≠Êúâ‰∫ëÔºö‚ÄúÂâçÁ´Ø‰∏ÄÂàÜÈíüÔºåÂêéÁ´ØÂçÅÂπ¥Âäü‚Äù„ÄÇ&lt;code>Postit&lt;/code> ÂëΩ‰ª§ÈúÄË¶Å‰∏Ä‰∫õÂü∫Á°ÄÊù•ÊîØÊíëÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-viml" data-lang="viml">&lt;span style="display:flex;">&lt;span>command Postit :&lt;span style="color:#ff6ac1">execute&lt;/span> &lt;span style="color:#5af78e">&amp;#34;w!&amp;#34;&lt;/span> . &lt;span style="color:#5af78e">&amp;#34;Path_to_jekyllsite_root/_posts/&amp;#34;&lt;/span> . strftime(&lt;span style="color:#5af78e">&amp;#34;%Y-%m-%d-&amp;#34;&lt;/span>) . expand(&lt;span style="color:#5af78e">&amp;#34;%:t&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ËøôËØ≠Âè•Ë¶ÅÂÜôÂú® &lt;code>.vimrc&lt;/code> Êñá‰ª∂ÈáåÔºåËøô‰∏™ÂÆûÈôÖ‰∏äÊòØËÆæÁΩÆ vim ÂëΩ‰ª§„ÄÇ&lt;/p>
&lt;p>Ê≥®: Ëøô‰∏™ÂëΩ‰ª§ÁöÑÂÆûÈôÖÊÑèÊÄùÊòØÔºöÂ∞ÜÂΩìÂâçÁºñËæëÁöÑÊñá‰ª∂‰ª• jekyll ÁöÑ post Êñá‰ª∂ÁöÑÊâÄË¶ÅÊ±ÇÊ†ºÂºè
ËøõË°åÂëΩÂêçÔºåÂπ∂‰øùÂ≠òÂà∞‰Ω†ÁöÑ jekyll È°πÁõÆÁöÑ _post/ ÁõÆÂΩï‰∏ã„ÄÇ&lt;/p>
&lt;p>Âõ†‰∏∫ jekyll ÁöÑ post ÂÖ®ÈÉ®Ë¶ÅÊ±Ç‰ª•‚ÄúÂπ¥-Êúà-Êó•-‚Äù‰Ωú‰∏∫Êñá‰ª∂ÂêçÂâçÁºÄ (ÊØîÂ¶ÇÔºö
‚Äú2022-02-02-Â•ΩÂ§ö‰∫å.md‚Äù)„ÄÇÂëΩ‰ª§‰∏≠ÁöÑ ‚Äú.‚Äù ÊòØ viml ËØ≠Ê≥ï‰∏≠ &amp;ldquo;concatenate&amp;rdquo; ÁöÑÊÑèÊÄù„ÄÇ&lt;/p>
&lt;p>ËôΩÁÑ∂Ë∞à‰∏ç‰∏äÈöæ‰ª•ÂøçÂèóÔºå‰ΩÜÊàë‰∏™‰∫∫‰∏çÊòØÂæàÁêÜËß£ËøôÁßçÁ°¨ÁºñÁ†ÅÔºåÊñá‰ª∂ÂëΩÂêçÊ†ºÂºè‰∏∫‰ΩïÈùûÂ¶ÇÊ≠§‰∏çÂèØÔºü
ÊòØbugËøòÊòØfeatureÔºåÈÇ£Â∞±ËßÅ‰ªÅËßÅÊô∫‰∫Ü„ÄÇÊàëÂê¨ËØ¥ËøòÊúâ‰∏Ä‰∏™ÈùôÊÄÅÁΩëÁ´ôÁîüÊàêÂô®ÂêçÂè´
&lt;a href="https://gohugo.io/">Hugo&lt;/a> ÁöÑÔºå‰∏çÁü•ÂÖ∂ÊâãËâ∫Â¶Ç‰Ωï &amp;hellip;&lt;/p>
&lt;hr>
&lt;p>ÂàöÂºÄÂßã‰ΩøÁî® vimwiki Êó∂Ôºå‰πüÊòØ‰∏çÂÅúÁöÑÊë∏Á¥¢ÔºåÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊòæÁ§∫‰∏ÄÁõ¥ÊòØ‰∏™Â§¥ÁñºÁöÑÈóÆÈ¢òÔºåÊØïÁ´üÔºå
‰ªé‚ÄúÊ≠™Êûú‰ªÅ‚ÄùÈÇ£ÈáåÂÄüÈâ¥Êù•ÁöÑÈªòËÆ§ÈÖçÁΩÆÈúÄË¶ÅËÉΩËÆøÈóÆÂ§ñÁΩëÔºåËÄåËøô‰∏™ÊÉ≥ÂøÖ‰∏çÊòØÊàë‰ª¨ËÉΩÂ§üÈªòËÆ§Âæó‰∫ÜÁöÑÔºå
Âè™ËÉΩÂÖàÂøçÁùÄÈÇ£‰∫õ‰ª§‰∫∫‰∏çÂø´ÁöÑÂ¶ÇÂêå‰π±Á†ÅÁöÑ latex ËØ≠Âè•ÊåÇÂú®È°µÈù¢‰∏äÔºå
ÔºàÂ∞§ÂÖ∂ÊòØÔºåÂΩì‰Ω†ÊòéÊòéÁü•ÈÅìËøô‰∫õÂÖ¨ÂºèÊú¨ÂèØ‰ª•‰ºòÈõÖÁöÑÂ±ïÁ§∫ÔºåÂç¥‰∏çËÉΩÂ§üÊó∂Ôºå‰∏çÂø´Â∞±ÂèòÊàê‰∫ÜÊÑ§ÊÄíÔºâ„ÄÇ&lt;/p>
&lt;p>Ê≤°ÊúâÂÉè‰Ωú‰∏∫Êñ∞ÊâãÊó∂Âú®‰∏çÂêåÁöÑ Linux ÂèëË°åÁâàË∑≥Êù•Ë∑≥ÂéªÈÇ£Ëà¨ÔºåÊàëÁü•ÈÅì vimwiki Â∞±ÊòØÊàëÁöÑËèúÔºå
‰ª•Ââç‰ΩøÁî® vimwiki ËøáÁ®ãÁ¢∞‰∏äÁöÑÂêÑÁßç‚ÄúÊöÇÊó∂‚ÄùÁöÑÊ≤°ÊúâËß£ÂÜ≥ÁöÑÈóÆÈ¢òÔºåÂºÄÂßãÂèòÂæó‰ª§‰∫∫‰∏çËÉΩÂøçÂèóÔºå
ÁâπÂà´ÊòØÂπ≥Êó∂ÊµèËßàÂÖ∂‰ªñÁΩëÈ°µÊó∂ÊØèÊØèÁúãÂà∞‰∫∫ÂÆ∂ÁöÑÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÂèàÂ§öÂèàÈùìÔºåÈÇ£ÁÆÄÁõ¥Â∞±ÊòØ‰∏ÄÂú∫‰æÆËæ±„ÄÇ
ÊàëËµ∞‰∫ÜËøô‰πàËøúÔºåÂ∞±Â∑ÆÈÇ£‰πàÊúÄÂêé‰∏ÄÊ≠•ÔºåÊàëÂøÉ‰∏çÁîò„ÄÇÊâÄ‰ª•ÊàëÂ∞±ÊäòËÖæÔºå‰∏ÄÂ§©Ôºå‰∏§Â§©Ôºå‰∏âÂ§© &amp;hellip;
ÊàëÊòØË¥™Â©™ÁöÑ‰∫∫ÔºåÊó†Ê≥ïÊîæÂºÉ‰∏ÄÊ≠•‰πãÈÅ•ÁöÑËØ±ÊÉëÔºåÂπ∏ËøêÁöÑÊòØÊàëËøòÊòØÊäòËÖæÂà∞‰∫ÜÊàëÊÉ≥Ë¶ÅÁöÑ‚ÄúÊõ¥Â§ö‚Äù„ÄÇ&lt;/p>
&lt;p>ÊòØÁöÑÔºåË∑ùÁ¶ªÂ§™ËøúÂèØËÉΩÁõ¥Êé•ÊîæÂºÉÔºåËÄå‰∏ÄÊ≠•‰πãÈÅ•ÊúÄÊòØÈîÄÈ≠Ç üòÇ&lt;/p>
&lt;p>ÂÖ∂ÂÆûÔºåËØ•Â∫ÜÂπ∏ÁöÑÊòØÊàëÁ¢∞‰∏äÁöÑÈóÆÈ¢òÊó©Â∞±Êúâ‰∫∫ÂÖàÁ¢∞‰∏ä‰πüÂÖàËß£ÂÜ≥‰∫ÜÔºåÊàëÂè™ÊòØÊâæÂà∞‰∫ÜËøô‰∏™ÊñπÊ≥ïÔºå
Â∫îËØ•ËØ¥ÊòØ‚ÄòÊêúÁ¥¢‚ÄôÂà∞‰∫ÜËøô‰∏™ÊñπÊ≥ïÔºåÂπ∂‰Ωú‰∫ÜÂøÖË¶ÅÁöÑË∞ÉÊï¥„ÄÇ‚ÄòÈóªÈÅìÊúâÂÖàÂêé‚ÄôÁöÑÂü∫Á°ÄÂ∞±ÊòØÂàÜ‰∫´Ôºå
ÂØπ‰∫éËøô‰∫õËß£ÂÜ≥‰∫ÜÂÆûÈôÖÈóÆÈ¢òÁöÑ‰∫∫ÔºåÂæàÊòØÊÑüÊøÄ‰ªñ‰ª¨ÁöÑÂàÜ‰∫´ÔºåÊ≤°ÊúâÂàÜ‰∫´ÔºåÂ§ßÂÆ∂ÁöÑÊó•Â≠ê‰∏ç‰ºöÂ•ΩËøá„ÄÇ&lt;/p></description></item><item><title>Âà©Âô®3: Jupyterlab and its Kernels</title><link>/posts/thecli/2022-07-22-jupyterwithdskernels/</link><pubDate>Fri, 22 Jul 2022 22:01:51 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/2022-07-22-jupyterwithdskernels/</guid><description>&lt;p>&lt;img alt="IMG_jupyterlab" src="images/jupyter_kernels.png">&lt;/p>
&lt;p>New machine needs to be tailored for a better personal workflow. And this is
about how &lt;a href="https://jupyter.org/">jupyter&lt;/a> can be a multi-language support
co-work platform for regular DataScience works. Also note that Julia community
has &lt;a href="https://github.com/fonsp/Pluto.jl">pluto&lt;/a> notebook, and R has its
&lt;a href="https://www.rstudio.com/">RStudio&lt;/a>.&lt;/p>
&lt;h1 id="jupyter-julia--python--r-all-in-one" >
&lt;div>
&lt;a href="#jupyter-julia--python--r-all-in-one">
##
&lt;/a>
JuPyteR: Julia + Python + R all in one
&lt;/div>
&lt;/h1>
&lt;p>1 check the Python came with system&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ which python3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If there is no python3 yet, install one:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ sudo apt install python3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this would install the latest version of Python.&lt;/p>
&lt;p>2 Poetry or Virtualenv (Poetry is way tooooo slow 2021-12)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># create virtual envs:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python3 -m venv dsvenv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># install jupyterlab:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./dsvenv/bin/python -m pip install jupyterlab
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this may create a venv without &lt;code>activate&lt;/code> scripts.
If this is the case, check &lt;a href="https://stackoverflow.com/questions/26215790/venv-doesnt-create-activate-script-python3">more discussion
here&lt;/a>&lt;/p>
&lt;p>3 &lt;a href="https://mirrors.tuna.tsinghua.edu.cn/CRAN/">install R&lt;/a>, then&lt;/p>
&lt;p>4 &lt;a href="https://julialang.org/downloads/platform/#linux_and_freebsd">install Julia&lt;/a>&lt;/p>
&lt;p>5 activate python&amp;rsquo;s dsvenv (source ./dsvenv/bin/activate), then&lt;/p>
&lt;p>a. Enter interactive R (REPL environment)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-R" data-lang="R">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># install &amp;#39;IRkernel&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#57c7ff">install.packages&lt;/span>(&lt;span style="color:#5af78e">&amp;#39;IRkernel&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># register the kernel in the current R installation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IRkernel&lt;span style="color:#ff6ac1">::&lt;/span>&lt;span style="color:#57c7ff">installspec&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># exit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#57c7ff">q&lt;/span>()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>b. Enter interactive Julia (./path-to/julia.1.6.5/bin/julia)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Julia" data-lang="Julia">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">using&lt;/span> Pkg
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Pkg&lt;span style="color:#ff6ac1">.&lt;/span>add(&lt;span style="color:#5af78e">&amp;#34;IJulia&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Optional: build IJulia&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Pkg&lt;span style="color:#ff6ac1">.&lt;/span>build(&lt;span style="color:#5af78e">&amp;#34;IJulia&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that if the path of binary of Julia somehow changed (reinstall etc),
one need to perform extra command &lt;code>Pkg.build(&amp;quot;IJulia&amp;quot;)&lt;/code> which tells jupyter
where to find the binary of Julia.&lt;/p>
&lt;p>6 launch jupyterlab server, and that&amp;rsquo;s it!&lt;/p></description></item><item><title>Âà©Âô®2: Vimwiki ‰πã Â¶Ç‰ΩïÁî® vimwiki ËÆ∞Á¨îËÆ∞</title><link>/posts/thecli/thevim/2022-07-15-vimwiki/</link><pubDate>Thu, 21 Jul 2022 23:37:31 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/thecli/thevim/2022-07-15-vimwiki/</guid><description>&lt;h2 id="‰∏™‰∫∫Áª¥Âü∫-personal-wiki" >
&lt;div>
&lt;a href="#%e4%b8%aa%e4%ba%ba%e7%bb%b4%e5%9f%ba-personal-wiki">
#
&lt;/a>
‰∏™‰∫∫Áª¥Âü∫ (personal wiki)
&lt;/div>
&lt;/h2>
&lt;p>ÈòÖËØªËøá„ÄäÁà±‰∏Ω‰∏ùÊº´Ê∏∏Â•áÂ¢É„ÄãÁöÑÂ∞è‰ºô‰º¥Â∫îËØ•‰ºöËßâÂæóÁÜüÊÇâÔºåÁª¥Âü∫Â∞±ÂÉèÈÇ£‰∏™‚ÄúÂÖîÂ≠êÊ¥û‚ÄùÔºå
ÊØè‰∏™ËØçÊù°ÈÉΩÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÈÄöÂæÄÂÖ∂‰ªñÂ•áÂ¢ÉÁöÑÂ§ßÈó®Ôºå‰ΩÜÂÆÉÂè™ÂØπÊúâÂ•ΩÂ•áÂøÉÁöÑ‰∫∫ÊâçÊâìÂºÄ„ÄÇ
ÔºàÊàñËÄÖËØ¥ÊúâÂ•ΩÂ•áÂøÉÁöÑ‰∫∫Êâç‰ºöÊâìÂºÄÂÆÉÔºåÂ∞ºÂ••Ë¶ÅÊòØÊ≤°ÊúâÈÄâÊã©Á∫¢Ëâ≤üíäÔºåÊïÖ‰∫ãÊÅêÊÄïÂ∞±Ê≤°Ë∂£ÂæóÂ§öÔºâ„ÄÇ
ËøôÊòØÂú®Áª¥Âü∫‰∏äÈòÖËØªÊó∂Â∏∏ÊúâÁöÑÊÑüÂèó„ÄÇÔºàÂÖ∂‰ªñÁΩëÁ´ôÂæàÂ§öÊó∂ÂÄôÂàôÊêûÁöÑÊõ¥ÂÉèÊòØ‚ÄúÁõò‰∏ùÊ¥û‚Äù„ÄÇÔºâ&lt;/p>
&lt;p>ÊØè‰∏™‰∫∫ÈÉΩÊúâÂàõ‰ΩúÁöÑÂÜ≤Âä®„ÄÇ
ÁÑ∂ËÄå‰∫ãÂÆûÂèØËÉΩÊõ¥ÁÆÄÂçï:Âπ≥Êó∂ËØª‰π¶Â≠¶‰π†Â∑•‰ΩúÊó∂ÂñúÊ¨¢‰∏úÂÜôË•øÂàíÊÄª‰ºöÁïô‰∏ã‰∏Ä‰∫õ‚ÄòÁ¨îËÆ∞‚Äô„ÄÇ
ÂØπÂà´‰∫∫Êù•ËØ¥Â∞±ÊòØ‰∫õËÉ°Ê∂Ç‰π±ÁîªÔºåÂèØËá™Â∑±ÊÄªËßâÂæó‚ÄúÁèçË¥µ‚Äù &amp;ndash; ‚ÄúÂ®ò‰∏çÂ´åÂÑø‰∏ë‚ÄùËØ¥ÁöÑÂ§ßÊ¶ÇÂ∞±ÊòØËøô‰∏™„ÄÇ
ÊÖ¢ÊÖ¢Âú∞ÔºåÁßØÁ¥ØÁöÑÂÜÖÂÆπË∂äÊù•Ë∂äÂ§öÔºåÊ∂âÂèäÁöÑËåÉÂõ¥Ë∂äÊù•Ë∂äÂπøÔºå‰πüÂ∞±ÊòæÂæóË∂äÊù•Ë∂äÈõ∂Á¢é„ÄÇ
ÔºàÂóØÔºå‰∏∫‰ªÄ‰πà‰∏çÂú®ÂºÄÂßãÁöÑÊó∂ÂÄôÂ∞±ËßÑÂàíÂ•ΩÂë¢ÔºüÁà∏Â¶àÈÄöÂ∏∏ÈÉΩÊòØÊèêËøôÁßçÂ•ΩÈóÆÈ¢òÁöÑÈ´òÊâã)„ÄÇ
‰∏çËøáËøôÂú®ÊàëÁúãÊù•Êõ¥ÂÉè‰∏ÄÁßç‚ÄúÈ©¨ÂêéÁÇÆ‚ÄùÂºèÁöÑË¥£ÈóÆÔºåÊÄªÊúâ‰∏Ä‰∏¢‰∏¢‚Äú‰ººÊòØËÄåÈùû‚ÄùÁöÑÊÑüËßâ„ÄÇ
Â≠¶‰π†ÁöÑËøáÁ®ãÊõ¥ÂÉè‰∏ÄÁßçÊé¢Á¥¢ËøáÁ®ãÔºåÂØπÂ§ßÈÉ®ÂàÜ‰∫∫Êù•ËØ¥Ôºå‰∫ãÂÖàÊÅêÊÄïÈÉΩÊó†Ê≥ïÊ∏ÖÊô∞Âú∞Áü•ÈÅì,
‰π¶Â±±ÁöÑË∑ØÈÄöÂêë‰ΩïÂ§ÑÔºåÂ≠¶Êµ∑ÁöÑËàüÂºï‰Ω†Ëµ∞ÂêëÂì™Èáå„ÄÇÂØπ‰∫é‰∏çËÉΩÈ¢ÑÂÖàÁü•ÈÅìÁªÜËäÇÁöÑÂÜÖÂÆπÔºå
ÈÄöÂ∏∏‰ºöÂú®ÂºÄÂßãÁöÑÊó∂ÂÄôÂÜ†‰ª•‚ÄúÊàëÁöÑÁ¨îËÆ∞‚ÄùËøô‰πà‰∏Ä‰∏™ÁõÆÂΩïÔºåÁÑ∂Âêé‰∏ãËÆæ‰∏Ä‰∫õÁ≤óÁï•ÁöÑÂ≠¶ÁßëÂêçÁß∞ÁõÆÂΩïÔºå
Áõ¥Âà∞‰∏çÂÜçÊª°ÊÑèËøôÊ†∑ÁöÑÂÆâÊéí„ÄÇÊØ´Êó†ÁñëÈóÆÔºå‰∏ã‰∏ÄÊ≠•Â∞±ÊòØË¶ÅÁî®‰∏ÄÁßçÂêàÈÄÇÁöÑÊñπÂºèÂ∞ÜÂÆÉ‰ª¨ÁªÑÁªáËµ∑Êù•„ÄÇ&lt;/p>
&lt;p>Áõ∏ÂØπÁã¨Á´ãÁöÑ‰∏ªÈ¢òËá™Êàê‰ΩìÁ≥ªÔºå‰∏çÂêå‰∏ªÈ¢ò‰πãÈó¥ÂÖ≥ËÅîÁöÑÂÜÖÂÆπÂèØ‰ª•ËøûÊé•Ëµ∑Êù•Ôºå
‰∏ç‰ªÖÂú®ÈòÖËØªÊó∂ÂèØ‰ª•‰∫´ÂèóËøôÁßç‰æøÊç∑ÁöÑËá™Áî±Ë∑≥ËΩ¨ÔºåÊúÄÂ•ΩÂú®ËÆæËÆ°ÂíåÁºñÂÜôÊó∂‰πüËÉΩÂ¶ÇÊ≠§„ÄÇ
ÂØπÊàëËÄåË®ÄÔºåÁ≠îÊ°àÂ∞±ÊòØ &lt;a href="https://github.com/vimwiki/vimwiki">vimwiki&lt;/a>.&lt;/p>
&lt;p>ÈòÖËØªÁöÑÊó∂ÂÄôËá™Áî±Ë∑≥ËΩ¨ÔºåËøô‰∏™Â•ΩÁêÜËß£ÔºåÂèØÁºñÂÜôÂÜÖÂÆπ‰∏çÈÉΩ‰∏ÄÁØá‰∏ÄÁØáÁöÑÊù•ÂêóÔºü
ÁºñÂÜôÁöÑÊó∂ÂÄô‰πü‚ÄúËÉ°Â¥©‰π±Ë∑≥‚ÄùÁöÑÔºåÂê¨Ëµ∑Êù•ÂèØ‰∏çÊÄé‰πàÁæéÂ¶ôÔºåÊÄï‰∏çÊòØÊúâÊØõÁóÖÔºü‰∏çÔºå‰∏çË¶ÅËØØ‰ºöÔºå
ËøôÂÖ∂ÂÆûÊòØÊåáÔºö1ÔºâÂêåÊó∂Âú®Â§ö‰∏™Á™óÊ†º(Pane)ÊâìÂºÄÂ§ö‰∏™ÊñáÊ°£Ôºõ2ÔºâÂú®Á™óÊ†º‰πãÈó¥ËøõË°å‚ÄúÈó™ËΩ¨ËÖæÊå™‚Äù„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúÊòØÁºñÂÜôËøáÊØîËæÉÂ§çÊùÇÁöÑÊ®°ÂùóÁöÑÂ∞è‰ºô‰º¥ÂèØËÉΩ‰ºöÊõ¥Ê∑±ÊúâÊÑüËß¶Ôºå
ËøôÁßç‰∏çÂêåÊñáÊ°£‰πãÈó¥ÁöÑ‚ÄúÊóãËΩ¨Ë∑≥Ë∑ÉÊàëÂºÄÂ§©Áúº‚ÄùÁöÑË¶ÅÊ±ÇÂÖ∂ÂÆûÂπ∂‰∏çËøáÂàÜ,
Âè™ÊÅ®Ëá™Â∑±ÁöÑÂ±èÂπï‰∏çÂ§üÂ§ö‰∏çÂ§üÂ§ßÊîæ‰∏ç‰∏ãÈÇ£ÊÆµÊÆµ‰ºòÈõÖÁöÑ‰ª£Á†Å
ÔºàÊçÆÂèØÈù†ÁöÑË∞£Ë®ÄÁß∞ÔºåÊúâ‰∫õÁ®ãÂ∫èÁåø‰∏ÄÂ§©80%ÁöÑÊó∂Èó¥ÊòØÂú®Ê¨£ËµèËá™Â∑±ÂÜôÁöÑ‰ª£Á†Å‰∏≠Â∫¶ËøáÔºâ„ÄÇ&lt;/p>
&lt;p>ÊúÄÂÖ∏ÂûãÁöÑËé´Ëøá‰∫é‰ªéÂáΩÊï∞Ë∞ÉÁî®ÁöÑÂú∞ÊñπÔºåË∑≥ËΩ¨Âà∞Âú®ÈÇ£ÈÅ•ËøúÁöÑÂú∞ÊñπÁöÑÂáΩÊï∞ÂÆö‰πâÊ®°ÂùóÔºå
Â¶ÇÊûúËøô‰∏§‰∏™Âú∞ÊñπÁöÑÂÖ≥Á≥ªÈöîÁùÄ‰∏â‰∫î‰∏™ÊàñËÄÖÊõ¥Â§öÁöÑÊ®°ÂùóÁöÑÊó∂ÂÄôÔºå(ÂèØËÉΩ‰πüÊòØÈúÄË¶ÅÈáçÊûÑÁöÑ‰ø°Âè∑)
ÈÇ£‰πàÔºåÂ¶ÇÊûú‰∏çËÉΩÂêåÊó∂Â∞ÜËøôÂá†‰∏™‚ÄòÂØÜÊé•‚ÄôÊñáÊ°£Èì∫Âú®ÁúºÂâçÔºåËÄåÊòØ‰∏Ä‰∏™ÁúãÂÆåÂÜçÁúãÂè¶‰∏Ä‰∏™Ôºå
ÊúãÂèãÔºåÊàë‰∏çÁü•ÈÅì‰Ω†ÁöÑÊÉÖÂÜµÂ¶Ç‰ΩïÔºåÂèçÊ≠£ÊàëÊòØÂæàÂÆπÊòìËø∑Â§±Âú®ËøôËåÇÂØÜÁöÑ‰ª£Á†Å‰∏õÊûóÈáåÔºåÊôïÂ§¥ËΩ¨Âêë„ÄÇ&lt;/p>
&lt;p>ÊÄªËÄåË®Ä‰πãÔºå
Ê†∏ÂøÉÁöÑÈóÆÈ¢òÂú®‰∫éÂ¶Ç‰ΩïÁªÑÁªáËøô‰∫õÊñáÊ°£Ôºå‰ΩøÂæóËá™Â∑±Âú®ÊûÑÂª∫‰∏™‰∫∫‚ÄòÁü•ËØÜÂ∫ì‚ÄôÁöÑËøáÁ®ãÊõ¥‰∏ùÊªëÁïÖÂø´„ÄÇ&lt;/p>
&lt;p>Áª¥Âü∫ÁöÑÁâπÁÇπÂ∞±ÊòØ‚Äú‰Ω†‰∏≠ÊúâÊàëÔºåÊàë‰∏≠Êúâ‰ªñ‚ÄùÔºåÊØîÂ¶ÇËØ¥Ôºå
Âú®ÁªÑÁªáÂíåÊûÑÂª∫ÂÖ≥‰∫é‚ÄúÁ∫øÊÄßÂõûÂΩí‚ÄùÂÜÖÂÆπÔºåÊàëÂ∏åÊúõÊúâ‰∏Ä‰∏™‚ÄúÂÖ•Âè£‚ÄùÊñáÊ°£ÔºåÁß∞‰πã‰∏∫‰∏ªÈ¢òÁõÆÂΩïÔºå
Ëøô‰∏™ÊñáÊ°£Âè™ÂØπË¶ÅÂÜôÁöÑÁ´†ËäÇÂÜÖÂÆπËøõË°åÁΩóÂàóÂíåÁÆÄ‰ªãÔºåÊõ¥ÂÉè‰∏ÄÁßç‚ÄúÈ°∂Á∫ßËèúÂçï‚Äù (top level manu) Ôºå
Ëøô‰∏™ËèúÂçïÂåÖÂê´‰∫õ‰ªÄ‰πàËèúÂë¢ÔºüÁÆÄ‰ªãÂ§ßÊ¶ÇÊòØËøôÊ†∑ÁöÑÔºö&lt;/p>
&lt;ol>
&lt;li>ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>ÊñúÁéáÂíåÊà™Ë∑ù&lt;/li>
&lt;li>ÊúÄÂ∞è‰∫å‰πòÊ≥ï&lt;/li>
&lt;li>ÂèÇÊï∞ËØÑ‰º∞ÊúâÂ§öÂáÜ
&lt;ul>
&lt;li>ÁΩÆ‰ø°Â∫¶Âå∫Èó¥&lt;/li>
&lt;li>ÂÅáËÆæÊ£ÄÈ™å&lt;/li>
&lt;li>PÂÄº: ÊúâÂ§öÂ∞ëÁà±ÂèØ‰ª•ÈáçÊù•&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>Ëá™ÂèòÈáèÂíåÂõ†ÂèòÈáèÂà∞Â∫ïÊúâÊ≤°ÊúâÂÖ≥Á≥ª&lt;/li>
&lt;li>Âõ†ÂèòÈáèÂíåÂì™‰∫õËá™ÂèòÈáèÊúâ‰ªÄ‰πàÂÖ≥Á≥ª&lt;/li>
&lt;li>Ê®°ÂûãÊãüÂêà
&lt;ul>
&lt;li>ÊÆãÂ∑Æ(RSE)&lt;/li>
&lt;li>$R^2$ ÁªüËÆ°Èáè&lt;/li>
&lt;li>Adjusted $R^2$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ë¥ùÂè∂ÊñØÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>ÊòØÂÖàÈ™åËøòÊòØÂÅèËßÅÔºü
&lt;ul>
&lt;li>ÊàëËÆ§‰∏∫ $\Theta$ ‰∏çÂè™‰∏Ä‰∏™&lt;/li>
&lt;li>Ë¥ùÂè∂ÊñØÂÆöÂæã&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ÂêéÈ™åÊ¶ÇÁéá‰º∞ËÆ°
&lt;ul>
&lt;li>MCMC&lt;/li>
&lt;li>ÂêâÂ∏ÉÊñØÈááÊ†∑&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Âπø‰πâÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>ÊåáÊï∞ÊóèÂàÜÂ∏ÉÔºö‰∏çÂè™ÊúâÊ≠£ÊÄÅÂàÜÂ∏É&lt;/li>
&lt;li>ÊúÄÂ§ßÁÜµÔºöÊàëÊù•Êïô‰Ω†ÊåëÈÄâÂàÜÂ∏É&lt;/li>
&lt;li>ÈìæÊé•ÂáΩÊï∞ÔºöÁ∫øÊÄßÁ©∫Èó¥Âà∞ÈùûÁ∫øÊÄßÁ©∫Èó¥ÁöÑÊò†Â∞Ñ&lt;/li>
&lt;li>Ê±âÂØÜÂ∞îÈ°øËíôÁâπÂç°Ê¥õ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>ËøôÂ∞±ÊòØÂÖ≥‰∫é‚ÄúÁ∫øÊÄßÂõûÂΩí‚ÄùÂÜÖÂÆπÁöÑ‰∏ªÈ¢òÁõÆÂΩïÔºå
ÂàóÂá∫Êù•ÁöÑÊØè‰∏™ÁÇπÈÉΩÂèØ‰ª•ËÆæÁΩÆ‰∏∫ÈìæÊé•ÁÇπÔºà‰ΩÜ‰∏çÊòØÂøÖÈ°ªËøô‰πàÂÅöÔºâÔºå
‰ªéËøô‰∏™‰∏ªÈ¢òÁõÆÂΩïÔºåÂèØ‰ª•ÂéªÂà∞‰ªª‰ΩïÁöÑÂçï‰∏™ÊñáÊ°£ÔºåÂú®ÊØè‰∏™ÊñáÊ°£ÈáåÔºå
ÈÉΩÂèØ‰ª•ËÆæÁΩÆÊåáÂêëÂÖ∂‰ªñÊñáÊ°£ÁöÑÈìæÊé•ÔºåÊàñËÄÖÊåáÂêëÁΩëÁªúËµÑÊ∫êÁöÑÈìæÊé•Ôºà‰∫ã‰∫ã‰∫≤‰∏∫Âπ∂ÈùûÂøÖË¶ÅÔºâ„ÄÇ&lt;/p>
&lt;p>Â§öÂπ¥‰ª•Êù•ÊàëÂ∑≤ÁªèÊàñÂ§öÊàñÂ∞ëÂú∞ÂÅö‰∫Ü‰∏Ä‰∫õÁ´†ËäÇÂÜÖÂÆπÁöÑÁ¨îËÆ∞ÔºåÁº∫ÁöÑÊ≠£ÊòØËøôÊ†∑ÁöÑ‰∏Ä‰∏™‰∏ªÈ¢òÁõÆÂΩï„ÄÇ&lt;/p>
&lt;p>Ëá≥‰∫éÁºñËæëÂÜÖÂÆπÁöÑÊó∂ÂÄôË¶Å‰∏çË¶ÅÂú®‰∏ÄÂùóÂ±èÂπï‰∏ä‚ÄúÂêåÊó∂‚ÄùÂπ∂ÂàóÂ±ïÂºÄÂ§ö‰∏™ÊñáÊ°£ÔºåÂπ∂‚ÄúË∑≥Êù•Ë∑≥Âéª‚ÄùÔºå
ÂèØ‰ª•ËØ¥ËøôÂè™ÊòØ‰∏Ä‰∫õ‰∫∫ÁöÑÁâπÊÆäÁôñÂ•ΩÔºåÂπ∂‰∏çÊòØÈùûÂ¶ÇÊ≠§‰∏çÂèØ„ÄÇ
ÊØîÂ¶ÇÔºå24ÂØ∏Â±èÂπï‰∏äÂ¶ÇÊûúÂè™ÊâìÂºÄ‰∏Ä‰∏™ÊñáÊ°£ÔºåÊàëËßâÂæóÊòØÂØπÁ©∫Èó¥ÁöÑÊûÅÂ§ßÊµ™Ë¥πÔºõ
‰ΩÜË¶ÅÁü•ÈÅìÔºåÂ¶ÇÊûúÂú®13ÂØ∏ÁöÑÂ±èÂπï‰∏äÔºåÊúâÊó∂Êàë‰πüËøõÂÖ• &lt;a href="https://github.com/junegunn/goyo.vim">goyo
Ê®°Âºè&lt;/a> „ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÁÇπÂºÄÂâçÊñá vimwiki ÁöÑÈìæÊé•ÔºåÈÇ£‰Ω†Â∫îËØ•Áü•ÈÅìÂÆÉÊòØ Vim ÁöÑ‰∏Ä‰∏™Êèí‰ª∂„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†Â∞ö‰∏çÊ∏ÖÊ•ö Vim ÊòØÂπ≤‰ªÄ‰πàÁöÑÔºåÊàñËÄÖ‰ªÖ‰ªÖÂê¨ËØ¥ Vim ÁöÑÂ≠¶‰π†Êõ≤Á∫øÊòØ‚ÄúÂá∫‰∫ÜÂêçÁöÑÂêì‰∫∫‚ÄùÔºå
ËøôÊòØ‰∏çÂØπÁöÑÔºåËá≥Â∞ë‰Ω†ÂèØ‰ª•ÁúãÂà∞Ôºö&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: center">&lt;img alt="learning_curves" src="https://fgg100y.github.io/img/posts/vimwiki_intro/editors_learning_curves.jpg">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: center">&lt;em>some text editors&amp;rsquo; learning curve&lt;/em>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Emacs ÊâçÊòØÊó†ÂèØ‰∫âËÆÆÁöÑ NO.1 üòÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰∏çÂπ∏ÁöÑÊòØËá≥‰ªäÊÇ®‰ªçÊóßÂè™Âú®‰∏çÂπ∏ÁöÑÊìç‰ΩúÁ≥ªÁªüËøáÊ¥ªÔºåüëâ ÊòØÁöÑÔºåÊàëÊåáÁöÑÂ∞±ÊòØÂ∑®Á°¨Á≥ªÁªü„ÄÇ
‰∏çÂ¶®Âéª&lt;a href="https://www.vim.org/download.php">‰∏ãËΩΩ Gvim&lt;/a>Êù•Â∞ùËØïÂà´Ê†∑ÁöÑÁºñËæëÁöÑÊªãÂë≥Ôºå&lt;/p>
&lt;p>ÊàñËÄÖÊÇ®ÊòØÂàöÂàöÂºÄÂêØ Linux Âπ∏Á¶è‰πãË∑ØÁöÑ‰∫∫ÔºåÊàñËÄÖÊÉ≥ËØï‰∏ÄËØï Vim Ôºå
ÂæàÁÆÄÂçïÔºåÁõ¥Êé•‰∏äÊâãÁ±ªÂûãÁöÑÔºåÂú® Terminal ‰∏≠ËæìÂÖ• &lt;code>vimtutor&lt;/code>ÔºåÈÄÄÂá∫ËæìÂÖ• &lt;code>:q&lt;/code>&lt;/p>
&lt;p>ÊúÄÂ•ΩÁöÑ vim Â≠¶‰π†ÊùêÊñô‰ºöÂú®ËæìÂÖ• &lt;code>:help&lt;/code> ‰πãÂêéÂá∫Áé∞Ôºà‰ΩÜ‰∏ç‰∏ÄÂÆöÊòØÊúÄÂ•ΩÁöÑÂÖ•Èó®ÊùêÊñôÔºâ„ÄÇ
ÊàñËÆ∏‰Ω†‰ºöÊÉ≥ÁúãÁúã‰∏Ä‰∫õÂÖ•Èó®ÁöÑÊùêÊñôÔºåÊØîÂ¶Ç &lt;a href="http://vimcasts.org/">vimcast&lt;/a> ËßÜÈ¢ëÔºå
ÊØîÂ¶ÇÔºåÂπ¥‰ª£Áï•ÂæÆ‰πÖËøúÁöÑ &lt;a href="https://vim.swaroopch.com/">A Byte of Vim&lt;/a>Ôºå
ÂÜÖÂÆπÂèäÂÖ∂ËØ¶Â∞ΩÁöÑ &lt;a href="http://www.oualline.com/vim-book.html">Vim Tutorial and Reference&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Last update: 2024-07-04 Thu&lt;/p></description></item><item><title>Linear Regression: frequentist and bayesian</title><link>/posts/regressionmodels/linear_regression/</link><pubDate>Wed, 25 Aug 2021 13:49:40 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/regressionmodels/linear_regression/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>**Important Note**:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Almost all the contents (text, images) are came from these great books and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>online resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Statistics, by David Freeman, Robert Pisani, and Roger Perves
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* ÁªüËÆ°Â≠¶, David Freeman *et.al* ËëóÔºåÈ≠èÂÆóËàí Á≠âËØëÔºå‰∏≠ÂõΩÁªüËÆ°Âá∫ÁâàÁ§æ
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Êú∫Âô®Â≠¶‰π†, Âë®ÂøóÂçé Ëëó (Â§ßÂêçÈºéÈºéÁöÑ‚ÄòË•øÁìú‰π¶‚Äô)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* An Introduction to Statistical Learning, by Gareth James, Daniela Witten,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Trevor Hastie, and Robert Tibshirani
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Deep Learning, a.k.a, the flower book, by Ian Goodfellow, Yoshua Bengio, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Aaron Courville
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Introduction to Machine Learning, Barnabas Poczos, Aarti Singh, CMU-10701
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Bayesian Methods, Nicholas Ruozzi, UT-DALLAS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 style="text-align: center;">Linear Regression&lt;/h1>
&lt;h2 id="part-0-regression-101" >
&lt;div>
&lt;a href="#part-0-regression-101">
#
&lt;/a>
Part-0: Regression 101
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>$\text{You&amp;rsquo;ve got to draw the line somewhere.}$&lt;/p>
&lt;/blockquote>
&lt;h3 id="introduction" >
&lt;div>
&lt;a href="#introduction">
##
&lt;/a>
Introduction
&lt;/div>
&lt;/h3>
&lt;p>The regression method describes how one variable depends on another. For example, take height and weight. Naturally, the taller men weighed more. How much of an increase in weight is associated with a unit increase in height? To get started, look at the scatter diagram (figure 1 on below). Height is plotted on the horizontal axis, and weight on the vertical. The summary statistics are&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches,\qquad SD \approx 3 \ inches$&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches, \qquad SD \approx 45 \ pounds, \qquad r \approx 0.40$&lt;/p>
&lt;p>The scales on the vertical and horizontal axes have been chosen so that one SD of height and one SD of weight cover the same distance on the page. This makes the SD line (dashed) rise at 45 degreed across the page. There is a fair amount of scatter around the line: $r$ is only 0.40.&lt;/p>
&lt;p>&lt;img alt="IMG: regression_line" src="images/stats_weight_height.png">&lt;/p>
&lt;p>Figure 1. Scatter diagram. Each point shows the height and weight for one of 471 men age 18-24 in a dataset. The vertical strip represents men who are about one SD above average in height. Those who are also one SD above average in weight would be plotted along the dashed SD line. Most of the men in the strip are below the SD line: they are only part of an SD above average in weight. The &lt;strong>solid&lt;/strong> regression line estimates average weight at each height.&lt;/p>
&lt;p>The vertical strip in figure 1 shows the men who were one SD above average in height were quite a bit less than one SD above average in weight. This is where the correlation of 0.40 comes in. Associated with an increase of one SD in height there is an increase of &lt;strong>only 0.40&lt;/strong> SDs in weight, on the average.&lt;/p>
&lt;p>To be more specific, take the men who are one SD above average in height:
$$
\text{average height} + SD\ \text{of height} = 70 \ in + 3 \ in = 73 \ in
$$
Their average weight will be above the overall average by $0.40 \times 45 \ lb = 18 \ lb$.&lt;/p>
&lt;p>So, the average weight of these men is around
$$
\text{average weight} + 0.40 \times (SD\ \text{of weight}) = 180 \ lb + 18 \ lb = 198 \ lb
$$
The point (73 inches, 198 pounds) is marked by a cross in figure 1 ( and the points that are 2SD above (76 in, 216 lb) and below (64 in, 144 lb) the average of height as well). All the points (height, estimate for average weight) fall on the solid line shown in figure 1. This is the &lt;em>regression line&lt;/em>. The line goes through the point of averages: men of average height should also be of average weight.&lt;/p>
&lt;p style="text-align:center;color:blue;">
The regression line estimates the average value for y corresponding to each value of x.
&lt;/p>
&lt;p>Along the regression line, associated with each increase of one SD in height there is an increase of only 0.40 SDs in weight. Remember where the 0.40 comes from. It is the correlation between height and weight. NOTE that: Two different SDs are involved here: the SD of $x$, to gauge change in $x$; and the SD of $y$, to gauge changes in $y$.&lt;/p>
&lt;p>This way of using the correlation coefficient to estimate the average value of $y$ for each value of $x$ is called the &lt;strong>regression method&lt;/strong>. The method can be stated as follows.
$$
\fbox{Associated with each increase of one SD in x there is an increase of only r SDs in y, on the average.}
$$&lt;/p>
&lt;blockquote>
&lt;p>Correlation: Like father, like son.&lt;/p>
&lt;blockquote>
&lt;p>If there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other.&lt;/p>
&lt;/blockquote>
&lt;p>The correlation coefficient is a measure of linear association, or clustering around a line. The relationship between two variables can be summarized by&lt;/p>
&lt;ul>
&lt;li>the average of the $x$-values, the SD of the $x$-values,&lt;/li>
&lt;li>the average of the $y$-values, the SD of the $y$-values,&lt;/li>
&lt;li>the correlation coefficient $r$.&lt;/li>
&lt;/ul>
&lt;p>Computing the correlation coefficient&lt;/p>
&lt;p>Here is the procedure for computing the correlation coefficient.
$$
\boxed{\text{Convert each variable to standard units.} \\
\text{The average of the products gives the correlation coefficient.}}
$$&lt;/p>
&lt;p>$$
\fbox{Convert each variable to standard units.
The average of the products gives the correlation coefficient.}
$$&lt;/p>
&lt;p>Recall that &amp;ldquo;Convert each variable to standard units&amp;rdquo; means &lt;strong>standardization&lt;/strong>. Let $x = {1, 3, 4, 5, 7 }, y = {5, 9, 7, 1, 13 } $ be vectors of variables, the mean is given by
$$
\mu = \frac{1}{|a|} \sum_i a_i
$$
And the SD is the &amp;ldquo;r.m.s size of the deviation from the average&amp;rdquo;, can be given by
$$
\sigma = \sqrt{\frac{1}{|a|} \sum_i (a_i - \mu)^2}
$$
where $|a|$ is the number of data points, $a_i$ is the $i$-th data point in the data set.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Table 1. Computing $r$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: center">x&lt;/th>
&lt;th style="text-align: center">y&lt;/th>
&lt;th style="text-align: center">x in standard units&lt;/th>
&lt;th style="text-align: center">y in standard units&lt;/th>
&lt;th style="text-align: center">Product&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: center">1&lt;/td>
&lt;td style="text-align: center">5&lt;/td>
&lt;td style="text-align: center">-1.5&lt;/td>
&lt;td style="text-align: center">-0.5&lt;/td>
&lt;td style="text-align: center">0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">3&lt;/td>
&lt;td style="text-align: center">9&lt;/td>
&lt;td style="text-align: center">-0.5&lt;/td>
&lt;td style="text-align: center">0.5&lt;/td>
&lt;td style="text-align: center">-0.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">4&lt;/td>
&lt;td style="text-align: center">7&lt;/td>
&lt;td style="text-align: center">0.0&lt;/td>
&lt;td style="text-align: center">0.0&lt;/td>
&lt;td style="text-align: center">0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">5&lt;/td>
&lt;td style="text-align: center">1&lt;/td>
&lt;td style="text-align: center">0.5&lt;/td>
&lt;td style="text-align: center">-1.5&lt;/td>
&lt;td style="text-align: center">-0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">7&lt;/td>
&lt;td style="text-align: center">13&lt;/td>
&lt;td style="text-align: center">1.5&lt;/td>
&lt;td style="text-align: center">1.5&lt;/td>
&lt;td style="text-align: center">2.25&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>$$
\begin{eqnarray}
r
&amp;amp;=&amp;amp; \text{average of [(x in standard units) times (x in standard units)]} \\
\\
&amp;amp;=&amp;amp; \frac{0.75 - 0.25 + 0.00 -0.75 + 2.25}{5} = 0.40
\end{eqnarray}
$$&lt;/p>
&lt;p>This complete the solution.&lt;/p>
&lt;/blockquote>
&lt;h3 id="slope-and-intercept" >
&lt;div>
&lt;a href="#slope-and-intercept">
##
&lt;/a>
Slope and Intercept
&lt;/div>
&lt;/h3>
&lt;p>Does education pay? Figure 1 shows the relationship between income and education, for a sample of 562 men age 25-29 in 2005. The summary statistics are
$$
\begin{eqnarray}
\text{average education} &amp;amp;\approx&amp;amp; 12.5\ \text{years}, &amp;amp;\qquad&amp;amp; SD \approx 3\ \text{years} \\
\text{average income} &amp;amp;\approx&amp;amp; $30000,\ &amp;amp;\qquad&amp;amp; SD \approx $24000, \qquad r \approx 0.25
\end{eqnarray}
$$
The regression estimates for average income at each educational level fall along the regression line shown in the figure. The line slopes up, showing that on the average, income does go up with education.&lt;/p>
&lt;p>&lt;img alt="Do education pay" src="./images/stats_education_income.png">&lt;/p>
&lt;p>Any line can be described in terms of its slope and intercept. The y-intercept is the height of the line when $x$ is $0$. And the slope is the rate at which $y$ increases, per unit increase in $x$. Slope and intercept are illustrated in figure 2.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept" src="images/stats_slope_intercept.png">&lt;/p>
&lt;p>&lt;strong>How do you get the slope of the regression line?&lt;/strong> Take the income-education example.
Associated with an increase of one SD in education, there is an increase of $r$ SDs in
income. On this basis, 3 extra years (one SD) of education are worth an extra
$r \times SD = 0.25 \times 24000 = 6000$ of income, on the average. So each extra year in
worth \$6000 / 3 = \$2000. The slope of the regression line is \$2000 per year.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept2" src="images/stats_slope_intercept2.png">&lt;/p>
&lt;p>The intercept of the regression line is the height when $x = 0$, corresponding to men with $0$ years of education. There men are 12.5 years below average in education.
Each year costs \$2000 -- that is what the slope says. A man with no education should have an income which is below average by
$$
12.5\ \text{years} \times 2000\ \text{per year} = 25000.
$$
His income should be $\$30000 - \$25000 = \$5000$. That is the intercept (figure 3): the predicted value of $y$ when $x = 0$.&lt;/p>
&lt;hr>
&lt;p style="text-align:justify;color:blue;">
Associated with a unit increase in x there is some average change in y. The slope of the regression line estimates this change. The formula for the slope is
&lt;/p>
&lt;p>$$
{r \times SD\ \text{of y} \over SD\ \text{of x}}
$$&lt;/p>
&lt;p style="text-align:justify;color:blue;">
The intercept of the regression line is just the predicted value for y when x is 0.
&lt;/p>
&lt;hr>
&lt;p>The equation of a line can be written in terms of the slope and intercept:
$$
y = \text{slope} \times x + \text{intercept},
$$
which is called the &lt;em>regression equation&lt;/em>. There is nothing new here. The regression equation is just a way of predicting $y$ from $x$ by the regression method.&lt;/p>
&lt;p>The regression line becomes unreliable when you are far from the center of the data, so a &lt;em>negative&lt;/em> intercept is not too disturbing (when the calculation results in some negative value which may seen absurd).&lt;/p>
&lt;p style="text-align:justify;color:blue;">
If you run an observational study, the regression line only describes the data that you see. The line cannot be relied on for predicting the results of interventions.
&lt;/p>
&lt;h3 id="the-least-squares" >
&lt;div>
&lt;a href="#the-least-squares">
##
&lt;/a>
The Least Squares
&lt;/div>
&lt;/h3>
&lt;p>Sometimes the points on a scatter diagram seem to be following a line. The problem discussed in this section is &lt;strong>how to find the line which best fits the points&lt;/strong>. Usually, this involves a compromise: moving the line closer to some points will increase it distance from others. To resolve the conflict, two steps are necessary.&lt;/p>
&lt;ul>
&lt;li>First, define an average distance from the line to all the points.&lt;/li>
&lt;li>Second, move the line around until this average distance is as small as possible.&lt;/li>
&lt;/ul>
&lt;p>To be more specific, suppose the line will be used to predict $y$ from $x$. Then the error made at each point is the vertical distance from the point to the line (a.k.a, the &lt;strong>residual&lt;/strong>, means the difference between the $i$th observed and the $i$th response that is predicted by linear model). In statistics, the usual way to define the average distance is by taking the root-mean-square of the errors. This measure of average distance is called the &lt;em>r.m.s error of the line&lt;/em>. (It was first proposed by Gauss)&lt;/p>
&lt;p>The second problem, how to move the line around to minimize the r.m.s error, was also solved by Gauss:&lt;/p>
&lt;hr>
&lt;p style="text-align:center;color:blue;">
Among all lines, the one that makes the smallest r.m.s error in predicting y form x is the regression line.
&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>The r.m.s error for regression says how far typical points are above or below the regression line.
$$
r.m.s\ error = \sqrt{\frac{1}{n} \sum^n_i (y_i - \hat{y_i})^2}
$$
where $n$ is the number of data points, $y_i$ the $i$-th actual value, $\hat{y_i}$ the corresponding predicted value.&lt;/p>
&lt;p>And the r.m.s error for the regression line of $y$ on $x$ can also be figured as
$$
\sqrt{1 - r^2} \times SD_y
$$
where $r$ is the correlation coefficient&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> between $x$ and $y$.&lt;/p>
&lt;/blockquote>
&lt;p>For this reason, the regression line is often called &lt;em>least squares line&lt;/em>: the errors are squared to compute the r.m.s error, and the regression line makes the r.m.s error as small as possible.&lt;/p>
&lt;p>In other words, the least squares approaches choose $\beta_0$ (the intercept) and $\beta_1$ (the slope) to minimize the &lt;em>residual sum of squares&lt;/em> (RSS) which is defined as
$$
RSS = e_1^2 + e_2^2 + \cdots + e_n^2 = \sum^n_{i=1} (y_i - \hat{y_i})^2
$$
where $e = y_i - \hat{y_i}$ is called the &lt;strong>residual&lt;/strong>. Obviously, the r.m.s error is the root of the mean of RSS.&lt;/p>
&lt;p>Linear regression is a very simple approach for supervised learning. In particular, linear regression is a useful tool for predicting a quantitative response. Many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression.&lt;/p>
&lt;p>$\color{Green}{\text{Example}}$&lt;/p>
&lt;p>According to Hooke&amp;rsquo;s law, the amount of stretch is proportional to the weight $x$. The new length of the spring is
$$
y = mx + b.
$$
In this equation, $m \in \mathbb{R}$ and $b \in \mathbb{R}$ are constants which depend on the spring. Their values are unknown, and have to be estimated using &lt;strong>experimental data&lt;/strong>.&lt;/p>
&lt;center>
Table 1. Data on Hooke's law.
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Weight (kg)&lt;/th>
&lt;th style="text-align: left">Length (cm)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">0&lt;/td>
&lt;td style="text-align: left">439.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">2&lt;/td>
&lt;td style="text-align: left">439.12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">4&lt;/td>
&lt;td style="text-align: left">439.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">6&lt;/td>
&lt;td style="text-align: left">439.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">8&lt;/td>
&lt;td style="text-align: left">439.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">10&lt;/td>
&lt;td style="text-align: left">439.50&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/center>
The correlation coefficient[^1] for the data in table 1 is 0.999, very close to 1 indeed. So the points almost form a straight line (figure 5), just as Hooke's law predicts. The minor deviations from linearity are probably due to measurement error; neither the weights nor the length have been measured with perfect accuracy. (Nothing ever is. [When it comes to measurement])
&lt;p>&lt;img alt="Hooke&amp;rsquo;s law" src="images/stats_hookes_law.png">&lt;/p>
&lt;p>Our goal is to estimate $\hat{m}$ and $\hat{b}$ in the equation of Hooke&amp;rsquo;s law for this spring:
$$
y = \hat{m} x + \hat{b}
$$
The graph of this equation is a perfect straight line. If the points in figure 5 happened to fall exactly on some line, the slope&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> of that line would estimate $m$, and its intercept would estimate $b$. However, the points do not line up perfectly. Many different lines could be drawn across the scatter diagram, each having a slightly different slope and intercept.&lt;/p>
&lt;p>Which line should be used? Hooke&amp;rsquo;s equation predicts the length from weight. As discussed above, it is natural to choose $m$ and $b$ so as to minimize the r.m.s error, the line $y = \hat{m} x + \hat{b}$ which does the job is the &lt;strong>regression line&lt;/strong>. This is the &lt;em>method of least squares&lt;/em>. In other words, $m$ in Hooke&amp;rsquo;s law should be estimated as the slope of the regression line, and $b$ as its intercept. These are called &lt;em>least squares estimate&lt;/em>, because they minimize root-mean-square error.&lt;/p>
&lt;p>Let&amp;rsquo;s do the arithmetic (in python code):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># X the weight data; y the length data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">2&lt;/span>, &lt;span style="color:#ff9f43">4&lt;/span>, &lt;span style="color:#ff9f43">6&lt;/span>, &lt;span style="color:#ff9f43">8&lt;/span>, &lt;span style="color:#ff9f43">10&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">439.00&lt;/span>, &lt;span style="color:#ff9f43">439.12&lt;/span>, &lt;span style="color:#ff9f43">439.21&lt;/span>, &lt;span style="color:#ff9f43">439.31&lt;/span>, &lt;span style="color:#ff9f43">439.40&lt;/span>, &lt;span style="color:#ff9f43">439.50&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># mean and Standard Deviation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ---------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># avg = sum(X) / len(X)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The means of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>mu_x, mu_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The means of X and y: (5.0, 439.25666666666666)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># std is the &amp;#34;r.m.s size of the deviation from the average&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The SDs of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>SD_x, SD_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The SDs of X and y: (3.415650255319866, 0.16799470891138593)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert X into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (X &lt;span style="color:#ff6ac1">-&lt;/span> mu_x) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert y into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (y &lt;span style="color:#ff6ac1">-&lt;/span> mu_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># correlation coefficient is the average of the products&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>r &lt;span style="color:#ff6ac1">=&lt;/span> (X_standard_unit&lt;span style="color:#ff6ac1">.&lt;/span>dot(y_standard_unit)) &lt;span style="color:#ff6ac1">/&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(X_standard_unit)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># r = 0.999167257319307&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the slope&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m_hat &lt;span style="color:#ff6ac1">=&lt;/span> (r &lt;span style="color:#ff6ac1">*&lt;/span> SD_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># m_hat = 0.0491428571428563&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the intercept, this is the *predicted length* when weight is 0,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>b_hat &lt;span style="color:#ff6ac1">=&lt;/span> mu_y &lt;span style="color:#ff6ac1">-&lt;/span> (mu_x &lt;span style="color:#ff6ac1">*&lt;/span> m_hat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># b_hat = 439.0109523809524&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>this gives us: $\hat{m} \approx 0.05$ per kg and $\hat{b} \approx 439.01$ cm.&lt;/p>
&lt;p>The length of the spring under no load is estimated as 439.01 cm. And each kilogram of load causes the spring to stretch by about 0.05 cm. Of course, even Hooke&amp;rsquo;s law has its limits: beyond some point, the spring will break. &lt;strong>Extrapolating beyond the data is risky&lt;/strong>.&lt;/p>
&lt;p>The method of least squares and the regression method involve the same mathematics; but the contexts may be different. In some fields, investigators talk about &amp;ldquo;least squares&amp;rdquo; when they are estimating parameters &amp;ndash; unknown constants of nature like $m$ and $b$ in Hooke&amp;rsquo;s law. In other fields, investigators talk about regression when they are studying the relationship between two variables, like income and education, using non-experimental data.&lt;/p>
&lt;p>&lt;strong>A technical point:&lt;/strong> The least squares estimate for the length of the spring under no load was 439.01 cm. This is a tiny bit longer than the measured length at no load (439.00 cm). A statistician might trust the least squares estimate over the measurement. Why? Because the least squares estimate takes advantage of all six measurements, not just once: some of the measurement error is likely to cancel out. Of course, the six measurements are tied together by a good theory &amp;ndash; Hooke‚Äôs law. Without the theory, the least squares estimate wouldn‚Äôt be worth much.&lt;/p>
&lt;h3 id="assessing-the-accuaracy-of-the-coefficient-estimates" >
&lt;div>
&lt;a href="#assessing-the-accuaracy-of-the-coefficient-estimates">
##
&lt;/a>
Assessing the Accuaracy of the Coefficient Estimates
&lt;/div>
&lt;/h3>
&lt;p>Assume that the &lt;em>true&lt;/em> relationship (e.g., the Hooke‚Äôs law) between $X$ and $Y$ takes the form $Y = f(X) + \epsilon$ for some unknown function $f$, where $\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as
$$
Y = \beta_0 + \beta_1 X + \epsilon.
$$
This is the &lt;em>population regression line&lt;/em>. Here $\beta_0$ is the intercept (the expected value of $Y$ when $X$ = 0) and the $\beta_1$ the slop (the average increase in $Y$ associated with a one-unit increase in $X$). The $\epsilon$ (error term, typically assumed to be independent of $X$) is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error.&lt;/p>
&lt;p>The model of &lt;em>population regression line&lt;/em> is the best linear approximation to the true relationship between $X$ and $Y$ (NOTE that the assumption of linearity is often a useful working model. However, it may be not true in reality). The true relationship is generally not known for real data, but the least squares line can always be computed using the cofficient estimation methods. A natural question is as follows: how accurate is the least square line as an estimate of the population regression line?&lt;/p>
&lt;p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of &lt;em>bias&lt;/em>. If we use the sample mean $\hat{\mu}$ to estimate $\mu$, this estimate is &lt;em>unbiased&lt;/em>, in the sense that on average, we expect $\hat{\mu}$ to equal $\mu$, if we could average a huge number of estimates of $\mu$ obtained from a huge number of sets of observations. Hence, an unbiased estimator does not &lt;em>systematically&lt;/em> over- or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates as well: if we estimate $\beta_0$ and $\beta_1$ on the basis of a particular data set, then our estimates won&amp;rsquo;t be exactly equal to $\beta_0$ and $\beta_1$. But if we could average the estimates obtained over a huge number of date sets, then the average would be spot on!&lt;/p>
&lt;p>So how far off will that single estimate of $\hat{\mu}$ be? In general, we answer this question by computing the &lt;em>standard error&lt;/em> of $\hat{\mu}$, written as $SE(\hat{\mu})$. We have the well-known formula
$$
Var(\hat{\mu}) = SE(\hat{\mu})^2 = {\sigma^2 \over n},
$$
where $\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$. NOTE that this formula holds iff the $n$ observations are uncorrelated. To compute the standard errors associated with $\hat{\beta_0}$ and $\hat{\beta_1}$, we use the following formulas:
$$
SE(\hat{\beta_0})^2 =
\sigma^2 [{1 \over n} + \frac{\bar{x}^2}{\sum^n_{i=1}(x_i - \bar{x})^2}],
&lt;br>
SE(\hat{\beta_1})^2 =\frac{\sigma^2}{\sum^n_{i=1}(x_i - \bar{x})^2}
$$
where $\sigma^2 = Var(\epsilon)$. In general, $\sigma^2$ is not known, but can be estimated from the data. This estimate of $\sigma$ is known as the &lt;em>residual standard error&lt;/em>, and is given by the formula
$$
\sigma = RSE = \sqrt{RSS / (n-2)}
$$&lt;/p>
&lt;h4 id="confidence-interval" >
&lt;div>
&lt;a href="#confidence-interval">
###
&lt;/a>
Confidence Interval
&lt;/div>
&lt;/h4>
&lt;p>Standard errors can be used to compute the &lt;em>confidence intervals&lt;/em>. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameters. The range is defined in terms of lower and upper limits computed from the sample of data.&lt;/p>
&lt;p>For linear regression, the 95% confidence interval for $\beta_1$ approximately takes the form
$$
\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1}).
$$
That is, there is approximately a 95% chance the true value of $\beta_1$ would be in this range.&lt;/p>
&lt;p>Similarly, for $\beta_0$, its 95% confidence interval takes the form
$$
\hat{\beta_0} \pm 2 \cdot SE(\hat{\beta_0}).
$$
NOTE that here we make an assumption that the errors are Gaussian. And the factor of $2$ in the formula will vary slightly depending on the number of observations $n$ in the linear regression.&lt;/p>
&lt;h4 id="hypothesis-tests" >
&lt;div>
&lt;a href="#hypothesis-tests">
###
&lt;/a>
Hypothesis tests
&lt;/div>
&lt;/h4>
&lt;p>Standard errors can also be used to perform &lt;em>hypothesis tests&lt;/em> on the coefficients. The most common hypothesis test involves testing the &lt;em>null hypothesis&lt;/em> of
$$
H_0 : \text{There is no relationship between X and Y}
$$
versus the &lt;em>alternative hypothesis&lt;/em>
$$
H_a : \text{There is some relationship between X and Y}.
$$
Mathematically, this corresponds to testing
$$
H_0 : \beta_1 = 0
$$
versus
$$
H_a : \beta_1 \ne 0,
$$
since if $\beta_1 = 0$ then the linear regression model reduces to $Y = \beta_0 + \epsilon$, and $X$ is not associated with $Y$.&lt;/p>
&lt;p>To test the null hypothesis, we need to determine whether $\hat{\beta_1}$, our estimate for $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero. How far is far enough? This is of course depends on the accuracy of $\hat{\beta_1}$ &amp;ndash; that is, it depends on $SE(\hat{\beta_1})$:&lt;/p>
&lt;ul>
&lt;li>If $SE(\hat{\beta_1})$ is small, then even relatively small values of $\hat{\beta_1}$ may provide strong evidence that $\beta_1 \ne 0$;&lt;/li>
&lt;li>if $SE(\hat{\beta_1})$ is large, then $\hat{\beta_1}$ must be large in absolute value in order for us to reject the null hypothesis.&lt;/li>
&lt;/ul>
&lt;p>In practice, we compute a &lt;em>t-statistic&lt;/em>, given by
$$
t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})},
$$
which measures the number of standard deviations that $\hat{\beta_1}$ is away from $0$.&lt;/p>
&lt;p>If there really is no relationship between $X$ and $Y$, then we expect that &lt;em>t-statistic&lt;/em> will have a $t$-distribution with $n-2$ degrees of freedom. Consequently, it is a simple matter to compute the probability of observing any number equal to $|t|$ or larger in absolute value, assuming $\beta_1 = 0$. We call this probability the &lt;em>p-value&lt;/em>.&lt;/p>
&lt;p>&lt;strong>p-value interpretation&lt;/strong>&lt;/p>
&lt;p>Roughly speaking, we interpret the p-value as follows: a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real relationship between $X$ and $Y$. Hence we &lt;em>reject the null hypothesis&lt;/em>, and declare a relationship to exist between $X$ and $Y$, if the p-value is small enough. Typical p-value cutoffs for rejecting the null hypothesis are 5% or 1%, when $n = 30$, these correspond to &lt;em>t-statistics&lt;/em> of around $2$, and $2.75$, respectively.&lt;/p>
&lt;blockquote>
&lt;p>A small p-value indicates that a particular sample is unlikely if the null hypothesis
is true. It is akin to a jury concluding that it was unlikely that it would have been
presented with this set of evidence if the defendant were innocent, and therefore
reaching a guilty verdict. Of course, that doesn&amp;rsquo;t mean that the defendant is actually
guilty. Perhaps the jury was presented with misleading evidence.
Analogously, a low p-value might be attributable to the null hypothesis actually being
false, or it could simply be that the sample is unrepresentative of the population from
which it is drawn, i.e., the evidence is misleading.&lt;/p>
&lt;/blockquote>
&lt;p>More info about p-value see &amp;ldquo;21.2 Beware of P-values&amp;rdquo; in the book of Prof. John Guttag &amp;lt;&amp;lt;Introduction to computation and programming using python&amp;gt;&amp;gt;.&lt;/p>
&lt;h3 id="assessing-the-accuracy-of-the-model" >
&lt;div>
&lt;a href="#assessing-the-accuracy-of-the-model">
##
&lt;/a>
Assessing the Accuracy of the Model
&lt;/div>
&lt;/h3>
&lt;p>Once we have rejected the null hypothesis in favor of the altervative hypothesis, it is natrual to want to quantify &lt;em>the extent to which the model fits the data&lt;/em>. The quality of a linear regression fit is typically assessed using two related quantities: the &lt;em>residual standard error&lt;/em> ($RSE$) and the $R^2$ statistic.&lt;/p>
&lt;h4 id="residual-standard-error" >
&lt;div>
&lt;a href="#residual-standard-error">
###
&lt;/a>
Residual Standard Error
&lt;/div>
&lt;/h4>
&lt;p>From the model $Y = \beta_0 + \beta_1 X + \epsilon$ that associated with each observation is an error term $\epsilon$. Due to the presence of these error terms, even if we knew the true regression line (i.e., $\beta$s were known), we would not be perfectly predict $Y$ from $X$. The $RSE$ is an estimate of the standard deviation of $\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula
$$
RSE
= \sqrt{{1 \over n-2} RSS}
= \sqrt{{1 \over n-2} \sum^n_{i=1}(y_i - \hat{y_i})^2}.
$$
NOTE that $RSE$ is slightly different from &lt;em>r.m.s error&lt;/em> which the latter using the number of all samples ($n$) as denominator while the former using $n-2$.&lt;/p>
&lt;p>The $RSE$ is considered a measure of the &lt;em>lack of fit&lt;/em> of the model to the data. The smaller $RSE$ the better the model fitted to the data.&lt;/p>
&lt;h4 id="r2-statistic" >
&lt;div>
&lt;a href="#r2-statistic">
###
&lt;/a>
$R^2$ Statistic
&lt;/div>
&lt;/h4>
&lt;p>The $RSE$ provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of $Y$, it is not always clear what consititues a good $RSE$. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a &lt;em>proportion&lt;/em>, the proportion of variance explained, and so it always takes on a value between $0$ and $1$, and is independent of the scale of $Y$.&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$
where $TSS = \sum(y_i - \bar{y})^2$ is the &lt;em>total sum of squares&lt;/em>. Hence $R^2$ measures the &lt;em>proportion of variability in $Y$ that can be explained using $X$&lt;/em>.&lt;/p>
&lt;h3 id="multiple-linear-regression" >
&lt;div>
&lt;a href="#multiple-linear-regression">
##
&lt;/a>
Multiple Linear Regression
&lt;/div>
&lt;/h3>
&lt;p>Simple linear regression[^3] is a useful approach for predicting a response on the basis of a single predictor variable. But in practice we often have more than one predictor. One option is to run multiple separate simple linear regression, each of which uses a different feature as a predictor. However, this approach is not entirely satisfactory[^4].&lt;/p>
&lt;p>Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model[^5] so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose we have $p$ distinct predictors. Then the multiple linear regression model takes the form&lt;/p>
&lt;p>$$
\tag{3.19}
\label{mlr}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + + \beta_p X_p + \epsilon
$$&lt;/p>
&lt;p>where $X_j$ represents the $j$-th predictor and $\beta_j$ quantifies the association
between that variable and the response. We interpret $\beta_j$ as the &lt;em>average&lt;/em> effect
on $Y$ of a unit increase in $X_j$, &lt;strong>holding all other predictors fixed&lt;/strong>[^6].&lt;/p>
&lt;p>As was the case in the simple linear regression setting, the regression coefficients
$\beta_0, \beta_1, \ldots, \beta_p$ in ($\ref{mlr}$) are unknown, and must be estimated.
Given estimates $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$, we can make
predictions using the formula&lt;/p>
&lt;p>$$
\tag{3.21}
\label{mlrpred}
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p.
$$&lt;/p>
&lt;p>Then parameters are estimated using the same &lt;strong>least squares&lt;/strong> approach that we saw in
the context of simple linear regression. We choose $\beta_0, \beta_1, \ldots, \beta_p$
to minimize the &lt;strong>sum of squared residuals&lt;/strong>&lt;/p>
&lt;p>$$
\begin{eqnarray}
RSS
&amp;amp;=&amp;amp; \sum^n_{i=1}(y_i - \hat{y}_i)^2 \\
\tag{3.22} \label{rss}
&amp;amp;=&amp;amp; \sum^n_{i=1} \big( y_i - (\hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p) \big)^2
\end{eqnarray}
$$&lt;/p>
&lt;p>The values $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$ that minimize
($\ref{rss}$) are the multiple least squares regression coefficient estimates. Unlike
the simple linear regression coefficient estimation (the Python code block in previous
section), the multiple regression coefficient estimates have somewhat complicated forms
that are most easily represented using matrix algebra (see detail in section of Normal
equation).&lt;/p>
&lt;p>When we perform multiple linear regression, we usually are interested in answering a few important questions.&lt;/p>
&lt;hr>
&lt;ol>
&lt;li>Is at least one of the predictors $X_1, X_2, \ldots, X_p$ useful in predicting the response?&lt;/li>
&lt;li>Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?&lt;/li>
&lt;li>How well does the model fit the data?&lt;/li>
&lt;li>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>We now address each of these questions in turn.&lt;/p>
&lt;h4 id="one-is-there-a-relationship-between-the-response-and-predictions" >
&lt;div>
&lt;a href="#one-is-there-a-relationship-between-the-response-and-predictions">
###
&lt;/a>
One: Is There a Relationship Between the Response and Predictions?
&lt;/div>
&lt;/h4>
&lt;p>Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether the slope equals $0$. In the multiple regression setting with $p$ predictors, we need to ask whether all of the regression coefficients are zero. We also use a &lt;strong>hypothesis test&lt;/strong> to answer this question. We test the &lt;strong>null hypothesis&lt;/strong>,
$$
H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0
$$
versus the alternative
$$
H_{\alpha}: \text{at least one } \beta_j \text{ is non-zero}
$$
This hypothesis test is performed by computing the &lt;strong>F-statistic&lt;/strong>,
$$
\tag{3.23}
F = \frac{(TSS - RSS) / p}{RSS / (n - p -1)}
$$
where, as with simple linear regression, $TSS = \sum(y_i - \bar{y})^2$ and $RSS = \sum(y_i - \hat{y})^2$ where $\bar{y} = {1 \over n} \sum^n_{i=1} y_i$ is the sample mean. If the linear model assumptions are correct, one can show that
$$
E {RSS / (n - p - 1) } = \sigma^2
$$
and that, provided $H_0$ is true,
$$
E {(TSS - RSS) / p } = \sigma^2
$$
Hence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to $1$. On the other hand, if $H_{\alpha}$ is true, then $E {(TSS - RSS) / p } &amp;gt; \sigma^2$, so we expect the F-statistic to be greater than $1$.&lt;/p>
&lt;p>However, what if the F-statistic had been closer to $1$? How large does the F-statistic need to be before we can reject $H_0$ and conclude that there is a relationship? It turns out that the answer depends on the values of $n$ and $p$.&lt;/p>
&lt;ul>
&lt;li>When $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$.&lt;/li>
&lt;li>Inctrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small.&lt;/li>
&lt;/ul>
&lt;p>When $H_0$ is true, and the errors $\epsilon_i$ have normal distribution, the F-statistic follows an F-distribution[^7]. For any given value of $n$ and $p$, any statistical software package can be used to compute the p-value[^8] associated with F-statistic using this distribution. Based on this p-value, we determine whether or not to reject $H_0$.&lt;/p>
&lt;p>Sometimes we want to test that a particular subset of $q$ of the coefficients are zero. This corresponds to a null hypothesis
$$
H_0 = \beta_{p-q+1} = \beta_{p-q+2} = \cdots = \beta_{p} = 0,
$$
where for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses all the variables except those last $q$. Suppose that the residual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is
$$
\tag{3.24} \label{ftest2}
F = \frac{(RSS_0 - RSS) / q}{RSS / (n - p -1)}.
$$
For each individual predictor a t-statistic and a p-value can be obtain, these statistics provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in (means $q=1$ in equation $\ref{ftest2}$). So it reports the &lt;em>partial effect&lt;/em> of adding that variable to the model.&lt;/p>
&lt;p>Given these individual p-values for each variable, why do we need to look at the over F-statistic? After all, it seems likely that if any one of the p-values for the individual variables is very small, then &lt;em>at least one of the predictors is related to the response&lt;/em>. However, this logic is flawed, especially when the number of predictors $p$ is large.&lt;/p>
&lt;p>For instance, consider an example in which $p = 100$ and $H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0$ is true, so no variable is truly associated with the response. In this situation, about &lt;strong>5%&lt;/strong> of the p-values associated with each variable will be below 0.05 by chance. &lt;strong>In other words, we expect to see approximately five small p-values even in the absence of any true association between the predictors and the response. In fact, we are almost guaranteed that we will observe at least one p-value below 0.05 by chance!&lt;/strong>&lt;/p>
&lt;p>Hence, if we use individual t-statistic and the associated p-value in order to confirm the association between any predictor and the response, there is a very high chance that we will incorrectly conclude that there is a relationship.&lt;/p>
&lt;p>However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. If $H_0$ is true, there is only a 5% chance that the F-statistic will result in a p-value below 0.05, regardless of the number of predictors or the number of observations.&lt;/p>
&lt;p>Note that when $p$ is larger than $n$, we cannot even fit the multiple linear regression model using least squares. Less flexible least squares models, such as forward stepwise selection, ridge regression, lasso regression and principal components regression, are particular useful for performing regression in the high-dimensional setting.&lt;/p>
&lt;h4 id="two-deciding-on-important-variable" >
&lt;div>
&lt;a href="#two-deciding-on-important-variable">
###
&lt;/a>
Two: Deciding on Important Variable
&lt;/div>
&lt;/h4>
&lt;p>If we conclude on the basis of the F-statistic and its associated p-value that at least one of the predictors is related to the response, then it is natural to wonder which are the guilty ones. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as &lt;strong>variable selection&lt;/strong>.&lt;/p>
&lt;p>Ideally, we would like to preform variable selection by trying out a lot of different models, each containing a different subset of the predictors. Unfortunately, there are a total $2^p$ models that contain subsets of $p$ variables (Note that even with a moderate value of $p$, say, $p=30$, then $2^{30}=1,073,741,824$ models make this infeasible). We need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:&lt;/p>
&lt;ul>
&lt;li>Forward selection. We begin with the &lt;strong>null model&lt;/strong> (which contains only the intercept). We then fit $p$ simple linear regressions and add to the null model the variable that results in the lowest $RSS$, and then add to that model the variable which results in the lowest $RSS$ for the new two-variable model. This approach is continued until some stopping rule is satisfied.&lt;/li>
&lt;li>Backward selection. We start with all variables in the model, and remove the variable with largest p-value. The new ($p - 1$)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached (such as when all remaining variables have a p-value below some threshold).&lt;/li>
&lt;li>Mixed selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. If at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform forward and backward steps until all variable in the model have a low p-value, and all the variables outside the model have a large p-value if added to the model.&lt;/li>
&lt;/ul>
&lt;p>Backward selection cannot be used when $p &amp;gt; n$, forward selection is a greedy approach, and might include variable early that later become redundant. Mixed selection can remedy this.&lt;/p>
&lt;h4 id="three-model-fit" >
&lt;div>
&lt;a href="#three-model-fit">
###
&lt;/a>
Three: Model Fit
&lt;/div>
&lt;/h4>
&lt;p>Two of the most common numerical measures of model fit are the $RSE$ and the $R^2$. Recall that in simple regression, $R^2$ is the square of the correlation coefficient between predictor and the response. In multiple linear regression, it turns out that it equals the square of the correlation coefficient between the response and the fitted model (this implies that the fitted model maximizes this correlation among all possible linear model).&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
\begin{eqnarray}
R^2
&amp;amp;=&amp;amp; \frac{\sum(y_i - \bar{y})^2 - \sum(y_i - \hat{y})^2}{\sum(y_i - \bar{y})^2} \\
\\
&amp;amp;=&amp;amp; \frac{TSS - RSS}{TSS} \\
\\
\tag{3.17}
&amp;amp;=&amp;amp; 1 - {RSS \over TSS},
\end{eqnarray}
$$
where ($\bar{y} = {1 \over n} \sum^n_{i=1} y_i$) is the sample mean, $\hat{y}$ is defined in ($\ref{mlrpred}$).&lt;/p>
&lt;p>An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the response variable. It turns out that $R^2$ will always increase when more variables are added to the model, even they are only weakly associated with the response. This is due to the fact that with more variable to the least squares equations must allow us to fit the training data more accurately (though not necessarily the testing data, a.k.a., over fitting).&lt;/p>
&lt;p>In general, $RSE$ is defined as
$$
\tag{3.25}
RSE = \sqrt{{1 \over {n - p - 1}} RSS}
$$
Thus model with more variables can have higher $RSE$ if the decrease in $RSS$ is small relative to the increase in $p$.&lt;/p>
&lt;h5 id="adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2" >
&lt;div>
&lt;a href="#adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2">
####
&lt;/a>
Adjusted $R^2$ &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">(From wiki)&lt;/a>
&lt;/div>
&lt;/h5>
&lt;p>The use of an adjusted $R^2$ (one common notation is $\bar{R}^2$; another is $R_{adj}^2$)
is an attempt to account for the phenomenon of the $R^2$ automatically and spuriously
increasing when extra explanatory variables are added to the model. There are many
different ways of adjusting, by far the most used one, to the point that it is typically
just referred to as &lt;em>adjusted $R^2$&lt;/em>, is the correction prosposed by Mordecai Ezekiel,
and adjusted $R^2$ is defined as
$$
\bar{R}^2 = 1 - (1 - R^2){\frac{n-1}{n-p-1}}
$$
where $p$ is the total number of explantory variables in the model (not including the constant term), and $n$ is the sample size. It can also be written as
$$
\bar{R}^2 = 1 - \frac{RSS / df_e}{TSS / df_t}
$$
where $df_t$ is the &lt;em>degrees of freedom&lt;/em> $n-1$ of the estimate of the population variance of the dependent variable, and the $df_e$ is the degrees of freedom $n-p-1$ of the estimate of the underlying population error variance.&lt;/p>
&lt;p>The adjusted $R^2$ can be negative, and its value will always be less than or equal to that of $R^2$. Unlike $R^2$, the adjusted $R^2$ increases only when the increase in $R^2$ (due to the inclusion of a new variable) is more than one would expect to see by chance. If a set of explanatory variables with a predtermined hierarchy of importance are introduced into a regression one at a time, with the adjusted $R^2$ computed each time, the level at which adjusted $R^2$ reaches a maximum, and decreases afterward, would be the regression with ideal combination of having the best fit without excess/unnecessary terms.&lt;/p>
&lt;blockquote>
&lt;p>Degrees of Freedom &lt;a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">(From wiki)&lt;/a>:&lt;br>
The number of &lt;em>degrees of freedom&lt;/em> is the number of values in the final calculation of a statistic that are free to vary.
Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter are called the degrees of freedom.
Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of &amp;ldquo;free&amp;rdquo; components (how many components need to be know before the vector is fully determined).
Suppose we have a sample of independent normally distributed observations, ${X_1, X_2, \ldots, X_n}$. This can be represented as an n-dimensional random vector:
$X^T$. Since this random vector can lie anywhere in n-dimensional space, it has $n$ degrees of freedom.
Now let $\bar{X}$ be the sample mean. The random vector can be decomposed as the sum of the sample mean plus a vector of residuals:&lt;/p>
&lt;/blockquote>
&lt;p>$$
\begin{pmatrix}
X_{1} \\
\vdots \\
X_{n} \\
\end{pmatrix}
= \bar{X} \cdot
\begin{pmatrix}
1 \\
\vdots \\
1 \\
\end{pmatrix}
+
\begin{pmatrix}
X_{1} - \bar{X} \\
\vdots \\
X_{n} - \bar{X} \\
\end{pmatrix}
$$&lt;/p>
&lt;p>The first vector on the right-hand side is constrained to be a multiple of the vector of $1$&amp;rsquo;s, and the only free quantity is $\bar{X}$. It therefore has only one degree of freedom.
The second vector is constrained by the relation $\sum(X_i - \bar{X}) = 0$. The first $n-1$ components of this vector can be anything. However, once you know the first $n-1$ components, the constraint tells you the value of the $n$th component. Therefore, this vector has $n-1$ degrees of freedom.&lt;/p>
&lt;h4 id="four-prediction" >
&lt;div>
&lt;a href="#four-prediction">
###
&lt;/a>
Four: Prediction
&lt;/div>
&lt;/h4>
&lt;p>Once we have fit the multiple regression model, it is straightforward to apply the fitted model $\hat{y} = \hat{f}(X) = \hat{\beta} X$ (a more verbose version see $\ref{mlrpred}$) in order to predict the response based on the values of the predictors. However, there are three sorts of uncertainty associated with this prediction.&lt;/p>
&lt;ol>
&lt;li>The coefficient estimate is the least squares estimation of the true coefficient which is unknown. The inaccuracy in the coefficient estimates is related to the &lt;em>reducible error&lt;/em>[^9]. We can compute a &lt;strong>confidence interval&lt;/strong> in order to determine how close $\hat{y}$ will be to $f(X)$.&lt;/li>
&lt;li>In practice assuming a linear model for $f(X)$ is almost always an approximation of reality, so if the true pattern is non-linear, there is an additional reducible error called &lt;em>model bias&lt;/em>.&lt;/li>
&lt;li>Even if we knew $f(X)$ &amp;ndash; that is, we knew the true value of $\beta$ &amp;ndash; the response value cannot be predicted perfectly, because of the random error $\epsilon$ in the model ($\ref{mlrpred}$), this is the &lt;em>irreducible error&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h3 id="summary" >
&lt;div>
&lt;a href="#summary">
##
&lt;/a>
Summary
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>The regression line can be specified by two descriptive statistics: the &lt;em>slope&lt;/em> and the &lt;em>intercept&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Among all lines, the regression line for $y$ on $x$ makes the smallest r.m.s error in predicting $y$ from $x$. For that reason, the regression line is often called the &lt;em>least squares line&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>With a controlled experiment, the slope can tell you the average change in $y$ that would be caused by a change in $x$. With an observational study, however, the slope cannot be relied on to predict the results of interventions. It takes a lot of hard work to draw causal inferences from observational data, with or without regression.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If the average of $y$ depends on $x$ in a non-linear way, the regression line can be quite misleading.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multiple regression is a powerful technique, but it is not a substitute for understanding. (Such as the poor investigator would fit a multiple regression equation of the form $ predicted\ area = a + b \times perimeter + c \times diagonal$ to predict the area of a rectangle).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="part-i-how-to-learn" >
&lt;div>
&lt;a href="#part-i-how-to-learn">
#
&lt;/a>
Part-I: How to learn
&lt;/div>
&lt;/h2>
&lt;p>Linear regression is of course an extremely simple and limited learning algorithm, but it provides an example of how a learning algorithm can work.&lt;/p>
&lt;h3 id="normal-equation" >
&lt;div>
&lt;a href="#normal-equation">
##
&lt;/a>
Normal Equation
&lt;/div>
&lt;/h3>
&lt;p>The goal is to build a system that can take a vector $x \in \mathbb{R^n}$ as input and predict the value of a scalar $y \in \mathbb{R}$ as its output. The output of linear regression is a linear function of the input. Let $\hat{y}$ be the value that our model predicts $y$ should take on. We define the output to be
$$
\begin{equation}
\tag{5.3}
\hat{y} = w^{\mathsf{T}}x
\end{equation}
$$&lt;/p>
&lt;p>where $w \in \mathbb{R^n}$ is a vector of &lt;strong>parameters&lt;/strong>.&lt;/p>
&lt;p>We thus have a definition of our task &lt;em>T&lt;/em> : to predict $y$ from $x$ by outputting $\hat{y}=w^{\mathsf{T}}x$.&lt;/p>
&lt;p>Next we need a definition of our performance measure, &lt;em>P&lt;/em>. One way of measuring the performance of the model is to compute the &lt;strong>mean squared error (MSE)&lt;/strong> of the model on the test set. If $\hat{y}^{(test)}$ gives the predictions of the model on the test set, then the MSE is given by
$$
\tag{5.4}
MSE_{test} = \frac{1}{m} \sum_i{(\hat{y}^{(test)} - {y}^{(test)})^2_i}
$$
Intuitively, one can see that this error measure decreases to 0 when $\hat{y}^{(test)} = {y}^{(test)}$. We can also see that
$$
\tag{5.5}
MSE_{test} = \frac{1}{m} ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2
$$
so the error increases whenever the Euclidean distance between the predictions and the targets increases.&lt;/p>
&lt;blockquote>
&lt;p>In machine learning, we usually measure the size of vectors using a function called a &lt;strong>norm&lt;/strong>. Formally, the $L^p$ norm is given by
$$
||x||_p = \bigg(\sum_i |x_i|^p \bigg)^{\frac{1}{p}}
$$
for $p \in \mathbb{R}, p \geq 1$.&lt;/p>
&lt;p>The $L^2$ norm, with $p = 2$, is known as the &lt;strong>Euclidean norm&lt;/strong>, often denoted simply as $||x||$. It is also common to measure the size of a vector using the squared $L^2$ norm, which can be calculated simply as $x^{\mathsf{T}}x$.&lt;/p>
&lt;/blockquote>
&lt;p>To make a machine learning algorithm, we need to design an algorithm that will improve the weights $w$ in a way that reduces $MSE_{test}$ when the algorithm is allowed to gain experience by observing a training set ($X^{(train)}, y^{(train)}$).&lt;/p>
&lt;p>One intuitive way of doing this is (to minimize $MSE_{test}$) just to minimize the MSE on the training set, $MSE_{train}$ . (Does this make any sense? Keep on reading.)&lt;/p>
&lt;p>To minimize $MSE_{train}$ , we can simply solve for where its gradient is &lt;strong>0&lt;/strong>:
$$
\begin{eqnarray}
\tag{5.6}
\nabla_w MSE_{train} = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(train)} - {y}^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{X^{(train)}w - y^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \nabla_w \big({X^{(train)}w - y^{(train)}}\big)^{\mathsf{T}} \big({X^{(train)}w - y^{(train)}}\big) = 0
\end{eqnarray}
$$&lt;/p>
&lt;p>$$
\tag{5.10}
\Rightarrow \nabla_w \big( w^{\mathsf{T}} X^{(train)\mathsf{T}} X^{(train)}w - 2w^{\mathsf{T}} X^{(train)\mathsf{T}} y^{(train)} + y^{(train)\mathsf{T}} y^{(train)} \big) = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.11}
\Rightarrow 2X^{(train)\mathsf{T}} X^{(train)}w - 2X^{(train)\mathsf{T}} y^{(train)} = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.12}
\Rightarrow w = \big(X^{(train)\mathsf{T}} X^{(train)}\big)^{-1} X^{(train)\mathsf{T}} y^{(train)}
$$&lt;/p>
&lt;p>The system of equations whose solution is given by equation 5.12 is known as the &lt;strong>normal equation&lt;/strong>. Evaluating equation 5.12 constitutes a simple learning algorithm.&lt;/p>
&lt;blockquote>
&lt;p>ÈìæÂºèÊ≥ïÂàô(Chain Rule)ÊòØËÆ°ÁÆóÂ§çÊùÇÂØºÊï∞Êó∂ÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÁÆÄÂçïÂú∞ËØ¥ÔºåËã•ÂáΩÊï∞ $f(x) = g(h(x))$ÔºåÂàôÊúâ
$$
\tag{A.31}
\frac{\partial{f(x)}}{\partial{x}} = \frac{\partial{g(h(x))}}{\partial{h(x)}} \cdot \frac{\partial{h(x)}}{\partial{x}}.
$$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>‰æãÂ¶ÇÂú®ËÆ°ÁÆó‰∏ãÂºèÊó∂ÔºåÂ∞Ü$(Ax - b)$Áúã‰Ωú‰∏Ä‰∏™Êï¥‰ΩìÂèØÁÆÄÂåñËÆ°ÁÆóÔºö
$$
\begin{align*}
&amp;amp;\frac{\partial}{\partial{x}}(Ax - b)^{\mathsf{T}} W(Ax - b) \\
&amp;amp;= \frac{\partial{(Ax - b)}}{\partial{x}} \cdot 2W(Ax - b) \\
&amp;amp;= 2AW(Ax - b)
\end{align*}
$$&lt;/p>
&lt;/blockquote>
&lt;p>It is worth noting that the term &lt;strong>linear regression&lt;/strong> is often used to refer to a slightly more sophisticated model with one additional parameter &amp;ndash; an intercept term $b$. In this model&lt;/p>
&lt;p>$$
\tag{5.13}
\hat{y} = w^{\mathsf{T}}x + b
$$&lt;/p>
&lt;p>so the mapping from parameters to predictions is still a linear function but the mapping from features to predictions is now an &lt;strong>affine function&lt;/strong> (which is in the form of equation 5.13, it means that the plot of model&amp;rsquo;s predictions still looks like a line, but it need not pass though the origin). One can continue to use the model with only weights but augment $x$ with an extra entry that is always set to 1. Then the weight corresponding to the extra 1 entry plays the role of the bias parameter (i.e., the intercept term, $b$, a.k.a., the bias term).&lt;/p>
&lt;blockquote>
&lt;p>The intercept term $b$ is often called the &lt;strong>bias&lt;/strong> parameter of the affine transformation. This terminology derives from the point of view that the output of the transformation is biased toward being $b$ in the absence of any input.&lt;/p>
&lt;p>This term is different from the idea of a statistical bias, in which a statistical estimation algorithm&amp;rsquo;s expected estimate of a quantity is not equal to the true quantity.&lt;/p>
&lt;/blockquote>
&lt;h3 id="gradient-based-method" >
&lt;div>
&lt;a href="#gradient-based-method">
##
&lt;/a>
Gradient-based method
&lt;/div>
&lt;/h3>
&lt;p>Example: Linear Least Squares&lt;/p>
&lt;p>Suppose we want to find the value of $x$ that minimizes&lt;/p>
&lt;p>$$
\tag{4.21}
f(x) = \frac{1}{2}||Ax - b||^2_2
$$&lt;/p>
&lt;p>Specialized linear algebra algorithms can solve this problem efficiently; however, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work.&lt;/p>
&lt;p>First, we need to obtain the gradient (Recall the Chain Rule):&lt;/p>
&lt;p>$$
\tag{4.42}
\nabla_x f(x) = A^{\mathsf{T}} (Ax - b) = A^{\mathsf{T}}Ax - A^{\mathsf{T}}b
$$&lt;/p>
&lt;p>We can then follow this gradient downhill, taking small steps.&lt;/p>
&lt;hr>
&lt;p>Algorithm 4.1 An algorithm to minimize $f(x) = \frac{1}{2}||Ax - b||^2_2$ with respect to $x$ using gradient descent, starting from an arbitrary value of $x$.&lt;/p>
&lt;hr>
&lt;p>Set the step size ($\epsilon$, a.k.a., learning rate) and tolerance ($\delta$) to small, positive numbers.&lt;/p>
&lt;p>&lt;strong>while&lt;/strong> $||A^{\mathsf{T}}Ax - A^{\mathsf{T}}b||_2 &amp;gt; \delta$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>$x \leftarrow x - \epsilon (A^{\mathsf{T}}Ax - A^{\mathsf{T}}b)$&lt;/p>
&lt;p>&lt;strong>end while&lt;/strong>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>ÂÖ≥‰∫éÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï(Gradient Descent)&lt;/p>
&lt;p>Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊòØ‰∏ÄÁßçÂ∏∏Áî®ÁöÑ‰∏ÄÈò∂(first-order)‰ºòÂåñÊñπÊ≥ï, ÊòØÊ±ÇËß£Êó†Á∫¶Êùü‰ºòÂåñÈóÆÈ¢òÊúÄÁÆÄÂçï,ÊúÄÁªèÂÖ∏ÁöÑÊñπÊ≥ï‰πã‰∏Ä.&lt;/p>
&lt;p>ËÄÉËôëÊó†Á∫¶Êùü‰ºòÂåñÈóÆÈ¢ò$min_x f(x)$,ÂÖ∂‰∏≠$f(x)$‰∏∫ËøûÁª≠ÂèØÂæÆÂáΩÊï∞.Ëã•ËÉΩÊûÑÈÄ†‰∏Ä‰∏™Â∫èÂàó$x^0, x^1, x^2, \ldots$ Êª°Ë∂≥
$$
\tag{B.15}
\label{eq_ngd}
f(x^{(t+1)}) &amp;lt; f(x^{(t)}), t = 0,1,2,\ldots
$$
Âàô‰∏çÊñ≠ÊâßË°åËØ•ËøáÁ®ãÂç≥ÂèØÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊûÅÂ∞èÁÇπ.Ê¨≤Êª°Ë∂≥Âºè($\ref{eq_ngd}$),Ê†πÊçÆÊ≥∞ÂãíÂ±ïÂºèÊúâ
$$
\tag{B.16}
f(x + \Delta x) \simeq f(x) + \Delta x^{\mathsf{T}} \nabla f(x)
$$
‰∫éÊòØ,Ê¨≤Êª°Ë∂≥$f(x + \Delta x) &amp;lt; f(x)$,ÂèØÈÄâÊã©
$$
\tag{B.17}
\Delta x = - \epsilon \nabla f(x)
$$
ÂÖ∂‰∏≠Ê≠•Èïø(step size)$\epsilon$ÊòØ‰∏Ä‰∏™Â∞èÂ∏∏Êï∞.ËøôÂ∞±ÊòØÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï.&lt;/p>
&lt;p>Ëã•ÁõÆÊ†áÂáΩÊï∞$f(x)$Êª°Ë∂≥‰∏Ä‰∫õÊù°‰ª∂,ÂàôÈÄöËøáÈÄâÂèñÂêàÈÄÇÁöÑÊ≠•Èïø,Â∞±ËÉΩÁ°Æ‰øùÈÄöËøáÊ¢ØÂ∫¶‰∏ãÈôçÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊûÅÂ∞èÁÇπ.‰æãÂ¶Ç,Ëã•$f(x)$Êª°Ë∂≥L-LipschitzÊù°‰ª∂(‰∫¶Âç≥,ÂØπ‰∫é‰ªªÊÑè$x$,Â≠òÂú®Â∏∏Êï∞$L$‰ΩøÂæó$||\nabla f(x)|| \leq L$ÊàêÁ´ã),ÂàôÂ∞ÜÊ≠•ÈïøËÆæÁΩÆ‰∏∫$1/(2L)$Âç≥ÂèØÁ°Æ‰øùÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊûÅÂ∞èÁÇπ.ÂΩìÁõÆÊ†áÂáΩÊï∞ÊòØÂá∏ÂáΩÊï∞Êó∂,Â±ÄÈÉ®ÊûÅÂ∞èÁÇπÂ∞±ÊòØÂÖ®Â±ÄÊúÄÂ∞èÁÇπ,Ê≠§Êó∂,Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂèØÁ°Æ‰øùÊî∂ÊïõÂà∞ÂÖ®Â±ÄÊúÄ‰ºòËß£.&lt;/p>
&lt;p>ÂΩìÁõÆÊ†áÂáΩÊï∞$f(x)$‰∫åÈò∂ËøûÁª≠ÂèØÂæÆÊó∂,ÂèØÂ∞ÜÂºè($B.16$)ÊõøÊç¢ÊàêÊõ¥‰∏∫Á≤æÁ°ÆÁöÑ‰∫åÈò∂Ê≥∞ÂãíÂ±ïÂºè,ËøôÊ†∑Â∞±ÂæóÂà∞‰∫ÜÁâõÈ°øÊ≥ï(Newton&amp;rsquo;s method).ÁâõÈ°øÊ≥ïÊòØÂÖ∏ÂûãÁöÑ‰∫åÈò∂ÊñπÊ≥ï,ÂÖ∂Ëø≠‰ª£ËΩÆÊï∞ËøúÂ∞è‰∫éÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï.‰ΩÜÁâõÈ°øÊ≥ï‰ΩøÁî®‰∫Ü‰∫åÈò∂ÂØºÊï∞$\nabla^2 f(x)$ (second derivative),ÂÖ∂ÊØèËΩÆËø≠‰ª£‰∏≠Ê∂âÂèäÂà∞Êµ∑Ê£ÆÁü©Èòµ(Hessian matrix)ÁöÑÊ±ÇÈÄÜ,ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Áõ∏ÂΩìÈ´ò,Â∞§ÂÖ∂Âú®È´òÁª¥ÈóÆÈ¢ò‰∏≠Âá†‰πé‰∏çÂèØË°å.ÂÖ∂Ê¨°,ÁâõÈ°øÊ≥ï‰ªÖÈÄÇÁî®‰∫éÈôÑËøëÁÇπÊúâÂ±ÄÈÉ®ÊûÅÂ∞èÁÇπÁöÑÊÉÖÂÜµ(‰πüÂ∞±ÊòØ,Êµ∑Ê£ÆÁü©Èòµ‰∏∫Ê≠£ÂÆöÁü©Èòµ,‰πüÂ∞±ÊòØÊµ∑Ê£ÆÁü©ÈòµÊâÄÊúâÁöÑÁâπÂæÅÂÄºÈÉΩÊòØÊ≠£Êï∞),Ëã•ÈôÑËøëÁÇπÊòØÈûçÁÇπ(saddle point)ÂàôÁâõÈ°øÊ≥ïÂ§±Êïà.ÁÑ∂ËÄå,Ê¢ØÂ∫¶‰∏ãÈôçÂç¥‰∏ç‰ºöË¢´ÈûçÁÇπÂõ∞‰Ωè.Ëã•ËÉΩ‰ª•ËæÉ‰ΩéÁöÑËÆ°ÁÆó‰ª£‰ª∑ÂØªÊâæÊµ∑Ê£ÆÁü©ÈòµÁöÑËøë‰ººÈÄÜÁü©Èòµ,ÂàôÂèØ‰ª•ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄ,ËøôÂ∞±ÊòØÊãüÁâõÈ°øÊ≥ï(quai-Newton method).&lt;/p>
&lt;/blockquote>
&lt;h3 id="newtons-method" >
&lt;div>
&lt;a href="#newtons-method">
##
&lt;/a>
Newton&amp;rsquo;s method
&lt;/div>
&lt;/h3>
&lt;p>Sometimes we need to find all the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a &lt;strong>Jacobian matrix&lt;/strong>. Specifically, if we have a function $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$, then the Jacobian matrix $\mathbf{J} \in \mathbb{R}^{m \times n}$ of $f$ is defined such that $J_{i, j} = \frac{\partial}{\partial x_j}f(x)_i$.&lt;/p>
&lt;p>We are also sometimes interested in a derivative of a derivative. This is known as a &lt;strong>second derivative&lt;/strong>. For example, for a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$, the derivative with respect to $x_i$ of the derivative of $f$ with respect to $x_j$ is denoted as $\frac{\partial^2}{\partial x_i \partial x_j}f$.&lt;/p>
&lt;p>In a single dimension, we can denote $\frac{d^2}{d x^2}$ by $f&amp;rsquo;&amp;rsquo;(x)$. The second derivative tells us how the first derivative will change when we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. We can think of the second derivative as measuring &lt;strong>curvature&lt;/strong>.&lt;/p>
&lt;p>Suppose we have a quadratic function (or in practice it can be approximated well as quadratic, at least locally). If such a function has:&lt;/p>
&lt;ul>
&lt;li>Second derivative of zero: there is no curvature, it is a flat line, its value can be predict using only the gradient. If the gradient is 1, set the step size of $\epsilon$ along the negative gradient, then the cost function will decrease by $\epsilon$.&lt;/li>
&lt;li>Second derivative is negative: the function curves downward, decrease by more than $\epsilon$.&lt;/li>
&lt;li>Second derivative is positive: the function curves upward, decrease by less than $\epsilon$.&lt;/li>
&lt;/ul>
&lt;p>When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the &lt;strong>Hessian matrix&lt;/strong>. The Hessian matrix $H(f)(x)$ is defined such that&lt;/p>
&lt;p>$$
\tag{4.6}
H(f)(x)_{i, j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x).
$$&lt;/p>
&lt;p>Equivalently, the Hessian is the Jacobian of the gradient.&lt;/p>
&lt;p>The (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation to the function $f(x)$ around the current point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.8}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}}g + \frac{1}{2} (x - x^{(0)})^{\mathsf{T}}H(x - x^{(0)}),
$$&lt;/p>
&lt;p>where $g$ is the gradient and $H$ is the Hessian at $x^{(0)}$. If we use a learning rate of $\epsilon$, then the new point $x$ will be given by $x^{(0)} - \epsilon g$. Substituting this into our approximation, we obtain&lt;/p>
&lt;p>$$
\tag{4.9}
f(x^{(0)} - \epsilon g) \approx f(x^{(0)}) - \epsilon g^{\mathsf{T}}g + \frac{1}{2} \epsilon^2 g^{\mathsf{T}}Hg.
$$&lt;/p>
&lt;p>There are three terms here:&lt;/p>
&lt;ul>
&lt;li>the original value of the function&lt;/li>
&lt;li>the expected improvement due to the slope of the function&lt;/li>
&lt;li>and the correction we must apply to account for the curvature of the function&lt;/li>
&lt;/ul>
&lt;p>When this last term is too large, the gradient descent step can actually move uphill.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is zero or negative, the Taylor series approximation predicts that increase $\epsilon$ forever will decrease $f$ forever. In practice, the Taylor series is unlikely to remain accurate for large $\epsilon$, so one must resort to more heuristic choices of $\epsilon$ in this case.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is positive, solving for the optimal step size that decrease the Taylor series approximation of the function the most yields&lt;/p>
&lt;p>$$
\tag{4.10}
\epsilon^* = \frac{g^{\mathsf{T}}g}{g^{\mathsf{T}}Hg}.
$$&lt;/p>
&lt;p>In the worst case, when $g$ aligns with the eigenvector of $H$ corresponding to the maximal eigenvalue $\lambda_{max}$, then this optimal step size is given by $\frac{1}{\lambda_{max}}$. (The eigenvalues of the Hessian determine the scale of the learning rate, if the function we minimized can be approximated well by a quadratic function.)&lt;/p>
&lt;p>Using the eigendecomposition of the Hessian matrix, we can generalize the &lt;strong>second derivative test&lt;/strong> to multiple dimensions. At a critical point, where $\nabla_x f(x) = 0$, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum, or saddle point.&lt;/p>
&lt;ul>
&lt;li>when the Hessian is positive definite (all its eigenvalues are positive): local minimum.&lt;/li>
&lt;li>when the Hessian is negative definite (all its eigenvalues are negative): local maximum.&lt;/li>
&lt;li>the test is inconclusive whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero.&lt;/li>
&lt;/ul>
&lt;p>In multiple dimensions, there is a different second derivative for each direction at a single point. The &lt;strong>condition number&lt;/strong> of the Hessian at this point measures how much the second derivative differ from each other. When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly, Gradient descent is unaware of this change in the derivative, so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.&lt;/p>
&lt;p>Poor condition number also makes choosing a good step size difficult. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature.&lt;/p>
&lt;p>This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method for doing so is known as &lt;strong>Newton&amp;rsquo;s method&lt;/strong>. Newton&amp;rsquo;s method is based on using a second-order Taylor series expansion to approximate $f(x)$ near some point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.11}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}} \nabla_x f(x^{(0)}) + \frac{1}{2}(x - x^{(0)})^{\mathsf{T}}H(f)(x^{(0)})(x - x^{(0)})
$$&lt;/p>
&lt;p>If we solve for the critical point of this function, we obtain&lt;/p>
&lt;p>$$
\tag{4.12}
x^* = x^{(0)} - H(f)(x^{(0)})^{-1} \nabla_x f(x^{(0)})
$$&lt;/p>
&lt;p>When $f$ is a positive definite quadratic function, Newton&amp;rsquo;s method consist of applying equation 4.12 once to jump to the minimum of the function directly. When $f$ is not truly quadratic but can be locally approximated as a positive definite quadratic, Newton&amp;rsquo;s method consists of applying equation 4.12 multiple times. NOTE that Newton&amp;rsquo;s method is only appropriate when the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient is not attracted to saddle points unless the gradient points toward them.&lt;/p>
&lt;h2 id="part_ii-why-that--work" >
&lt;div>
&lt;a href="#part_ii-why-that--work">
#
&lt;/a>
Part_II: Why that Work
&lt;/div>
&lt;/h2>
&lt;h3 id="linear-regression-as-maximum-likelihood" >
&lt;div>
&lt;a href="#linear-regression-as-maximum-likelihood">
##
&lt;/a>
Linear Regression as Maximum Likelihood
&lt;/div>
&lt;/h3>
&lt;p>Previously, we motivated linear regression as an algorithm that learns to take an input $x$ and produce an output value $\hat{y}$. The mapping from $x$ to $\hat{y}$ is chosen to minimize mean squared error, a criterion that we introduced more or less arbitrarily. We now revisit linear regression from the point of view of maximum likelihood estimation.&lt;/p>
&lt;p>Instead of producing a single prediction $\hat{y}$, we now think of the model as producing a conditional distribution $p(y | x)$. We can imagine that with an infinitely large training set, we might see several training examples with the same input value $x$ but different values of $y$. The goal of the learning algorithm is now to fit the distribution $p(y | x)$ to all those different $y$ values that are all compatible with $x$.&lt;/p>
&lt;p>To derive the same linear regression algorithm we obtained before, we &lt;strong>define&lt;/strong> $p(y | x) = \mathcal{N}(y; \hat{y}(x; w), \sigma^2)$. In this example, we assume that the variance is fixed to some constant $\sigma^2$ chosen by user.&lt;/p>
&lt;p>Since the examples are assumed to be i.i.d., the conditional log-likelihood is given by&lt;/p>
&lt;p>$$
\begin{split}
&amp;amp;\sum^m_{i=1} log \ p(y^{(i)}|x^{(i)}; \theta) \\
&amp;amp;= -m \ log \ \sigma - \frac{m}{2} log(2 \pi) - \sum^m_{i=1} \frac{||\hat{y}^{(i)} - {y}^{(i)}||^2}{2 \sigma^2},
\end{split}
$$&lt;/p>
&lt;p>where $\hat{y}^{(i)}$ is the output of the linear regression on the $i$-th input $x^{(i)}$ and m is the number of the training examples. Comparing the log-likelihood with the mean squared error,&lt;/p>
&lt;p>$$
MSE_{train} = \frac{1}{m} \sum^m_{i=1} ||\hat{y}^{(i)} - {y}^{(i)}||^2,
$$&lt;/p>
&lt;p>we immediately see that maximizing the log-likelihood with respect to $w$ yields the same estimate of the parameters $w$ as does minimizing the mean squared error. The two criteria have different values but the same location of the optimum.&lt;/p>
&lt;p>This justifies the use of MSE as a maximum likelihood estimation procedure.&lt;/p>
&lt;h4 id="maximum-likelihood-estimation" >
&lt;div>
&lt;a href="#maximum-likelihood-estimation">
###
&lt;/a>
Maximum Likelihood Estimation
&lt;/div>
&lt;/h4>
&lt;p>Rather than guessing that some function might make a good estimator and then analyzing its bias and variance, we would like to have some principle from which we can derive specific functions that are good estimators for different models.&lt;/p>
&lt;p>The most common such principle is the maximum likelihood principle.&lt;/p>
&lt;p>Consider a set of m examples $\mathbb{X} = {x^{(1)}, \cdots, x^{(m)}}$ are i.i.d from true but unknown data-generating distribution $p_{data}(\mathbf{x})$.&lt;/p>
&lt;p>Let $p_{model}(\mathbf{x}; \mathbf{\theta})$ be a parametric family of probability distribution over the same space indexed by $\mathbf{\theta}$. In other words, $p_{model}({x}; \mathbf{\theta})$ maps any configuration $x$ to a real number estimating the true probability $p_{data}({x})$.&lt;/p>
&lt;p>The maximum likelihood estimator for $\mathbf{\theta}$ is then defined as&lt;/p>
&lt;p>$$
\begin{eqnarray}
\mathbf{\theta_{ML}}
\tag{5.56}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} {p_{model}(\mathbb{x}; \mathbf{\theta})} \\
\tag{5.57}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \prod^m_{i=1}{p_{model}(x^{(i)}; \mathbf{\theta})}
\end{eqnarray}
$$&lt;/p>
&lt;p>This product over many probabilities can be inconvenient for various reasons. Such as it&amp;rsquo;s prone to numerical underflow. We observe that taking the logarithm of the likelihood does not change its argmax but does conveniently transform a product into a sum:&lt;/p>
&lt;p>$$
\tag{5.58}
\mathbf{\theta_{ML}} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \sum^m_{i=1}{\text{log} \ p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>Because the argmax does not change when we rescale the cost function, we can divide by
$m$ to obtain a version of the criterion that is expressed as an expectation with
respect to the empirical distribution $\hat{p}_{data}$ defined by the training data:&lt;/p>
&lt;p>FIXME: \mathbb in equation(5.59, 5.60, 5.61) not work.&lt;/p>
&lt;p>$$
\tag{5.59}
\mathbf{\theta_{ML}} = {\underset{\mathbf{\theta}}{\operatorname{argmax}}}\
\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {\text{log} p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>One way to interpret maximum likelihood estimation is to view it as minimizing the
dissimilarity between the empirical distribution $\hat{p}_{data}$, defined by the
training set and the model distribution, with the degree of dissimilarity between the
two measured by the KL divergence. The KL divergence is given by&lt;/p>
&lt;p>$$
\tag{5.60}
D_{KL}(\hat{p}_{data} || p_{model}) = \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\log \hat{p}_{data}(x) - \log {p}_{model}(x)]}.
$$&lt;/p>
&lt;p>The term on the left is a function only of the data-generating process, not the model.
This means when we train the model to minimize the KL divergence, we need only minimize&lt;/p>
&lt;p>$$
\tag{5.61}
-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\text{log} \hat{p}_{data}(x)]},
$$&lt;/p>
&lt;p>which is of course the same as the maximization in equation 5.59.&lt;/p>
&lt;p>Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution and the model distribution. For example, MSE is the cross-entropy between the empirical distribution and a Gaussian model.&lt;/p>
&lt;p>We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution $\hat{p}_{data}$. While the optimal $\mathbf{\theta}$ is the same regardless of whatever we are maximizing the likelihood or minimizing the KL divergence, the values of the objective functions are different.&lt;/p>
&lt;p>In software, we often phrase both as minimizing a cost function.&lt;/p>
&lt;p>Maximum likelihood thus becomes minimization of the negative log-likelihood (NLL), or equivalently, minimization of the cross-entropy.&lt;/p>
&lt;p>ÂÖ≥‰∫éKLÊï£Â∫¶&lt;/p>
&lt;p>KLÊï£Â∫¶(Kullback-Leibler divergence), ‰∫¶Áß∞Áõ∏ÂØπÁÜµ(relative entropy)Êàñ‰ø°ÊÅØÊï£Â∫¶(information divergence), ÂèØÁî®‰∫éÂ∫¶Èáè‰∏§‰∏™Ê¶ÇÁéáÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇ. ÁªôÂÆö‰∏§‰∏™ËøûÁª≠ÂûãÊ¶ÇÁéáÂàÜÂ∏É$P$Âíå$Q$, ‰∫åËÄÖ‰πãÈó¥ÁöÑKLÊï£Â∫¶ÂÆö‰πâ‰∏∫&lt;/p>
&lt;p>$$
\label{eq_kld} \tag{C.34}
KL(P||Q) = \int^{\infty}_{-\infty} p(x) \text{log}\frac{p(x)}{q(x)} dx,
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠,$p(x)$Âíå$q(x)$ÂàÜÂà´ÊòØ$P$Âíå$Q$ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞.&lt;/p>
&lt;p>KLÊï£Â∫¶Êª°Ë∂≥ÈùûË¥üÊÄß, Âç≥&lt;/p>
&lt;p>$$
\tag{C.35}
KL(P||Q) \geq 0,
$$&lt;/p>
&lt;p>ÂΩì‰∏î‰ªÖÂΩì$P=Q$Êó∂$KL(P||Q)=0$. ‰ΩÜÊòØ, KLÊï£Â∫¶‰∏çÊª°Ë∂≥ÂØπÁß∞ÊÄß, Âç≥&lt;/p>
&lt;p>$$
\tag{C.36}
KL(P||Q) \neq KL(Q||P),
$$&lt;/p>
&lt;p>Âõ†Ê≠§, KLÊï£Â∫¶‰∏çÊòØ‰∏Ä‰∏™Â∫¶Èáè(metric).&lt;/p>
&lt;p>Ëã•Â∞ÜKLÊï£Â∫¶ÁöÑÂÆö‰πâ($\ref{eq_kld}$)Â±ïÂºÄ, ÂèØÂæó&lt;/p>
&lt;p>$$
\begin{eqnarray}
KL(P||Q)
&amp;amp;=&amp;amp; \int^{\infty}_{-\infty} p(x)\log p(x)dx - \int^{\infty}_{-\infty} p(x)\log q(x)dx \\
\tag{C.37}
&amp;amp;=&amp;amp; -H(P) + H(P, Q),
\end{eqnarray}
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠$H(P)$‰∏∫ÁÜµ(entropy), $H(P,Q)$‰∏∫‰∫§ÂèâÁÜµ(cross-entropy).&lt;/p>
&lt;h3 id="bayesian-linear-regression" >
&lt;div>
&lt;a href="#bayesian-linear-regression">
##
&lt;/a>
Bayesian Linear Regression
&lt;/div>
&lt;/h3>
&lt;p>So far we have discussed &lt;strong>frequentist statistics&lt;/strong> and approaches based on estimating a single value of $\theta$, then making all predictions thereafter based on that one estimate. An other approach is to consider all possible values of $\theta$ when making a prediction. The latter is the domain of &lt;strong>Bayesian statistics&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>Freqentist: the true parameter value $\theta$ is fixed but unknown, while the point estimate $\hat{\theta}$ is a random variable on account of it being a function of the dataset (which is seen as random).&lt;/li>
&lt;li>Bayesian: the Bayesian uses probability to reflect degrees of certainty in states of knowledge. The dataset is directly observed and so is not random. On the other hand, the true parameter $\theta$ is unknown or uncertain and thus is represented as random variable.&lt;/li>
&lt;/ul>
&lt;p>Before observing the data, we represent our knowledge of $\theta$ using the &lt;strong>prior probability distribution&lt;/strong>, $p(\theta)$ (a.k.a., &amp;ldquo;the prior&amp;rdquo;). Generally, the machine learning practitioner selects a prior distribution that quite broad (i.e., with high entropy, such as uniform distribution) to reflect a high degree of uncertainty in the value of $\theta$ before observing any data.&lt;/p>
&lt;p>Now consider that we have a set of data samples {$x^{(1)}, \ldots, x^{(m)}$}. We can recover the effect of data on our belief about $\theta$ by combining the data likelihood $p(x^{(1)}, \ldots, x^{(m)} | \theta)$ with the prior via Bayes&amp;rsquo; rule:
$$
\tag{5.67}
p(\theta | x^{(1)}, \ldots, x^{(m)}) = \frac{p(x^{(1)}, \ldots, x^{(m)} | \theta) p(\theta)}{p(x^{(1)}, \ldots, x^{(m)})}
$$&lt;/p>
&lt;p>In the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation of the data usually causes the posterior to lose entropy and concentrate around a few highly likely values of the parameters.&lt;/p>
&lt;p>Bayesian estimation offers two important differences from MLE:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Unlike the MLE approach that makes predictions using a point estimate of $\theta$, the Bayesian approach is to make predictions using a full distribution over $\theta$. For example, after observing $m$ examples, the predicted distribution over the next data sample, $x^{(m+1)}$, is given by
$$
\tag{5.68}
p(x^{(m+1)} | x^{(1)}, \ldots, x^{(m)}) = \int p(x^{(m+1)} | \theta) p(\theta | x^{(1)}, \ldots, x^{(m)}) d \theta
$$
Here each value of $\theta$ with positive probability density contributes to the prediction of the next example, with the contribution weighted by the posterior density itself.&lt;/p>
&lt;p>After having observed {$x^{(1)}, \ldots, x^{(m)}$}, if we are still uncertain about the value of $\theta$, then this uncertainty is incorporated into any predictions we might make.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The prior has an influence by shifting probability mass density towards regions of the parameter space that are preferred a priori. In practice, the prior often expresses a preference for models that are simpler or more smooth.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Critics of the Bayesian approach identify the prior as a source of subjective human judgment affecting the predictions.&lt;/p>
&lt;p>Bayesian methods typically generalize much better when limited training data is available but typically suffer from high computational cost when the number of training examples is large.&lt;/p>
&lt;p>&lt;strong>$\color{Green}{\mathbf{Example}}$&lt;/strong>&lt;/p>
&lt;p>Here we consider the Bayesian estimation approach to learning the linear regression parameters. In linear regression, we learn a linear mapping from an input vector $x \in \mathbb{R}^n$ to predict the value of a scalar $y \in \mathbb{R}$. The prediction is parameterized by the vector $w \in \mathbb{R}^n$:
$$
\tag{5.69}
\hat{y} = w^{\mathsf{T}}x.
$$
Given a set of $m$ training samples ($X^{(train)}, y^{(train)}$), we can express the prediction of $y$ over the entire training set as
$$
\tag{5.70}
\hat{y}^{(train)} = X^{(train)}w.
$$
Expressed as a Gaussian conditional distribution on $y^{(train)}$, we have
$$
\begin{eqnarray}
p(y^{(train)} | X^{(train)}, w)
\tag{5.71}
&amp;amp;=&amp;amp; \mathcal{N}(y^{(train)} ; X^{(train)}w, I) \\
\tag{5.72}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(
- \frac{1}{2}(y^{(train)} - X^{(train)}w)^{\mathsf{T}} (y^{(train)} - X^{(train)}w)
\bigg),
\end{eqnarray}
$$
where we follow the standard MSE formulation in assuming that the Gaussian variance on $y$ is one.&lt;/p>
&lt;p>In what follows, to reduce the notational burden, we refer to ($X^{(train)}, y^{(train)}$) as simply ($X, y$).&lt;/p>
&lt;p>To determine the posterior distribution over the model parameter vector $w$, we first need to specify a prior distribution. For real-valued parameters it is common to use a Gaussian as a prior distribution,
$$
\tag{5.73}
p(w) = \mathcal{N}(w; \mu_0, \Lambda_0) \varpropto \text{exp} \bigg(
-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg),
$$
where $\mu_0$ and $\Lambda_0$ are the prior distribution mean vector and covariance matrix respectively. (We assume a diagonal covariance matrix $\Lambda_0 = diag(\lambda_0)$, unless there is a reason to use a particular covariance structure.)&lt;/p>
&lt;p>With the prior thus specified, we can now proceed in determining the &lt;strong>posterior&lt;/strong> distribution over the model parameters:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.74}
&amp;amp;\varpropto&amp;amp; p(y | X, w)p(w) \\
\tag{5.75}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(y - Xw)^{\mathsf{T}} (y - Xw) \bigg) \text{exp} \bigg(-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg) \\
\tag{5.76}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} \big( -2y^{\mathsf{T}}Xw + w^{\mathsf{T}}X^{\mathsf{T}}Xw + w^{\mathsf{T}} \Lambda_0^{-1}w - 2\mu_0^{\mathsf{T}}\Lambda_0^{-1}w \big) \bigg)
\end{eqnarray}
$$
We now define $\Lambda_m = (X^{\mathsf{T}}X + \Lambda_0^{-1})^{-1}$ and $\mu_m = \Lambda_m (X^{\mathsf{T}}y + \Lambda_0^{-1} \mu_0)$. Using these new variables, we find that the posterior may be rewritten as a Gaussian distribution:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.77}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) + \frac{1}{2} \mu_m^{\mathsf{T}}\Lambda_m^{-1}\mu_m \bigg) \\
\tag{5.78}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} (w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) \bigg)
\end{eqnarray}
$$
All terms that do not include the parameter vector $w$ have been omitted; they are implied by the fact that the distribution must be normalized to integrate to 1.&lt;/p>
&lt;p>Equation 3.23 shows how to normalize a multivariate Gaussian distribution:
$$
\tag{3.23}
\mathcal{N}(x; \mu, \Sigma) = \sqrt{\frac{1}{(2\pi)^n \text{det}(\Sigma)}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \Sigma^{-1}(x - \mu) \bigg).
$$
When we wish to evaluate the PDF several times for many different values of the parameters, the covariance is not a computationally efficient way to parametrize the distribution, since we need to invert $\Sigma$ to evaluate the PDF. We can instead use a &lt;strong>precision matrix $\beta$&lt;/strong>:
$$
\tag{3.24}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{\frac{\text{det}(\beta)}{(2\pi)^n}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \beta (x - \mu) \bigg).
$$&lt;/p>
&lt;h4 id="maximum-a-posteriori-map-estimation" >
&lt;div>
&lt;a href="#maximum-a-posteriori-map-estimation">
###
&lt;/a>
Maximum A Posteriori (MAP) Estimation
&lt;/div>
&lt;/h4>
&lt;p>While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter $\theta$, it is still often desirable to have a single point estimate. One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation.&lt;/p>
&lt;p>Rather than simply returning to the MLE, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate. One rational way to do this id to choose the &lt;strong>maximum a posteriori&lt;/strong> (MAP) point estimate. The MAP estimate chooses the point of maximal posterior probability ( or maximal probability density in the more common case of continuous $\theta$):
$$
\tag{5.79}
\theta_{MAP}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {p(\mathbf{\theta} | x)}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {\text{log} \ p(x | \mathbf{\theta})} + \text{log} \ p(\mathbf{\theta})
$$
We recognize, on the righthand side, $\text{log} \ p(x | \mathbf{\theta})$, that is, the standard log-likelihood term, and $\text{log} \ p(\mathbf{\theta})$, corresponding to the prior distribution.&lt;/p>
&lt;p>As an example, consider a linear regression model with a Gaussian prior on the weights $w$. If this prior is given by $\mathcal{N}(w; 0, \frac{1}{\lambda}I^2)$, then the log-prior term in equation 5.79 is proportional to the familiar $\lambda w^{\mathsf{T}}w$ weight decay penalty, plus a term that does not depend on $w$ and does not affect the learning process. MAP Bayesian inference with a Gaussian prior on the weights thus corresponds to weight decay.&lt;/p>
&lt;h3 id="mle-and-map-ÊÆäÈÄîÂêåÂΩí" >
&lt;div>
&lt;a href="#mle-and-map-%e6%ae%8a%e9%80%94%e5%90%8c%e5%bd%92">
##
&lt;/a>
MLE and MAP: ÊÆäÈÄîÂêåÂΩí
&lt;/div>
&lt;/h3>
&lt;h4 id="binary-variables" >
&lt;div>
&lt;a href="#binary-variables">
###
&lt;/a>
Binary Variables
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Coin flipping: heads = 1, tails = 0 with bias $\mu$
$$
p(X = 1 | \mu) = \mu
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Bernoulli Distribution
$$
Bern(x | \mu) = \mu^x \cdot (1 - \mu)^{1 - x} \\
\mathbf{E}[X] = \mu \\
var(X) = \mu \cdot (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>N coin flips: $X_1, \ldots, X_N$
$$
p(\Sigma_i X_i = m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Binomial Distribution
$$
p(m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
\mathbf{E}[\Sigma_i X_i] = N \mu \\
var[\Sigma_i X_i] = N \mu (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="the-bias-of-a-coin" >
&lt;div>
&lt;a href="#the-bias-of-a-coin">
###
&lt;/a>
The Bias of a Coin
&lt;/div>
&lt;/h4>
&lt;p>Suppose that we have a coin, and we would like to figure out what the probability is that it will flip up heads.&lt;/p>
&lt;ul>
&lt;li>How should we estimate the bias?&lt;/li>
&lt;/ul>
&lt;p>With these coin flips result: &lt;strong>[tail, head, tail, head, head]&lt;/strong>, our estimate of the bias is: 3/5 (&amp;ldquo;the frequency of heads&amp;rdquo;).&lt;/p>
&lt;ul>
&lt;li>
&lt;p>why is this a good estimate of the bias?&lt;/p>
&lt;p>- how good is this estimation?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$P(Heads) = \theta, \ P(Tails) = 1 - \theta$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Flips are i.i.d.&lt;/p>
&lt;ul>
&lt;li>Independent events&lt;/li>
&lt;li>Identically distributed according to Binomial distribution&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Our training data consists of $\alpha_H$ heads and $\alpha_T$ tails
$$
p(D | \theta) = \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="mle" >
&lt;div>
&lt;a href="#mle">
###
&lt;/a>
MLE
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Data: Observed set of $\alpha_H$ heads and $\alpha_T$ tails&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hypothesis: Coin flips follow a binomial distribution&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Learning: Find the &amp;ldquo;best&amp;rdquo; $\theta$&lt;/p>
&lt;p>Maximum Likelihood Estimation: Choose $\theta$ to maximize probability of $D$ given $\theta$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\hat{\theta}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;ul>
&lt;li>Set derivative to zero, and solve!&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\frac{d}{d\theta} \text{ln} \ P(D | \theta)
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}] \\
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\alpha_H \text{ln} \ \theta + \alpha_T \text{ln} (1 - \theta)] \\
&amp;amp;=&amp;amp; \alpha_H \frac{d}{d\theta} \text{ln} \ \theta + \alpha_T \frac{d}{d\theta} \text{ln} \ (1 - \theta) \\
&amp;amp;=&amp;amp; \frac{\alpha_H}{\theta} - \frac{\alpha_T}{1 - \theta} = 0 \\
\\
\Rightarrow \hat{\theta}_{MLE} &amp;amp;=&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;p>As we can see now, that&amp;rsquo;s exactly the &amp;ldquo;Frequency of the heads&amp;rdquo;! In other words, the frequency of heads is exactly the &lt;strong>maximum likelihood estimator&lt;/strong> for this problem.&lt;/p>
&lt;h4 id="map" >
&lt;div>
&lt;a href="#map">
###
&lt;/a>
MAP
&lt;/div>
&lt;/h4>
&lt;p>Suppose we have 5 coin flips all of which are heads, Our estimate of the bias is: ???&lt;/p>
&lt;ul>
&lt;li>MLE would give $\theta_{MLE} = 1$&lt;/li>
&lt;li>This event occurs with probability $1 / 2^5 = 1/32$ for a fair coin&lt;/li>
&lt;li>Are we willing to commit to such a strong conclusion with such little evidence?&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Priors&lt;/strong> are a Bayesian mechanism that allow us to take into account &amp;ldquo;prior&amp;rdquo; knowledge about our belief in the outcome. Rather than estimating a single $\theta$, consider a distribution over possible values of $\theta$ given the data:&lt;/p>
&lt;ul>
&lt;li>Without any data observed, our best guess of $\theta$ is obeyed a Beta(2, 2),&lt;/li>
&lt;li>After we see some data (such as observed flips:[tails, tails]), we update our prior to Beta(3, 2).&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Bayesian Learning&lt;/strong>
$$
\begin{eqnarray}
\tag{L1}
p(\theta | D) = \frac{p(D | \theta) \ p(\theta)}{p(D)} \\
\tag{L2} \label{eq_map}
\Rightarrow p(\theta | D) \varpropto p(D | \theta) \ p(\theta)
\end{eqnarray}
$$
where&lt;/p>
&lt;ul>
&lt;li>$p(\theta | D)$ is the posterior,&lt;/li>
&lt;li>$p(D | \theta)$ is the data likelihood,&lt;/li>
&lt;li>$p(\theta)$ is the prior,&lt;/li>
&lt;li>$p(D)$ is the normalization factor.&lt;/li>
&lt;/ul>
&lt;p>We update the prior according to the observed data to get the posterior by applying Bayes rule.&lt;/p>
&lt;p>&lt;strong>Picking Priors&lt;/strong>&lt;/p>
&lt;p>How do we pick a good prior distribution?&lt;/p>
&lt;ul>
&lt;li>Priors could represent expert domain knowledge&lt;/li>
&lt;li>Statisticians choose them to make the posterior distribution &amp;ldquo;nice&amp;rdquo; (conjugate priors, which makes the posterior the same form as the prior)&lt;/li>
&lt;/ul>
&lt;p>What is a good prior for the bias in the coin flipping problem?&lt;/p>
&lt;ul>
&lt;li>Truncated Gaussian (tough to work with)&lt;/li>
&lt;li>Beta distribution (works well for binary random variables)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Coin Flips with Beta Distribution&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Likelihood function: $p(D | \theta) = \theta^{\alpha_H} (1 - \theta)^{\alpha_T}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Prior: $p(\theta) = \frac{\theta^{\beta_H - 1} (1 - \theta)^{\beta_T - 1}}{B(\beta_H, \beta_T)} \sim Beta(\beta_H, \beta_T)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Posterior:
$$
\begin{eqnarray}
p(\theta | D)
&amp;amp;\varpropto&amp;amp; \theta^{\alpha_H}(1 - \theta)^{\alpha_T} \theta^{\beta_H - 1}(1 - \theta)^{\beta_T - 1} \\
&amp;amp;=&amp;amp; \theta^{\alpha_H + \beta_H - 1}(1 - \theta)^{\alpha_T + \beta_T - 1} \\
&amp;amp;=&amp;amp; Beta(\alpha_H + \beta_H, \alpha_T + \beta_T)
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>MAP Estimation&lt;/strong>&lt;/p>
&lt;p>Choosing $\theta$ to maximize the posterior distribution is called &amp;ldquo;maximum a posteriori (MAP)&amp;rdquo; estimation
$$
\theta_{MAP} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \ {p(\mathbf{\theta} | D)}
$$
The only difference between $\theta_{MLE}$ and $\theta_{MAP}$ is that one assumes a &lt;strong>uniform&lt;/strong> prior (MLE) and the other allows an arbitrary prior.&lt;/p>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>With uniform prior $p(\theta) \varpropto 1$, according to $\ref{eq_map}$, the posterior $p(\theta | D) \varpropto p(D | \theta)$.&lt;/p>
&lt;/blockquote>
&lt;p>Suppose we have 5 coin flips all of which are heads,&lt;/p>
&lt;ul>
&lt;li>
&lt;p>MLE would give $\theta_{MLE} = 1$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>MLE with a Beta(2, 2) prior gives $\theta_{MAP} = \frac{5 + 2 - 1}{5+2+0+2 - 2} = \frac{6}{7} \approx .857$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>As we see more data, the effect of the prior diminishes
$$
\begin{eqnarray}
\theta_{MAP}
&amp;amp;=&amp;amp; \frac{\alpha_H + \beta_H - 1}{\alpha_H + \beta_H + \alpha_T + \beta_T - 2} \\
\\
&amp;amp;\approx&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T} \ (\text{for large number of observations})
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="sample-complexity" >
&lt;div>
&lt;a href="#sample-complexity">
###
&lt;/a>
Sample Complexity
&lt;/div>
&lt;/h4>
&lt;p>How many coin flips do we need in order to guarantee that our learned parameter does not differ too much from the true parameter (with high probability)? Say, I want to know the coin parameter $\theta$, within $\epsilon = 0.1$ error with probability at least $1 - \delta = 0.95$.&lt;/p>
&lt;p>Using the Chernoff bound, we have
$$
p(|\theta_{true} - \theta_{MLE}| \geq \epsilon) \leq 2e^{-2N \epsilon^2} \\
\delta \geq 2e^{-2N \epsilon^2} \Rightarrow N \geq \frac{1}{2\epsilon^2} \text{ln} \frac{1}{\delta}
$$&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Convert each variable to standard units. The average of the products gives the correlation coefficient (may be more intuitively in the python code)&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Associated with a unit increase in $x$ there is some average change in $y$. The slope of the regression line estimates this change. The formula for the slope is $\frac{r \times SD_y}{SD_x}$. And the intercept of the regression line is just the predicted value for $y$ when $x$ is $0$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Book Notes: Probability and Information Theory</title><link>/posts/lossfunctions/probability_information_theory/</link><pubDate>Fri, 07 Aug 2020 21:17:59 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/lossfunctions/probability_information_theory/</guid><description>&lt;h1 id="probability-and-information-theory" >
&lt;div>
&lt;a href="#probability-and-information-theory">
##
&lt;/a>
Probability and Information Theory
&lt;/div>
&lt;/h1>
&lt;p>Probability theory is a mathematical framework for representing uncertain statements. It provides a means of quantifying uncertainty as well as axioms (ÂÖ¨ÁêÜ) for deriving new uncertain statements. In artificial intelligence applications, we use probability theory in two major ways:&lt;/p>
&lt;ul>
&lt;li>The laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using probability theory;&lt;/li>
&lt;li>We can use probability and statistics to theoretically analyze the behavior of proposed AI systems.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>While probability theory allows us to make uncertain statements and to reason in the presence of uncertainty, information theory enables us to quantify the amount of uncertainty in a probability distribution.&lt;/strong>&lt;/p>
&lt;h2 id="why-probability" >
&lt;div>
&lt;a href="#why-probability">
#
&lt;/a>
Why Probability?
&lt;/div>
&lt;/h2>
&lt;p>Machine learning must aways deal with uncertain quantities and sometimes stochastic (nondeterministic) quantities. Uncertainty and stochasticity can arise from many sources. There are three possible sources of uncertainty:&lt;/p>
&lt;ul>
&lt;li>Inherent stochasticity in the system being modeled. For example, most interpretations of quantum mechanics describe the dynamics of subatomic particles as being probabilistic.&lt;/li>
&lt;li>Incomplete observability. Even deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system. For example, there are three doors, two doors lead to a goat while a third leads to a car. The show contestant was asked to choose among three doors and wins the prize. The outcome given the contestant&amp;rsquo;s choice is deterministic, but from the contestant&amp;rsquo;s point of view, the outcome is uncertain.&lt;/li>
&lt;li>Incomplete modeling. When we use a model that must discard some of the information we have observed, the discarded information results in uncertainty in the model&amp;rsquo;s predictions.&lt;/li>
&lt;/ul>
&lt;p>While it should be clear that we need a means of representing and reasoning about uncertainty, it is not immediately obvious that probability theory can provide all the tools we want for AI applications. Probability theory was originally developed to analyze the frequencies of events. It is easy to see how probability theory can be used to study events like drawing a certain hand of cards in a poker game. These kinds of events are often repeatable. When we say that an outcome has a probability $p$ of occurring, it means that if we repeated the experiment (e.g., drawing a hand of cards, flipping a coin) infinitely many times, then proportion $p$ of the repetitions would result in that outcome. This kind of reasoning does not seem immediately applicable to propositions that are not repeatable. If a doctor analyzes a patient and says that the patient has a 40 percent chance of having the flu, this means something very different &amp;ndash; we cannot make infinitely many replicas of the patient, nor is there any reason to believe that different replicas of the patient would present with the same symptoms yet have varying underlying conditions. In the case of the doctor diagnosing the patient, we use probability to represent a &lt;strong>degree of belief&lt;/strong>, with $1$ indicating absolute certainty that the patient has the flu and $0$ indicating absolute certainty that the patient does not have the flu. The former kind of probability, related directly to the rates at which events occur, is known as &lt;strong>frequentist probability&lt;/strong>, while the latter, related to qualitative levels of certainty, is known as &lt;strong>Bayesian probability&lt;/strong>.&lt;/p>
&lt;p>Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions (ÂëΩÈ¢ò) are implied to be true or false given the assumption that some other set of propositions is true of false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions.&lt;/p>
&lt;h2 id="random-variables" >
&lt;div>
&lt;a href="#random-variables">
#
&lt;/a>
Random Variables
&lt;/div>
&lt;/h2>
&lt;p>A &lt;strong>random variable&lt;/strong> is a variable that can take on different values randomly. On its own, a random variable is just a description of the states that are possible; it must be coupled with probability distribution that specifies how likely each of these states are.&lt;/p>
&lt;p>&lt;em>Quantitative&lt;/em> random variables may be discrete or continuous. This is not a hard-and-fast distinction, but it is a useful one. For a discrete variable, the values can only differ by fixed amounts. Family size is discrete. Two families can differ in size by 0 or 1 or 2, and so on. Nothing in between is possible. Age, on the other hand, is a continuous variable. This doesn&amp;rsquo;t refer to the fact that a person is continuously getting older; it just means that the difference in age between two people can be arbitrarily small &amp;ndash; a year, a month, a day, a hour, $\ldots$ And there are variables are &lt;em>qualitative&lt;/em> : examples are marital status (single, married, widowed, divorced, separated) and employment status (employed, unemployed, not in the labor force). Finally, the terms &lt;em>qualitative, quantitative, discrete&lt;/em>, and &lt;em>continuous&lt;/em> are also used to describe data &amp;ndash; qualitative data are collected on a qualitative variable, and so on.&lt;/p>
&lt;p>&lt;img alt="random variables" src="./images/stats_random_variables.png">&lt;/p>
&lt;h2 id="probability-distributions" >
&lt;div>
&lt;a href="#probability-distributions">
#
&lt;/a>
Probability Distributions
&lt;/div>
&lt;/h2>
&lt;p>A &lt;strong>probability distribution&lt;/strong> is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous.&lt;/p>
&lt;h3 id="discrete-variables-and-probability-mass-functions-pmf" >
&lt;div>
&lt;a href="#discrete-variables-and-probability-mass-functions-pmf">
##
&lt;/a>
Discrete Variables and Probability Mass Functions (PMF)
&lt;/div>
&lt;/h3>
&lt;p>A probability distribution over discrete variables may be described using a &lt;strong>Probability Mass Functions (PMF)&lt;/strong>. We typically denote probability mass functions with a capital $P$. Often we associate each random variable with different probability mass function and the reader must infer which PMF to use based on the identity of the random variable, rather than on the name of the function; $P(\bf{x})$ is usually not the same as $P(\bf{y})$.&lt;/p>
&lt;p>The PMF maps from a state of a random variable to the probability of that random variable taking on that state. The probability that $\mathbf{x} = x$ is denoted explicitly as $P(\mathbf{x}=x)$ or $P(x)$ in brevity, with a probability of $1$ indicating that $\mathbf{x} = x$ is certain and a probability of $0$ indicating that $\mathbf{x} = x$ is impossible. Sometimes we define a variable first, then use $\sim$ notation: $\mathbf{x} \sim P(\mathbf{x})$.&lt;/p>
&lt;p>PMF can act on many variables at the same time. Such a probability distribution over many variables is known as a &lt;strong>join probability distribution&lt;/strong>. $P(\mathbf{x} = x, \mathbf{y} = y)$ denotes the probability that $\mathbf{x}=x, \mathbf{y}=y$ simultaneously. We may also write $P(x, y)$ for brevity.&lt;/p>
&lt;p>To be a PMF on a random variable $\bf{x}$, a function $P$ must satisfy the following properties:&lt;/p>
&lt;ul>
&lt;li>The domain of $P$ must be the set of all possible states of $\bf{x}$.&lt;/li>
&lt;li>$\forall x \in \mathbf{x}, 0 \le P(x) \le 1$. An impossible event has probability $0$, and no state can be less probable than that. Likewise, an event that is guaranteed to happen has probability $1$, and no state can have a greater chance of occurring.&lt;/li>
&lt;li>$\sum_{x \in \mathbf{x}} P(x) = 1$. We refer to this property as being &lt;strong>normalized&lt;/strong>. Without this property, we could obtain probabilities greater than one by computing the probability of one of many events occurring.&lt;/li>
&lt;/ul>
&lt;p>For example, consider a single discrete random variable $\bf{x}$ with $k$ different states. We can place a &lt;strong>uniform distribution&lt;/strong> on $\bf{x}$ &amp;ndash; that is, make each of its states equally likely &amp;ndash; by setting its PMF to
$$
\tag{3.1}
P(\mathbf{x}=x_i) = {1 \over k}
$$
for all $i$. We can see that this fits the requirements for a probability mass function. The value $1 \over k$ is positive because $k$ is positive integer. We also see that
$$
\tag{3.2}
\sum_i P(\mathbf{x}=x_i) = \sum_i {1 \over k} = {k \over k} = 1,
$$
so the distribution is properly normalized.&lt;/p>
&lt;h3 id="continuous-variables-and-probability-density-functions-pdf" >
&lt;div>
&lt;a href="#continuous-variables-and-probability-density-functions-pdf">
##
&lt;/a>
Continuous Variables and Probability Density Functions (PDF)
&lt;/div>
&lt;/h3>
&lt;p>When working with continuous random variables, we describe probability distributions using a &lt;strong>Probability Density Functions (PDF)&lt;/strong> rather than a PMF. To be a Probability Density Function, a function $p$ must satisfy the following properties:&lt;/p>
&lt;ul>
&lt;li>The domain of $p$ must be the set of all possible states of $\bf{x}$.&lt;/li>
&lt;li>$\forall x \in \mathbf{x}, p(x) \ge 0$. Note that we do not require $p(x) \le 1$.&lt;/li>
&lt;li>$\int p(x)dx = 1$.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>A probability density function $p(x)$ does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume $\delta x$ is given by $p(x) \delta x$.&lt;/strong> Specifically, the probability that $x$ lies in some set $\mathbb{S}$ is given by the integral of density function $p(x)$ over that set. In the univariate example, the probability that $x$ lies in the interval $[a, b]$ is given by $\int_{[a, b]}p(x)dx$.&lt;/p>
&lt;p>For an example of a PDF corresponding to a specific probability density over a continuous random variable, consider a uniform distribution on an interval of the real numbers. We can do this with a function $u(x; a, b)$, where $a$ and $b$ are the endpoints of the interval, with $b &amp;gt; a$. The &amp;ldquo;$;$&amp;rdquo; notation means &amp;ldquo;parametrized by&amp;rdquo;; we consider $x$ to be the argument of the function, while $a$ and $b$ are parameters that define the function. To ensure that there is no probability mass outside the interval, we say $u(x; a, b) = 0\ \text{for all}\ x \notin [a, b]$. Within $[a, b]$, $u(x; a, b) = {1 \over b-a}$. we can see that this is non-negative everywhere. Additionally, it integrates to $1$. We often denote that $x$ follows the uniform distribution on $[a, b]$ by writing $\mathbf{x} \sim U(a, b)$.&lt;/p>
&lt;h3 id="marginal-probability" >
&lt;div>
&lt;a href="#marginal-probability">
##
&lt;/a>
Marginal Probability
&lt;/div>
&lt;/h3>
&lt;p>Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the &lt;strong>marginal probability distribution&lt;/strong>.&lt;/p>
&lt;p>For example, suppose we have discrete random variables $\bf{x}$ and $\bf{y}$ , and we know $P(\bf{x}, y)$. We can find $P(\bf{x})$ with &lt;strong>sum rule&lt;/strong>:
$$
\tag{3.3}
\forall x \in \mathbf{x}, P(\mathbf{x} = x) = \sum_y P(\mathbf{x}=x, \mathbf{y}=y) .
$$
The name &amp;ldquo;marginal probability&amp;rdquo; comes from the process of computing marginal probabilities on paper. When the values of $P(\bf{x}, y)$ are written in a grid with different values of $x$ in rows and different values of $y$ in columns, it is natural to sum across a row of the grid, then write $P(\bf{x})$ in the marginal of the paper just to the right of the row (see Table 4.1 from the book [Doing Bayesian Data Analysis]).&lt;/p>
&lt;p>&lt;img alt="discrete marginal probability" src="./images/DBDA_marginal_probability.png">&lt;/p>
&lt;p>For continuous variables, we need to use integration instead of summation:
$$
\tag{3.4}
p(x) = \int p(x, y)dy .
$$&lt;/p>
&lt;h3 id="conditional-probability" >
&lt;div>
&lt;a href="#conditional-probability">
##
&lt;/a>
Conditional Probability
&lt;/div>
&lt;/h3>
&lt;p>In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a &lt;strong>conditional probability&lt;/strong>. We denote the conditional probability that $\mathbf{y}=y$ given $\mathbf{x}=x$ as $P(\mathbf{y}=y | \mathbf{x}=x)$. This conditional probability can be computed with the formula
$$
\tag{3.5}
P(\mathbf{y}=y | \mathbf{x}=x) = \frac{P(\mathbf{y}=y , \mathbf{x}=x)}{P(\mathbf{x}=x)} .
$$
The conditional probability is only defined when $P(\mathbf{x}=x) &amp;gt; 0$. We cannot compute the conditional probability conditioned on an event that never happens.&lt;/p>
&lt;p>It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability that a person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change. Computing the consequences of an action is called making an &lt;strong>intervention query&lt;/strong>. Intervention queries are the domain of &lt;strong>causal modeling&lt;/strong>.&lt;/p>
&lt;h3 id="the-chain-rule-of-conditional-probability" >
&lt;div>
&lt;a href="#the-chain-rule-of-conditional-probability">
##
&lt;/a>
The Chain Rule of Conditional Probability
&lt;/div>
&lt;/h3>
&lt;p>Any join probability distribution over many random variables may be decomposed into conditional distribution over only one variable:
$$
\tag{3.6}
P(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}) = P(\mathbf{x}^{(1)}) \prod^n_{i=2} P(\mathbf{x}^{(i)} | \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(i-1)}) .
$$
This observation is known as the &lt;strong>chain rule&lt;/strong>, or &lt;strong>product rule&lt;/strong>, of probability. It follows immediately from the definition of conditional probability in equation 3.5.&lt;/p>
&lt;p>For example, applying the definition twice, we get
$$
\begin{eqnarray}
P(a, b, c) &amp;amp;=&amp;amp; P(a | b, c) P(b, c) \
\
P(b, c) &amp;amp;=&amp;amp; P(b | c) P(c) \
\
P(a, b, c) &amp;amp;=&amp;amp; P(a | b, c) P(b | c) P(c) .
\end{eqnarray}
$$&lt;/p>
&lt;h3 id="independence-and-conditional-independence" >
&lt;div>
&lt;a href="#independence-and-conditional-independence">
##
&lt;/a>
Independence and Conditional Independence
&lt;/div>
&lt;/h3>
&lt;p>Two random variables $\bf{x}$ and $\bf{y}$ are &lt;strong>independent&lt;/strong> if their probability distribution can be expressed as a product of two factors, one involving only $\bf{x}$ and one involving only $\bf{y}$ :
$$
\tag{3.7}
\forall x \in \mathbf{x}, y \in\mathbf{y}, p(\mathbf{x}=x, \mathbf{y}=y) = p(\mathbf{x}=x) p(\mathbf{y}=y) .
$$
Two random variables $\bf{x}$ and $\bf{y}$ are &lt;strong>conditional independent&lt;/strong> given a random variable $z$ if the conditional probability distribution over $\bf{x}$ and $\bf{y}$ factorizes in this way for every value of $z$ :
$$
\tag{3.8}
\forall x \in \mathbf{x}, y \in\mathbf{y}, z \in \mathbf{z},
p(\mathbf{x}=x, \mathbf{y}=y | \mathbf{z}=z) =
p(\mathbf{x}=x | \mathbf{z}=z) p(\mathbf{y}=y | \mathbf{z}=z) .
$$
We can denote &lt;strong>independent&lt;/strong> and &lt;strong>conditional independent&lt;/strong> with compact notation:&lt;/p>
&lt;ul>
&lt;li>$\bf{x} \perp y$ means that $\bf{x}$ and $\bf{y}$ are independent,&lt;/li>
&lt;li>$\bf{x} \perp y | z$ means that $\bf{x}$ and $\bf{y}$ are conditional independent given $z$.&lt;/li>
&lt;/ul>
&lt;h3 id="expectation-variance-and-covariance" >
&lt;div>
&lt;a href="#expectation-variance-and-covariance">
##
&lt;/a>
Expectation, Variance and Covariance
&lt;/div>
&lt;/h3>
&lt;p>The &lt;strong>expectation&lt;/strong>, or &lt;strong>expected value&lt;/strong>, of some function $f(x)$ with respect to a probability distribution $P(\mathbf{x})$ is the average, or mean value, that $f$ takes on when $x$ is drawn from $P$. For discrete variables this can be computed with summation:
$$
\tag{3.9}
\mathbb{E}&lt;em>{\mathbf{x} \sim P} [f(x)] = \sum_x P(x)f(x),
$$
while for continuous variables, it is computed with an integral:
$$
\tag{3.10}
\mathbb{E}&lt;/em>{\mathbf{x} \sim P} [f(x)] = \int p(x)f(x)dx,
$$
Expectations are linear, for example,
$$
\tag{3.11}
\mathbb{E}&lt;em>{\mathbf{x}} [\alpha f(x) + \beta g(x)] =
\alpha \mathbb{E}&lt;/em>{\mathbf{x}}[f(x)] + \beta \mathbb{E}_{\mathbf{x}}[g(x)] ,
$$
when $\alpha$ and $\beta$ are not dependent on $x$.&lt;/p>
&lt;p>The &lt;strong>variance&lt;/strong> gives a measure of how much the values of a function of a random variable $\bf{x}$ vary as we sample different values of $x$ from its probability distribution:
$$
\tag{3.12}
\text{Var}(f(x)) = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2] .
$$
When the variance is low, the values of $f(x)$ cluster near their expected value. The square root of the variance is known as the &lt;strong>standard deviation&lt;/strong>.&lt;/p>
&lt;p>The &lt;strong>covariance&lt;/strong> gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:
$$
\tag{3.13}
\text{Cov}(f(x), g(y)) = \mathbb{E} \bigg[\big(
f(x) - \mathbb{E}[f(x)])\ (g(y) - \mathbb{E}[g(y)]
\big)\bigg] .
$$
High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.&lt;/p>
&lt;ul>
&lt;li>If the sign of the covariance is positive, both variables tend to take on relatively high values simultaneously,&lt;/li>
&lt;li>If the sign of the covariance is negative, one variable tend to take on relatively high values and the other takes on a relatively low values and vice versa.&lt;/li>
&lt;/ul>
&lt;p>The notions of covariance and dependence are related but distinct concepts.&lt;/p>
&lt;ul>
&lt;li>Two independent variables have zero covariance, and two variables have nonzero covariance are dependent.&lt;/li>
&lt;li>Zero covariance means there must be no linear dependence, While independent also excludes nonlinear relationships between two variables.&lt;/li>
&lt;li>It is possible for two variables to be dependent but have zero covariance.&lt;/li>
&lt;/ul>
&lt;p>The &lt;strong>covariance matrix&lt;/strong> of a random vector $x \in \R^n$ is an $n \times n$ matrix, such that
$$
\tag{3.14}
\text{Cov}(\mathbf{x})_{i, j} = \text{Cov}(\mathbf{x}_i, \mathbf{x}_j) .
$$
The diagonal elements of the covariance give the variance:
$$
\tag{3.15}
\text{Cov}(\mathbf{x}_i, \mathbf{x}_i) = \text{Var}(\mathbf{x}_i) .
$$&lt;/p>
&lt;h3 id="common-probability-distribution" >
&lt;div>
&lt;a href="#common-probability-distribution">
##
&lt;/a>
Common Probability Distribution
&lt;/div>
&lt;/h3>
&lt;h4 id="bernoulli-distribution" >
&lt;div>
&lt;a href="#bernoulli-distribution">
###
&lt;/a>
Bernoulli Distribution
&lt;/div>
&lt;/h4>
&lt;p>The Bernoulli Distribution is a distribution over a single binary random variable. It is controlled by a single parameter $\phi \in [0, 1]$, which gives the probability of the random variable being equal to $1$. It has the following properties:
$$
\begin{eqnarray}
\tag{3.16}
P(\mathbf{x} = 1) &amp;amp;=&amp;amp; \phi \
\
\tag{3.17}
P(\mathbf{x} = 0) &amp;amp;=&amp;amp; 1 - \phi \
\
\tag{3.18}
P(\mathbf{x} = x) &amp;amp;=&amp;amp; \phi^{x} (1 - \phi)^{1-x} \
\
\tag{3.19}
\mathbb{E}&lt;em>\mathbf{x}[\mathbf{x}] &amp;amp;=&amp;amp; \phi \
\
\tag{3.20}
\text{Var}&lt;/em>\mathbf{x}(\mathbf{x}) &amp;amp;=&amp;amp; \phi (1 - \phi)
\end{eqnarray}
$$&lt;/p>
&lt;h4 id="gaussian-distribution" >
&lt;div>
&lt;a href="#gaussian-distribution">
###
&lt;/a>
Gaussian Distribution
&lt;/div>
&lt;/h4>
&lt;p>The most commonly used distribution over real numbers is the normal distribution, also known as the Gaussian distribution:
$$
\tag{3.21}
\mathcal{N}(x; \mu, \sigma^2) = \sqrt{{1 \over {2 \pi \sigma^2}}} \exp \bigg(-{1 \over {2 \sigma^2}} (x - \mu)^2 \bigg).
$$
The two parameters $\mu \in \R$ and $\sigma \in (0, \infin)$ control the normal distribution.&lt;/p>
&lt;p>When we evaluate the PDF, we need to square and invert $\sigma$. When we need to frequently evaluate the PDF with different parameter values, a more efficient way of parametrizing the distribution is to use a parameter $\beta \in (0, \infin)$ to control the &lt;strong>precision&lt;/strong>, or inverse variance, of the distribution:
$$
\tag{3.22}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{{\beta \over {2 \pi}}} \exp \bigg(-{1 \over 2} \beta (x - \mu)^2 \bigg).
$$
&lt;img alt="bell curve" src="./images/DL_gaussian_distribution.png">&lt;/p>
&lt;p>Normal distributions are a sensible choice for many applications, for two major reasons:&lt;/p>
&lt;ul>
&lt;li>First, many distributions we wish to model are truly close to being normal distributions. The &lt;strong>central limit theorem&lt;/strong> shows that the sum of many independent random variables is approximately normally distributed. This means that in practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior.&lt;/li>
&lt;li>Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model.&lt;/li>
&lt;/ul>
&lt;p>The normal distribution generalizes to $\R^n$, in which case it is known as the &lt;strong>multivariate normal distribution&lt;/strong>. It may be parametrized with a positive definite symmetric matrix $\Sigma$ :
$$
\tag{3.23}
\mathcal{N}(x; \mu, \Sigma) = \sqrt{{1 \over {(2 \pi)^n |\Sigma|}}}
\exp \bigg(-{1 \over 2} (x - \mu)^{\mathsf{T}} \Sigma^{-1} (x - \mu) \bigg).
$$
The parameter $\mu$ still gives the mean of the distribution, though now it is vector valued. The parameter $\Sigma$ gives the covariance matrix of the distribution. $|\Sigma|$ denotes the &lt;strong>determinant&lt;/strong> of $\Sigma$ .&lt;/p>
&lt;p>As in the univariate case, when we wish to evaluate the PDF multiple times with more efficient computation, we need to invert $\Sigma$ , and instead use a &lt;strong>precision matrix $\beta$&lt;/strong> :
$$
\tag{3.24}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{{|\beta| \over (2 \pi)^n}} \exp \bigg(-{1 \over 2} (x - \mu)^{\mathsf{T}} \beta (x - \mu) \bigg).
$$
We often fix the covariance matrix to be a diagonal matrix. An even simpler version is the &lt;strong>isotropic&lt;/strong> Gaussian distribution, whose covariance matrix is a scalar times the identity matrix.&lt;/p>
&lt;h4 id="exponential-and-laplace-distributions" >
&lt;div>
&lt;a href="#exponential-and-laplace-distributions">
###
&lt;/a>
Exponential and Laplace Distributions
&lt;/div>
&lt;/h4>
&lt;p>In the context of deep learning, we often want to have a probability distribution with a sharp point at $x = 0$. To accomplish this, we can use the &lt;strong>exponential distribution&lt;/strong>:
$$
\tag{3.25}
p(x; \lambda) = \lambda \mathbf{1}&lt;em>{x \ge 0} \exp(- \lambda x) .
$$
The exponential distribution uses the &lt;strong>indicator function&lt;/strong> $\mathbf{1}&lt;/em>{x \ge 0}$ to assign probability zero to all negative values of $x$.&lt;/p>
&lt;p>A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point $\mu$ is the &lt;strong>Laplace distribution&lt;/strong>
$$
\tag{3.26}
\text{Laplace}(x; \mu, \gamma) = {1 \over 2\gamma} \exp(- {|x - \mu| \over \gamma}).
$$&lt;/p>
&lt;h2 id="information-theory" >
&lt;div>
&lt;a href="#information-theory">
#
&lt;/a>
Information Theory
&lt;/div>
&lt;/h2>
&lt;p>Information theory is a branch of applied mathematics that resolves around quantifying how much information is present in a signal. In the context of machine learning, we can apply information theory to characterize probability distributions or quantify similarity between probability distributions.&lt;/p>
&lt;p>The basic intuition behind information theory is that learning that an unlikely event has has occurred is more informative than learning that a likely event has occurred. A message saying &amp;ldquo;the sun rose this morning&amp;rdquo; is so uninformative (to human being on the earth) as to be unnecessary to send, but a message saying &amp;ldquo;there are a solar eclipse this morning&amp;rdquo; is very informative.&lt;/p>
&lt;p>We would like to quantify information in a way that formalizes this intuition.&lt;/p>
&lt;ul>
&lt;li>Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.&lt;/li>
&lt;li>Less likely events should have higher information content.&lt;/li>
&lt;li>Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as head once.&lt;/li>
&lt;/ul>
&lt;p>To satisfy all three of these properties, we define the &lt;strong>self-information&lt;/strong> of an event $\mathbf{x} = x$ to be
$$
\tag{3.48}
I(x) = - \log P(x) .
$$
In this series, we always use log to mean the natural logarithm, with base $e$. Our definition of $I(x)$ is therefore written in units of &lt;strong>nats&lt;/strong>. One nat is the amount of information gained by observing an event of probability $1 \over e$. Other texts use base-2 logarithms and units called &lt;strong>bits&lt;/strong> or &lt;strong>shannons&lt;/strong>; information measured in bits is just a rescaling of information measured in nats.&lt;/p>
&lt;p>When $\bf{x}$ is continuous, an event with unit density still has zero information, despite not being an event that is guaranteed to occur.&lt;/p>
&lt;h3 id="entropy" >
&lt;div>
&lt;a href="#entropy">
##
&lt;/a>
Entropy
&lt;/div>
&lt;/h3>
&lt;p>Self-information deals only with a single outcome. We can quantify the amount of uncertainty in an entire probability distribution using the &lt;strong>Shannon entropy&lt;/strong>,
$$
\tag{3.49}
H(\mathbf{x})
= \mathbb{E}&lt;em>{\mathbf{x} \sim P}[I(x)]
= - \mathbb{E}&lt;/em>{\mathbf{x} \sim P}[\log P(x)],
$$
also denoted $H(P)$. In other words, the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives a lower bound on the number of nats needed on average to encode symbols drawn from a distribution $P$. Distributions that are nearly deterministic (where outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy. When $\bf{x}$ is continuous, the Shannon entropy is known as the &lt;strong>differential entropy&lt;/strong>.&lt;/p>
&lt;p>&lt;img alt="Shannon entropy" src="./images/DL_binary_Shannon_entropy.png">&lt;/p>
&lt;h3 id="kl-divergence" >
&lt;div>
&lt;a href="#kl-divergence">
##
&lt;/a>
KL divergence
&lt;/div>
&lt;/h3>
&lt;p>If we have two separate probability distributions $P(x)$ and $Q(x)$ over the same random variable $\bf{x}$, we can measure how different these two distributions are using the &lt;strong>Kullback-Leibler (KL) divergence&lt;/strong>:
$$
\tag{3.50} \label{eq_kld}
D_{KL}(P || Q)
= \mathbb{E}&lt;em>{\mathbf{x} \sim P} \bigg[\log {P(x) \over Q(x)} \bigg]
= \mathbb{E}&lt;/em>{\mathbf{x} \sim P}[\log P(x) - \log Q(x)] .
$$&lt;/p>
&lt;p>In the case of discrete variables, it is the extra amount of information (in bits or nats) needed to send a message containing symbols drawn from probability distribution $P$, when we use a code that was designed to minimize the length of messages drawn from probability distribution $Q$.&lt;/p>
&lt;p>The KL divergence has many useful properties, most notably being non-negative. The KL divergence is $0$ if and only if $P$ and $Q$ are the same distribution in the case of discrete variables, or equal &amp;ldquo;almost everywhere&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&amp;rdquo; in the case of continuous variables. Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions. It is not a true distance measure because it is not symmetric: $D_{KL}(P || Q) \ne D_{KL}(Q || P)$ for some $P$ and $Q$. This asymmetriy means that there are important consequences to the choice of whether to use $D_{KL}(P || Q)$ or $D_{KL}(Q || P)$. See figure 3.6 for more detail.&lt;/p>
&lt;p>&lt;img alt="KL divergence" src="./images/DL_kl_divergence.png">&lt;/p>
&lt;p>A quantity that is closely related to the KL divergence is the &lt;strong>cross-entropy&lt;/strong> $H(P, Q) = H(P) + D_{KL}(P||Q)$, which is similar to the KL divergence ($\ref{eq_kld}$) but lacking the term on the left:
$$
\tag{3.51}
H(P, Q) = - \mathbb{E}_{\mathbf{x} \sim P} \log Q(x) .
$$
Minimizing the cross-entropy with respect to $Q$ is equivalent to minimizing the KL divergence, because $Q$ does not participate in the omitted term.&lt;/p>
&lt;p>When computing many of these quantities, it is common to encounter expressions of the form $0\log0$. By convention, in the context of information theory, we treat these expressions as $\lim_{x \rightarrow 0} x \log x = 0$.&lt;/p>
&lt;h3 id="kl-Êï£Â∫¶" >
&lt;div>
&lt;a href="#kl-%e6%95%a3%e5%ba%a6">
##
&lt;/a>
KL Êï£Â∫¶
&lt;/div>
&lt;/h3>
&lt;p>KL Êï£Â∫¶Ôºå‰πüÁß∞‰∏∫Áõ∏ÂØπÁÜµ(relative entropy)Êàñ‰ø°ÊÅØÊï£Â∫¶(information divergence)ÔºåÂèØÁî®‰∫éÂ∫¶Èáè‰∏§‰∏™Ê¶ÇÁéáÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÁªôÂÆö‰∏§‰∏™(ËøûÁª≠Âûã)Ê¶ÇÁéáÂàÜÂ∏É $P$ Âíå $Q$ Ôºå‰∫åËÄÖ‰πãÈó¥ÁöÑ KL Êï£Â∫¶ÂÆö‰πâ‰∏∫
$$
\tag{C.34}
KL(P||Q) = \int^{\infin}_{-\infin} p(x) \log {p(x) \over q(x)} dx,
$$
ÂÖ∂‰∏≠Ôºå$p(x)$ Âíå $q(x)$ ÂàÜÂà´‰∏∫ $P$ Âíå $Q$ ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞„ÄÇKL Êï£Â∫¶Êª°Ë∂≥ÈùûË¥üÊÄßÔºå‰ΩÜ‰∏çÊª°Ë∂≥ÂØπÁß∞ÊÄßÔºåÂõ†Ê≠§ KL Êï£Â∫¶‰∏çÊòØ‰∏Ä‰∏™Â∫¶Èáè(metric)&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>„ÄÇ&lt;/p>
&lt;p>Ëã•Â∞Ü KL Êï£Â∫¶ÁöÑÂÆö‰πâÂ±ïÂºÄÔºåÂèØÂæó
$$
\begin{eqnarray}
KL(P||Q)
&amp;amp;=&amp;amp; \int^{\infin}&lt;em>{-\infin} p(x) \log {p(x)} dx - \int^{\infin}&lt;/em>{-\infin} p(x) \log {q(x)} dx \
\
\tag{C.37}
&amp;amp;=&amp;amp; -H(P) + H(P,Q)
\end{eqnarray}
$$
ÂÖ∂‰∏≠Ôºå$H(P)$ ‰∏∫ÁÜµ(entropy)Ôºå$H(P,Q)$ ‰∏∫ $P$ Âíå $Q$ Âæó‰∫§ÂèâÁÜµ(cross-entropy)„ÄÇÂú®‰ø°ÊÅØËÆ∫‰∏≠ÔºåÁÜµ $H(P)$ Ë°®Á§∫ÂØπÊù•Ëá™ $P$ ÁöÑÈöèÊú∫ÂèòÈáèËøõË°åÁºñÁ†ÅÊâÄÈúÄË¶ÅÁöÑÊúÄÂ∞èÂ≠óËäÇÊï∞ÔºåËÄå‰∫§ÂèâÁÜµ $H(H,Q)$ ÂàôË°®Á§∫‰ΩøÁî®Âü∫‰∫é $Q$ ÁöÑÁºñÁ†ÅÂØπÊù•Ëá™ $P$ ÁöÑÂèòÈáèËøõË°åÁºñÁ†ÅÊâÄÈúÄË¶ÅÁöÑ ‚ÄúÈ¢ùÂ§ñ‚Äù ÁöÑÂ≠óËäÇÊï∞ÔºõÊòæÁÑ∂ÔºåÈ¢ùÂ§ñÂ≠óËäÇÊï∞ÂøÖÁÑ∂ÈùûË¥üÔºåÂΩì‰∏î‰ªÖÂΩì $P = Q$ Êó∂È¢ùÂ§ñÂ≠óËäÇÊï∞‰∏∫Èõ∂„ÄÇ&lt;/p>
&lt;h2 id="density-estimation" >
&lt;div>
&lt;a href="#density-estimation">
#
&lt;/a>
Density Estimation
&lt;/div>
&lt;/h2>
&lt;h3 id="histogram" >
&lt;div>
&lt;a href="#histogram">
##
&lt;/a>
Histogram
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>&amp;ldquo;In a (standard) histogram, the areas of the blocks represent percentages.&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;With the density scale on the vertical axis, the areas of the blocks come out in percent. The area under the histogram over an interval equals the percentage of cases in that interval. The total area under the histogram is 100%.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;p>A &lt;strong>histogram&lt;/strong> is a plot designed to show the distribution of values in a set of data. The values are first sorted, and then divided into a fixed number of equal-width bins. A plot is then drawn that shows the number of elements in each bin. (Note that the histograms in Figure 15.20 were not turn the counts into normalized probability density.)&lt;/p>
&lt;p>&lt;img alt="histograms" src="./images/ICPP_histograms_coin_flips.png">&lt;/p>
&lt;p>A histogram is a depiction of a &lt;strong>frequency distribution&lt;/strong>. It tells us how often a random variable has taken on a value in some range, e.g., how often the fraction of times a coin came up heads was between 0.4 and 0.5. It also provides information about the relative frequency of various ranges. For example, we can easily see that the fraction of heads falls between 0.4 and 0.5 far more frequently than it falls between 0.3 and 0.4 (see the one in the left in Figure 15.20). Notice that while the means in both plots (Figure 15.20) are about the same, the standard deviations are quite different. The spread of outcomes is much tighter when we flip the coin 1000 times per trail than when we flip the coin 100 times per trial&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>A &lt;strong>probability distribution&lt;/strong> captures the notion of relative frequency by giving the probability of a random value taking on a value within a range. Probability distributions fall into two groups: discrete probability distributions and continuous probability distributions, depending upon whether they define the probability distribution for a discrete or a continuous random variable.&lt;/p>
&lt;p>&lt;strong>Discrete probability distributions&lt;/strong> are easier to describe. Since there are a finite number of values that the variable can take on, the distribution can be described by simply listing the probability of each value.&lt;/p>
&lt;p>&lt;strong>Continuous probability distributions&lt;/strong> are trickier. Since there are an infinite number of possible values, the probability that a continuous random variable will take on a specific value is usually 0. For example, the probability that a car is traveling at exactly 81.3457283 miles per hour is probably 0. Mathematicians like to describe continuous probability distributions using a &lt;strong>Probability Density Function (PDF)&lt;/strong>. A &lt;strong>PDF&lt;/strong> describes the probability of a random variable lying between two values. Think of the PDF as defining a curve where the values on the x-axis lie between the minimum and maximum value of the random variable. Under the assumption that $x_1$ and $x_2$ lie in the domain of the random variable, the probability of the variable having a value between $x_1$ and $x_2$ is the area under the PDF curve between $x_1$ and $x_2$ .&lt;/p>
&lt;p>&lt;img alt="pdf" src="./images/ICPP_probability_density_functions.png">&lt;/p>
&lt;p>The &lt;code>random.random()&lt;/code> returns a value lies in interval [0, 1]. The area under the curve of PDF for &lt;code>random.random()&lt;/code> from 0 to 1 is 1. On the other hand, if we consider the area under the part of the curve between 0.2 and 0.4, it is 0.2 ($(0.4 - 0.2) \times 1.00$). Similarly, the area under the curve for &lt;code>random.random()+random.random()&lt;/code> from 0 to 2 is also 1, and the area under the curve from 0 to 1 is 0.5. Notice that the same length of interval has the same probability in PDF for &lt;code>random.random()&lt;/code> while some intervals are more probable than others in PDF for &lt;code>random.random()+random.random()&lt;/code>.&lt;/p>
&lt;h3 id="histogram-for-density-estimation" >
&lt;div>
&lt;a href="#histogram-for-density-estimation">
##
&lt;/a>
Histogram for density estimation
&lt;/div>
&lt;/h3>
&lt;p>Density estimation can be used of probability distributions (such Gaussian, Beta, Dirichlet distributions) having specific functional forms governed by a small number of parameters whose values are to be determined from a data set. This is called the &lt;em>parametric&lt;/em> approach to density modeling. An important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. For instance, if the process that generates the data is multimodal (Â§öÂ≥∞ÁöÑ), then this aspect of the distribution can never be captured by a Gaussian, which is necessarily unimodal&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We consider some &lt;em>nonparametric&lt;/em> approaches to density estimation that make few assumptions about the form of the distribution. Here we focus mainly on simple frequentist methods. Let&amp;rsquo;s start with a discussion of histogram methods for density estimation, we explore the properties of histogram density model, focusing on the case of a single continuous variable $x$.&lt;/p>
&lt;p>Standard histograms simply partition $x$ into distinct bins of with $\Delta_i$ and then count the number $n_i$ of observations of $x$ falling in bin $i$. In order to turn this count into a normalized probability density, we simply divide by the total number $N$ of observations and by the width $\Delta_i$ of the bins to obtain probability values for each bin given by
$$
\tag{2.241}
p_i = \frac{n_i}{N \Delta_i}
$$
for which it is easily seen that $\int p(x)dx = 1$. This gives a model for the density $p(x)$ that is constant over the width of each bin, and often the bins are chosen to have the same width $\Delta_i = \Delta$.&lt;/p>
&lt;p>&lt;img alt="histogram models" src="./images/PRML_histogram_models.png">&lt;/p>
&lt;p>From above figure, we can see that&lt;/p>
&lt;ul>
&lt;li>when $\Delta$ is very small (top right), the resulting density model is very spiky (lost a lot of structure of underlying distribution),&lt;/li>
&lt;li>if $\Delta$ is too large (bottom right), the result is a model that is too smooth (fails to capture the bimodal property of the green curve).&lt;/li>
&lt;/ul>
&lt;p>The best results are obtained for some intermediate value of $\Delta$ (middle right). In principle, a histogram density model is also dependent on the choice of edge location for the bins, though this is typically much less significant than the value of $\Delta$.&lt;/p>
&lt;p>Note that the histogram method has the property that once the histogram has been computed, the data set itself can be discarded, which can be advantageous if the data set is large. Also, the histogram approach is easily applied if the data points are arriving sequentially.&lt;/p>
&lt;p>In practice, the histogram technique can be useful for obtaining a quick visualization of data in one or two dimensions but is unsuited to most density estimation applications. The limitations are&lt;/p>
&lt;ul>
&lt;li>the estimated density has discontinuities that are due to the bin edges,&lt;/li>
&lt;li>in a space of high dimensionality, the number of bins (M) are exponential scaling with the number of dimension (D), which is $M^D$. The quantity of data needed to provide meaningful estimates of local probability density would be prohibitive.&lt;/li>
&lt;/ul>
&lt;p>Two widely used nonparametric techniques for density estimation, &lt;strong>kernel estimators&lt;/strong> and &lt;strong>nearest neighbors&lt;/strong>, which have better scaling with dimensionality than the simple histogram model.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>technical view of continuous variables [more advance mathematics knowledge required].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Â∫¶Èáè(metric)Â∫îÊª°Ë∂≥Âõõ‰∏™Âü∫Êú¨ÊÄßË¥®ÔºöÈùûË¥üÊÄß($dist(x_i,x_j) \ge 0$)ÔºõÂêå‰∏ÄÊÄß($dist(x_i, x_j)=0 \iff x_i=x_j$)ÔºõÂØπÁß∞ÊÄß($dist(x_i, x_j) = dist(x_j, x_i)$)ÔºõÁõ¥ÈÄíÊÄß($dist(x_i, x_j) \le dist(x_i, x_k) + dist(x_k, x_j)$)&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Recall that this is also an example of the law of the average.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Consider Gaussian Mixture Models in this case.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Book Notes: semi-supervised clustering methods</title><link>/posts/ml101/clusteringmethods/semi-supervised_clustering/</link><pubDate>Mon, 13 Jul 2020 12:11:47 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/clusteringmethods/semi-supervised_clustering/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>Ê≥®ÊòéÔºö
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ÂéüÁêÜÈÉ®ÂàÜÁöÑÂÜÖÂÆπÂùáÊù•Ëá™Âë®ÂøóÂçéÁöÑË•øÁìú‰π¶ÔºåÁúüÊ≠£ÁöÑÂ§ßÂ∏à‰πã‰Ωú„ÄÇ
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ÂÖ∂‰ªñÂÜÖÂÆπÊù•Ëá™ÂºÄÊ∫êÂåÖÊñáÊ°£„ÄÅÂºÄÊ∫êÁîµÂ≠ê‰π¶„ÄÅipynbÊñáÊ°£Á≠â„ÄÇ
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ÂçäÁõëÁù£ËÅöÁ±ª-semi-supervised-clustering" >
&lt;div>
&lt;a href="#%e5%8d%8a%e7%9b%91%e7%9d%a3%e8%81%9a%e7%b1%bb-semi-supervised-clustering">
#
&lt;/a>
ÂçäÁõëÁù£ËÅöÁ±ª Ôºàsemi-supervised clusteringÔºâ
&lt;/div>
&lt;/h2>
&lt;p>ËÅöÁ±ªÊòØ‰∏ÄÁßçÂÖ∏ÂûãÁöÑÊó†ÁõëÁù£Â≠¶‰π†‰ªªÂä°ÔºåÁÑ∂ËÄåÂú®Áé∞ÂÆûËÅöÁ±ª‰ªªÂä°‰∏≠Êàë‰ª¨ÂæÄÂæÄËÉΩËé∑Âæó‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÁõëÁù£‰ø°ÊÅØÔºå‰∫éÊòØÂèØ‰ª•ÈÄöËøáÂçäÁõëÁù£ËÅöÁ±ªÊù•Âà©Áî®È¢ùÂ§ñÁõëÁù£‰ø°ÊÅØ‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑËÅöÁ±ªÊïàÊûú„ÄÇ&lt;/p>
&lt;p>ËÅöÁ±ª‰ªªÂä°‰∏≠Ëé∑ÂæóÈ¢ùÂ§ñÁõëÁù£‰ø°ÊÅØÂ§ßËá¥Êúâ‰∏§ÁßçÁ±ªÂûãÔºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Ê†∑Êú¨Á∫¶ÊùüÔºö&lt;/p>
&lt;p>ÂøÖËøû (must-link): ÊåáÁöÑÊòØÊ†∑Êú¨ÂøÖÂ±û‰∫éÂêå‰∏Ä‰∏™Á∞á&lt;/p>
&lt;p>ÂãøËøû (cannot-link): Ê†∑Êú¨ÂøÖ‰∏çÂ±û‰∫éÂêå‰∏Ä‰∏™Á∞á&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ê†∑Êú¨Ê†áÁ≠æÔºö&lt;/p>
&lt;p>ÁõëÁù£‰ø°ÊÅØÊù•Ëá™Â∞ëÈáèÂ∏¶ÊúâÊ†áÁ≠æÁöÑÊ†∑Êú¨&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="Á∫¶ÊùükÂùáÂÄºÁÆóÊ≥ï-pseudo-code" >
&lt;div>
&lt;a href="#%e7%ba%a6%e6%9d%9fk%e5%9d%87%e5%80%bc%e7%ae%97%e6%b3%95-pseudo-code">
##
&lt;/a>
Á∫¶Êùü$k$ÂùáÂÄºÁÆóÊ≥ï (pseudo-code)
&lt;/div>
&lt;/h3>
&lt;p>Á∫¶Êùü$k$ÂùáÂÄºÁÆóÊ≥ï (Constrained k-means) ÊòØÂà©Áî®Á¨¨‰∏ÄÁ±ªÁõëÁù£‰ø°ÊÅØÁöÑ‰ª£Ë°®„ÄÇÁªôÂÆöÊ†∑Êú¨ÈõÜ $D={x_1, x_2, \ldots, x_m}$ ‰ª•Âèä ‚ÄúÂøÖËøû‚Äù ÂÖ≥Á≥ªÈõÜÂêà $\cal{M}$ Âíå ‚ÄúÂãøËøû‚Äù ÂÖ≥Á≥ªÈõÜÂêà $\cal{C}$ Ôºå$(x_i, x_j) \in \cal{M}$ Ë°®Á§∫ $x_i, x_j$ ÂøÖÂ±û‰∫éÂêåÁ∞áÔºå$(x_i, x_j) \in \cal{C}$ Ë°®Á§∫ $x_i, x_j$ ÂøÖ‰∏çÂ±û‰∫éÂêåÁ∞á„ÄÇËØ•ÁÆóÊ≥ïÊòØ $k$-means ÁÆóÊ≥ïÁöÑÊâ©Â±ïÔºåÂÆÉÂú®ËÅöÁ±ªËøáÁ®ã‰∏≠Ë¶ÅÁ°Æ‰øùÊ†∑Êú¨ÁöÑÁ∫¶ÊùüÂæóÂà∞Êª°Ë∂≥ÔºåÂê¶ÂàôËøîÂõûÈîôËØØÊèêÁ§∫ÔºåÁÆóÊ≥ïÂ¶Ç‰∏ãÔºö&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>Ôºö Ê†∑Êú¨ÈõÜ $D = {x_1, x_2, \ldots, x_m}$;&lt;/p>
&lt;p>‚Äã ÂøÖËøûÁ∫¶ÊùüÈõÜÂêà $\cal{M}$ ;&lt;/p>
&lt;p>‚Äã ÂãøËøûÁ∫¶ÊùüÈõÜÂêà $\cal{C}$ ;&lt;/p>
&lt;p>‚Äã ËÅöÁ±ªÁ∞áÊï∞ $k$.&lt;/p>
&lt;p>ËøáÁ®ãÔºö&lt;/p>
&lt;p>01: ‰ªé $D$ ‰∏≠ÈöèÊú∫ÈÄâÂèñ $k$ ‰∏™Ê†∑Êú¨‰Ωú‰∏∫ÂàùÂßãÂùáÂÄºÂêëÈáè ${\mu_1, \mu_2, \ldots, \mu_k}$;&lt;/p>
&lt;p>02: &lt;strong>repeat&lt;/strong>&lt;/p>
&lt;p>03: $C_j = \phi (1 \le j \le k)$;&lt;/p>
&lt;p>04: &lt;strong>for&lt;/strong> $i = 1, 2, \ldots, m$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>05: ËÆ°ÁÆóÊ†∑Êú¨ $x_i$ ‰∏éÂêÑ‰∏™ÂùáÂÄºÂêëÈáè $\mu_j (1 \le j \le k)$ ÁöÑË∑ùÁ¶ªÔºö $d_{ij} = ||x_i - \mu_j||_2$;&lt;/p>
&lt;p>06: $\cal{K} = {1, 2, \ldots, k}$;&lt;/p>
&lt;p>07: $\text{is_merged} = false$;&lt;/p>
&lt;p>08: &lt;strong>while&lt;/strong> $\neg\ \text{is_merged}$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>09: Âü∫‰∫é $\cal{K}$ ÊâæÂá∫‰∏éÊ†∑Êú¨ $x_i$ Ë∑ùÁ¶ªÊúÄËøëÁöÑÁ∞áÔºö $r = argmin_{j \in \cal{K}}\ d_{ij}$;&lt;/p>
&lt;p>10: Ê£ÄÊµãÂ∞Ü $x_i$ ÂàíÂÖ•ËÅöÁ±ªÁ∞á $C_r$ ÊòØÂê¶‰ºöËøùËÉå $\cal{K}$ ‰∏é $\cal{C}$ ‰∏≠ÁöÑÁ∫¶ÊùüÔºõ&lt;/p>
&lt;p>11Ôºö &lt;strong>if&lt;/strong> $\neg\ \text{is_voilated}$ &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>12: $C_r = C_r \cup {x_i}$;&lt;/p>
&lt;p>13: $\text{is_merged} = true$&lt;/p>
&lt;p>14: &lt;strong>else&lt;/strong>&lt;/p>
&lt;p>15: $\cal{K} = K \setminus {r}$;&lt;/p>
&lt;p>16: &lt;strong>if&lt;/strong> $\cal{K} = \phi$ &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>17: &lt;strong>break&lt;/strong> Âπ∂ËøîÂõûÈîôËØØÊèêÁ§∫&lt;/p>
&lt;p>18: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>19: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>20: &lt;strong>end while&lt;/strong>&lt;/p>
&lt;p>21: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>22: &lt;strong>for&lt;/strong> $j = 1, 2, \ldots, k$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>23: $\mu_j = {1 \over |C_j|} \sum_{x \in C_j} x$;&lt;/p>
&lt;p>24: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>25: &lt;strong>until&lt;/strong> ÂùáÂÄºÂêëÈáèÂùá‰∏∫Êõ¥Êñ∞&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>ÔºöÁ∞áÂàíÂàÜ ${C_1, C_2, \ldots, C_k}$&lt;/p>
&lt;hr>
&lt;hr>
&lt;h3 id="Á∫¶ÊùüÁßçÂ≠ê-k-ÂùáÂÄºÁÆóÊ≥ï-pseudo-code" >
&lt;div>
&lt;a href="#%e7%ba%a6%e6%9d%9f%e7%a7%8d%e5%ad%90-k-%e5%9d%87%e5%80%bc%e7%ae%97%e6%b3%95-pseudo-code">
##
&lt;/a>
Á∫¶ÊùüÁßçÂ≠ê $k$ ÂùáÂÄºÁÆóÊ≥ï (pseudo-code)
&lt;/div>
&lt;/h3>
&lt;p>Á∫¶ÊùüÁßçÂ≠ê $k$ ÂùáÂÄºÁÆóÊ≥ï (Constrained Seed k-means) Âà©Áî®Á¨¨‰∫åÁßçÁõëÁù£‰ø°ÊÅØÔºåÂç≥Â∞ëÈáèÊúâÊ†áËÆ∞Ê†∑Êú¨ÔºàÊ≠§Â§ÑÊ†∑Êú¨Ê†áËÆ∞ÊåáÁöÑÊòØÁ∞áÊ†áËÆ∞‚Äòcluster label‚ÄôÔºåËÄå‰∏çÊòØÁ±ªÂà´Ê†áËÆ∞‚Äòclass label‚ÄôÔºâ„ÄÇÁªôÂÆöÊ†∑Êú¨ÈõÜ $D = {x_1, x_2, \ldots, x_m}$ÔºåÂÅáÂÆöÂ∞ëÈáèÁöÑÊúâÊ†áËÆ∞Ê†∑Êú¨‰∏∫ $S = \cup^k_{j=1} S_j \subset D$ÔºåÂÖ∂‰∏≠Ôºå$S_j \ne \phi$ ‰∏∫Èö∂Â±û‰∫éÁ¨¨ $j$ ‰∏™ËÅöÁ±ªÁ∞áÁöÑÊ†∑Êú¨„ÄÇËøôÊ†∑ÁöÑÁõëÁù£‰ø°ÊÅØÂà©Áî®Ëµ∑Êù•ÂæàÂÆπÊòìÔºöÁõ¥Êé•Â∞Ü‰ªñ‰ª¨‰Ωú‰∏∫‚ÄòÁßçÂ≠ê‚ÄôÔºåÁî®‰ªñ‰ª¨ÂàùÂßãÂåñ $k$ ÂùáÂÄºÁÆóÊ≥ïÁöÑ $k$ ‰∏™ËÅöÁ±ª‰∏≠ÂøÉÔºåÂπ∂‰∏îÂú®ËÅöÁ±ªÁ∞áËø≠‰ª£Êõ¥Êñ∞ËøáÁ®ã‰∏≠‰∏çÊîπÂèòÁßçÂ≠êÊ†∑Êú¨ÁöÑÁ∞áÈö∂Â±ûÂÖ≥Á≥ª„ÄÇÂÖ∂ÁÆóÊ≥ïÊèèËø∞Â¶Ç‰∏ãÔºö&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>: Ê†∑Êú¨ÈõÜ $D = {x_1, x_2, \ldots, x_m}$;&lt;/p>
&lt;p>‚Äã Â∞ëÈáèÊúâÊ†áËÆ∞Ê†∑Êú¨ $S = \cup^k_{j=1} S_j$;&lt;/p>
&lt;p>‚Äã ËÅöÁ±ªÁ∞áÊï∞ $k$.&lt;/p>
&lt;p>ËøáÁ®ã:&lt;/p>
&lt;p>01: &lt;strong>for&lt;/strong> $j = 1, 2, \ldots, k$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>02: $\mu_j = {1 \over |S_j|} \sum_{x \in S} x$&lt;/p>
&lt;p>03: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>04: &lt;strong>repeat&lt;/strong>&lt;/p>
&lt;p>05: $C_j = \phi (1 \le j \le k)$;&lt;/p>
&lt;p>06: &lt;strong>for&lt;/strong> $j = 1, 2, \ldots, k$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>07: &lt;strong>for all&lt;/strong> $x \in S_j$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>08: $C_j = C_j \cup {x}$&lt;/p>
&lt;p>09: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>10: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>11: &lt;strong>for all&lt;/strong> $x_i \in D \setminus S$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>12: ËÆ°ÁÆóÊ†∑Êú¨ $x_i$ ‰∏éÂêÑ‰∏™ÂùáÂÄºÂêëÈáè $\mu_j (1 \le j \le k)$ ÁöÑË∑ùÁ¶ªÔºö $d_{ij} = ||x_i -\mu_j||_2$;&lt;/p>
&lt;p>13: ÊâæÂá∫‰∏éÊ†∑Êú¨ $x_i$ Ë∑ùÁ¶ªÊúÄËøëÁöÑÁ∞áÔºö $r = argmin_{j \in {1, 2, \ldots, k}} d_{ij}$;&lt;/p>
&lt;p>14: Â∞ÜÊ†∑Êú¨ $x_i$ ÂàíÂÖ•Áõ∏Â∫îÁöÑÁ∞áÔºö $C_r = C_r \cup {x_i}$&lt;/p>
&lt;p>15: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>16: &lt;strong>for&lt;/strong> $j = 1, 2, \ldots, k$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>17: $\mu_j = {1 \over |C_j|} \sum_{x \in C_j} x$;&lt;/p>
&lt;p>18: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>19: &lt;strong>until&lt;/strong> ÂùáÂÄºÂêëÈáèÂùáÊú™Êõ¥Êñ∞&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>: Á∞áÂàíÂàÜ ${C_1, C_2, \ldots, C_k}$&lt;/p>
&lt;hr></description></item><item><title>Book Notes: clustering methods</title><link>/posts/ml101/clusteringmethods/prototype-based_clustering/</link><pubDate>Sat, 13 Jun 2020 16:11:47 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/clusteringmethods/prototype-based_clustering/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>Ê≥®ÊòéÔºö
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ÂéüÁêÜÈÉ®ÂàÜÁöÑÂÜÖÂÆπÂùáÊù•Ëá™Âë®ÂøóÂçéÁöÑË•øÁìú‰π¶ÔºåÁúüÊ≠£ÁöÑÂ§ßÂ∏à‰πã‰Ωú„ÄÇ
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ÂÖ∂‰ªñÂÜÖÂÆπÊù•Ëá™ÂºÄÊ∫êÂåÖÊñáÊ°£„ÄÅÂºÄÊ∫êÁîµÂ≠ê‰π¶„ÄÅipynbÊñáÊ°£Á≠â„ÄÇ
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ÂéüÂûãËÅöÁ±ª" >
&lt;div>
&lt;a href="#%e5%8e%9f%e5%9e%8b%e8%81%9a%e7%b1%bb">
#
&lt;/a>
ÂéüÂûãËÅöÁ±ª
&lt;/div>
&lt;/h2>
&lt;p>ÂéüÂûã&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>ËÅöÁ±ª‰πüÁß∞‰∏∫ ‚ÄúÂü∫‰∫éÂéüÂûãÁöÑËÅöÁ±ª(prototype-based clustering)‚ÄùÔºåÊ≠§Á±ªÁÆóÊ≥ïÂÅáËÆæËÅöÁ±ªÁªìÊûÑËÉΩÂ§üÈÄöËøá‰∏ÄÁªÑÂéüÂûãÂàªÁîªÔºåÂú®Áé∞ÂÆûËÅöÁ±ª‰ªªÂä°‰∏≠ÊûÅ‰∏∫Â∏∏Áî®„ÄÇÈÄöÂ∏∏ÊÉÖÂΩ¢‰∏ãÔºåÁÆóÊ≥ïÂÖàÂØπÂéüÂûãËøõË°åÂàùÂßãÂåñÔºåÁÑ∂ÂêéÂØπÂéüÂûãËøõË°åËø≠‰ª£Êõ¥Êñ∞Ê±ÇËß£„ÄÇÈááÁî®‰∏çÂêåÁöÑÂéüÂûãË°®Á§∫„ÄÅ‰∏çÂêåÁöÑÊ±ÇËß£ÊñπÂºèÔºåÂ∞Ü‰∫ßÁîü‰∏çÂêåÁöÑÁÆóÊ≥ï„ÄÇ&lt;/p>
&lt;h3 id="k-ÂùáÂÄºËÅöÁ±ªÁÆóÊ≥ï" >
&lt;div>
&lt;a href="#k-%e5%9d%87%e5%80%bc%e8%81%9a%e7%b1%bb%e7%ae%97%e6%b3%95">
##
&lt;/a>
$k$ ÂùáÂÄºËÅöÁ±ªÁÆóÊ≥ï
&lt;/div>
&lt;/h3>
&lt;p>ÁªôÂÆöÊ†∑Êú¨ÈõÜ $D = {x_1, \ldots, x_m }$ Ôºå‚Äú$k$ ÂùáÂÄº($k$-means)‚Äù ÁÆóÊ≥ïÈíàÂØπËÅöÁ±ªÊâÄÂæóÁ∞áÂàíÂàÜ $\mathcal{C} = {C_1, \ldots, C_k }$ ÊúÄÂ∞èÂåñÂπ≥ÊñπËØØÂ∑Æ
$$
\tag{9.24}
E = \sum^k_{i=1} \sum_{x \in C_i} ||x - \mu_i||^2_2 ,
$$
ÂÖ∂‰∏≠Ôºå$\mu_i = {1 \over |C_i|} \sum_{x \in C_i} x$ ÊòØÁ∞á $C_i$ ÁöÑÂùáÂÄºÂêëÈáè„ÄÇÁõ¥ËßÇÊù•ÁúãÔºåÂºè(9.24)Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÂàªÁîª‰∫ÜÁ∞áÂÜÖÊ†∑Êú¨Âõ¥ÁªïÁ∞áÂùáÂÄºÂêëÈáèÁöÑÁ¥ßÂØÜÁ®ãÂ∫¶Ôºå$E$ ÂÄºË∂äÂ∞èÂàôÁ∞áÂÜÖÊ†∑Êú¨Áõ∏‰ººÂ∫¶Ë∂äÈ´ò„ÄÇ&lt;/p>
&lt;p>ÊúÄÂ∞èÂåñÂºè(9.24)Âπ∂‰∏çÂÆπÊòìÔºåÊâæÂà∞ÂÆÉÁöÑÊúÄ‰ºòËß£ÈúÄËÄÉÂØüÊ†∑Êú¨ÈõÜ $D$ ÁöÑÊâÄÊúâÂèØËÉΩÁ∞áÂàíÂàÜÔºåËøôÊòØ‰∏Ä‰∏™ NP ÈöæÈóÆÈ¢ò&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> „ÄÇÂõ†Ê≠§Ôºå$k$ ÂùáÂÄºÁÆóÊ≥ïÈááÁî®‰∫ÜË¥™ÂøÉÁ≠ñÁï•ÔºåÈÄöËøáËø≠‰ª£‰ºòÂåñÊù•Ëøë‰ººÊ±ÇËß£Âºè(9.24)„ÄÇÁÆóÊ≥ïÊµÅÁ®ãÂ¶Ç‰∏ãÔºåÂÖ∂‰∏≠Á¨¨1Ë°åÂØπÂùáÂÄºÂêëÈáèËøõË°åÂàùÂßãÂåñÔºåÂú®Á¨¨4-8Ë°å‰∏éÁ¨¨9-16Ë°å‰æùÊ¨°ÂØπÂΩìÂâçÁ∞áÂàíÂàÜÂèäÂùáÂÄºÂêëÈáèËø≠‰ª£Êõ¥Êñ∞ÔºåËã•Ëø≠‰ª£Êõ¥Êñ∞ÂêéËÅöÁ±ªÁªìÊûú‰øùÊåÅ‰∏çÂèòÔºåÂàôÂú®Á¨¨18Ë°åÂ∞ÜÂΩìÂâçÁöÑÁ∞áÂàíÂàÜÁªìÊûúËøîÂõû„ÄÇ&lt;/p>
&lt;hr>
&lt;p>&lt;code>k ÂùáÂÄºÁÆóÊ≥ïÊµÅÁ®ã&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>ÔºöÊ†∑Êú¨ÈõÜ $D = {x_1, \ldots, x_m }$ Ôºõ&lt;/p>
&lt;p>‚Äã ËÅöÁ±ªÁ∞áÊï∞ $k$&lt;/p>
&lt;p>&lt;strong>ËøáÁ®ã&lt;/strong>Ôºö&lt;/p>
&lt;p>1: ‰ªé $D$ ‰∏≠ÈöèÊú∫ÈÄâÊã© $k$ ‰∏™Ê†∑Êú¨‰Ωú‰∏∫ÂàùÂßãÂùáÂÄºÂêëÈáè ${\mu_1, \ldots, \mu_k }$&lt;/p>
&lt;p>2: &lt;strong>repeat&lt;/strong>&lt;/p>
&lt;p>3: ‰ª§ $C_i = \phi \quad (1 \le i \le k)$&lt;/p>
&lt;p>4: &lt;strong>for&lt;/strong> $j = 1, 2, \ldots, m$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>5: ËÆ°ÁÆóÊ†∑Êú¨ $x_j$ ‰∏éÂêÑ‰∏™ÂùáÂÄºÂêëÈáè $\mu_i \ (1 \le i \le k)$ ÁöÑË∑ùÁ¶ªÔºö$d_{ji} = ||x_j - \mu_i||_2$ Ôºõ&lt;/p>
&lt;p>6: Ê†πÊçÆË∑ùÁ¶ªÊúÄËøëÁöÑÂùáÂÄºÂêëÈáèÁ°ÆÂÆö $x_j$ ÁöÑÁ∞áÊ†áËÆ∞&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>Ôºö$\lambda_j = \text{argmin}&lt;em>{i \in {1, 2, \ldots, k}} d&lt;/em>{ji}$ Ôºõ&lt;/p>
&lt;p>7: Â∞ÜÊ†∑Êú¨ $x_j$ ÂàíÂÖ•Áõ∏Â∫îÁöÑÁ∞áÔºö$C_{\lambda_j} = C_{\lambda_j} \cup {x_j}$ Ôºõ&lt;/p>
&lt;p>8: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>9: &lt;strong>for&lt;/strong> $i = 1, 2, \ldots, k$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>10: ËÆ°ÁÆóÊñ∞ÂùáÂÄºÂêëÈáèÔºö$\mu_i&amp;rsquo; = {1 \over |C_i|} \sum_{x \in C_i} x$ Ôºõ&lt;/p>
&lt;p>11: &lt;strong>if&lt;/strong> $\mu_i&amp;rsquo; \ne \mu_i$ &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>12: Â∞ÜÂΩìÂâçÂùáÂÄºÂêëÈáè $\mu_i$ Êõ¥Êñ∞‰∏∫ $\mu_i'$&lt;/p>
&lt;p>13: &lt;strong>else&lt;/strong>&lt;/p>
&lt;p>14: ‰øùÊåÅÂΩìÂâçÂùáÂÄºÂêëÈáè‰∏çÂèò&lt;/p>
&lt;p>15: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>16: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>17: &lt;strong>until&lt;/strong> ÂΩìÂâçÂùáÂÄºÂêëÈáèÂùáÊú™Êõ¥Êñ∞&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>ÔºöÁ∞áÂàíÂàÜ $\mathcal{C} = {C_1, C_2, \ldots, C_k }$&lt;/p>
&lt;hr>
&lt;h3 id="k-means-in-action" >
&lt;div>
&lt;a href="#k-means-in-action">
##
&lt;/a>
K-Means in Action
&lt;/div>
&lt;/h3>
&lt;h4 id="finding-the-optimal-number-of-clusters" >
&lt;div>
&lt;a href="#finding-the-optimal-number-of-clusters">
###
&lt;/a>
Finding the optimal number of clusters
&lt;/div>
&lt;/h4>
&lt;p>In general, it will not be easy to know how to set $k$, and the result might be quite bad if you set it to the wrong value (see Figure 9-7).&lt;/p>
&lt;p>&lt;img alt="cluster No." src="./images/handson_cluster_numbers.png">&lt;/p>
&lt;p>You might be thinking that we could just pick the model with lowest inertia. Unfortunately, the inertia is not a good performance metric when trying to choose $k$ because it keeps getting lower as we increase $k$. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be (see Figure 9-8: a plot of the inertia as a function of $k$).&lt;/p>
&lt;p>&lt;img alt="cluster inertia" src="./images/handson_cluster_inertia.png">&lt;/p>
&lt;p>This technique for choosing the best value for the number of clusters is rather coarse. A more precise approach (but more computationally expensive) is to use the &lt;strong>silhouette score&lt;/strong>, which is the mean silhouette coefficient (ËΩÆÂªìÁ≥ªÊï∞) over all the instances. An instance&amp;rsquo;s silhouette coefficient is equal to $(b - a) / \max(a, b)$, where $a$ is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and $b$ is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next cluster, defined as the one that minimizes $b$, excluding the instance&amp;rsquo;s own cluster).&lt;/p>
&lt;p>The silhouette coefficient can vary between -1 and +1.&lt;/p>
&lt;ul>
&lt;li>silhouette coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters&lt;/li>
&lt;li>silhouette coefficient close to 0 means that it is close to a cluster boundary&lt;/li>
&lt;li>silhouette coefficient close to -1 means that the instance may have been assigned to the wrong cluster&lt;/li>
&lt;/ul>
&lt;p>To compute the silhouette score, you can use sklearn&amp;rsquo;s &lt;code>silhouette_score()&lt;/code> function:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.metrics &lt;span style="color:#ff6ac1">import&lt;/span> silhouette_score
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># give it all the instances in the dataset&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># and the labels they were assigned&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>silhouette_score(X, kmeans_model&lt;span style="color:#ff6ac1">.&lt;/span>labels_)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can also compare the silhouette scores for different numbers of clusters (see Figure 9-9):&lt;/p>
&lt;p>&lt;img alt="silhouette scores" src="./images/handson_cluster_silhouette_scores.png">&lt;/p>
&lt;p>As the Figure 9-9 shows, this visualization is much richer than the previous one: although it confirms that $k = 4$ is a very good choice, it also underlines the fact that $k = 5$ is quite good as well, and much better than $k &amp;gt; 5$. This is not visible when comparing the inertias.&lt;/p>
&lt;p>&lt;strong>silhouette diagram&lt;/strong>&lt;/p>
&lt;p>An even more informative visualization is obtained when you plot every instance&amp;rsquo;s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a &lt;em>silhouette diagram&lt;/em> (see Figure 9-10). Each diagram contains one knife shape per cluster. The shape&amp;rsquo;s height indicates the number of instances the cluster contains, and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better). The dashed line indicated the mean silhouette coefficient.&lt;/p>
&lt;p>&lt;img alt="silhouette coefficient sorted" src="./images/handson_cluster_silhouette_coefficients_sorted.png">&lt;/p>
&lt;p>The dashed line represents the mean silhouette score for each number of clusters. When most of the instances in a cluster have a lower coefficient than this score, then the cluster is rather bad since this means its instances are much too close to other clusters (such as when $k = 3, k = 6$). But when $k =4$ or $k = 5$ , the clusters look pretty good: most instances extend beyond the dashed line. When $k = 4$ , the cluster at index 1 is rather big. When $k = 5$ , all clusters have similar sizes. So, even though the overall silhouette score from $k=4$ is slightly greater than for $k=5$ , it seems like a good idea to use $k=5$ to get clusters of similar sizes.&lt;/p>
&lt;h4 id="limits-of-k-means" >
&lt;div>
&lt;a href="#limits-of-k-means">
###
&lt;/a>
Limits of K-Means
&lt;/div>
&lt;/h4>
&lt;p>Despite its many merits, most notably being fast and scalable, K-Means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very well when the clusters have varying sizes, different densities, or non-spherical shapes (see Figure 9-11).&lt;/p>
&lt;p>&lt;img alt="cluster shapes" src="./images/handson_cluster_shapes.png">&lt;/p>
&lt;p>As Figure 9-11 shows, neither of these solutions is any good (the solution on the right is just terrible even though its inertia is lower). So, depending on the data, different clustering algorithms may perform better. On these types of elliptical clusters, &lt;em>Gaussian mixture models&lt;/em> work great.&lt;/p>
&lt;h3 id="Â≠¶‰π†ÂêëÈáèÈáèÂåñ" >
&lt;div>
&lt;a href="#%e5%ad%a6%e4%b9%a0%e5%90%91%e9%87%8f%e9%87%8f%e5%8c%96">
##
&lt;/a>
Â≠¶‰π†ÂêëÈáèÈáèÂåñ
&lt;/div>
&lt;/h3>
&lt;p>‰∏é $k$ ÂùáÂÄºÁÆóÊ≥ïÁ±ª‰ººÔºå‚ÄúÂ≠¶‰π†ÂêëÈáèÈáèÂåñ(Learning Vector Quantization, LVQ)‚Äù ‰πüÊòØËØïÂõæÊâæÂà∞‰∏ÄÁªÑÂéüÂûãÂêëÈáèÊù•ÂàªÁîªËÅöÁ±ªÁªìÊûÑÔºå‰ΩÜ‰∏é‰∏ÄËà¨ËÅöÁ±ªÁÆóÊ≥ï‰∏çÂêåÁöÑÊòØÔºåLVQ ÂÅáËÆæÊï∞ÊçÆÊ†∑Êú¨Â∏¶ÊúâÁ±ªÂà´Ê†áËÆ∞&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> ÔºåÂ≠¶‰π†ËøáÁ®ãÂà©Áî®Ê†∑Êú¨ÁöÑËøô‰∫õÁõëÁù£‰ø°ÊÅØÊù•ËæÖÂä©ËÅöÁ±ª„ÄÇ&lt;/p>
&lt;p>ÁªôÂÆöÊ†∑Êú¨ÈõÜ $D = {(x_1, y_1), \ldots, (x_m, y_m) }$ ÔºåÊØè‰∏™Ê†∑Êú¨ $x_j$ ÊòØÁî± $n$ ‰∏™Â±ûÊÄßÊèèËø∞ÁöÑÁâπÂæÅÂêëÈáè $(x_{j1}; x_{j2}; \ldots;x_{jn})$ Ôºå $y_j \in \mathcal{Y}$ ÊòØÊ†∑Êú¨ $x_j$ ÁöÑÁ±ªÂà´Ê†áËÆ∞„ÄÇLVQ ÁöÑÁõÆÊ†áÊòØÂ≠¶Âæó‰∏ÄÁªÑ $n$ Áª¥ÂéüÂûãÂêëÈáè ${p_1, p_2, \ldots, p_q }$ ÔºåÊØè‰∏™ÂéüÂûãÂêëÈáè‰ª£Ë°®‰∏Ä‰∏™ËÅöÁ±ªÁ∞áÔºåÁ∞áÊ†áËÆ∞ $t_i \in \mathcal{Y}$ „ÄÇ&lt;/p>
&lt;p>&lt;code>LVQ ÁÆóÊ≥ï&lt;/code> ÊèèËø∞Â¶Ç‰∏ãÔºöÁÆóÊ≥ïÁ¨¨1Ë°åÂÖàÂØπÂéüÂûãÂêëÈáèËøõË°åÂàùÂßãÂåñÔºå‰æãÂ¶ÇÂØπÁ¨¨q‰∏™Á∞áÂèØ‰ª•‰ªéÁ±ªÂà´Ê†áËÆ∞‰∏∫ $t_q$ ÁöÑÊ†∑Êú¨‰∏≠ÈöèÊú∫ÈÄâÂèñ‰∏Ä‰∏™‰Ωú‰∏∫ÂéüÂûãÂêëÈáè„ÄÇÁÆóÊ≥ïÁ¨¨2-12Ë°åÂØπÂéüÂûãÂêëÈáèËøõË°åËø≠‰ª£‰ºòÂåñ„ÄÇÂú®ÊØè‰∏ÄËΩÆËø≠‰ª£‰∏≠ÔºåÁÆóÊ≥ïÈöèÊú∫ÈÄâÂèñ‰∏Ä‰∏™ÊúâÊ†áËÆ∞ÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÔºåÊâæÂá∫‰∏éÂÖ∂Ë∑ùÁ¶ªÊúÄËøëÁöÑÂéüÂûãÂêëÈáèÔºåÂπ∂Ê†πÊçÆ‰∏§ËÄÖÁöÑÁ±ªÂà´Ê†áËÆ∞ÊòØÂê¶‰∏ÄËá¥Êù•ÂØπÂéüÂûãÂêëÈáèËøõË°åÁõ∏Â∫îÁöÑÊõ¥Êñ∞„ÄÇÂú®Á¨¨12Ë°å‰∏≠ÔºåËã•ÁÆóÊ≥ïÁöÑÂÅúÊ≠¢Êù°‰ª∂Â∑≤Êª°Ë∂≥Ôºà‰æãÂ¶ÇËææÂà∞ÊúÄÂ§ßËø≠‰ª£ËΩÆÊï∞ÔºåÊàñÂéüÂûãÂêëÈáèË∂ã‰∫éÁ®≥ÂÆöÔºâÔºåÂàôÂ∞ÜÂΩìÂâçÂéüÂûãÂêëÈáè‰Ωú‰∏∫ÊúÄÁªàÁªìÊûúËøîÂõû„ÄÇ&lt;/p>
&lt;hr>
&lt;p>&lt;code>LVQ ÁÆóÊ≥ï&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>ÔºöÊ†∑Êú¨ÈõÜ $D = {(x_1, y_1), \ldots, (x_m, y_m) }$ Ôºõ&lt;/p>
&lt;p>‚Äã ÂéüÂûãÂêëÈáè‰∏™Êï∞ $q$ ÔºåÂêÑÂéüÂûãÂêëÈáèÈ¢ÑËÆæÁ±ªÂà´Ê†áËÆ∞ ${ t_1, t_2, \ldots, t_q}$ Ôºõ&lt;/p>
&lt;p>‚Äã Â≠¶‰π†Áéá $\eta \in (0, 1)$ .&lt;/p>
&lt;p>&lt;strong>ËøáÁ®ã&lt;/strong>Ôºö&lt;/p>
&lt;p>01: ÂàùÂßãÂåñ‰∏ÄÁªÑÂéüÂûãÂêëÈáè ${p_1, p_2, \ldots, p_q }$&lt;/p>
&lt;p>02: &lt;strong>repeat&lt;/strong>&lt;/p>
&lt;p>03: ‰ªéÊ†∑Êú¨ÈõÜ $D$ ÈöèÊú∫ÈÄâÂèñÊ†∑Êú¨ $(x_j, y_j)$ Ôºõ&lt;/p>
&lt;p>04: ËÆ°ÁÆóÊ†∑Êú¨ $x_j$ ‰∏é $p_i (1 \le i \le q)$ ÁöÑË∑ùÁ¶ªÔºö$d_{ji} = ||x_j - p_i||_2$ Ôºõ&lt;/p>
&lt;p>05: ÊâæÂá∫‰∏é $x_j$ Ë∑ùÁ¶ªÊúÄËøëÁöÑÂéüÂûãÂêëÈáè $p_{i*}$ Ôºå$i* = \text{argmin}&lt;em>{i \in {1, 2, \ldots, q}} d&lt;/em>{ji}$ Ôºõ&lt;/p>
&lt;p>06: &lt;strong>if&lt;/strong> $y_j = t_{i*}$ &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>07: $p&amp;rsquo; = p_{i*} + \eta \cdot (x_j - p_{i*})$&lt;/p>
&lt;p>08: &lt;strong>else&lt;/strong>&lt;/p>
&lt;p>09: $p&amp;rsquo; = p_{i*} - \eta \cdot (x_j - p_{i*})$&lt;/p>
&lt;p>10: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>11: Â∞ÜÂéüÂûãÂêëÈáè $p_{i*}$ Êõ¥Êñ∞‰∏∫ $p'$&lt;/p>
&lt;p>12: &lt;strong>until&lt;/strong> Êª°Ë∂≥ÂÅúÊ≠¢Êù°‰ª∂&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>ÔºöÂéüÂûãÂêëÈáè ${p_1, p_2, \ldots, p_q }$&lt;/p>
&lt;hr>
&lt;p>ÊòæÁÑ∂ÔºåLVQ ÁöÑÂÖ≥ÈîÆÊòØÁ¨¨6-10Ë°åÔºåÂç≥Â¶Ç‰ΩïÊõ¥Êñ∞ÂéüÂûãÂêëÈáè„ÄÇÁõ¥ËßÇ‰∏äÁúãÔºåÂØπÊ†∑Êú¨ $x_j$ ÔºåËã•ÊúÄËøëÁöÑÂéüÂûãÂêëÈáè $p_{i*}$ ‰∏é $x_j$ ÁöÑÁ±ªÂà´Ê†áËÆ∞Áõ∏ÂêåÔºåÂàô‰ª§ $p_{i*}$ ‰∏é $x_j$ ÁöÑÊñπÂêëÈù†Êã¢ÔºåÂ¶Ç&lt;code>LVQ ÁÆóÊ≥ï&lt;/code> Á¨¨7Ë°åÊâÄÁ§∫ÔºåÊ≠§Êó∂Êñ∞ÂéüÂûãÂêëÈáè‰∏∫
$$
\tag{9.25}
p&amp;rsquo; = p_{i*} + \eta \cdot (x_j - p_{i*}) ,
$$
$p&amp;rsquo;$ ‰∏é $x_j$ ‰πãÈó¥ÁöÑË∑ùÁ¶ª‰∏∫
$$
\begin{eqnarray}
||p&amp;rsquo; - x_j||&lt;em>2
&amp;amp;=&amp;amp; ||p&lt;/em>{i*} + \eta \cdot (x_j - p_{i*}) - x_j||&lt;em>2 \
\
\tag{9.26}
&amp;amp;=&amp;amp; (1 - \eta) \cdot ||p&lt;/em>{i*} - x_j||&lt;em>2 .
\end{eqnarray}
$$
‰ª§Â≠¶‰π†Áéá $\eta \in (0, 1)$ ÔºåÂàôÂéüÂûãÂêëÈáè $p&lt;/em>{i*}$ Âú®Êõ¥Êñ∞‰∏∫ $p&amp;rsquo;$ ‰πãÂêéÂ∞ÜÊõ¥Êé•Ëøë $x_j$ „ÄÇ&lt;/p>
&lt;p>Á±ª‰ººÁöÑÔºåËã• $p_{i*}$ ‰∏é $x_j$ ÁöÑÁ±ªÂà´Ê†áËÆ∞‰∏çÂêåÔºåÂàôÊõ¥Êñ∞ÂêéÁöÑÂéüÂûãÂêëÈáè‰∏é $x_j$ ‰πãÈó¥ÁöÑË∑ùÁ¶ªÂ∞ÜÂ¢ûÂ§ß $(1 + \eta) \cdot ||p_{i*} - x_j||_2$ Ôºå‰ªéËÄåÊõ¥ËøúÁ¶ª $x_j$ „ÄÇ&lt;/p>
&lt;p>Âú®Â≠¶Âæó‰∏ÄÁªÑÂéüÂûãÂêëÈáè ${p_1, p_2, \ldots, p_q }$ ÂêéÔºåÂç≥ÂèØÂÆûÁé∞ÂØπÊ†∑Êú¨Á©∫Èó¥ $\cal{X}$ ÁöÑÁ∞áÂàíÂàÜ„ÄÇÂØπ‰ªªÊÑèÊ†∑Êú¨ $x$ ÔºåÂÆÉÂ∞ÜË¢´ÂàíÂÖ•‰∏éÂÖ∂Ë∑ùÁ¶ªÊúÄËøëÁöÑÂéüÂûãÂêëÈáèÊâÄ‰ª£Ë°®ÁöÑÁ∞á‰∏≠ÔºõÊç¢Ë®Ä‰πãÔºåÊØè‰∏™ÂéüÂûãÂêëÈáè $p_i$ ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™‰∏é‰πãÁõ∏ÂÖ≥ÁöÑ‰∏Ä‰∏™Âå∫Âüü $R_i$&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> ÔºåËØ•Âå∫Âüü‰∏≠ÊØè‰∏™Ê†∑Êú¨‰∏é $p_i$ ÁöÑË∑ùÁ¶ª‰∏çÂ§ß‰∫éÂÆÉ‰∏éÂÖ∂‰ªñÂéüÂûãÂêëÈáè $p_{i&amp;rsquo;} (i \ne i&amp;rsquo;)$ ÁöÑË∑ùÁ¶ªÔºåÂç≥
$$
\tag{9.27}
R_i = {x \in \mathcal{X} \quad \text{so that} \quad ||x - p_i||&lt;em>2 \le ||x - p&lt;/em>{i&amp;rsquo;}||_2, i&amp;rsquo; \ne i } .
$$
Áî±Ê≠§ÂΩ¢Êàê‰∫ÜÂØπÊ†∑Êú¨Á©∫Èó¥ $\cal{X}$ ÁöÑÁ∞áÂàíÂàÜ ${R_1, R_2, \ldots, R_q }$ ÔºåËØ•ÂàíÂàÜÈÄöÂ∏∏Áß∞‰∏∫ ‚ÄúVoronoi ÂâñÂàÜ(Voronoi tessellation)‚Äù „ÄÇ&lt;/p>
&lt;h2 id="È´òÊñØÊ∑∑ÂêàËÅöÁ±ªgmm" >
&lt;div>
&lt;a href="#%e9%ab%98%e6%96%af%e6%b7%b7%e5%90%88%e8%81%9a%e7%b1%bbgmm">
#
&lt;/a>
È´òÊñØÊ∑∑ÂêàËÅöÁ±ª(GMM)
&lt;/div>
&lt;/h2>
&lt;p>‰∏é $k$ ÂùáÂÄº„ÄÅLVQ Áî®ÂéüÂûãÂêëÈáèÁ±ªÂàªÁîªËÅöÁ±ªÁªìÊûÑ‰∏çÂêåÔºåÈ´òÊñØÊ∑∑Âêà(Mixture-of-Gaussian) ËÅöÁ±ªÁÆóÊ≥ïÈááÁî®Ê¶ÇÁéáÊ®°ÂûãÊù•Ë°®ËææËÅöÁ±ªÂéüÂûã„ÄÇ&lt;/p>
&lt;blockquote>
&lt;p>ÁÆÄÂçïÂõûÈ°æ | Â§öÂÖÉÈ´òÊñØÂàÜÂ∏É&lt;/p>
&lt;p>Â§öÂÖÉÈ´òÊñØÂàÜÂ∏ÉÁöÑÂÆö‰πâÔºöÂØπ $n$ Áª¥Ê†∑Êú¨Á©∫Èó¥ $\cal{X}$ ‰∏≠ÈöèÊú∫ÂêëÈáè $x$ ÔºåËã• $x$ Êúç‰ªéÈ´òÊñØÂàÜÂ∏ÉÔºåÂÖ∂Ê¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞‰∏∫
$$
\tag{9.28}
p(x) = {1 \over {(2 \pi)^{n \over 2} |\Sigma|^{1 \over 2}}} \exp {\bigg(-{1\over2}(x - \mu)^{\mathsf{T}} \Sigma^{-1} (x - \mu) \bigg)} ,
$$
ÂÖ∂‰∏≠Ôºå$\exp(x) = e^x$ Ôºå$\mu$ ÊòØ $n$ Áª¥ÂùáÂÄºÂêëÈáèÔºå$\Sigma$ ÊòØ $n \times n$ ÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ(Âπ∂‰∏îÊòØ‚ÄúÂØπÁß∞Ê≠£ÂÆöÁü©Èòµ‚ÄùÔºå Ê≠£ÂÆöÁü©ÈòµÊÑèÊÄùÊòØÂÖ∂eigenvaluesÈÉΩÂ§ß‰∫éÈõ∂)Ôºå$|\Sigma|$ ÊòØÂÖ∂Ë°åÂàóÂºèÔºå$\Sigma^{-1}$ ÊòØÂÖ∂ÈÄÜÁü©Èòµ„ÄÇÁî±Âºè(9.28)ÂèØÁúãÂá∫ÔºåÈ´òÊñØÂàÜÂ∏ÉÂÆåÂÖ®Áî±ÂùáÂÄºÂêëÈáè $\mu$ ÂíåÂçèÊñπÂ∑ÆÁü©Èòµ $\Sigma$ Ëøô‰∏§‰∏™ÂèÇÊï∞Á°ÆÂÆö„ÄÇ‰∏∫‰∫ÜÊòéÁ°ÆÊòæÁ§∫È´òÊñØÂàÜÂ∏É‰∏éÁõ∏Â∫îÂèÇÊï∞ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÂ∞ÜÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ËÆ∞‰∏∫ $p(x | \mu, \Sigma)$ „ÄÇ&lt;/p>
&lt;/blockquote>
&lt;p>Êàë‰ª¨ÂèØ‰ª•ÂÆö‰πâÈ´òÊñØÊ∑∑ÂêàÂàÜÂ∏É&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>
$$
\tag{9.29}
p_{\cal{M}} (x) = \sum^k_{i=1} \alpha_i \cdot p(x | \mu_i, \Sigma_i) ,
$$
ËØ•ÂàÜÂ∏ÉÂÖ±Áî± $k$ ‰∏™Ê∑∑ÂêàÊàêÂàÜÁªÑÊàêÔºåÊØè‰∏™Ê∑∑ÂêàÊàêÂàÜÂØπÂ∫î‰∏Ä‰∏™È´òÊñØÂàÜÂ∏É„ÄÇÂÖ∂‰∏≠ $\mu_i$ ‰∏é $\Sigma_i$ ÊòØÁ¨¨ $i$ ‰∏™È´òÊñØÊ∑∑ÂêàÊàêÂàÜÁöÑÂèÇÊï∞ÔºåËÄå $\alpha_i &amp;gt; 0$ ‰∏∫Áõ∏Â∫îÁöÑ ‚ÄúÊ∑∑ÂêàÁ≥ªÊï∞(mixture coefficient)‚Äù Ôºå‰∏îÊúâ $\sum^k_{i=1} \alpha_i = 1$ „ÄÇ&lt;/p>
&lt;p>ÂÅáËÆæÊ†∑Êú¨ÁöÑÁîüÊàêËøáÁ®ãÁî±È´òÊñØÊ∑∑ÂêàÂàÜÂ∏ÉÁªôÂá∫ÔºöÈ¶ñÂÖàÔºåÊ†πÊçÆ $\alpha_1, \alpha_2, \ldots, \alpha_k$ ÂÆö‰πâÁöÑÂÖàÈ™åÂàÜÂ∏ÉÈÄâÊã©È´òÊñØÊ∑∑ÂêàÊàêÂàÜÔºåÂÖ∂‰∏≠ $\alpha_i$ ‰∏∫ÈÄâÊã©Á¨¨ $i$ ‰∏™Ê∑∑ÂêàÊàêÂàÜÁöÑÊ¶ÇÁéáÔºõÁÑ∂ÂêéÔºåÊ†πÊçÆË¢´ÈÄâÊã©ÁöÑÊ∑∑ÂêàÊàêÂàÜÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞ËøõË°åÈááÊ†∑Ôºå‰ªéËÄåÁîüÊàêÁõ∏Â∫îÁöÑÊ†∑Êú¨„ÄÇ&lt;/p>
&lt;p>Ëã•ËÆ≠ÁªÉÈõÜ $D = {x_1, \ldots, x_m }$ Áî±‰∏äËø∞ËøáÁ®ãÁîüÊàêÔºå‰ª§ÈöèÊú∫ÂèòÈáè $z_j \in {1, 2, \ldots, k }$ Ë°®Á§∫ÁîüÊàêÊ†∑Êú¨ $x_j$ ÁöÑÈ´òÊñØÊ∑∑ÂêàÊàêÂàÜÔºåÂÖ∂ÂèñÂÄºÊú™Áü•„ÄÇÊòæÁÑ∂Ôºå$z_j$ ÁöÑÂÖàÈ™åÊ¶ÇÁéá $P(z_j = i)$ ÂØπÂ∫î‰∫é $\alpha_i (i = 1, 2, \ldots, k)$ „ÄÇÊ†πÊçÆË¥ùÂè∂ÊñØÂÆöÁêÜÔºå$z_j$ ÁöÑÂêéÈ™åÊ¶ÇÁéáÂàÜÂ∏ÉÂØπÂ∫î‰∫é
$$
\begin{eqnarray}
p_{\mathcal{M}}(z_j = i | x_j)
&amp;amp;=&amp;amp; \frac{P(z_j = i) \cdot p_{\mathcal{M}}(x_j | z_j = i)}{p_{\mathcal{M}}(x_j)} \
\
\tag{9.30} \label{eq_bayes_posterior}
&amp;amp;=&amp;amp; \frac{\alpha_i \cdot p(x_j | \mu_i, \Sigma_i) }{\sum^k_{l=1} \alpha_l \cdot p(x_j | \mu_l, \Sigma_l)} .
\end{eqnarray}
$$
Êç¢Ë®Ä‰πãÔºå$p_{\mathcal{M}}(z_j = i | x_j)$ ÁªôÂá∫‰∫ÜÊ†∑Êú¨ $x_j$ Áî±Á¨¨ $i$ ‰∏™È´òÊñØÊ∑∑ÂêàÊàêÂàÜÁîüÊàêÁöÑÂêéÈ™åÊ¶ÇÁéá„ÄÇ‰∏∫Êñπ‰æøÂèôËø∞ÔºåÂ∞ÜÂÖ∂ÁÆÄËÆ∞‰∏∫ $\gamma_{ji} (i = 1, 2, \ldots, k)$ „ÄÇ&lt;/p>
&lt;p>ÂΩìÈ´òÊñØÊ∑∑ÂêàÂàÜÂ∏É(Âºè9.29)Â∑≤Áü•Êó∂ÔºåÈ´òÊñØÊ∑∑ÂêàËÅöÁ±ªÂ∞ÜÊääÊ†∑Êú¨ÈõÜ $D$ ÂàíÂàÜ‰∏∫ $k$ ‰∏™Á∞á $\mathcal{C} = {C_1, \ldots, C_k }$ ÔºåÊØè‰∏™Ê†∑Êú¨ $x_j$ ÁöÑÁ∞áÊ†áËÆ∞ $\lambda_j$ Â¶Ç‰∏ãÁ°ÆÂÆöÔºö
$$
\tag{9.31} \label{eq_cluster_idx}
\lambda_j
= \underset{i \in {1, 2, \ldots, k }}{\operatorname{argmax}} \gamma_{ji} .
$$
Âõ†Ê≠§Ôºå‰ªéÂéüÂûãËÅöÁ±ªÁöÑËßíÂ∫¶Êù•ÁúãÔºåÈ´òÊñØÊ∑∑ÂêàËÅöÁ±ªÊòØÈááÁî®Ê¶ÇÁéáÊ®°Âûã(È´òÊñØÂàÜÂ∏É)ÂØπÂéüÂûãËøõË°åÂàªÁîªÔºåÁ∞áÂàíÂàÜÂàôÁî±ÂéüÂûãÂØπÂ∫îÂêéÈ™åÊ¶ÇÁéáÁ°ÆÂÆö„ÄÇ&lt;/p>
&lt;p>ÈÇ£‰πàÔºåÂØπ‰∫éÂºè(9.29)ÔºåÊ®°ÂûãÂèÇÊï∞ ${\alpha_i, \mu_i, \Sigma_i | 1 \le i \le k }$ Â¶Ç‰ΩïÊ±ÇËß£Âë¢ÔºüÊòæÁÑ∂ÔºåÁªôÂÆöÊ†∑Êú¨ÈõÜ $D$ ÔºåÂèØÈááÁî®ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºåÂç≥ÊúÄÂ§ßÂåñ‰ººÁÑ∂(ÂØπÊï∞‰ººÁÑ∂)
$$
\begin{eqnarray}
LL(D)
&amp;amp;=&amp;amp; \ln \bigg(\prod^m_{j=1} p_{\mathcal{M}}(x_j) \bigg) \
\
\tag{9.32}
&amp;amp;=&amp;amp; \sum^m_{j=1} \ln \bigg(\sum^k_{i=1} \alpha_i \cdot p(x_j | \mu_i, \Sigma_i) \bigg) ,
\end{eqnarray}
$$
Â∏∏Áî® EM ÁÆóÊ≥ï&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> ËøõË°åËø≠‰ª£‰ºòÂåñÊ±ÇËß£ÔºåÂæóÂà∞
$$
\tag{9.34}
\mu_i = \frac{\sum^m_{j=1} \gamma_{ji} x_j}{\sum^m_{j=1} \gamma_{ji}} ,
$$
Âç≥ÂêÑ&lt;strong>Ê∑∑ÂêàÊàêÂàÜÁöÑÂùáÂÄº&lt;/strong>ÂèØÈÄöËøáÊ†∑Êú¨Âä†ÊùÉÂπ≥ÂùáÊù•‰º∞ËÆ°ÔºåÊ†∑Êú¨ÊùÉÈáçÊòØÊØè‰∏™Ê†∑Êú¨Â±û‰∫éËØ•ÊàêÂàÜÁöÑÂêéÈ™åÊ¶ÇÁéá„ÄÇÁ±ª‰ººÁöÑÔºåÂØπ&lt;strong>Ê∑∑ÂêàÊàêÂàÜÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ&lt;/strong>Êúâ
$$
\tag{9.35}
\Sigma_i = \frac{\sum^m_{j=1} \gamma_{ji}(x_j - \mu_i)(x_j - \mu_i)^{\mathsf{T}}}{\sum^m_{j=1} \gamma_{ji}} .
$$
ÂØπÂ∫îÊ∑∑ÂêàÁ≥ªÊï∞ $\alpha_i$ ÔºåÈô§‰∫ÜË¶ÅÊúÄÂ§ßÂåñ $LL(D)$ ÔºåËøòÈúÄË¶ÅÊª°Ë∂≥ $\alpha_i \le 0,\sum^k_{i=1} \alpha_i = 1$ „ÄÇËÄÉËôëËß£ $LL(D)$ ÁöÑÊãâÊ†ºÊúóÊó•ÂΩ¢Âºè
$$
\tag{9.36}
LL(D) + \lambda \bigg(\sum^k_{i=1} \alpha_i - 1 \bigg) ,
$$
ÂÖ∂‰∏≠ $\lambda$ ‰∏∫ÊãâÊ†ºÊúóÊó•‰πòÂ≠ê„ÄÇÁî±Âºè(9.36)ÂØπ $\alpha_i$ ÁöÑÂØºÊï∞‰∏∫ 0ÔºåÊúâ
$$
\tag{9.37}
\sum^m_{j=1} \frac{p(x_j | \mu_i, \Sigma_i)}{\sum^k_{l=1} \alpha_l \cdot p(x_j | \mu_l, \Sigma_l)} + \lambda = 0 ,
$$
‰∏§ËæπÂêå‰πò‰ª• $\alpha_i$ ÔºåÂØπÊâÄÊúâÊ†∑Êú¨Ê±ÇÂíåÂèØÁü• $\lambda = -m$ ÔºåÊúâ
$$
\tag{9.38}
\alpha_i = {1 \over m} \sum^m_{j=1} \gamma_{ji} ,
$$
Âç≥ÊØè‰∏™&lt;strong>È´òÊñØÊàêÂàÜÁöÑÊ∑∑ÂêàÁ≥ªÊï∞&lt;/strong>Áî±Ê†∑Êú¨Â±û‰∫éËØ•ÊàêÂàÜÁöÑÂπ≥ÂùáÂêéÈ™åÊ¶ÇÁéáÁ°ÆÂÆö„ÄÇ&lt;/p>
&lt;p>Áî±‰∏äËø∞Êé®ÂØºÂç≥ÂèØËé∑ÂæóÈ´òÊñØÊ∑∑ÂêàÊ®°ÂûãÁöÑ EM ÁÆóÊ≥ïÔºöÂú®ÊØèÊ≠•Ëø≠‰ª£‰∏≠ÔºåÂÖàÊ†πÊçÆÂΩìÂâçÂèÇÊï∞Êù•ËÆ°ÁÆóÊØè‰∏™Ê†∑Êú¨Â±û‰∫éÊØè‰∏™È´òÊñØÊ∑∑ÂêàÊàêÂàÜÁöÑÂêéÈ™åÊ¶ÇÁéá $\gamma_{ji}$ ÔºàE Ê≠•ÔºâÔºåÂÜçÊ†πÊçÆÂºè(9.34)„ÄÅ(9.35)Âíå(9.38)Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞ ${\alpha_i, \mu_i, \Sigma_i | 1 \le i \le k }$ ÔºàM Ê≠•Ôºâ„ÄÇ&lt;/p>
&lt;hr>
&lt;p>&lt;code>È´òÊñØÊ∑∑ÂêàËÅöÁ±ªÁÆóÊ≥ï&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>ÔºöÊ†∑Êú¨ÈõÜ $D = {x_1, \ldots, x_m }$ Ôºõ&lt;/p>
&lt;p>‚Äã È´òÊñØÊ∑∑ÂêàÊàêÂàÜ‰∏™Êï∞ $k$ „ÄÇ&lt;/p>
&lt;p>&lt;strong>ËøáÁ®ã&lt;/strong>Ôºö&lt;/p>
&lt;p>01: ÂàùÂßãÂåñÈ´òÊñØÊ∑∑ÂêàÂàÜÂ∏ÉÁöÑÊ®°ÂûãÂèÇÊï∞ ${\alpha_i, \mu_i, \Sigma_i | 1 \le i \le k }$&lt;/p>
&lt;p>02: &lt;strong>repeat&lt;/strong>&lt;/p>
&lt;p>03: &lt;strong>for&lt;/strong> $j = 1, 2, \ldots, m$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>04: Ê†πÊçÆÂºè($\ref{eq_bayes_posterior}$)ËÆ°ÁÆóÊ†∑Êú¨ $x_j$ Áî±ÂêÑÊ∑∑ÂêàÊàêÂàÜÁîüÊàêÁöÑÂêéÈ™åÊ¶ÇÁéáÔºåÂç≥ $\gamma_{ji} = p_{\mathcal{M}}(z_j = i | x_j) (1 \le i \le k)$&lt;/p>
&lt;p>05: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>06: &lt;strong>for&lt;/strong> $i = 1, 2, \ldots, k$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>07: ËÆ°ÁÆóÊñ∞ÁöÑÂùáÂÄºÂêëÈáèÔºö$\mu_i&amp;rsquo; = \frac{\sum^m_{j=1} \gamma_{ji} x_j}{\sum^m_{j=1} \gamma_{ji}}$&lt;/p>
&lt;p>08: ËÆ°ÁÆóÊñ∞ÁöÑÂçèÊñπÂ∑ÆÁü©ÈòµÔºö$\Sigma_i&amp;rsquo; = \frac{\sum^m_{j=1} \gamma_{ji}(x_j - \mu_i)(x_j - \mu_i)^{\mathsf{T}}}{\sum^m_{j=1} \gamma_{ji}}$&lt;/p>
&lt;p>09: ËÆ°ÁÆóÊñ∞ÁöÑÊ∑∑ÂêàÁ≥ªÊï∞Ôºö$\alpha_i&amp;rsquo; = {1 \over m} \sum^m_{j=1} \gamma_{ji}$&lt;/p>
&lt;p>10: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>11: Â∞ÜÊ®°ÂûãÂèÇÊï∞ ${\alpha_i, \mu_i, \Sigma_i | 1 \le i \le k }$ Êõ¥Êñ∞‰∏∫ ${\alpha_i&amp;rsquo;, \mu_i&amp;rsquo;, \Sigma_i&amp;rsquo; | 1 \le i \le k }$&lt;/p>
&lt;p>12: &lt;strong>until&lt;/strong> Êª°Ë∂≥ÂÅúÊ≠¢Êù°‰ª∂&lt;/p>
&lt;p>13: $C_i = \phi (1 \le i \le k)$&lt;/p>
&lt;p>14: &lt;strong>for&lt;/strong> $j = 1, 2, \ldots, m$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>15: Ê†πÊçÆÂºè($\ref{eq_cluster_idx}$)Á°ÆÂÆö $x_j$ ÁöÑÁ∞áÊ†áËÆ∞ $\lambda_j$ ;&lt;/p>
&lt;p>16: Â∞Ü $x_j$ ÂàíÂÖ•Áõ∏Â∫îÁöÑÁ∞áÔºö$C_{\lambda_j} = C_{\lambda_j} \cup {x_j }$&lt;/p>
&lt;p>17: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>ÔºöÁ∞áÂàíÂàÜ $\mathcal{C} = {C_1, C_2, \ldots, C_k }$&lt;/p>
&lt;hr>
&lt;h3 id="gaussian-mixture-models-in-action" >
&lt;div>
&lt;a href="#gaussian-mixture-models-in-action">
##
&lt;/a>
Gaussian Mixture Models in Action
&lt;/div>
&lt;/h3>
&lt;p>A &lt;em>Gaussian mixture model (GMM)&lt;/em> is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution from a cluster that typically looks like an ellipsoid with different shape, sizes, density and orientation.&lt;/p>
&lt;p>There are several GMM variants. In the simplest variant, implemented in the &lt;code>GaussianMixture&lt;/code> class, you must know in advance the number $k$ of Gaussian distributions. The dataset $\bf{X}$ is assumed to have been generated through the following probabilistic process:&lt;/p>
&lt;ul>
&lt;li>For each instance, a cluster is picked randomly from among $k$ clusters. The probability of choosing the $j^{th}$ cluster is defined by the cluster&amp;rsquo;s weight, $\phi^{(j)}$. The index of the cluster chosen for the $i^{th}$ instance is noted as $z^{(i)}$ .&lt;/li>
&lt;li>If $z^{(i)} = j$, meaning the $i^{th}$ instance has been assigned to the $j^{th}$ cluster, the location $\bf{x}^{(i)}$ of this instance is sampled randomly from the Gaussian distribution with mean $\mathbf{\mu}^{(j)}$ and covariance matrix $\mathbf{\Sigma}^{(j)}$ . This is noted $\mathbf{x}^{(i)} \sim \mathcal{N}(\mathbf{\mu}^{(j)}, \mathbf{\Sigma}^{(j)})$.&lt;/li>
&lt;/ul>
&lt;p>This generative process can be represented as a graphical model (Figure 9-16).&lt;/p>
&lt;p>&lt;img alt="gmm" src="./images/handson_cluster_gmm.png">&lt;/p>
&lt;h4 id="gmm-for-clustering" >
&lt;div>
&lt;a href="#gmm-for-clustering">
###
&lt;/a>
GMM for Clustering
&lt;/div>
&lt;/h4>
&lt;p>So, what can you do with such a model? Well, given the dataset $\bf{X}$ , you typically want to start by estimating the weights $\phi$ and all the distribution parameters $\mathbf{\mu}^{(1)}$ to $\mathbf{\mu}^{(k)}$ and $\mathbf{\Sigma}^{(1)}$ to $\mathbf{\Sigma}^{(k)}$ . Sklearn&amp;rsquo;s &lt;code>GaussianMixture&lt;/code> class makes this super easy:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.mixture &lt;span style="color:#ff6ac1">import&lt;/span> GaussianMixture
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># This class relies on the Expectation-Maximization(EM) algorithm,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># which has many similarities with K-Means algorithm:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># it also initializes the cluster parameters randomly,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># then it repeats two steps until convergence:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># * first assigning instances to clusters (E step)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># * then updating the clusters (M step)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Think of EM as a generalization of K-Means that not only finds&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># * the clusters (mu_1 to mu_k), but also&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># * their size, shape, and orientation (Sigma_1 to Sigma_k), as well as&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># * their relative weights (phi_1 to phi_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Unlike K-Means, EM uses soft clustering assignments, not hard assignments,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># unfortunately, just like K-Means, EM can end up converging to poor solutions,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># so it needs to be run several times, keeping only the best solution. This is&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># why we set n_init=10. (By default, n_init=1)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gmm &lt;span style="color:#ff6ac1">=&lt;/span> GaussianMixture(n_components&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">3&lt;/span>, n_init&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">10&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gmm&lt;span style="color:#ff6ac1">.&lt;/span>fit(X)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(gmm&lt;span style="color:#ff6ac1">.&lt;/span>converged_) &lt;span style="color:#78787e"># True or False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(gmm&lt;span style="color:#ff6ac1">.&lt;/span>n_iter_) &lt;span style="color:#78787e"># how many EM iterations using&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(gmm&lt;span style="color:#ff6ac1">.&lt;/span>weights_) &lt;span style="color:#78787e"># cluster weights&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(gmm&lt;span style="color:#ff6ac1">.&lt;/span>means_) &lt;span style="color:#78787e"># means vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(gmm&lt;span style="color:#ff6ac1">.&lt;/span>covariances_) &lt;span style="color:#78787e"># covariance matrices&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># now that the gmm can easily&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># 1. assign each instance to the most likely cluster (hard clustering)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># 2. estimate the probability that it belongs to a particular cluster (soft clustering)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>res_hc &lt;span style="color:#ff6ac1">=&lt;/span> gmm&lt;span style="color:#ff6ac1">.&lt;/span>predict(X) &lt;span style="color:#78787e"># hard clustering&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>res_sc &lt;span style="color:#ff6ac1">=&lt;/span> gmm&lt;span style="color:#ff6ac1">.&lt;/span>predict_proba(x) &lt;span style="color:#78787e"># soft clustering&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># estimate the density of the model at any given location&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>log_pdf_scores &lt;span style="color:#ff6ac1">=&lt;/span> gmm&lt;span style="color:#ff6ac1">.&lt;/span>score_samples(X)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pdf_values &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>exp(log_pdf_scores)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># these pdf_values are not probabilities, but probability densities,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># to estimate the probability that an instance will fall within a&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># particular region, one would have to integrate the PDF over that region.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># A GMM is a generative model, meaning you can sample new instances form it&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># (note that they are ordered by cluster index):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X_new, y_new &lt;span style="color:#ff6ac1">=&lt;/span> gmm&lt;span style="color:#ff6ac1">.&lt;/span>sample(&lt;span style="color:#ff9f43">6&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Figure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the density contours of this model.&lt;/p>
&lt;p>&lt;img alt="trained gmm" src="./images/handson_trained_gmm.png">&lt;/p>
&lt;p>It seems the algorithm clearly found an excellent solution. Of course, we made its task easy by generating the data using a set of 2D Gaussian distributions (real life data is not always so Gaussian and low-dimensional). We also gave the algorithm the correct number of clusters.&lt;/p>
&lt;p>When there are many dimensions, or many clusters, or few instances, EM can struggle to converge to the optimal solution. In such cases, we might need to reduce the difficulty of the task by limiting the number of parameters that the algorithm has to learn. One way to do this is to constraints the covariance matrices (limited the range of shapes and orientations the clusters can have) by setting the &lt;code>covariance_type&lt;/code> hyperparameter to one of the following values:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>covariance_type=&amp;quot;spherical&amp;quot;&lt;/code> :&lt;/p>
&lt;p>All clusters must be spherical, but can have different diameters (i.e., different variances)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>covariance_type=&amp;quot;diag&amp;quot;&lt;/code> :&lt;/p>
&lt;p>Clusters can take on any ellipsoidal shape of any size, but ellipsoid&amp;rsquo;s axes must parallel to the coordinate axes&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>covariance_type=&amp;quot;tied&amp;quot;&lt;/code> :&lt;/p>
&lt;p>All the cluster must have the same ellipsoidal shape, size, and orientation (i.e., all share one covariance matrix)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>covariance_type=&amp;quot;full&amp;quot;&lt;/code> : (by default)&lt;/p>
&lt;p>This means that each cluster can take on any shape, size, and orientation. If there is a large numbers of features, it will not scale well.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="constrained gmm" src="./images/handson_constrained_gmm.png">&lt;/p>
&lt;h4 id="gmm-for-anomaly-detection" >
&lt;div>
&lt;a href="#gmm-for-anomaly-detection">
###
&lt;/a>
GMM for Anomaly Detection
&lt;/div>
&lt;/h4>
&lt;p>&lt;em>Anomaly detection&lt;/em> (a.k.a., &lt;em>outlier detection&lt;/em>) is the task of detecting instances that deviate strongly from the norm. Using GMM for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. So one must define what density threshold to use.&lt;/p>
&lt;p>For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually well known. Say it is equal to 4%. You then set the density threshold ($\rho$) to be the value that results in having 4% of the instances located in areas below $\rho$:&lt;/p>
&lt;ul>
&lt;li>If getting too many false positives (good products flagged as defective), lower the value of $\rho$&lt;/li>
&lt;li>If getting too many false negatives (defective products not flag as defective), lower the value of $\rho$&lt;/li>
&lt;/ul>
&lt;p>This is the usual precision/recall trade-off&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> .&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># defective products example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>densities &lt;span style="color:#ff6ac1">=&lt;/span> gmm&lt;span style="color:#ff6ac1">.&lt;/span>score_samlpe(X)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>density_threshold &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>percentile(densities, &lt;span style="color:#ff9f43">4&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>anomalies &lt;span style="color:#ff6ac1">=&lt;/span> X[densities &lt;span style="color:#ff6ac1">&amp;lt;&lt;/span> density_threshold]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img alt="anomaly detection" src="./images/handson_gmm_anomaly_detection.png">&lt;/p>
&lt;h4 id="selecting-the-number-of-clusters" >
&lt;div>
&lt;a href="#selecting-the-number-of-clusters">
###
&lt;/a>
Selecting the Number of Clusters
&lt;/div>
&lt;/h4>
&lt;p>With K-Means, we could use the inertia or the silhouette score to select the appropriate number of clusters. But with GMM, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have different sizes. Instead, we can try to find the model that minimizes a &lt;em>theoretical information criterion&lt;/em>, such the &lt;em>Bayes information criterion (BIC)&lt;/em> or the &lt;em>Akaike information criterion (AIC)&lt;/em>, defined as follows
$$
\begin{eqnarray}
BIC &amp;amp;=&amp;amp; \log(m)p - 2 \log(\hat{L}) \
\
AIC &amp;amp;=&amp;amp; 2p - 2 \log(\hat{L})
\end{eqnarray}
$$
where $m$ is the number of instances, $p$ is the number of parameters learned by the model, and $\hat{L}$ is the maximized value of the &lt;em>likelihood function&lt;/em> of the model.&lt;/p>
&lt;p>Both the $BIC$ and $AIC$ penalize models that have more parameters to learn (e.g., more clusters) and reward models that fit the data well. They often end up selecting the same model. When they differ, $BIC$ tends to select simpler model (fewer parameters) while not fit the data quite as well as $AIC$ (especially true for larger datasets).&lt;/p>
&lt;p>To compute the $BIC$ and $AIC$ , call the &lt;code>bic()&lt;/code> and &lt;code>aic()&lt;/code> methods:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># gmms contains gmm trained with different k&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> gmm &lt;span style="color:#ff6ac1">in&lt;/span> gmms:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bic_scores&lt;span style="color:#ff6ac1">.&lt;/span>append(gmm&lt;span style="color:#ff6ac1">.&lt;/span>bic(x))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> aic_scores&lt;span style="color:#ff6ac1">.&lt;/span>append(gmm&lt;span style="color:#ff6ac1">.&lt;/span>aic(x))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img alt="bic aic metrics" src="./images/handson_gmm_bic_aic_metrics.png">&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>‚ÄúÂéüÂûã‚Äù ÊòØÊåáÊ†∑Êú¨Á©∫Èó¥‰∏≠ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑÁÇπ„ÄÇ&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>NP hardness problem (&lt;a href="https://wikimili.com/en/NP_(complexity)">non-deterministic polynomial-time&lt;/a> hardness): wait&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>$\lambda_j$ ÂÆûÈôÖÊòØ ${1, 2, \ldots, k }$ ‰∏≠ÁöÑÊüê‰∏™Êï∞ÔºåÊòØ $k$ ‰∏™‰∏çÂêåËÅöÁ±ªÁ∞á $\cal{C}$ ÁöÑ‰∏ãÊ†áÔºåÂõ†‰∏∫ $\text{argmin}_{i \in {1, 2, \ldots, k}} (\text{expression}_i)$ ÂáΩÊï∞Â∞±ÊòØËøîÂõû‰ΩøÂæó $\text{expression}$ ÊúÄÂ∞èÁöÑÈÇ£‰∏™ $i$ „ÄÇ&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>SOM ÊòØÂü∫‰∫éÊó†Ê†áËÆ∞Ê†∑Êú¨ÁöÑËÅöÁ±ªÁÆóÊ≥ïÔºåËÄå LVQ ÂèØÁúã‰Ωú SOM Âü∫‰∫éÁõëÁù£‰ø°ÊÅØÁöÑÊâ©Â±ï„ÄÇSOMÔºàSelf-Organizing Map, Ëá™ÁªÑÁªáÊò†Â∞ÑÔºâÁΩëÁªúÊòØ‰∏ÄÁßçÁ´û‰∫âÂûãÂ≠¶‰π†(competitive learning) ÁöÑÊó†ÁõëÁù£Á•ûÁªèÁΩëÁªúÔºåÂÆÉËÉΩÂ∞ÜÈ´òÁª¥ËæìÂÖ•Êï∞ÊçÆÊò†Â∞ÑÂà∞‰ΩéÁª¥Á©∫Èó¥(ÈÄöÂ∏∏‰∫åÁª¥) ÔºåÂêåÊó∂‰øùÊåÅËæìÂÖ•Êï∞ÊçÆÂú®È´òÁª¥Á©∫Èó¥ÁöÑÊãìÊâëÁªìÊûÑÔºåÂç≥Â∞ÜÈ´òÁª¥Á©∫Èó¥‰∏≠Áõ∏‰ººÁöÑÊ†∑Êú¨ÁÇπÊò†Â∞ÑÂà∞ÁΩëÁªúËæìÂá∫Â±ÇÁöÑÈÇªËøëÁ•ûÁªèÂÖÉ„ÄÇ&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>Á´û‰∫âÂûãÂ≠¶‰π†ÊòØÁ•ûÁªèÁΩëÁªú‰∏≠Â∏∏Áî®ÁöÑ‰∏ÄÁßçÊó†ÁõëÁù£Â≠¶‰π†Á≠ñÁï•ÔºåÂú®‰ΩøÁî®ËØ•Á≠ñÁï•Êó∂ÔºåÁΩëÁªúÁöÑËæìÂá∫Á•ûÁªèÂÖÉ‰∫íÁõ∏Á´û‰∫âÔºåÊØè‰∏ÄÊó∂Âàª‰ªÖÊúâ‰∏Ä‰∏™Á´û‰∫âËé∑ËÉúÁöÑÁ•ûÁªèÂÖÉË¢´ÊøÄÊ¥ªÔºåÂÖ∂‰ªñÁ•ûÁªèÂÖÉÁöÑÁä∂ÊÄÅË¢´ÊäëÂà∂Ôºà‚ÄúËÉúËÄÖÈÄöÂêÉ(winner-take-all)ÂéüÂàô‚ÄùÔºâ„ÄÇ&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>Ëã•Â∞Ü $R_i$ ‰∏≠Ê†∑Êú¨ÂÖ®Áî®ÂéüÂûãÂêëÈáè $p_i$ Ë°®Á§∫ÔºåÂàôÂèØÂÆûÁé∞Êï∞ÊçÆÁöÑ ‚ÄúÊúâÊçüÂéãÁº©(lossy compression)‚ÄùÔºåËøôÁß∞‰∏∫ ‚ÄúÂêëÈáèÈáèÂåñ( vector quantization)‚Äù ÔºõLVQ Áî±Ê≠§ËÄåÂæóÂêç„ÄÇ&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>$p_{\cal{M}} (¬∑)$ ‰πüÊòØÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞Ôºå$\int {p_{\cal{M}}(x)dx} = 1$.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>Expectation-Maximization ÁÆóÊ≥ï(EMÔºåÊúüÊúõÊúÄÂ§ßÂåñÁÆóÊ≥ï) ÊòØÂ∏∏Áî®ÁöÑ‰º∞ËÆ°ÂèÇÊï∞ÈöêÂèòÈáèÁöÑÂà©Âô®ÔºåÂÆÉÊòØ‰∏ÄÁßçËø≠‰ª£ÂºèÁöÑÊñπÊ≥ïÔºåÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØÔºöËã•Ê®°ÂûãÂèÇÊï∞ $\Theta$ Â∑≤Áü•ÔºåÂàôÂèØÊ†πÊçÆËÆ≠ÁªÉÊï∞ÊçÆÊé®Êñ≠Âá∫ÊúÄ‰ºòÈöêÂèòÈáè $\mathbf{Z}$ ÁöÑÂÄºÔºàE Ê≠•ÔºâÔºõÂèç‰πãÔºåËã• $\mathbf{Z}$ ÁöÑÂÄºÂ∑≤Áü•ÔºåÂàôÂèØÊñπ‰æøÂú∞ÂØπÂèÇÊï∞ $\Theta$ ÂÅöÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºàM Ê≠•Ôºâ„ÄÇËøõ‰∏ÄÊ≠•ÔºåËã•Êàë‰ª¨‰∏çÊòØÂèñ $\mathbf{Z}$ ÁöÑÊúüÊúõÔºåËÄåÊòØÂü∫‰∫é$\Theta$ ËÆ°ÁÆóÈöêÂèòÈáè $\mathbf{Z}$ ÁöÑÊ¶ÇÁéáÂàÜÂ∏É $P(\bf{Z} | X, \Theta)$ ÔºåÂàô EM ÁÆóÊ≥ïÁöÑ‰∏§‰∏™Ê≠•È™§ÊòØÔºö‰ª•ÂΩìÂâçÂèÇÊï∞ $\Theta^t$ Êé®Êñ≠ $P(\bf{Z} | X, \Theta^t)$ ÔºåÂπ∂ËÆ°ÁÆóÂØπÊï∞‰ººÁÑ∂ $LL(\bf{\Theta} | X, Z)$ ÂÖ≥‰∫é $\bf{Z}$ ÁöÑÊúüÊúõÔºåÂç≥ $\mathbb{E}(\Theta | \Theta^t)$ÔºàE Ê≠•ÔºâÔºõÂØªÊâæÂèÇÊï∞ÊúÄÂ§ßÂåñÊúüÊúõ‰ººÁÑ∂ÔºåÂç≥ $\Theta^{t+1} = \text{argmax}_{\Theta} \mathbb{E}(\Theta | \Theta^t)$ ÔºàM Ê≠•Ôºâ„ÄÇEM ÁÆóÊ≥ïÂèØÁúã‰ΩúÁî® ‚ÄúÂùêÊ†á‰∏ãÈôçÊ≥ï‚Äù Êù•ÊúÄÂ§ßÂåñÂØπÊï∞‰ººÁÑ∂‰∏ãÁïåÁöÑËøáÁ®ã„ÄÇ&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>Evaluate model performance in classification task.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Book Notes: Deep-Forest Model</title><link>/posts/ml101/treebasedmodels/deepforest/</link><pubDate>Sat, 31 Aug 2019 14:11:27 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/treebasedmodels/deepforest/</guid><description>&lt;h1 id="deep-foresthttpsarxivorgabs170208835" >
&lt;div>
&lt;a href="#deep-foresthttpsarxivorgabs170208835">
##
&lt;/a>
&lt;a href="https://arxiv.org/abs/1702.08835">Deep Forest&lt;/a>
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>online paper, follow the link to all the details.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>In this paper, we extend our preliminary study which proposes the &lt;a href="https://fgg100y.github.io/">gcForest&lt;/a> (multi-Grained Cascade Forest) approach for constructing deep forest, a non-NN style deep model. This is a novel decision tree ensemble, with a cascade structure which enables representation learning by forests. Its representational learning ability can be further enhanced by multi-grained scanning, potentially enabling gcForest to be contextual or structural aware. The cascade levels can be automatically determined such that the model complexity can be determined in a data-dependent way rather than manually designed before training; this enables gcForest to work well even on small-scale data, and enables users to control training costs according to computational resource available. Moreover, the gcForest has much fewer hyper-parameters than DNNs. Even better news is that its performance is quite robust to hyper-parameter settings; our experiments show that in most cases, it is able to get excellent performance by using the default setting, even across different data from different domains.&lt;/p>
&lt;/blockquote>
&lt;h2 id="inspiration-from-dnns1" >
&lt;div>
&lt;a href="#inspiration-from-dnns1">
#
&lt;/a>
Inspiration from DNNs&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>
&lt;/div>
&lt;/h2>
&lt;p>It is widely recognized that the &lt;em>representation learning&lt;/em> ability is crucial for the success of deep neural networks. We believe&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> that the model complexity itself cannot explain the success of DNNs (e.g., large flat networks are not as successful as deep ones) and the &lt;em>layer-by-layer processing&lt;/em> is what really crucial for representation learning in DNNs. Figure 1 provides an illustration&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img alt="layer-by-layer_processing" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_crucial_of_representation_learning.png">&lt;/p>
&lt;p>Learning models, e.g., decision trees and Boosting machines, which also conduct layer-by-layer processing, why they are not as successful as DNNs? We believe that the most important distinguishing factor is that, in contrast to DNNs where new features are generated as illustrated in Figure 1, decision trees and Boosting machines always work on the original feature representation without creating new features during the learning process, or in other words, there is no in-model feature transformation. Moreover, DTs and Boosting machines can only have limited model complexity.&lt;/p>
&lt;p>Overall, we conjecture that behind the mystery of DNNs there are three crucial characteristics, i.e., layer-by-layer processing, in-model feature transformation, and sufficient model complexity. We will try to endow these characteristics to our non-NN style deep model.&lt;/p>
&lt;h2 id="inspiration-from-ensemble-learning" >
&lt;div>
&lt;a href="#inspiration-from-ensemble-learning">
#
&lt;/a>
Inspiration from Ensemble Learning
&lt;/div>
&lt;/h2>
&lt;p>It is well known that an ensemble (multiple learners are trained and combined) can usually achieve better generalization performance than single learners.&lt;/p>
&lt;p>To construct a good ensemble, the individual learners should be &lt;em>accurate&lt;/em> and &lt;em>diverse&lt;/em>. Combining only accurate learners is often inferior to combining some accurate learners with some relatively weaker ones, because the complementarity is more important than pure accuracy. Here is the equation derived from &lt;em>error-ambiguity decomposition&lt;/em>&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>:
$$
\tag{1}
E = \bar{E} - \bar{A},
$$
where $E$ denotes the error of an ensemble, $\bar{E}$ denotes the average error of individual classifiers in the ensemble, and $\bar{A}$ denotes the average &lt;em>ambiguity&lt;/em>, later called &lt;em>diversity&lt;/em>, among the individual classifiers. Eq. 1 reveals that, the more accurate and more diverse the individual classifiers, the better the ensemble. However, it could not be taken as an objective function for optimization, because the &lt;em>ambiguity&lt;/em> term is mathematically defined in the derivation and cannot be operated directly&lt;sup id="fnref1:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Actually, &lt;em>&amp;ldquo;what is diversity?&amp;rdquo;&lt;/em> remains the holy grail problem in ensemble learning.&lt;/p>
&lt;p>In practice, the basic strategy of diversity enhancement is to inject randomness based on some heuristics during the training process. Roughly speaking, there are four major category of mechanisms&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>data sample manipulation&lt;/strong>: which works by generating different data samples to trian individual learners.&lt;/p>
&lt;p>E.g., bootstrap sampling is exploited by Bagging; sequential importance sampling is adopted by AdaBoost.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>input feature manipulation&lt;/strong>: which works by generating different feature subspaces to train individual learners.&lt;/p>
&lt;p>E.g., the Random Subspace approach randomly picks a subset of features.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>learning parameter manipulation&lt;/strong>: which works by using different parameter settings of the base learning algorithm to generate diverse individual learners.&lt;/p>
&lt;p>E.g., different initial selections can be applied to individual neural networks; different split selections can be applied to individual decision trees.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>output representation manipulation&lt;/strong>: which works by using different output representations to generate diverse individual learners.&lt;/p>
&lt;p>E.g., the ECOC approach employs error-correcting output codes; the Flipping Output method randomly changes the labels of some training instances.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Note that, however, these mechanisms are not always effective. More information about ensemble learning can be found in the book Ensemble Methods&lt;sup id="fnref1:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Next we give you, the gcForest, which can be viewed as a decision tree ensemble approach that utilizes almost all categories of mechanisms for diversity enhancement.&lt;/p>
&lt;h2 id="the-gcforest-approach" >
&lt;div>
&lt;a href="#the-gcforest-approach">
#
&lt;/a>
The gcForest Approach
&lt;/div>
&lt;/h2>
&lt;p>First introduce the cascade forest structure, and then the multi-grained scanning, followed by the overall architecture.&lt;/p>
&lt;h3 id="cascade-forest-structure" >
&lt;div>
&lt;a href="#cascade-forest-structure">
##
&lt;/a>
Cascade Forest Structure
&lt;/div>
&lt;/h3>
&lt;p>Representation learning in DNNs mostly relies on the layer-by-layer processing of raw features. Inspired by this recognition, gcForest employs a cascade structure, as illustrated in Figure 2, where each level of cascade receives feature information processed by its preceding level, and outputs its processing result to the next level.&lt;/p>
&lt;p>&lt;img alt="cascade-forest structure" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_cascade_forest_structure.png">&lt;/p>
&lt;p>Each level is an ensemble of decision tree forests, i.e., an &lt;em>ensemble&lt;/em> of &lt;em>ensembles&lt;/em>. Here, we include different types of forests to encourage the &lt;em>diversity&lt;/em>, because diversity is crucial for ensemble construction.&lt;/p>
&lt;p>For simplicity, suppose that we use two completely-random tree forests and two random forests. Each completely-random tree forest contains 500 completely-random trees, generated by randomly selecting a feature for split at each node of the tree, and growing tree until pure leaf, i.e., each leaf node contains only the same class of instances. Similarly, each random forest contains 500 trees, by randomly selecting $\sqrt{d}$ number of features as candidate ($d$ is the number of input features) and choosing the one with the best &lt;em>gini&lt;/em> value for split. (Note that the number of trees in each forest is a hyper-parameter.)&lt;/p>
&lt;p>Given an instance, each forest will produce an estimate of class distribution, by counting the percentage of different classes of training examples at the leaf node where concerned instance falls, and then averaging across all trees in the same forest, as illustrated in Figure 3, where red color highlights paths along which the instance traverses to leaf nodes.&lt;/p>
&lt;p>&lt;img alt="class-vector generation" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_class-vector_generation.png">&lt;/p>
&lt;p>The estimated class distribution forms a class vector, which is then concatenated with the original feature vector to be input to the next level of cascade. For example, suppose there are three classes, then each of the four forests will produce a three-dimensional class vector; thus the next level of cascade will receive 12 ($= 3 \times 4$) augmented features.&lt;/p>
&lt;p>Note that here we take the simplest form of class vectors, i.e., the class distribution at the leaf nodes into which the concerned instance falls. The more complex form of class vectors can be constructed by getting more distributions such as class distribution of the parent nodes which express prior distribution, the sibling nodes which express complementary distribution, etc.&lt;/p>
&lt;p>To reduce the risk of over-fitting, class vector produced by each forest is generated by $k$-fold cross validation. In detail, each instance will be used as training data for $k - 1$ times, resulting $k - 1$ class vectors, which are then averaged to produce the final class vector as augmented features for the next level of cascade. After expanding a new level, the performance of the whole cascade can be estimated on validation set, and the training procedure will terminate if there is no significant performance gain; thus, the number of cascade levels is automatically determined. Note that the training error rather than cross validation error can also be used to control the cascade growth when the training cost is concerned or limited computation resource available.&lt;/p>
&lt;h3 id="multi-grained-scanning" >
&lt;div>
&lt;a href="#multi-grained-scanning">
##
&lt;/a>
Multi-Grained Scanning
&lt;/div>
&lt;/h3>
&lt;p>DNNs are powerful in handling feature relationships, e.g., convolutional-NN are effective on image data where spatial relationships among the raw pixels are critical; recurrent-NN are effective on sequence data where sequential relationships are critical. Inspired by this recognition, we enhance cascade forest with a procedure of multi-grained scanning.&lt;/p>
&lt;p>&lt;img alt="sliding-windows scanning" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_sliding-windows_scanning.png">&lt;/p>
&lt;p>As Figure 4 illustrates, sliding windows are used to scan the raw features. Suppose there are 400 raw features and a window size of 100 features is used. For sequence data, a 100-dimensional feature vector will be generated by sliding the window for one feature; in total 301 feature vectors are produced. If the raw features are with spacial relationships, such as a $20 \times 20$ panel of 400 image pixels, then a $10 \times 10$ window will produce 121 feature vectors. All feature vectors extracted from positive/negative training examples are regarded as positive/negative instances, which will then be used to generate class vectors like in Section [Cascade Forest Structure](###Cascade Forest Structure): the instance extracted from the same size of windows will be used to train a completely-random tree forest and a random forest, and then the class vectors are generated and concatenated as transformed features. As Figure 4 illustrates, suppose that there are 3 classes and a 100-dimensional window is used; then, 301 three-dimensional class vectors are produced by each forest, leading to a 1860-dimensional transformed feature vector corresponding to the original 400-dimensional raw feature vector.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>convolution operations&lt;/strong>: padding and strides&lt;/p>
&lt;p>when the input shape is $(n_h \times n_w)$, the &lt;em>convolution kernel&lt;/em>&amp;rsquo;s shape is $(k_h \times k_w)$,&lt;/p>
&lt;p>&lt;strong>with no padding and stride (default with $s_h = s_w = 1$)&lt;/strong>, then output shape will be:
$$
(n_h - k_h + 1, n_w - k_w + 1),
$$
&lt;strong>with padding (add $p_h$ rows and $p_w$ columns ) and stride $(s_h = s_w = 1)$,&lt;/strong> then output shape will be:
$$
(n_h - k_h + p_h + 1, n_w - k_w + p_w + 1),
$$
&lt;strong>with padding (add $p_h$ rows and $p_w$ columns ) and stride $(s_h, s_w)$,&lt;/strong> then output shape will be:
$$
\bigg((n_h-k_h+p_h+s_h)/s_h, (n_w-k_w+p_w+s_w)/s_w \bigg)
$$&lt;/p>
&lt;p>If we set $p_h=k_h-1$ and $p_w=k_w-1$, then the output shape will be simplified to:
$$
\bigg((n_h+s_h-1)/s_h, (n_w+s_w-1)/s_w \bigg)
$$&lt;/p>
&lt;p>Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be:
$$
\bigg((n_h/s_h)Ôºå (n_w/s_w)\bigg)
$$&lt;/p>
&lt;hr>
&lt;p>For the instances extracted from the windows, we simply assign them with the label of the original training example. Here, some label assignments are inherently incorrect. For example, suppose the original training example is a positive image about &amp;ldquo;car&amp;rdquo;; it is clearly that many extracted instances do not contain a car, and therefore, they are incorrectly labeled as positive. This is actually related to the Flipping Output method&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, a representative of output representation manipulation for ensemble diversity enhancement.&lt;/p>
&lt;p>Note that when transformed feature vectors are too long to be accommodated, feature sampling can be performed, e.g., by subsampling the instances generated by sliding window scanning, since completely-random trees do not rely on feature split selection whereas random forests are quite insensitive to inaccurate feature split selection. Such a feature sampling process is also related to the Random Subspace method&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, a representative of input feature manipulation for ensemble diversity enhancement.&lt;/p>
&lt;p>Figure 4 shows only one size of sliding window. By using multiple sizes of sliding windows, differently grained feature vectors will be generated, as show in Figure 5.&lt;/p>
&lt;p>&lt;img alt="multi-grained scanning" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_multi-grained_scanning.png">&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>DNNs, in a more technically view, is &amp;ldquo;multiple layers of parameterized differentiable nonlinear modules that can be trained by back-propagation.&amp;rdquo; Also note that back-propagation requires differentiability.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>There is no rigorous justification yet.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge, MA, 2016.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>A. Krogh and J. Vedelsby. Neural network ensembles, cross validation, and active learning. In G. Tesauro, D. S.Touretzky, and T. K. Leen, editors, Advances in Neural Information Processing Systems 7, pages 231{238. 1995.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>Z.-H. Zhou. Ensemble Methods: Foundations and Algorithms. CRC, Boca Raton, FL, 2012.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>L. Breiman. Randomizing outputs to increase prediction accuracy. Machine Learning, 40(3):113‚Äì120, 2000.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Analysis and Machine Intelligence, 20(8):832‚Äì844, 1998.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Book Notes: Tree-based Models</title><link>/posts/ml101/treebasedmodels/treemodels/</link><pubDate>Sat, 31 Aug 2019 11:11:27 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/treebasedmodels/treemodels/</guid><description>&lt;h1 id="tree-based-models" >
&lt;div>
&lt;a href="#tree-based-models">
##
&lt;/a>
Tree-based models
&lt;/div>
&lt;/h1>
&lt;h2 id="part-i-theorist-views" >
&lt;div>
&lt;a href="#part-i-theorist-views">
#
&lt;/a>
Part-I: Theorist views
&lt;/div>
&lt;/h2>
&lt;p>&lt;strong>Âü∫Êú¨ÊúØËØ≠ÂíåÁ¨¶Âè∑Á∫¶ÂÆö&lt;/strong>&lt;/p>
&lt;p>‰∏ÄËà¨Âú∞Ôºå‰ª§ $D = {x_1, x_2, \ldots, x_m }$ Ë°®Á§∫ÂåÖÂê´ $m$ ‰∏™Á§∫‰æãÁöÑÊï∞ÊçÆÈõÜÔºåÊØè‰∏™Á§∫‰æãÁî± $d$ ‰∏™Â±ûÊÄßÊèèËø∞ÔºåÂàôÊØè‰∏™Á§∫‰æã $x_i = (x_{i1}, x_{i2}, \ldots, x_{id})$ ÊòØ $d$ Áª¥Ê†∑Êú¨Á©∫Èó¥ $\mathcal{X}$ ÁöÑ‰∏Ä‰∏™ÂêëÈáè&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>Ôºå$x_i \in \mathcal{X}$, ÂÖ∂‰∏≠ $x_{ij}$ ÊòØ $x_i$ Âú®Á¨¨ $j$ ‰∏™Â±ûÊÄß‰∏äÁöÑÂèñÂÄºÔºå $d$ Áß∞‰∏∫Ê†∑Êú¨ $x_i$ ÁöÑ‚ÄúÁª¥Êï∞‚ÄùÔºàdimensionalityÔºâ„ÄÇ&lt;/p>
&lt;p>Ë¶ÅÂª∫Á´ã‰∏Ä‰∏™ÂÖ≥‰∫é‚ÄúÈ¢ÑÊµã(prediction)‚ÄùÁöÑÊ®°ÂûãÔºåÂçïÊúâÁ§∫‰æãÊï∞ÊçÆÔºà‰πüÁß∞‰∏∫Ê†∑Êú¨ÔºåsampleÔºâËøò‰∏çË°åÔºåÊàë‰ª¨ËøòÈúÄË¶ÅËé∑ÂæóËÆ≠ÁªÉÊ†∑Êú¨ÁöÑ‚ÄúÁªìÊûú‚Äù‰ø°ÊÅØÔºå‰æãÂ¶ÇÔºå‰∏Ä‰∏™ÊèèËø∞Ë•øÁìúÁöÑËÆ∞ÂΩï‚ÄúÔºàÔºàËâ≤Ê≥Ω=ÈùíÁªøÔºõÊ†πËíÇ=Ëú∑Áº©ÔºõÊï≤Â£∞=ÊµäÂìçÔºâÔºåÂ•ΩÁìúÔºâ‚Äù„ÄÇËøôÈáåÔºåÂÖ≥‰∫éÁ§∫‰æãÁªìÊûúÁöÑ‰ø°ÊÅØÔºå‰æãÂ¶Ç ‚ÄúÂ•ΩÁìú‚Äù ÔºåÁß∞‰∏∫ ‚ÄúÊ†áËÆ∞(label)‚ÄùÔºõÊã•Êúâ‰∫ÜÊ†áËÆ∞‰ø°ÊÅØÁöÑÁ§∫‰æãÔºåÂàôÁß∞‰πã‰∏∫ &amp;ldquo;Ê†∑‰æã(example)&amp;quot;„ÄÇ&lt;/p>
&lt;p>‰∏ÄËà¨Âú∞ÔºåÁî® $(x_i, y_i)$ Ë°®Á§∫Á¨¨ $i$ ‰∏™Ê†∑‰æãÔºåÂÖ∂‰∏≠ $y_i \in \mathcal{Y}$ ÊòØÁ§∫‰æã $x_i$ ÁöÑÊ†áËÆ∞Ôºå $\mathcal{Y}$ ÊòØÊâÄÊúâÊ†áËÆ∞ÁöÑÈõÜÂêàÔºå‰∫¶Áß∞‚ÄúÊ†áËÆ∞Á©∫Èó¥(label space)‚ÄùÊàñ‚ÄúËæìÂá∫Á©∫Èó¥‚Äù„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúÊàë‰ª¨ÊÉ≥Ë¶ÅÈ¢ÑÊµãÁöÑÊòØÁ¶ªÊï£ÂÄºÔºå‰æãÂ¶Ç ‚ÄúÂ•ΩÁìú‚Äù ‚ÄúÂùèÁìú‚ÄùÔºåÊ≠§Á±ªÂ≠¶‰π†‰ªªÂä°Áß∞‰∏∫ ‚ÄúÂàÜÁ±ª(classification)‚ÄùÔºõÂ¶ÇÊûúË¶ÅÈ¢ÑÊµãÁöÑÊòØËøûÁª≠ÂÄºÔºå ‰æãÂ¶ÇË•øÁìúÁöÑÊàêÁÜüÂ∫¶0.9Ôºå0.4ÔºåÊ≠§Á±ªÂ≠¶‰π†‰ªªÂä°Áß∞‰∏∫ ‚ÄúÂõûÂΩí(regression)‚Äù„ÄÇ‰∫åÂàÜÁ±ª(binary classification)‰ªªÂä°‰∏≠ÔºåÈÄöÂ∏∏‰ª§ $\mathcal{Y} = {-1, +1 }$ Êàñ $\mathcal{Y} = {0, 1 }$ÔºõÂØπ‰∫éÂ§öÂàÜÁ±ª(multi-class classification), $|\mathcal{Y}| &amp;gt; 2$ÔºõÂØπÂõûÂΩí‰ªªÂä°Ôºå$\mathcal{Y} = \R$Ôºå$\R$ ‰∏∫ÂÆûÊï∞ÈõÜ„ÄÇ&lt;/p>
&lt;h2 id="decision-tree" >
&lt;div>
&lt;a href="#decision-tree">
#
&lt;/a>
Decision Tree
&lt;/div>
&lt;/h2>
&lt;h3 id="ÂÜ≥Á≠ñÊ†ëÁîüÊàêÁÆóÊ≥ï" >
&lt;div>
&lt;a href="#%e5%86%b3%e7%ad%96%e6%a0%91%e7%94%9f%e6%88%90%e7%ae%97%e6%b3%95">
##
&lt;/a>
ÂÜ≥Á≠ñÊ†ëÁîüÊàêÁÆóÊ≥ï
&lt;/div>
&lt;/h3>
&lt;p>‰∏ÄËà¨ÁöÑÔºå‰∏ÄÊ£µÂÜ≥Á≠ñÊ†ëÂåÖÂê´‰∏Ä‰∏™Ê†πÁªìÁÇπ„ÄÅËã•Âπ≤‰∏™ÂÜÖÈÉ®ÁªìÁÇπÂíåËã•Âπ≤‰∏™Âè∂ÁªìÁÇπÔºõÂè∂ÁªìÁÇπÂØπÂ∫î‰∫éÂÜ≥Á≠ñÁªìÊûúÔºåÂÖ∂‰ªñÊØè‰∏™ÁªìÁÇπÂØπÂ∫î‰∫é‰∏Ä‰∏™Â±ûÊÄßÊµãËØïÔºõÊØè‰∏™ÁªìÁÇπÂåÖÂê´ÁöÑÊ†∑Êú¨ÈõÜÂêàÊ†πÊçÆÂ±ûÊÄßÊµãËØïÁöÑÁªìÊûúË¢´ÂàíÂàÜÂà∞Â≠êÁªìÁÇπ‰∏≠ÔºõÊ†πÁªìÁÇπÂåÖÂê´Ê†∑Êú¨ÂÖ®ÈõÜ„ÄÇ‰ªéÊ†πÁªìÁÇπÂà∞ÊØè‰∏™Âè∂ÁªìÁÇπÁöÑË∑ØÂæÑÂØπÂ∫î‰∫Ü‰∏Ä‰∏™Âà§ÂÆöÊµãËØïÂ∫èÂàó„ÄÇ&lt;/p>
&lt;p>ÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫Ü‰∫ßÁîü‰∏ÄÊ£µÊ≥õÂåñÊÄßËÉΩÂº∫ÁöÑÂÜ≥Á≠ñÊ†ëÔºå‰∫¶Âç≥Â§ÑÁêÜÊú™ËßÅÁ§∫‰æãÔºàunseen samplesÔºâÁöÑËÉΩÂäõÂº∫ÁöÑÂÜ≥Á≠ñÊ†ë„ÄÇÂÖ∂Âü∫Êú¨ÊµÅÁ®ãÈÅµÂæ™ÁÆÄÂçï‰∏îÁõ¥ËßÇÁöÑ‚ÄúÂàÜËÄåÊ≤ª‰πã‚ÄùÔºàdivide-and-conquerÔºâÁ≠ñÁï•ÔºåÂ¶Ç&lt;code>ÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†Âü∫Êú¨ÁÆóÊ≥ï&lt;/code>ÊâÄÁ§∫„ÄÇ&lt;/p>
&lt;hr>
&lt;p>&lt;code>ÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†Âü∫Êú¨ÁÆóÊ≥ï&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>Ôºö ËÆ≠ÁªÉÈõÜ $D = {(x_1, y_1), \dots, ({x_m, y_m}) }$; \
Â±ûÊÄßÈõÜ $A = {a_1, \ldots, a_d }$&lt;/p>
&lt;p>&lt;strong>ËøáÁ®ã&lt;/strong>Ôºö ÂáΩÊï∞ $\text{TreeGenerate}(D, A)$&lt;/p>
&lt;p>1: ÁîüÊàêÁªìÁÇπ $\text{node}$;&lt;/p>
&lt;p>2: &lt;strong>if&lt;/strong> $D$ ‰∏≠Ê†∑Êú¨ÂÖ®Â±û‰∫éÂêå‰∏ÄÁ±ªÂà´ $C$ &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>3: Â∞Ü $\text{node}$ Ê†áËÆ∞‰∏∫ $C$ Á±ªÂè∂ÁªìÁÇπÔºõ&lt;strong>return&lt;/strong>&lt;/p>
&lt;p>4: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>5: &lt;strong>if&lt;/strong> $A = \phi$ &lt;strong>OR&lt;/strong> $D$ ‰∏≠Ê†∑Êú¨Âú®$A$ ‰∏äÂèñÂÄºÁõ∏Âêå &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>6: Â∞Ü $\text{node}$ Ê†áËÆ∞‰∏∫Âè∂ÁªìÁÇπÔºåÂÖ∂Á±ªÂà´Ê†áËÆ∞‰∏∫ $D$ ‰∏≠Ê†∑Êú¨Êï∞ÈáèÊúÄÂ§öÁöÑÁ±ªÔºõ&lt;strong>return&lt;/strong>&lt;/p>
&lt;p>7: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>&lt;strong>8&lt;/strong>: ‰ªé $A$ ‰∏≠ÈÄâÊã©ÊúÄ‰ºòÂàíÂàÜÂ±ûÊÄß $a_*$;&lt;/p>
&lt;p>9: &lt;strong>for&lt;/strong> Â±ûÊÄß $a_&lt;em>$ ÁöÑÊØè‰∏Ä‰∏™ÂÄº $a_&lt;/em>^v$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>10: ‰∏∫ $\text{node}$ ÁîüÊàê‰∏Ä‰∏™ÂàÜÊîØÔºõ‰ª§ $D_v$ Ë°®Á§∫ $D$ ‰∏≠Âú® $a_&lt;em>$ ‰∏äÂèñÂÄº‰∏∫ $a_&lt;/em>^v$ ÁöÑÊ†∑Êú¨Â≠êÈõÜÔºõ&lt;/p>
&lt;p>11: &lt;strong>if&lt;/strong> $D_v$ ‰∏∫Á©∫ &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>12: Â∞ÜÂàÜÊîØÁªìÁÇπÊ†áËÆ∞‰∏∫Âè∂ÁªìÁÇπÔºåÂÖ∂Á±ªÂà´Ê†áËÆ∞‰∏∫ $D$ ‰∏≠Ê†∑Êú¨Êï∞ÈáèÊúÄÂ§öÁöÑÁ±ªÔºõ&lt;strong>return&lt;/strong>&lt;/p>
&lt;p>13: &lt;strong>else&lt;/strong>&lt;/p>
&lt;p>14: ‰ª• $\text{TreeGenerate}(D_v, A - a_*)$ ‰∏∫ÂàÜÊîØÁªìÁÇπ&lt;/p>
&lt;p>15: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>16: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>Ôºö ‰ª• $\text{node}$ ‰∏∫Ê†πÁªìÁÇπÁöÑ‰∏ÄÊ£µÂÜ≥Á≠ñÊ†ë&lt;/p>
&lt;hr>
&lt;p>ÊòæÁÑ∂ÔºåÂÜ≥Á≠ñÊ†ëÁöÑÁîüÊàêÊó∂‰∏Ä‰∏™ÈÄíÂΩíËøáÁ®ãÔºåÂú®&lt;code>ÂÜ≥Á≠ñÊ†ëÂü∫Êú¨ÁÆóÊ≥ï&lt;/code>‰∏≠ÔºåÊúâ‰∏âÁßçÊÉÖÂΩ¢‰ºöÂØºËá¥ÈÄíÂΩíËøîÂõûÔºö&lt;/p>
&lt;ol>
&lt;li>ÂΩìÂâçÁªìÁÇπÂåÖÂê´ÁöÑÊ†∑Êú¨ÂÖ®ÈÉ®Â±û‰∫éÂêå‰∏ÄÁ±ªÂà´ ÔºàÊó†ÈúÄËøõ‰∏ÄÊ≠•ÂàíÂàÜÔºâ&lt;/li>
&lt;li>ÂΩìÂâçÂ±ûÊÄßÈõÜ‰∏∫Á©∫ÔºåÊàñÊòØÊâÄÊúâÊ†∑Êú¨Âú®ÊâÄÊúâÂ±ûÊÄß‰∏äÂèñÂÄºÁõ∏Âêå ÔºàÊó†Ê≥ïËøõ‰∏ÄÊ≠•ÂàíÂàÜÔºâ&lt;/li>
&lt;li>ÂΩìÂâçÁªìÁÇπÂåÖÂê´ÁöÑÊ†∑Êú¨ÈõÜÂêà‰∏∫Á©∫ Ôºà‰∏çËÉΩËøõ‰∏ÄÊ≠•ÂàíÂàÜÔºâ&lt;/li>
&lt;/ol>
&lt;p>Âú®Á¨¨2ÁßçÊÉÖÂΩ¢‰∏ãÔºåÊàë‰ª¨ÊääÂΩìÂâçÁªìÁÇπÊ†áËÆ∞‰∏∫Âè∂ÁªìÁÇπÔºåÂπ∂Â∞ÜÂÖ∂Á±ªÂà´ËÆæÂÆö‰∏∫ËØ•ÁªìÁÇπ‰∏≠Ê†∑Êú¨Êï∞ÈáèÊúÄÂ§öÁöÑÁ±ªÂà´ÔºõÂú®Á¨¨3ÁßçÊÉÖÂΩ¢‰∏ãÔºåÂêåÊ†∑ÊääÂΩìÂâçÁªìÁÇπÊ†áËÆ∞‰∏∫Âè∂ÁªìÁÇπÔºå‰ΩÜÂ∞ÜÂÖ∂Á±ªÂà´ËÆæÂÆö‰∏∫ÂÖ∂Áà∂ÁªìÁÇπÊâÄÂê´Ê†∑Êú¨ÊúÄÂ§öÁöÑÁ±ªÂà´ÔºåÊ≥®ÊÑèËøô‰∏§ÁßçÊÉÖÂΩ¢Â§ÑÁêÜÂÆûË¥®‰∏çÂêåÔºöÊÉÖÂΩ¢2‰∏≠ÊòØÂà©Áî®ÂΩìÂâçÁªìÁÇπÁöÑÂêéÈ™åÂàÜÂ∏ÉÔºåËÄåÊÉÖÂΩ¢3‰∏≠ÂàôÊòØÊääÁà∂ÁªìÁÇπÁöÑÊ†∑Êú¨ÂàÜÂ∏É‰Ωú‰∏∫ÂΩìÂâçÁªìÁÇπÁöÑÂÖàÈ™åÂàÜÂ∏É„ÄÇ&lt;/p>
&lt;blockquote>
&lt;p>„ÄäThe hundred-Page Machine Learning„Äã&lt;/p>
&lt;p>&lt;img alt="build tree the 1st split" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\looPagesML_dtree_build.png">&lt;/p>
&lt;p>The ID3 learning algorithm works as follows. Let $\cal{S}$ denotes a set of labeled examples. In the begining, the decision tree only has a start noed (root node) that contains all examples: $\mathcal{S} = {(\mathbb{x}&lt;em>i, y_i) }^N_i$. Start with a constant model $f&lt;/em>{ID3}$ :
$$
\tag{6}
f_{ID3} = {1 \over |\mathcal{S}|} \sum_{(\mathbb{x},y) \in \mathcal{S}} y .
$$
The prediction given by the above model, $f_{ID3}(\mathbb{x})$, would be the same for any input $\mathbb{x}$. The corresponding decision tree is shown in fig4(a).&lt;/p>
&lt;p>The we search through all features $j = 1, \ldots, D$ and all thresholds $t$, and split the set $\cal{S}$ into two subsets:&lt;/p>
&lt;ul>
&lt;li>$\mathcal{S}_{_} = {(\mathbb{x},y) | (\mathbb{x},y) \in \mathcal{S}, x^{(j)} &amp;lt; t }$ and&lt;/li>
&lt;li>$\mathcal{S}_{+} = {(\mathbb{x},y) | (\mathbb{x},y) \in \mathcal{S}, x^{(j)} \ge t }$ .&lt;/li>
&lt;/ul>
&lt;p>The new two subsets would go to two new leaf nodes (or inter nodes), and we evaluate, for all possible pairs $(j, t)$ how good the split with pieces $\mathcal{S}&lt;em>{_}$ and $\mathcal{S}&lt;/em>{+}$ is (see the followed section &lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/###ÂàíÂàÜÈÄâÊã©">ÂàíÂàÜÈÄâÊã©&lt;/a>). Finally, we pick the best such values $(j, t)$ for splitting $\cal{S}$ into $\mathcal{S}&lt;em>{_}$ and $\mathcal{S}&lt;/em>{+}$ , from two new leaf nodes, and continue recursively on $\mathcal{S}&lt;em>{_}$ and $\mathcal{S}&lt;/em>{+}$ (or quit if reach some criterion). A decision tree after one split is illustrated in fig4(b).&lt;/p>
&lt;/blockquote>
&lt;h3 id="ÂàíÂàÜÈÄâÊã©" >
&lt;div>
&lt;a href="#%e5%88%92%e5%88%86%e9%80%89%e6%8b%a9">
##
&lt;/a>
ÂàíÂàÜÈÄâÊã©
&lt;/div>
&lt;/h3>
&lt;p>ÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ÁöÑÂÖ≥ÈîÆÊòØÂ¶Ç‰ΩïÈÄâÊã©ÊúÄ‰ºòÂàíÂàÜÂ±ûÊÄßÔºà&lt;code>ÂÜ≥Á≠ñÊ†ëÂü∫Êú¨ÁÆóÊ≥ï&lt;/code> Á¨¨8Ë°åÔºâ„ÄÇ‰∏ÄËà¨ËÄåË®ÄÔºåÈöèÁùÄÂàíÂàÜËøáÁ®ã‰∏çÊñ≠ËøõË°åÔºåÊàë‰ª¨Â∏åÊúõÂÜ≥Á≠ñÊ†ëÁöÑÂàÜÊîØÁªìÁÇπÊâÄÂåÖÂê´ÁöÑÊ†∑Êú¨Â∞ΩÂèØËÉΩÂ±û‰∫éÂêå‰∏ÄÁ±ªÂà´ÔºåÂç≥ÁªìÁÇπÁöÑ‚ÄúÁ∫ØÂ∫¶‚ÄùÔºàpurityÔºâË∂äÊù•Ë∂äÈ´ò„ÄÇ&lt;/p>
&lt;h4 id="‰ø°ÊÅØÂ¢ûÁõä" >
&lt;div>
&lt;a href="#%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a">
###
&lt;/a>
‰ø°ÊÅØÂ¢ûÁõä
&lt;/div>
&lt;/h4>
&lt;p>‚Äú‰ø°ÊÅØÁÜµ‚ÄùÔºàinformation entropyÔºâÊòØÂ∫¶ÈáèÊ†∑Êú¨ÈõÜÂêàÁ∫ØÂ∫¶ÊúÄÂ∏∏Áî®ÁöÑ‰∏ÄÁßçÊåáÊ†á„ÄÇÂÅáÂÆöÂΩìÂâçÊ†∑Êú¨ÈõÜÂêà $D$ ‰∏≠Á¨¨ $k$ Á±ªÊ†∑Êú¨ÊâÄÂç†ÁöÑÊØî‰æã‰∏∫ $p_k \ (k=1, \ldots, |\mathcal{Y}|)$ÔºåÂàô $D$ ‰ø°ÊÅØÁÜµÂÆö‰πâ‰∏∫
$$
\tag{4.1}
\text{Ent}{(D)} = - \sum^{|\mathcal{Y}|}_{k=1} p_k \text{log}_2 p_k.
$$
$\text{Ent}(D)$ ÁöÑÂÄºË∂äÂ∞èÔºåÂàô $D$ ÁöÑÁ∫ØÂ∫¶Ë∂äÈ´ò„ÄÇ&lt;/p>
&lt;p>ÂÅáÂÆöÁ¶ªÊï£Â±ûÊÄß $a$ Êúâ $V$ ‰∏™ÂèØËÉΩÁöÑÂèñÂÄº ${a^1, \ldots, a^V }$ÔºåËã•‰ΩøÁî® $a$ Êù•ÂØπÊ†∑Êú¨ÈõÜ $D$ ËøõË°åÂàíÂàÜÔºåÂàô‰ºö‰∫ßÁîü $V$ ‰∏™ÂàÜÊîØÁªìÁÇπÔºåÂÖ∂‰∏≠Á¨¨ $v$ ‰∏™ÂàÜÊîØÁªìÁÇπÂåÖÂê´‰∫Ü $D$ ‰∏≠ÊâÄÊúâÂú®Â±ûÊÄß $a$ ‰∏äÂèñÂÄº‰∏∫ $a^v$ ÁöÑÊ†∑Êú¨ÔºåËÆ∞‰∏∫ $D^v$„ÄÇÊàë‰ª¨Ê†πÊçÆÂºè(4.1)ËÆ°ÁÆóÂá∫ $D^v$ ÁöÑ‰ø°ÊÅØÁÜµÔºåÂÜçËÄÉËôëÂà∞‰∏çÂêåÁöÑÂàÜÊîØÁªìÁÇπÊâÄÂåÖÂê´ÁöÑÊ†∑Êú¨Êï∞‰∏çÂêåÔºåÁªôÂàÜÊîØÁªìÁÇπËµã‰∫àÊùÉÈáç ${|D^v| \over |D|}$ÔºåÂç≥Ê†∑Êú¨Êï∞Ë∂äÂ§öÁöÑÂàÜÊîØÁªìÁÇπÁöÑÂΩ±ÂìçË∂äÂ§ßÔºå‰∫éÊòØÂèØ‰ª•ËÆ°ÁÆóÂá∫Áî®Â±ûÊÄß $a$ ÂØπÊ†∑Êú¨ÈõÜ $D$ ËøõË°åÂàíÂàÜÊâÄËé∑ÂæóÁöÑ‚Äú‰ø°ÊÅØÂ¢ûÁõä(information gain)‚Äù
$$
\tag{4.2}
\text{Gain}(D, a) = \text{Ent}(D) - \sum^V_{v=1} {|D^v| \over |D|} \text{Ent}(D^v).
$$
‰∏ÄËà¨ËÄåË®ÄÔºå‰ø°ÊÅØÂ¢ûÁõäË∂äÂ§ßÔºåÂàôÊÑèÂë≥ÁùÄ‰ΩøÁî®Â±ûÊÄß $a$ Êù•ËøõË°åÂàíÂàÜÊâÄËé∑ÂæóÁöÑ‚ÄúÁ∫ØÂ∫¶ÊèêÂçá‚ÄùË∂äÂ§ß„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•Áî®‰ø°ÊÅØÂ¢ûÁõäÊù•ËøõË°åÂÜ≥Á≠ñÊ†ëÁöÑÂàíÂàÜÂ±ûÊÄßÈÄâÊã©ÔºåÂç≥ÈÄâÊã©Â±ûÊÄß $a_* = argmax_{(a \in A)} \text{Gain}(D, a)$„ÄÇ&lt;/p>
&lt;p>$\color{Green}{\bold{‰æãÂ≠ê}}$&lt;/p>
&lt;p>&lt;img alt="xgs d2.0" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree_dataset1.png">&lt;/p>
&lt;p>‰ª•Ë°®4.1‰∏≠ÁöÑË•øÁìúÊï∞ÊçÆÈõÜ2.0‰∏∫‰æã„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´17‰∏™ËÆ≠ÁªÉÊ†∑‰æãÔºåÁî®‰ª•Â≠¶‰π†‰∏ÄÊ£µËÉΩÈ¢ÑÊµãÊ≤°ÊúâÂ∞ùËøáÁöÑÊòØ‰∏çÊòØÂ•ΩÁìúÁöÑÂÜ≥Á≠ñÊ†ë„ÄÇÊòæÁÑ∂ÔºåÂàÜÁ±ªÁöÑÁ±ªÂà´ÂÖ±‰∏§Á±ªÔºàÊòØÂ•ΩÁìúÔºå‰∏çÊòØÂ•ΩÁìúÔºâÔºå$|\mathcal{Y}| = 2$„ÄÇÂú®ÂÜ≥Á≠ñÊ†ëÂºÄÂßãÂ≠¶‰π†Êó∂ÔºåÊ†πÁªìÁÇπÂåÖÂê´ $D$ ‰∏≠ÊâÄÊúâÁöÑÊ†∑‰æãÔºåÂÖ∂‰∏≠Ê≠£‰æãÂç† $p_1 = 8 / 17$ ÔºåÂèç‰æãÂç† $p_1 = 9 / 17$„ÄÇ‰∫éÊòØÔºåÊ†πÊçÆÂºè(4.1)ÂèØËÆ°ÁÆóÂá∫Ê†πÁªìÁÇπÁöÑ‰ø°ÊÅØÁÜµ‰∏∫
$$
\text{Ent}(D) = - \sum^2_{k=1} p_k \text{log}_2 p_k = - \bigg({8 \over 17}\text{log}_2 {8 \over 17} + {9 \over 17}\text{log}_2 {9 \over 17} \bigg) \approx 0.998 .
$$
ÁÑ∂ÂêéÔºåÊàë‰ª¨Ë¶ÅËÆ°ÁÆóÂá∫ÂΩìÂâçÂ±ûÊÄßÈõÜÂêàÔΩõËâ≤Ê≥ΩÔºåÊ†πËíÇÔºåÊï≤Â£∞ÔºåÁ∫πÁêÜÔºåËÑêÈÉ®ÔºåËß¶ÊÑüÔΩù‰∏≠ÊØè‰∏™Â±ûÊÄßÁöÑ‰ø°ÊÅØÂ¢ûÁõä„ÄÇ‰ª•Â±ûÊÄß ‚ÄúËâ≤Ê≥Ω‚Äù ‰∏∫‰æãÔºåÂÆÉÊúâ3‰∏™ÂèØËÉΩÁöÑÂèñÂÄºÔºöÔΩõÈùíÁªøÔºå‰πåÈªëÔºåÊµÖÁôΩÔΩù„ÄÇËã•‰ΩøÁî®ËØ•Â±ûÊÄßÂØπ $D$ ËøõË°åÂàíÂàÜÔºåÂàôÂèØÂæóÂà∞3‰∏™Â≠êÈõÜÔºåÂàÜÂà´ËÆ∞‰∏∫Ôºö$D^1 (Ëâ≤Ê≥Ω=ÈùíÁªø)ÔºåD^2 (Ëâ≤Ê≥Ω=‰πåÈªë)ÔºåD^3 (Ëâ≤Ê≥Ω=ÊµÖÁôΩÔºâ$„ÄÇ&lt;/p>
&lt;p>Áî±Ë°®4.1ÂèØÂæóÔºåÂ≠êÈõÜ $D^1$ ÂåÖÂê´ÁºñÂè∑‰∏∫ÔΩõ1Ôºå4Ôºå6Ôºå10Ôºå13Ôºå17ÔΩùÁöÑ6‰∏™Ê†∑‰æãÔºåÂÖ∂‰∏≠Ê≠£‰æãÂç† $p_1 = 3 / 6$ ÔºåÂèç‰æãÂç† $p_2 = 3 / 6$ÔºõÂ≠êÈõÜ $D^2$ ÂåÖÂê´ÁºñÂè∑‰∏∫ÔΩõ2Ôºå3Ôºå7Ôºå8Ôºå9Ôºå15ÔΩùÁöÑ6‰∏™Ê†∑‰æãÔºåÂÖ∂‰∏≠Ê≠£‰æãÂç† $p_1 = 4 / 6$ ÔºåÂèç‰æãÂç† $p_2 = 2 / 6$ÔºõÂ≠êÈõÜ $D^3$ ÂåÖÂê´ÁºñÂè∑‰∏∫ÔΩõ5Ôºå11Ôºå12Ôºå14Ôºå16ÔΩùÁöÑ5‰∏™Ê†∑‰æãÔºåÂÖ∂‰∏≠Ê≠£‰æãÂç† $p_1 = 1 / 5$ ÔºåÂèç‰æãÂç† $p_2 = 4 / 5$„ÄÇÊ†πÊçÆÂºè(4.1)ÂèØËÆ°ÁÆóÂá∫Áî® ‚ÄúËâ≤Ê≥Ω‚Äù ÂàíÂàÜ‰πãÂêéÊâÄÂæóÂà∞ÁöÑ3‰∏™ÂàÜÊîØÁªìÁÇπÁöÑ‰ø°ÊÅØÁÜµ‰∏∫
$$
\begin{eqnarray}
\text{Ent}(D^1) &amp;amp;=&amp;amp; - \bigg({3 \over 6}\text{log}_2 {3 \over 6} + {3 \over 6}\text{log}_2 {3 \over 6} \bigg) = 1.000, \
\text{Ent}(D^2) &amp;amp;=&amp;amp; - \bigg({4 \over 6}\text{log}_2 {4 \over 6} + {2 \over 6}\text{log}_2 {2 \over 6} \bigg) = 0.918, \
\text{Ent}(D^3) &amp;amp;=&amp;amp; - \bigg({1 \over 5}\text{log}_2 {1 \over 5} + {4 \over 5}\text{log}&lt;em>2 {4 \over 5} \bigg) = 0.772, \
\end{eqnarray}
$$
‰∫éÊòØÔºåÊ†πÊçÆÂºè(4.2)ÂèØËÆ°ÁÆóÂá∫Â±ûÊÄß ‚ÄúËâ≤Ê≥Ω‚Äù ÁöÑ‰ø°ÊÅØÂ¢ûÁõä‰∏∫
$$
\begin{eqnarray}
\text{Gain}(D, Ëâ≤Ê≥Ω)
&amp;amp;=&amp;amp; \text{Ent}(D) - \sum^3&lt;/em>{v=1} {|D^v| \over |D|} \text{Ent}(D^v) \
&amp;amp;=&amp;amp; 0.998 - \bigg({6 \over 17} \times 1.000 + {6 \over 17} \times 0.918 + {5 \over 17} \times 0.772 \bigg) \
&amp;amp;=&amp;amp; 0.109 .
\end{eqnarray}
$$
Á±ª‰ººÁöÑÔºåÊàë‰ª¨ÂèØ‰ª•ËÆ°ÁÆóÂá∫ÂÖ∂‰ªñÂ±ûÊÄßÁöÑ‰ø°ÊÅØÂ¢ûÁõäÔºö
$$
\text{Gain}(D, Ê†πËíÇ) = 0.143;\text{Gain}(D, Êï≤Â£∞) = 0.141 \
\text{Gain}(D, Á∫πÁêÜ) = 0.381;\text{Gain}(D, ËÑêÈÉ®) = 0.289 \
\text{Gain}(D, Ëß¶ÊÑü) = 0.006.\qquad \qquad \qquad \qquad \quad \ &lt;br>
$$
ÊòæÁÑ∂ÔºåÂ±ûÊÄß ‚ÄúÁ∫πÁêÜ‚Äù ÁöÑ‰ø°ÊÅØÂ¢ûÁõäÊúÄÂ§ßÔºå‰∫éÊòØÂÆÉË¢´ÈÄâ‰∏∫ÂàíÂàÜÂ±ûÊÄß„ÄÇÂõæ4.3ÁªôÂá∫‰∫ÜÂü∫‰∫é ‚ÄúÁ∫πÁêÜ‚Äù ÂØπÊ†πÁªìÁÇπËøõË°åÂàíÂàÜÁöÑÁªìÊûúÔºåÂêÑÂàÜÊîØÁªìÁÇπÊâÄÂåÖÂê´ÁöÑÊ†∑‰æãÂ≠êÈõÜÊòæÁ§∫Âú®ÁªìÁÇπ‰∏≠„ÄÇ&lt;/p>
&lt;p>&lt;img alt="tree first split" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree1.png">&lt;/p>
&lt;p>ÁÑ∂ÂêéÔºåÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ÁÆóÊ≥ïÂ∞ÜÂØπÊØè‰∏™ÂàÜÊîØÁªìÁÇπÂÅöËøõ‰∏ÄÊ≠•ÂàíÂàÜ„ÄÇ‰ª•Âõæ4.3‰∏≠Á¨¨‰∏Ä‰∏™ÂàÜÊîØÁªìÁÇπÔºà‚ÄúÁ∫πÁêÜ=Ê∏ÖÊô∞‚ÄùÔºâ‰∏∫‰æãÔºåËØ•ÁªìÁÇπÂåÖÂê´ÁöÑÊ†∑‰æãÈõÜÂêà $D^1$ ‰∏≠ÊúâÁºñÂè∑‰∏∫ ÔΩõ1Ôºå2Ôºå3Ôºå4Ôºå5Ôºå6Ôºå8Ôºå10Ôºå15ÔΩùÁöÑ9‰∏™Ê†∑‰æãÔºåÂèØÁî®Â±ûÊÄßÈõÜÂêà‰∏∫ ÔΩõËâ≤Ê≥ΩÔºåÊ†πËíÇÔºåÊï≤Â£∞ÔºåËÑêÈÉ®ÔºåËß¶ÊÑüÔΩù&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>„ÄÇÂü∫‰∫é $D^1$ ËÆ°ÁÆóÂá∫ÂêÑ‰∏™Â±ûÊÄßÁöÑ‰ø°ÊÅØÂ¢ûÁõäÔºö
$$
\text{Gain}(D, Ê†πËíÇ) = 0.458;\text{Gain}(D, Êï≤Â£∞) = 0.331 \
\text{Gain}(D, Ëâ≤Ê≥Ω) = 0.043;\text{Gain}(D, ËÑêÈÉ®) = 0.458 \
\text{Gain}(D, Ëß¶ÊÑü) = 0.458.\qquad \qquad \qquad \qquad \quad \ &lt;br>
$$
‚ÄúÊ†πËíÇ‚Äù„ÄÅ‚ÄúËÑêÈÉ®‚Äù„ÄÅ‚ÄúËß¶ÊÑü‚Äù 3‰∏™Â±ûÊÄßÂùáÂèñÂæóÊúÄÂ§ßÁöÑ‰ø°ÊÅØÂ¢ûÁõäÔºåÂèØ‰ªªÈÄâÂÖ∂‰∏≠‰πã‰∏Ä‰Ωú‰∏∫ÂàíÂàÜÂ±ûÊÄß„ÄÇÁ±ª‰ººÁöÑÔºåÂØπÊØè‰∏™ÂàÜÊîØÁªìÁÇπËøõË°å‰∏äËø∞Êìç‰ΩúÔºåÊúÄÁªàÂæóÂà∞ÁöÑÂÜ≥Á≠ñÊ†ëÂ¶ÇÂõæ4.4ÊâÄÁ§∫„ÄÇ&lt;/p>
&lt;p>&lt;img alt="tree first split" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree2.png">&lt;/p>
&lt;h4 id="Â¢ûÁõäÁéá" >
&lt;div>
&lt;a href="#%e5%a2%9e%e7%9b%8a%e7%8e%87">
###
&lt;/a>
Â¢ûÁõäÁéá
&lt;/div>
&lt;/h4>
&lt;p>Âú®‰∏äÈù¢ÁöÑ‰æãÂ≠ê‰∏≠ÔºåÊàë‰ª¨ÊúâÊÑèÂøΩÁï•‰∫ÜË°®4.1‰∏≠ÁöÑ ‚ÄúÁºñÂè∑‚Äù Ëøô‰∏ÄÂàó„ÄÇÂ¶ÇÊûúÊää ‚ÄúÁºñÂè∑‚Äù ‰πü‰Ωú‰∏∫‰∏Ä‰∏™ÂÄôÈÄâÂàíÂàÜÂ±ûÊÄßÔºåÂàôÊ†πÊçÆÂºè(4.2)ÂèØËÆ°ÁÆóÂá∫ÂÆÉÁöÑ‰ø°ÊÅØÂ¢ûÁõä‰∏∫$0.998$ÔºåËøúÂ§ß‰∫éÂÖ∂‰ªñÂÄôÈÄâÂàíÂàÜÂ±ûÊÄß„ÄÇËøôÂæàÂÆπÊòìÁêÜËß£Ôºö‚ÄúÁºñÂè∑‚Äù Â∞Ü‰∫ßÁîü17‰∏™ÂàÜÊîØÔºåÊØè‰∏™ÂàÜÊîØÁªìÁÇπ‰ªÖÂåÖÂê´‰∏Ä‰∏™Ê†∑Êú¨ÔºåËøô‰∫õÂàÜÊîØÁªìÁÇπÁöÑÁ∫ØÂ∫¶Â∑≤ËææÂà∞ÊúÄÂ§ß„ÄÇÁÑ∂ËÄåÔºåËøôÊ†∑ÁöÑÂÜ≥Á≠ñÊ†ëÊòæÁÑ∂‰∏çÂÖ∑ÊúâÊ≥õÂåñËÉΩÂäõÔºåÊó†Ê≥ïÂØπÊñ∞Ê†∑Êú¨ËøõË°åÊúâÊïàÈ¢ÑÊµã„ÄÇ&lt;/p>
&lt;p>ÂÆûÈôÖ‰∏äÔºå‰ø°ÊÅØÂ¢ûÁõäÂáÜÂàôÂØπÂèØÂèñÂÄºÊï∞ÁõÆËæÉÂ§öÁöÑÂ±ûÊÄßÊúâÊâÄÂÅèÂ•Ω„ÄÇ‰∏∫ÂáèÂ∞ëËøôÁßçÂÅèÂ•ΩÂèØËÉΩÂ∏¶Êù•ÁöÑ‰∏çÂà©ÂΩ±ÂìçÔºåC4.5ÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ï‰ΩøÁî® ‚ÄúÂ¢ûÁõäÁéá(gain ratio)‚Äù Êù•ÈÄâÊã©ÊúÄ‰ºòÂàíÂàÜÂ±ûÊÄß„ÄÇÈááÁî®‰∏éÂºè4.2Áõ∏ÂêåÁöÑÁ¨¶Âè∑Ë°®Á§∫ÔºåÂ¢ûÁõäÁéáÂÆö‰πâ‰∏∫
$$
\tag{4.3}
\text{Gain_ratio}(D, a) = \frac{\text{Gain}(D, a)}{\text{IV}(a)},
$$
ÂÖ∂‰∏≠
$$
\tag{4.4}
\text{IV}(a) = - \sum^V_{v=1} {|D^v| \over |D|} \text{log}_2 {|D^v| \over |D|}
$$
Áß∞‰∏∫Â±ûÊÄß $a$ ÁöÑ ‚ÄúÂõ∫ÊúâÂÄº(intrinsic value)‚Äù„ÄÇÂ±ûÊÄß $a$ ÁöÑÂèØËÉΩÂèñÂÄºÊï∞ÁõÆË∂äÂ§öÔºàÂç≥ $V$ Ë∂äÂ§ßÔºâÔºåÂàô ${\text{IV}(a)}$ ÁöÑÂÄº‰πüË∂äÂ§ß„ÄÇ&lt;/p>
&lt;p>Ê≥®ÔºöÂ¢ûÁõäÁéáÂáÜÂàôÂØπÂèØÂèñÂÄºÊï∞ÁõÆËæÉÂ∞ëÁöÑÂ±ûÊÄßÊúâÊâÄÂÅèÂ•ΩÔºåÂõ†Ê≠§ÔºåC4.5ÁÆóÊ≥ïÂπ∂‰∏çÊòØÁõ¥Êé•ÈÄâÊã©Â¢ûÁõäÁéáÊúÄÂ§ßÁöÑÂÄôÈÄâÂàíÂàÜÂ±ûÊÄßÔºåËÄåÊòØ‰ΩøÁî®‰∏Ä‰∏™&lt;strong>ÂêØÂèëÂºè&lt;/strong>ÔºöÂÖà‰ªéÂÄôÈÄâÂàíÂàÜÂ±ûÊÄß‰∏≠ÊâæÂá∫ &lt;em>‰ø°ÊÅØÂ¢ûÁõä&lt;/em> È´ò‰∫éÂπ≥ÂùáÊ∞¥Âπ≥ÁöÑÂ±ûÊÄßÔºåÂÜç‰ªé‰∏≠ÈÄâÊã© &lt;em>Â¢ûÁõäÁéá&lt;/em> ÊúÄÂ§ßÁöÑ„ÄÇ&lt;/p>
&lt;h4 id="Âü∫Â∞ºÊåáÊï∞" >
&lt;div>
&lt;a href="#%e5%9f%ba%e5%b0%bc%e6%8c%87%e6%95%b0">
###
&lt;/a>
Âü∫Â∞ºÊåáÊï∞
&lt;/div>
&lt;/h4>
&lt;p>CART(Classification and Regression Tree) ÂÜ≥Á≠ñÊ†ë‰ΩøÁî®Âü∫Â∞ºÊåáÊï∞ÔºàGini indexÔºâÊù•ÈÄâÊã©ÂàíÂàÜÂ±ûÊÄß„ÄÇÊï∞ÊçÆÈõÜDÁöÑÁ∫ØÂ∫¶ÂèØÁî®Âü∫Â∞ºÊåáÊï∞Êù•Â∫¶ÈáèÔºö
$$
\begin{eqnarray}
Gini(D)
&amp;amp;=&amp;amp; \sum^{|\mathcal{Y}|}&lt;em>{k=1} \sum&lt;/em>{k&amp;rsquo; \ne k} p_k p_{k&amp;rsquo;} \
&amp;amp;=&amp;amp; 1 - \sum^{|\mathcal{Y}|}&lt;em>{k=1} {p&lt;/em>{k}}^2.
\end{eqnarray}
$$
Áõ¥ËßÇÊù•ËØ¥Ôºå$Gini(D)$ÂèçÊò†‰∫Ü‰ªéÊï∞ÊçÆÈõÜ$D$‰∏≠ÈöèÊú∫ÊäΩÂèñ‰∏§‰∏™Ê†∑Êú¨ÔºåÂÖ∂Á±ªÂà´Ê†áËÆ∞‰∏ç‰∏ÄËá¥ÁöÑÊ¶ÇÁéá„ÄÇÂõ†Ê≠§$Gini(D)$Ë∂äÂ∞èÔºåÂàôÊï∞ÊçÆÈõÜ$D$ÁöÑÁ∫ØÂ∫¶Ë∂äÈ´ò„ÄÇ&lt;/p>
&lt;p>Â±ûÊÄß $a$ ÁöÑÂü∫Â∞ºÊåáÊï∞ÂÆö‰πâ‰∏∫
$$
\text{Gini_index}(D, a) = \sum^{V}&lt;em>{v=1} \frac{|D^v|}{|D|}Gini(D^v)
$$
‰∫éÊòØÔºåÊàë‰ª¨ÂÄôÈÄâÂ±ûÊÄßÈõÜÂêà$A$‰∏≠ÔºåÈÄâÊã©ÈÇ£‰∏™‰ΩøÂæóÂàíÂàÜÂêéÂü∫Â∞ºÊåáÊï∞ÊúÄÂ∞èÁöÑÂ±ûÊÄß‰Ωú‰∏∫ÊúÄ‰ºòÂàíÂàÜÂ±ûÊÄßÔºåÂç≥$a^* = argmin&lt;/em>{(a \in A)} \text{Gini_index}(D, a)$.&lt;/p>
&lt;blockquote>
&lt;p>„Äähands-on Machine Learning with sklearn, Keras and tensorflow„Äã&lt;/p>
&lt;p>The CART Training Algorithm&lt;/p>
&lt;p>&lt;strong>1. Classification Task&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Sklearn&lt;/strong> uses the CART algorithm to train Decision Tree (i.e., &amp;ldquo;growing&amp;rdquo; tree). The algorithm works by first splitting the training set into two subsets using a single feature $k$ and a threshold $t_k$ (e.g., &amp;ldquo;petal length $\le$ 2.45 cm&amp;rdquo; which is a feature in iris data). How does it choose $k$ and $t_k$ ? It searches for the pair ($k$, $t_k$) that produces the purest subsets (weighted by their size).
$$
\tag{6.2}
J(k, t_k) = {m_{left} \over m} G_{left} + {m_{right} \over m} G_{right}
$$
where&lt;/p>
&lt;ul>
&lt;li>$m_{left/right}$ is the number of instances in the left/right subset,&lt;/li>
&lt;li>$G_{left/right}$ measures the impurity of the left/right subset.&lt;/li>
&lt;/ul>
&lt;p>Equation 6.2 gives the cost function for classification task that the algorithm tries to minimize.&lt;/p>
&lt;p>Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (&lt;code>max_depth&lt;/code>), or if it cannot find a split that will reduce impurity. There are other additional stopping conditions hyperparameters such as &lt;code>min_samples_split&lt;/code>, &lt;code>min_samples_leaf&lt;/code>, &lt;code>min_weight_fraction_leaf&lt;/code>, and &lt;code>max_leaf_nodes&lt;/code>. Increasing &lt;code>min_*&lt;/code> hyperparameters or reducing &lt;code>max_*&lt;/code> hyperparameters will regularize the model.&lt;/p>
&lt;p>&lt;strong>2. Regresssion Task&lt;/strong>&lt;/p>
&lt;p>The CART algorithm works mostly the same as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE.
$$
\tag{6.2}
J(k, t_k) = {m_{left} \over m} \text{MSE}&lt;em>{left} + {m&lt;/em>{right} \over m} \text{MSE}_{right}
$$
where&lt;/p>
&lt;ul>
&lt;li>$MSE_{node} = \sum_{i \in node} (\hat{y}_{node} - y^{(i)})$ ,&lt;/li>
&lt;li>$\hat{y}&lt;em>{node} = {1 \over m&lt;/em>{node}} \sum_{i \in node} y^{(i)}$&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>3. Instability&lt;/strong>&lt;/p>
&lt;p>Decision Trees produce orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to trianing set rotation (The model on the right of figure 6-7 will not generalize well). Ony way to limit this problem is to use Principal Component Analysis (PCA), which often results in a better orientation of the training data.&lt;/p>
&lt;p>&lt;img alt="dtree instability" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_dtree_instability.png">&lt;/p>
&lt;p>More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Actually, since the training algorithm used by Sklearn is stochastic (means it randomly selects the set of features to evaluate at each node), it may produces very different models even on the same training data (unless you set the &lt;code>random_state&lt;/code> hyperparameter).&lt;/p>
&lt;/blockquote>
&lt;h3 id="Ââ™ÊûùÂ§ÑÁêÜ" >
&lt;div>
&lt;a href="#%e5%89%aa%e6%9e%9d%e5%a4%84%e7%90%86">
##
&lt;/a>
Ââ™ÊûùÂ§ÑÁêÜ
&lt;/div>
&lt;/h3>
&lt;p>Ââ™Êûù (pruning)ÊòØÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ÁÆóÊ≥ïÂØπ‰ªò ‚ÄúËøáÊãüÂêà‚Äù ÁöÑ‰∏ªË¶ÅÊâãÊÆµ„ÄÇÂú®ÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ËøáÁ®ã‰∏≠Ôºå‰∏∫‰∫ÜÂ∞ΩÂèØËÉΩÊ≠£Á°ÆÂàÜÁ±ªËÆ≠ÁªÉÊ†∑Êú¨ÔºåÁªìÁÇπÂàíÂàÜËøáÁ®ãÂ∞Ü‰∏çÊñ≠ÈáçÂ§çÔºåÊúâÊó∂‰ºöÈÄ†ÊàêÂÜ≥Á≠ñÊ†ëÂàÜÊîØËøáÂ§öÔºå‰ª•Ëá¥‰∫éÊääËÆ≠ÁªÉÈõÜËá™Ë∫´ÁöÑ‰∏Ä‰∫õÁâπÁÇπÂΩì‰ΩúÊâÄÊúâÊï∞ÊçÆÈÉΩÂÖ∑ÊúâÁöÑ‰∏ÄËà¨ÊÄßË¥®‰ªéËÄåÂØºËá¥ËøáÊãüÂêà„ÄÇÂõ†Ê≠§ÔºåÈÄöËøá‰∏ªÂä®ÂéªÊéâ‰∏Ä‰∫õÂàÜÊîØÊù•Èôç‰ΩéËøáÊãüÂêàÁöÑÈ£éÈô©„ÄÇ&lt;/p>
&lt;p>ÂÜ≥Á≠ñÊ†ëÂâ™ÊûùÁöÑÂü∫Êú¨Á≠ñÁï•Êúâ &lt;strong>È¢ÑÂâ™Êûù(prepruning)&lt;/strong> Âíå &lt;strong>ÂêéÂâ™Êûù(post-pruning)&lt;/strong> „ÄÇ&lt;/p>
&lt;p>È¢ÑÂâ™ÊûùÊòØÊåáÂú®ÂÜ≥Á≠ñÊ†ëÁîüÊàêËøáÁ®ã‰∏≠ÔºåÂØπÊØè‰∏™ÁªìÁÇπÂú®ÂàíÂàÜÂâçÂÖàËøõË°å‰º∞ËÆ°ÔºåËã•ÂΩìÂâçÁªìÁÇπÁöÑÂàíÂàÜ‰∏çËÉΩÂ∏¶Êù•ÂÜ≥Á≠ñÊ†ëÊ≥õÂåñÊÄßËÉΩÁöÑÊèêÂçáÔºåÂàôÂÅúÊ≠¢ÂàíÂàÜÔºåÂπ∂Â∞ÜÂΩìÂâçÁªìÁÇπÊ†áËÆ∞‰∏∫Âè∂ÁªìÁÇπ„ÄÇ&lt;/p>
&lt;p>ÂêéÂâ™ÊûùÂàôÊòØÂÖà‰ªéËÆ≠ÁªÉÈõÜÁîüÊàê‰∏ÄÊ£µÂÆåÊï¥ÁöÑÂÜ≥Á≠ñÊ†ëÔºåÁÑ∂ÂêéËá™Â∫ïÂêë‰∏äÂú∞ÂØπÈùûÂè∂ÁªìÁÇπËøõË°åËÄÉÂØüÔºåËã•Â∞ÜËØ•ÁªìÁÇπÂØπÂ∫îÁöÑÂ≠êÊ†ëÊõøÊç¢‰∏∫Âè∂ÁªìÁÇπËÉΩÂ∏¶Êù•ÂÜ≥Á≠ñÊ†ëÊ≥õÂåñÊÄßËÉΩÁöÑÊèêÂçáÔºåÂàôÂ∞ÜËØ•Â≠êÊ†ëÊõøÊç¢‰∏∫Âè∂ÁªìÁÇπ„ÄÇ&lt;/p>
&lt;p>&lt;strong>Â¶Ç‰ΩïÂà§Êñ≠ÂÜ≥Á≠ñÊ†ëÊ≥õÂåñÊÄßËÉΩÊòØÂê¶ÊèêÂçáÂë¢Ôºü&lt;/strong> ËøôÂèØ‰ª•‰ΩøÁî®Â∏∏Áî®ÁöÑÊÄßËÉΩËØÑ‰º∞ÊñπÊ≥ïËøõË°åÔºåÂ¶Ç ‚ÄúÁïôÂá∫Ê≥ï‚Äù„ÄÅ‚Äú‰∫§ÂèâÈ™åËØÅÊ≥ï‚Äù ‰ª•Âèä ‚ÄúËá™Âä©Ê≥ï‚Äù Á≠âÊñπÊ≥ï„ÄÇ&lt;/p>
&lt;p>È¢ÑÂâ™Êûù‰ºö‰ΩøÂæóÂÜ≥Á≠ñÊ†ëÁöÑÂæàÂ§öÂàÜÊîØÊ≤°Êúâ ‚ÄúÂ±ïÂºÄ‚ÄùÔºåËøô‰∏ç‰ªÖËÉΩÈôç‰ΩéËøáÊãüÂêàÁöÑÈ£éÈô©ÔºåËøò‰ºöÊòæËëóÂáèÂ∞ëËÆ≠ÁªÉÂíåÊµãËØïÁöÑÊó∂Èó¥ÂºÄÈîÄ„ÄÇ‰ΩÜÂè¶‰∏ÄÊñπÈù¢ÔºåÊúâ‰∫õÂàÜÊîØÁöÑÂΩìÂâçÂàíÂàÜËôΩ‰∏çËÉΩÊèêÂçáÊ≥õÂåñÊÄßËÉΩÔºàÁîöËá≥ÂèØËÉΩÂØºËá¥Ê≥õÂåñÊÄßËÉΩÊöÇÊó∂‰∏ãÈôçÔºâÔºå‰ΩÜÂú®ÂÖ∂Âü∫Á°Ä‰∏äËøõË°åÁöÑÂêéÁª≠ÂàíÂàÜÂç¥ÊúâÂèØËÉΩ‰ΩøÂæóÊ≥õÂåñÊÄßËÉΩÊòæËëóÊèêÈ´òÔºõÈ¢ÑÂâ™ÊûùÂü∫‰∫é ‚ÄúË¥™ÂøÉ‚Äù Êú¨Ë¥®Á¶ÅÊ≠¢Ëøô‰∫õÂàÜÊîØÂ±ïÂºÄÔºåËøôÁªôÈ¢ÑÂâ™ÊûùÂÜ≥Á≠ñÊ†ëÂ∏¶Êù•Ê¨†ÊãüÂêàÁöÑÈ£éÈô©„ÄÇ&lt;/p>
&lt;p>ÂêéÂâ™ÊûùÂÜ≥Á≠ñÊ†ëÈÄöÂ∏∏‰ºöÊØîÈ¢ÑÂâ™ÊûùÂÜ≥Á≠ñÊ†ë‰øùÁïôÊõ¥Â§öÁöÑÂàÜÊîØ„ÄÇ‰∏ÄËà¨ÊÉÖÂΩ¢‰∏ãÔºåÂêéÂâ™ÊûùÂÜ≥Á≠ñÊ†ëÊ¨†ÊãüÂêàÈ£éÈô©ÂæàÂ∞èÔºåÊ≥õÂåñÊÄßËÉΩÂæÄÂæÄ‰ºò‰∫éÈ¢ÑÂâ™ÊûùÂÜ≥Á≠ñÊ†ë„ÄÇ‰ΩÜÂêéÂâ™ÊûùÂÜ≥Á≠ñÊ†ëÁöÑËÆ≠ÁªÉÊó∂Èó¥ÂºÄÈîÄÂàôÂ§ßÂæóÂ§ö„ÄÇ&lt;/p>
&lt;h3 id="ËøûÁª≠ÂÄºÂ±ûÊÄßÂíåÁº∫Â§±ÂÄº" >
&lt;div>
&lt;a href="#%e8%bf%9e%e7%bb%ad%e5%80%bc%e5%b1%9e%e6%80%a7%e5%92%8c%e7%bc%ba%e5%a4%b1%e5%80%bc">
##
&lt;/a>
ËøûÁª≠ÂÄºÂ±ûÊÄßÂíåÁº∫Â§±ÂÄº
&lt;/div>
&lt;/h3>
&lt;p>&lt;strong>ËøûÁª≠ÂÄºÂ§ÑÁêÜ&lt;/strong>&lt;/p>
&lt;p>Áî±‰∫éËøûÁª≠Â±ûÊÄßÁöÑÂèØÂèñÂÄºÊï∞ÁõÆ‰∏çÂÜçÊúâÈôêÔºåÂõ†Ê≠§Ôºå‰∏çËÉΩÁõ¥Êé•Ê†πÊçÆËøûÁª≠Â±ûÊÄßÁöÑÂèØÂèñÂÄºÊù•ÂØπÁªìÁÇπËøõË°åÂàíÂàÜ„ÄÇÊ≠§Êó∂Ôºå&lt;strong>ËøûÁª≠Â±ûÊÄßÁ¶ªÊï£Âåñ&lt;/strong>ÊäÄÊúØÂèØÊ¥æ‰∏äÁî®Âú∫„ÄÇÊúÄÁÆÄÂçïÁöÑÁ≠ñÁï•ÊòØÈááÁî®‰∫åÂàÜÊ≥ï(bi-partition)ÂØπËøûÁª≠Â±ûÊÄßËøõË°åÂ§ÑÁêÜÔºåËøôÊ≠£ÊòØC4.5ÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ï‰∏≠ÈááÁî®ÁöÑÊú∫Âà∂„ÄÇ&lt;/p>
&lt;p>ÁªôÂÆöÊ†∑Êú¨ÈõÜ $D$ ÂíåËøûÁª≠Â±ûÊÄß $a$ÔºåÂÅáÂÆö $a$ Âú® $D$ ‰∏äÂá∫Áé∞‰∫Ü $n$ ‰∏™‰∏çÂêåÁöÑÂèñÂÄºÔºåÂ∞ÜËøô‰∫õÂèñÂÄº‰ªéÂ∞èÂà∞Â§ßËøõË°åÊéíÂ∫èÔºåËÆ∞‰∏∫ {$a^1, \ldots, a^n$}„ÄÇÂü∫‰∫éÂàíÂàÜÁÇπ $t$ ÂèØÂ∞Ü $D$ ÂàÜ‰∏∫Â≠êÈõÜ $D^-_t$ Âíå $D^+&lt;em>t$ ÔºåÂÖ∂‰∏≠ $D^-&lt;em>t$ ÂåÖÂê´Âì™‰∫õÂú®Â±ûÊÄß $a$ ‰∏äÂèñÂÄº‰∏çÂ§ß‰∫é $t$ ÁöÑÊ†∑Êú¨ÔºåËÄå $D^+&lt;em>t$ ÂàôÂåÖÂê´ÈÇ£‰∫õÂ§ß‰∫é $t$ ÁöÑÊ†∑Êú¨„ÄÇÊòæÁÑ∂ÔºåÂØπÁõ∏ÈÇªÁöÑÂ±ûÊÄßÂèñÂÄº $a^i$ ‰∏é $a^{i+1}$ Êù•ËØ¥Ôºå$t$ Âú®Âå∫Èó¥ [$a^i, a^{i+1}$) ‰∏≠Âèñ‰ªªÊÑèÂÄºÊâÄ‰∫ßÁîüÁöÑÂàíÂàÜÁªìÊûúÁõ∏Âêå„ÄÇÂõ†Ê≠§ÔºåÂØπËøûÁª≠Â±ûÊÄß $a$ÔºåÊàë‰ª¨ÂèØËÄÉÂØüÂåÖÂê´ $n - 1$ ‰∏™ÂÖÉÁ¥†ÁöÑÂÄôÈÄâÂàíÂàÜÁÇπÈõÜÂêà
$$
\tag{4.7}
T_a = \bigg{{a^i + a^{i+1} \over 2} | 1 \le i \le n-1 \bigg},
$$
Âç≥ÊääÂå∫Èó¥ [$a^i, a^{i+1}$) ÁöÑ‰∏≠‰ΩçÁÇπ‰Ωú‰∏∫ÂÄôÈÄâÂàíÂàÜÁÇπ&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>„ÄÇÁÑ∂ÂêéÊàë‰ª¨Â∞±ÂèØ‰ª•ÂÉèÁ¶ªÊï£Â±ûÊÄßÂÄº‰∏ÄÊ†∑Êù•ËÄÉËôëËøô‰∫õÂàíÂàÜÁÇπÔºåÈÄâÂèñÊúÄ‰ºòÁöÑÂàíÂàÜÁÇπËøõË°åÊ†∑Êú¨ÈõÜÂêàÁöÑÂàíÂàÜ„ÄÇ‰æãÂ¶ÇÔºåÂèØÂØπÂºè4.2Á®çÂä†ÊîπÈÄ†Ôºö
$$
\begin{eqnarray}
\text{Gain}(D, a)
&amp;amp;=&amp;amp; \text{max}&lt;/em>{(t \in T_a)} \text{Gain}(D, a, t) \
\tag{4.8}
&amp;amp;=&amp;amp; \text{max}&lt;/em>{(t \in T_a)} \text{Ent}(D) - \sum&lt;/em>{\lambda \in {-, + }} {|D^{\lambda}_t| \over |D|} \text{Ent}(D^{\lambda}_t),
\end{eqnarray}
$$
ÂÖ∂‰∏≠Ôºå$\text{Gain}(D, a, t)$ ÊòØÊ†∑Êú¨ÈõÜ $D$ Âü∫‰∫éÂàíÂàÜÁÇπ $t$ ‰∫åÂàÜÂêéÁöÑ‰ø°ÊÅØÂ¢ûÁõä„ÄÇ‰∫éÊòØÔºåÊàë‰ª¨Â∞±ÂèØÈÄâÊã©‰Ωø $\text{Gain}(D, a, t)$ ÊúÄÂ§ßÂåñÁöÑÂàíÂàÜÁÇπ„ÄÇ&lt;/p>
&lt;p>&lt;strong>Áº∫Â§±ÂÄºÂ§ÑÁêÜ&lt;/strong>&lt;/p>
&lt;p>Áé∞ÂÆû‰ªªÂä°‰∏≠Â∏∏‰ºöÈÅáÂà∞‰∏çÂÆåÊï¥Ê†∑Êú¨ÔºåÂç≥Ê†∑Êú¨ÁöÑÊüê‰∫õÂ±ûÊÄßÂÄºÁº∫Â§±„ÄÇÂú®Â±ûÊÄßÊï∞ÁõÆËæÉÂ§öÁöÑÊÉÖÂΩ¢‰∏ãÔºåÂæÄÂæÄ‰ºöÊúâÂ§ßÈáèÊ†∑Êú¨Âá∫Áé∞Áº∫Â§±ÂÄº„ÄÇÂ¶ÇÊûúÁÆÄÂçïÂú∞ÊîæÂºÉ‰∏çÂÆåÊï¥Ê†∑Êú¨Ôºå‰ªÖ‰ΩøÁî®Êó†Áº∫Â§±ÂÄºÁöÑÊ†∑Êú¨ËøõË°åÂ≠¶‰π†ÔºåÊòæÁÑ∂ÊòØÂØπÊï∞ÊçÆ‰ø°ÊÅØÁöÑÊûÅÂ§ßÊµ™Ë¥π„ÄÇ&lt;/p>
&lt;p>Êàë‰ª¨ÈúÄË¶ÅËß£ÂÜ≥‰∏§‰∏™ÈóÆÈ¢òÔºö&lt;/p>
&lt;ol>
&lt;li>Â¶Ç‰ΩïÂú®Â±ûÊÄßÂÄºÁº∫Â§±ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÂàíÂàÜÂ±ûÊÄßÁöÑÈÄâÊã©Ôºü&lt;/li>
&lt;li>ÁªôÂÆöÂàíÂàÜÂ±ûÊÄßÔºåÂ¶ÇÊûúÊ†∑Êú¨Âú®ËØ•Â±ûÊÄß‰∏äÁöÑÂÄºÁº∫Â§±ÔºåÂ¶Ç‰ΩïÂØπÊ†∑Êú¨ËøõË°åÂàíÂàÜÔºü&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>The real handling approaches to missing data does not use data point with missing values in the evaluation of a split. However, when child nodes are created and trained, those instances are distributed somehow.&lt;/p>
&lt;p>I know about the following approaches to distribute the missing value instances to child nodes:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>simply ignoring the missing values (like ID3 and other old algorithms does) or treating the missing values as another category (in case of a nominal feature). Those approachs were used in the early stages of decision tree development.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>all goes to the node which already has the biggest number of instances (CART, but not its primary rule)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>distribute to all children, but with diminished weights, proportional with the number of instances from each child node (C4.5 and others)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>distribute randomly to only one single child node, eventually according with a categorical distribution (various implementations of C4.5 and CART for faster funing time)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>build, sort and use surrogates to distribute instances to a child node, where surrogates are input features which resembles best how the test feature send data instances to left or right child node (CART, if that fails, the majority rule is used)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This answer was copied from &lt;a href="https://stats.stackexchange.com/questions/96025/how-do-decision-tree-learning-algorithms-deal-with-missing-values-under-the-hoo">here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="Â§öÂèòÈáèÂÜ≥Á≠ñÊ†ë" >
&lt;div>
&lt;a href="#%e5%a4%9a%e5%8f%98%e9%87%8f%e5%86%b3%e7%ad%96%e6%a0%91">
##
&lt;/a>
Â§öÂèòÈáèÂÜ≥Á≠ñÊ†ë
&lt;/div>
&lt;/h3>
&lt;p>Â¶ÇÊûúÊàë‰ª¨ÊääÊØè‰∏™Â±ûÊÄßËßÜ‰∏∫ÂùêÊ†áÁ©∫Èó¥‰∏≠ÁöÑ‰∏Ä‰∏™ÂùêÊ†áËΩ¥ÔºåÂàô $d$ ‰∏™Â±ûÊÄßÊèèËø∞ÁöÑÊ†∑Êú¨Â∞±ÂØπÂ∫î‰∫Ü $d$ Áª¥Á©∫Èó¥‰∏≠ÁöÑ‰∏Ä‰∏™Êï∞ÊçÆÁÇπÔºåÂØπÊ†∑Êú¨ÂàÜÁ±ªÊÑèÂë≥ÁùÄÂú®Ëøô‰∏™ÂùêÊ†áÁ©∫Èó¥‰∏≠ÂØªÊâæ‰∏çÂêåÊ†∑Êú¨‰πãÈó¥ÁöÑÂàÜÁ±ªËæπÁïå„ÄÇ&lt;/p>
&lt;p>ÂÜ≥Á≠ñÊ†ëÊâÄÂΩ¢ÊàêÁöÑÂàÜÁ±ªËæπÁïåÊúâ‰∏Ä‰∏™ÊòéÊòæÁöÑÁâπÁÇπÔºöËΩ¥Âπ≥Ë°å(axis-parallel)ÔºåÂç≥ÂÆÉÁöÑÂàÜÁ±ªËæπÁïåÁî±Ëã•Âπ≤‰∏™‰∏éÂùêÊ†áËΩ¥Âπ≥Ë°åÁöÑÂàÜÊÆµÁªÑÊàê„ÄÇ&lt;/p>
&lt;p>&lt;img alt="data3a" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree_dataset3a.png">&lt;/p>
&lt;p>‰ª•Ë°®4.5‰∏≠ÁöÑË•øÁìúÊï∞ÊçÆ$3.0 \alpha$‰∏∫‰æãÔºåÂ∞ÜÂÆÉ‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜÂ≠¶‰π†ÂæóÂõæ4.10ÊâÄÁ§∫ÁöÑÂÜ≥Á≠ñÊ†ëÔºåÂÖ∂ÂàÜÁ±ªËæπÁïåÂ¶ÇÂõæ4.11ÊâÄÁ§∫„ÄÇ&lt;/p>
&lt;p>&lt;img alt="tree3a" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree3a2.png">&lt;/p>
&lt;p>ÊòæÁÑ∂ÔºåÂàÜÁ±ªËæπÁïåÁöÑÊØè‰∏ÄÊÆµÈÉΩÊòØ‰∏éÂùêÊ†áËΩ¥Âπ≥Ë°åÁöÑ„ÄÇËøôÊ†∑ÁöÑÂàÜÁ±ªËæπÁïå‰ΩøÂæóÂ≠¶‰π†ÁªìÊûúÊúâËæÉÂ•ΩÁöÑÂèØËß£ÈáäÊÄßÔºåÂõ†‰∏∫ÊØè‰∏ÄÊÆµÂàíÂàÜÈÉΩÁõ¥Êé•ÂØπÂ∫î‰∫ÜÊüê‰∏™Â±ûÊÄßÂèñÂÄº„ÄÇ‰ΩÜÂú®Â≠¶‰π†‰ªªÂä°ÁöÑÁúüÂÆûÂàÜÁ±ªËæπÁïåÊØîËæÉÂ§çÊùÇÊó∂ÔºåÂøÖÈ°ª‰ΩøÁî®ÂæàÂ§öÊÆµÂàíÂàÜÊâçËÉΩËé∑ÂæóËæÉÂ•ΩÁöÑËøë‰ººÔºåÂ¶ÇÂõæ4.12ÊâÄÁ§∫ÔºõÊ≠§Êó∂ÁöÑÂÜ≥Á≠ñÊ†ë‰ºöÁõ∏ÂΩìÂ§çÊùÇÔºåÁî±‰∫éÈúÄË¶ÅËøõË°åÂ§ßÈáèÂ±ûÊÄßÊµãËØïÔºåÈ¢ÑÊµãÊó∂Èó¥ÂºÄÈîÄ‰ºöÂæàÂ§ß„ÄÇ&lt;/p>
&lt;p>&lt;img alt="tree3a3" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree3a3.png">&lt;/p>
&lt;p>Â¶ÇÊûúËÉΩÂ§ü‰ΩøÁî®ÊñúÁöÑÂàíÂàÜËæπÁïåÔºåÂ¶ÇÂõæ4.12‰∏≠ÁöÑÁ∫¢Ëâ≤Á∫øÊÆµÊâÄÁ§∫ÔºåÂàôÂÜ≥Á≠ñÊ†ëÊ®°ÂûãÂ∞ÜÂ§ß‰∏∫ÁÆÄÂåñ„ÄÇ&lt;/p>
&lt;p>&lt;strong>‚ÄúÂ§öÂèòÈáèÂÜ≥Á≠ñÊ†ë‚Äù(multivariate decision tree)&lt;/strong> Â∞±ÊòØËÉΩÂÆûÁé∞ËøôÊ†∑ÁöÑ ‚ÄúÊñúÂàíÂàÜ‚Äù ÁîöËá≥Êõ¥Â§çÊùÇÂàíÂàÜÁöÑÂÜ≥Á≠ñÊ†ë„ÄÇ‰ª•ÂÆûÁé∞ÊñúÂàíÂàÜÁöÑÂ§öÂèòÈáèÂÜ≥Á≠ñÊ†ë‰∏∫‰æãÔºåÂú®Ê≠§Á±ªÂÜ≥Á≠ñÊ†ë‰∏≠ÔºåÈùûÂè∂ÁªìÁÇπ‰∏çÂÜçÊòØ‰ªÖÂØπÊüê‰∏™Â±ûÊÄßÔºåËÄåÊòØÂØπÂ±ûÊÄßÁöÑÁ∫øÊÄßÁªÑÂêàËøõË°åÊµãËØïÔºõÊç¢Ë®Ä‰πãÔºåÊØè‰∏™ÈùûÂè∂ÁªìÁÇπÊòØ‰∏Ä‰∏™ÂΩ¢Â¶Ç $\sum^d_{i=1} w_i a_i = t$ ÁöÑÁ∫øÊÄßÂàÜÁ±ªÂô®ÔºåÂÖ∂‰∏≠ $w_i$ ÊòØÂ±ûÊÄß $a_i$ ÁöÑÊùÉÈáçÔºå $w_i$ Âíå $t$ ÂèØ‰ª•Âú®ËØ•ÁªìÁÇπÊâÄÂê´ÁöÑÊ†∑Êú¨ÈõÜÂíåÂ±ûÊÄßÈõÜ‰∏äÂ≠¶Âæó&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>„ÄÇ‰∫éÊòØÔºå‰∏é‰º†ÁªüÁöÑ ‚ÄúÂçïÂèòÈáèÂÜ≥Á≠ñÊ†ë(univariate decision tree)‚Äù ‰∏çÂêåÔºåÂú®Â§öÂèòÈáèÂÜ≥Á≠ñÊ†ëÁöÑÂ≠¶‰π†ËøáÁ®ã‰∏≠Ôºå‰∏çÊòØ‰∏∫ÊØè‰∏™ÈùûÂè∂ÁªìÁÇπÂØªÊâæ‰∏Ä‰∏™ÊúÄ‰ºòÂàíÂàÜÂ±ûÊÄßÔºåËÄåÊòØËØïÂõæÂª∫Á´ã‰∏Ä‰∏™ÂêàÈÄÇÁöÑÁ∫øÊÄßÂàÜÁ±ªÂô®„ÄÇ&lt;/p>
&lt;p>‰æãÂ¶ÇÂØπË•øÁìúÊï∞ÊçÆ$3.0 \alpha$ÔºåÊàë‰ª¨ÂèØ‰ª•Â≠¶ÂæóÂõæ4.13ËøôÊ†∑ÁöÑÂ§öÂèòÈáèÂÜ≥Á≠ñÊ†ëÔºåÂÖ∂ÂàÜÁ±ªËæπÁïåÂ¶ÇÂõæ4.14ÊâÄÁ§∫„ÄÇ&lt;/p>
&lt;p>&lt;img alt="tree3a4" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree3a4.png">&lt;/p>
&lt;h3 id="ÈòÖËØªÊùêÊñô" >
&lt;div>
&lt;a href="#%e9%98%85%e8%af%bb%e6%9d%90%e6%96%99">
##
&lt;/a>
ÈòÖËØªÊùêÊñô
&lt;/div>
&lt;/h3>
&lt;p>Âú®&lt;strong>‰ø°ÊÅØÂ¢ûÁõä„ÄÅÂ¢ûÁõäÁéá„ÄÅÂü∫Â∞ºÊåáÊï∞&lt;/strong>‰πãÂ§ñÔºå‰∫∫‰ª¨ËøòËÆæËÆ°‰∫ÜËÆ∏Â§öÂÖ∂‰ªñÁöÑÂáÜÂàôÁî®‰∫éÂÜ≥Á≠ñÊ†ëÂàíÂàÜÈÄâÊã©ÔºåÁÑ∂ËÄåÊúâÂÆûÈ™åÁ†îÁ©∂Ë°®Êòé&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>ÔºåËøô‰∫õÂáÜÂàôËôΩÁÑ∂ÂØπÂÜ≥Á≠ñÊ†ëÁöÑÂ∞∫ÂØ∏ÊúâËæÉÂ§ßÂΩ±ÂìçÔºå‰ΩÜÂØπÊ≥õÂåñÊÄßËÉΩÁöÑÂΩ±ÂìçÂæàÊúâÈôêÔºõÂØπ‰ø°ÊÅØÂ¢ûÁõäÂíåÂü∫Â∞ºÊåáÊï∞ËøõË°åÁöÑÁêÜËÆ∫ÂàÜÊûê&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>‰πüÊòæÁ§∫Âá∫ÔºåÂÆÉ‰ª¨‰ªÖÂú® $2%$ ÁöÑÊÉÖÂÜµ‰∏ã‰ºöÊúâÊâÄ‰∏çÂêå„ÄÇËÄåÂâ™ÊûùÊñπÊ≥ïÂíåÂâ™ÊûùÁ®ãÂ∫¶ÂØπÂÜ≥Á≠ñÊ†ëÁöÑÊ≥õÂåñÊÄßËÉΩÂΩ±ÂìçÊòæËëóÔºåÊúâÂÆûÈ™åÁ†îÁ©∂&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>Ë°®ÊòéÔºåÂú®Êï∞ÊçÆÂ∏¶ÊúâÂô™Â£∞Êó∂ÔºåÈÄöËøáÂâ™ÊûùÁîöËá≥ÂèØÂ∞ÜÂÜ≥Á≠ñÊ†ëÁöÑÊ≥õÂåñÊÄßËÉΩÊèêÈ´ò $25%$„ÄÇ&lt;/p>
&lt;p>Â§öÂèòÈáèÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ï‰∏ªË¶ÅÊúâ $OC1$&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>Ôºå$OC1$ ÁÆóÊ≥ïÂÖàË¥™ÂøÉÂú∞ÂØªÊâæÊØè‰∏™Â±ûÊÄßÁöÑÊúÄ‰ºòÊùÉÂÄºÔºåÂú®Â±ÄÈÉ®‰ºòÂåñÁöÑÂü∫Á°Ä‰∏äÂÜçÂØπÂàÜÁ±ªËæπÁïåËøõË°åÈöèÊú∫Êâ∞Âä®‰ª•ËØïÂõæÊâæÂà∞Êõ¥Â•ΩÁöÑËæπÁïåÔºõBrodley and Utgoff&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> ÂàôÁõ¥Êé•ÂºïÂÖ•‰∫ÜÁ∫øÊÄßÂàÜÁ±ªÂô®Â≠¶‰π†ÁöÑÊúÄÂ∞è‰∫å‰πòÊ≥ï„ÄÇËøòÊúâ‰∏Ä‰∫õÁÆóÊ≥ïËØïÂõæÂú®ÂÜ≥Á≠ñÊ†ëÁöÑÂè∂ÁªìÁÇπ‰∏äÂµåÂÖ•Á•ûÁªèÁΩëÁªúÔºå‰ª•ÁªìÂêàËøô‰∏§ÁßçÂ≠¶‰π†Êú∫Âà∂ÁöÑ‰ºòÂäøÔºå‰æãÂ¶Ç ‚ÄúÊÑüÁü•Êú∫Ê†ë(Perceptron tree)‚Äù&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup> Âú®ÊØè‰∏™Âè∂ÁªìÁÇπ‰∏äËÆ≠ÁªÉ‰∏Ä‰∏™ÊÑüÁü•Êú∫Ôºå‰πüÊúâÁõ¥Êé•Âú®Âè∂ÁªìÁÇπ‰∏äÂµåÂÖ•Â§öÂ±ÇÁ•ûÁªèÁΩëÁªúÁöÑÊ®°Âûã&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>„ÄÇ&lt;/p>
&lt;p>Êúâ‰∏Ä‰∫õÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ÁÆóÊ≥ïÂèØËøõË°å ‚ÄúÂ¢ûÈáèÂ≠¶‰π†(incremental learning)‚ÄùÔºåÂç≥Âú®Êé•Êî∂Âà∞Êñ∞Ê†∑Êú¨ÂêéÂèØÂØπÂ∑≤Â≠¶ÂæóÁöÑÊ®°ÂûãËøõË°åË∞ÉÊï¥ÔºåËÄå‰∏çÁî®ÂÆåÂÖ®ÈáçÊñ∞Â≠¶‰π†„ÄÇ‰∏ªË¶ÅÊú∫Âà∂ÊòØÈÄöËøáË∞ÉÊï¥ÂàÜÊîØË∑ØÂæÑ‰∏äÁöÑÂàíÂàÜÂ±ûÊÄßÊ¨°Â∫èÊù•ÂØπÊ†ëËøõË°åÈÉ®ÂàÜÈáçÊûÑÔºå‰ª£Ë°®ÊÄßÁÆóÊ≥ïÊúâID4&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>„ÄÅID5R&lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>„ÄÅITI&lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>Á≠â„ÄÇÂ¢ûÈáèÂ≠¶‰π†ÂèØÊúâÊïàÈôç‰ΩéÊØèÊ¨°Êé•Êî∂Âà∞Êñ∞Ê†∑Êú¨ÂêéÁöÑËÆ≠ÁªÉÊó∂Èó¥ÂºÄÈîÄÔºå‰ΩÜÂ§öÊ≠•Â¢ûÈáèÂ≠¶‰π†ÂêéÁöÑÊ®°Âûã‰ºö‰∏éÂü∫‰∫éÂÖ®ÈÉ®Êï∞ÊçÆËÆ≠ÁªÉËÄåÂæóÁöÑÊ®°ÂûãÊúâËæÉÂ§ßÂ∑ÆÂà´„ÄÇ&lt;/p>
&lt;h2 id="ÈõÜÊàêÂ≠¶‰π†ensemble" >
&lt;div>
&lt;a href="#%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0ensemble">
#
&lt;/a>
ÈõÜÊàêÂ≠¶‰π†(Ensemble)
&lt;/div>
&lt;/h2>
&lt;p>Ê†πÊçÆ‰∏™‰ΩìÂ≠¶‰π†Âô®ÁöÑÁîüÊàêÊñπÂºèÔºåÁõÆÂâçÈõÜÊàêÂ≠¶‰π†&lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>ÊñπÊ≥ïÂ§ßËá¥ÂèØÂàÜ‰∏∫‰∏§Â§ßÁ±ªÔºå&lt;/p>
&lt;ul>
&lt;li>‰∏™‰ΩìÂ≠¶‰π†Âô®‰πãÈó¥Â≠òÂú®Âº∫‰æùËµñÂÖ≥Á≥ª„ÄÅÂøÖÈ°ª‰∏≤Ë°åÁîüÊàêÁöÑÂ∫èÂàóÂåñÊñπÊ≥ïÔºå‰ª£Ë°®ÁÆóÊ≥ïBoosting&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>;&lt;/li>
&lt;li>‰∏™‰ΩìÂ≠¶‰π†Âô®‰πãÈó¥‰∏çÂ≠òÂú®Âº∫‰æùËµñÂÖ≥Á≥ª„ÄÅÂèØÂêåÊó∂ÁîüÊàêÁöÑÂπ∂Ë°åÂåñÊñπÊ≥ïÔºå‰ª£Ë°®ÁÆóÊ≥ïBaggingÂíå ‚ÄúÈöèÊú∫Ê£ÆÊûó‚Äù.&lt;/li>
&lt;/ul>
&lt;p>ÊÉ≥Ë¶ÅÂæóÂà∞Ê≥õÂåñÊÄßËÉΩÂº∫ÁöÑÈõÜÊàêÔºåÈõÜÊàê‰∏≠ÁöÑ‰∏™‰ΩìÂ≠¶‰π†Âô®Â∫îÂ∞ΩÂèØËÉΩÁõ∏‰∫íÁã¨Á´ãÔºõËôΩÁÑ∂ ‚ÄúÁã¨Á´ã‚Äù Âú®Áé∞ÂÆû‰ªªÂä°‰∏≠Êó†Ê≥ïÂÅöÂà∞Ôºå‰ΩÜÂèØ‰ª•ËÆæÊ≥ï‰ΩøÂü∫Â≠¶‰π†Âô®Â∞ΩÂèØËÉΩÂÖ∑ÊúâËæÉÂ§ßÁöÑÂ∑ÆÂºÇ„ÄÇ&lt;/p>
&lt;p>ÁªôÂÆö‰∏Ä‰∏™ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºå‰∏ÄÁßçÂèØËÉΩÁöÑÂÅöÊ≥ïÊòØÂØπËÆ≠ÁªÉÊ†∑Êú¨ËøõË°åÈááÊ†∑Ôºå‰∫ßÁîüÂá∫Ëã•Âπ≤‰∏™‰∏çÂêåÁöÑÂ≠êÈõÜÔºåÂÜç‰ªéÊØè‰∏™Êï∞ÊçÆÂ≠êÈõÜ‰∏≠ËÆ≠ÁªÉÂá∫‰∏Ä‰∏™Âü∫Â≠¶‰π†Âô®„ÄÇËøôÊ†∑ÔºåÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏çÂêåÔºåÊàë‰ª¨Ëé∑ÂæóÁöÑÂü∫Â≠¶‰π†Âô®ÂèØÊúõÂÖ∑ÊúâÊØîËæÉÂ§ßÁöÑÂ∑ÆÂºÇ„ÄÇÁÑ∂ËÄåÔºå‰∏∫Ëé∑ÂæóÂ•ΩÁöÑÈõÜÊàêÔºåÊàë‰ª¨ÂêåÊó∂ËøòÂ∏åÊúõ‰∏™‰ΩìÂ≠¶‰π†Âô®‰∏çËÉΩÂ§™Â∑Æ„ÄÇÂ¶ÇÊûúÈááÊ†∑Âá∫ÁöÑÊØè‰∏™Â≠êÈõÜÈÉΩÂÆåÂÖ®‰∏çÂêåÔºåÂàôÊÑèÂë≥ÁùÄÊØè‰∏™Âü∫Â≠¶‰π†Âô®Âè™Áî®Âà∞‰∫Ü‰∏ÄÂ∞èÈÉ®ÂàÜËÆ≠ÁªÉÊï∞ÊçÆÔºåÁîöËá≥ÂèØËÉΩ‰∏çË∂≥‰ª•ËøõË°åÊúâÊïàÂ≠¶‰π†ÔºåËøôÂ∞±Êó†Ê≥ï‰øùËØÅ‰∫ßÂá∫ÊØîËæÉÂ•ΩÁöÑÂü∫Â≠¶‰π†Âô®„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂèØËÄÉËôë‰ΩøÁî®‰∫íÁõ∏Êúâ‰∫§Âè†ÁöÑÈááÊ†∑Â≠êÈõÜ„ÄÇ&lt;/p>
&lt;p>&lt;img alt="hard voting classifier" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_majority_vote.png">&lt;/p>
&lt;p>&lt;em>Figure 7-2. Hard voting classifier predictions. Copy from the book„Äähands-on Machine Learning with sklearn, Keras and tensorflow„Äã&lt;/em>&lt;/p>
&lt;h3 id="bagging" >
&lt;div>
&lt;a href="#bagging">
##
&lt;/a>
BAGGING
&lt;/div>
&lt;/h3>
&lt;p>Bagging (Bootstrap AGGregatING) ÊòØÂπ∂Ë°åÂºèÈõÜÊàêÂ≠¶‰π†ÊñπÊ≥ïÊúÄËëóÂêçÁöÑ‰ª£Ë°®„ÄÇ‰ªéÂêçÂ≠óÂç≥ÂèØÁúãÂá∫ÔºåÂÆÉÁõ¥Êé•Âü∫‰∫éËá™Âä©ÈááÊ†∑Ê≥ï&lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨Áü•ÈÅìÂàùÂßãËÆ≠ÁªÉÈõÜ‰∏≠Á∫¶Êúâ $63.2%$ ÁöÑÊ†∑Êú¨Âá∫Áé∞Âú®ÈááÊ†∑ÈõÜ‰∏≠„ÄÇÊàë‰ª¨ÂèØ‰ª•ÈááÊ†∑Âá∫ $T$ ‰∏™Âê´ $m$ ‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÈááÊ†∑ÈõÜÔºåÁÑ∂ÂêéÂü∫‰∫éÊØè‰∏™ÈááÊ†∑ÈõÜËÆ≠ÁªÉÂá∫‰∏Ä‰∏™Âü∫Â≠¶‰π†Âô®ÔºåÂÜçÂ∞ÜËøô‰∫õÂü∫Â≠¶‰π†Âô®ËøõË°åÁªìÂêà„ÄÇËøôÂ∞±ÊòØBaggingÁöÑÂü∫Êú¨ÊµÅÁ®ã„ÄÇ&lt;/p>
&lt;p>Âú®ÂØπÈ¢ÑÊµãËæìÂá∫ËøõË°åÁªìÂêàÊó∂ÔºåBaggingÈÄöÂ∏∏ÂØπÂàÜÁ±ª‰ªªÂä°‰ΩøÁî®ÁÆÄÂçïÊäïÁ•®Ê≥ïÔºåÂØπÂõûÂΩí‰ªªÂä°ÈááÁî®ÁÆÄÂçïÂπ≥ÂùáÊ≥ï„ÄÇ&lt;/p>
&lt;hr>
&lt;p>Bagging ÁÆóÊ≥ï&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>: ËÆ≠ÁªÉÈõÜ $D = {(x_1, y_1), \ldots, (x_m, y_m) }$;&lt;/p>
&lt;p>‚Äã Âü∫Â≠¶‰π†ÁÆóÊ≥ï $\mathcal{L}$;&lt;/p>
&lt;p>‚Äã ËÆ≠ÁªÉËΩÆÊï∞ $T$.&lt;/p>
&lt;p>&lt;strong>ËøáÁ®ã&lt;/strong>Ôºö&lt;/p>
&lt;p>1: &lt;strong>for&lt;/strong> $t = 1, \ldots, T$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>2: $h_t = \mathcal{L} (D, D_{bs})$&lt;/p>
&lt;p>3: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>: $H(x) = \text{argmax}&lt;em>{(y \in \mathcal{Y})} \sum^T&lt;/em>{t=1} \mathbf{I}(h_t(x) = y)$&lt;/p>
&lt;hr>
&lt;p>ÂÖ∂‰∏≠Ôºå$D_{bs}$ ÊòØËá™Âä©ÈááÊ†∑‰∫ßÁîüÁöÑÊ†∑Êú¨ÂàÜÂ∏É„ÄÇ&lt;/p>
&lt;p>‰ªéÂÅèÂ∑Æ-ÊñπÂ∑ÆÂàÜËß£ÁöÑËßíÂ∫¶ÁúãÔºåBagging‰∏ªË¶ÅÂÖ≥Ê≥®Èôç‰ΩéÊñπÂ∑ÆÔºåÂõ†Ê≠§Âõ†Ê≠§ÂÆÉÂú®‰∏çÂâ™ÊûùÂÜ≥Á≠ñÊ†ë„ÄÅÁ•ûÁªèÁΩëÁªúÁ≠âÊòìÂèóÂà∞Ê†∑Êú¨Êâ∞Âä®ÁöÑÂ≠¶‰π†Âô®‰∏äÊïàÁî®Êõ¥‰∏∫ÊòéÊòæ„ÄÇ&lt;/p>
&lt;blockquote>
&lt;p>„Äähands-on Machine Learning with sklearn, Keras and tensorflow„Äã&lt;/p>
&lt;p>&lt;strong>Bagging and Pasting&lt;/strong>&lt;/p>
&lt;p>One way to get a diverse set of classifiers is to use very different training algorithms (such as SVMs, LR, DTs etc). Another approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed &lt;strong>with replacement&lt;/strong>, this method is called &lt;strong>bagging&lt;/strong>, when sampling is preformed &lt;strong>without replacement&lt;/strong>, it is called &lt;strong>pasting&lt;/strong>.&lt;/p>
&lt;p>In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictiors, but only bagging allows training instances to be sampled several times for the same predictor. This sampling and training process is represented in Figure 7-4.&lt;/p>
&lt;p>&lt;img alt="bagging pasting" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_bagging_pasting.png">&lt;/p>
&lt;p>Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions all predictors. The aggregation function is typically&lt;/p>
&lt;ul>
&lt;li>the &lt;em>statistical mode&lt;/em>&lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup> for classification, or&lt;/li>
&lt;li>the &lt;em>statistical average&lt;/em> for regression.&lt;/li>
&lt;/ul>
&lt;p>Generally, the net result is that the ensemble has a similar bias but a lower variance than single predictor trained on the original train set.&lt;/p>
&lt;/blockquote>
&lt;h4 id="ÈöèÊú∫Ê£ÆÊûó-rf" >
&lt;div>
&lt;a href="#%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97-rf">
###
&lt;/a>
ÈöèÊú∫Ê£ÆÊûó RF
&lt;/div>
&lt;/h4>
&lt;p>ÈöèÊú∫Ê£ÆÊûóÊòØBaggingÁöÑ‰∏Ä‰∏™Êâ©Â±ïÂèò‰Ωì„ÄÇRFÂú®‰ª•ÂÜ≥Á≠ñÊ†ë‰∏∫Âü∫Â≠¶‰π†Âô®ÊûÑÂª∫BaggingÈõÜÊàêÁöÑÂü∫Á°Ä‰∏äÔºåËøõ‰∏ÄÊ≠•Âú®ÂÜ≥Á≠ñÊ†ëÁöÑËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂºïÂÖ•‰∫ÜÈöèÊú∫Â±ûÊÄßÈÄâÊã©„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰º†ÁªüÂÜ≥Á≠ñÊ†ëÂú®ÈÄâÊã©ÂàíÂàÜÂ±ûÊÄßÊó∂ÊòØÂú®ÂΩìÂâçÁªìÁÇπÁöÑÂ±ûÊÄßÈõÜÔºàÂÅáÂÆöÊúâ $d$ ‰∏™Â±ûÊÄßÔºâ‰∏≠ÈÄâÊã©‰∏Ä‰∏™ÊúÄ‰ºòÂ±ûÊÄßÔºõËÄåÂú® RF ‰∏≠ÔºåÂØπÂü∫ÂÜ≥Á≠ñÊ†ëÁöÑÊØè‰∏™ÁªìÁÇπÔºåÂÖà‰ªéËØ•ÁªìÁÇπÁöÑÂ±ûÊÄßÈõÜ‰∏≠ÈöèÊú∫ÈÄâÊã©‰∏Ä‰∏™ÂåÖÂê´ $k$ ‰∏™Â±ûÊÄßÁöÑÂ≠êÈõÜÔºåÁÑ∂ÂêéÂÜç‰ªéËøô‰∏™Â≠êÈõÜ‰∏≠ÈÄâÊã©‰∏Ä‰∏™ÊúÄ‰ºòÂ±ûÊÄßÁî®‰∫éÂàíÂàÜ„ÄÇËøôÈáåÁöÑÂèÇÊï∞ $k$ ÊéßÂà∂‰∫ÜÈöèÊú∫ÊÄßÁöÑÂºïÂÖ•Á®ãÂ∫¶Ôºö&lt;/p>
&lt;ul>
&lt;li>$k = d$, ÂàôÂü∫ÂÜ≥Á≠ñÊ†ëÁöÑÊûÑÂª∫‰∏é‰º†ÁªüÂÜ≥Á≠ñÊ†ëÁõ∏ÂêåÔºõ&lt;/li>
&lt;li>$k = 1$, ÂàôÊòØÈöèÊú∫ÈÄâÊã©‰∏Ä‰∏™Â±ûÊÄßÁî®‰∫éÂàíÂàÜÔºõ‰∏ÄËà¨Êé®Ëçê $k = \text{log}_2 d$.&lt;/li>
&lt;/ul>
&lt;p>ÂèØ‰ª•ÁúãÂá∫ÔºåÈöèÊú∫Ê£ÆÊûóÂØπBaggingÂè™ÂÅö‰∫ÜÂ∞èÊîπÂä®Ôºå‰ΩÜÊòØ‰∏éBagging‰∏≠Âü∫Â≠¶‰π†Âô®ÁöÑ ‚ÄúÂ§öÊ†∑ÊÄß‚Äù ‰ªÖÈÄöËøáÊ†∑Êú¨Êâ∞Âä®ËÄåÊù•‰∏çÂêåÔºåÈöèÊú∫Ê£ÆÊûó‰∏≠Âü∫Â≠¶‰π†Âô®ÁöÑÂ§öÊ†∑ÊÄß‰∏ç‰ªÖÊù•Ëá™Ê†∑Êú¨Êâ∞Âä®ÔºåËøòÊù•Ëá™Â±ûÊÄßÊâ∞Âä®ÔºåËøôÂ∞±‰ΩøÂæóÊúÄÁªàÈõÜÊàêÁöÑÊ≥õÂåñÊÄßËÉΩÂèØÈÄöËøá‰∏™‰ΩìÂ≠¶‰π†Âô®‰πãÈó¥Â∑ÆÂºÇÁ®ãÂ∫¶ÁöÑÂ¢ûÂä†ËÄåËøõ‰∏ÄÊ≠•ÊèêÂçá„ÄÇ&lt;/p>
&lt;p>ÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåÈöèÊú∫Ê£ÆÊûóÁöÑËÆ≠ÁªÉÊïàÁéáÈÄöÂ∏∏‰ºò‰∫éBaggingÔºåÂõ†‰∏∫Âú®‰∏™‰ΩìÂÜ≥Á≠ñÊ†ëÁöÑÊûÑÂª∫ËøáÁ®ã‰∏≠ÔºåBagging ‰ΩøÁî®ÁöÑÊòØ ‚ÄúÁ°ÆÂÆöÂûã‚Äù ÂÜ≥Á≠ñÊ†ëÔºåÂú®ÈÄâÊã©ÂàíÂàÜÂ±ûÊÄßÊó∂Ë¶ÅÂØπÁªìÁÇπÁöÑÊâÄÊúâÂ±ûÊÄßËøõË°åËÄÉÂØüÔºåËÄåÈöèÊú∫Ê£ÆÊûó‰ΩøÁî®ÁöÑ ‚ÄúÈöèÊú∫Âûã‚Äù ÂÜ≥Á≠ñÊ†ëÂàôÂè™ÈúÄËÄÉÂØü‰∏Ä‰∏™Â±ûÊÄßÂ≠êÈõÜ„ÄÇ&lt;/p>
&lt;blockquote>
&lt;p>„Äähands-on Machine Learning with sklearn, Keras and tensorflow„Äã&lt;/p>
&lt;p>&lt;strong>1. Random Forest&lt;/strong>&lt;/p>
&lt;p>A Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with &lt;code>max_samples&lt;/code> set to the size of the training set. Instead of building a &lt;code>BaggingClassifier&lt;/code> and passing it a &lt;code>DecisionTreeClassifier&lt;/code>, you can instead use the &lt;code>RandomForestClassifier&lt;/code> class, which is more convenients and optimized for Decision Trees&lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup> (similarly, there is a &lt;code>RandomForestRegressor&lt;/code> class for regression tasks).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.ensemble &lt;span style="color:#ff6ac1">import&lt;/span> RandomForestClassifier
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rf_clf &lt;span style="color:#ff6ac1">=&lt;/span> RandomForestClassifier(n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">500&lt;/span>, max_leaf_nodes&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">16&lt;/span>, n_jobs&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rf_clf&lt;span style="color:#ff6ac1">.&lt;/span>fit(X_train, y_train)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> rf_clf&lt;span style="color:#ff6ac1">.&lt;/span>predict(X_test)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With a few exceptions, a &lt;code>RandomForestClassifier&lt;/code> has all the hypeparameters of a &lt;code>DecisionTreeClassifier&lt;/code> (to control how trees are grown), plus all the hypeparameters of a &lt;code>BaggingClassifier&lt;/code> to control the ensemble itself.&lt;/p>
&lt;p>&lt;strong>2. Extra-Trees&lt;/strong>&lt;/p>
&lt;p>When you are growing a tree in a Random Forest, at each node only a random subset of the features (the $k$ set) is considered for splitting. It is possible to make trees even more random by also using random thresholds (the $t_k$ value) for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).&lt;/p>
&lt;p>A forest with such extremely random trees is called an &lt;em>Extremely Randomized Trees&lt;/em> ensemble (or &lt;em>Extra-Trees&lt;/em> for short). Once again, this technique trades more bias for a lower variance. It also makes &lt;em>Extra-Trees&lt;/em> much faster to train than regular Random Forests&lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>It is hard to tell in advance whether a &lt;code>RandomForestClassifier&lt;/code> will preform better or worse than an &lt;code>ExtraTreesClassifier&lt;/code> . Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters uisng grid search).&lt;/p>
&lt;/blockquote>
&lt;h3 id="boosting" >
&lt;div>
&lt;a href="#boosting">
##
&lt;/a>
BOOSTING
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>from „ÄäA Gentle Introduction to Gradient Boosting„Äã, &lt;a href="mailto:chengli@ccs.neu.edu">chengli@ccs.neu.edu&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>What is Gradient Boosting&lt;/strong>&lt;/p>
&lt;p style="text-align: center;color=green;font-size=20">
Gradient Boosting = Gradient Descent + Boosting
&lt;/p>
&lt;h4 id="adaboost" >
&lt;div>
&lt;a href="#adaboost">
###
&lt;/a>
AdaBoost
&lt;/div>
&lt;/h4>
&lt;p>&lt;img alt="adaboost" src="./images/adaboost.png">&lt;/p>
&lt;p>AdaBoost training:&lt;/p>
&lt;ul>
&lt;li>Fit an additive model (ensemble) $\sum_t \rho_t h_t(x)$ in a forward stage-wise manner.&lt;/li>
&lt;li>In each stage, introduce a weak learner to compensate the &lt;em>shortcomings&lt;/em> of existing weak learners.&lt;/li>
&lt;li>In AdaBoost, &lt;em>shortcomings&lt;/em> are identified by high-weight data points.&lt;/li>
&lt;/ul>
&lt;p>$$
H(x) = \sum_t \rho_t h_t(x)
$$
&lt;img alt="adaboost" src="./images/adaboost2.png">&lt;/p>
&lt;p style="text-align: center">
Figure: AdaBoost. Source: Figure 1.2 of [Schapire and Freund, 2012]
&lt;/p>
&lt;h4 id="gradient-boosting" >
&lt;div>
&lt;a href="#gradient-boosting">
###
&lt;/a>
Gradient Boosting
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>Fit an additive model (ensemble) $\sum_t \rho_t h_t(x)$ in a forward stage-wise manner.&lt;/li>
&lt;li>In each stage, introduce a weak learner to compensate the &lt;em>shortcomings&lt;/em> of existing weak learners.&lt;/li>
&lt;li>In Gradient Boosting, &lt;em>shortcomings&lt;/em> are identified by &lt;strong>gradients&lt;/strong>. (Recall that&lt;strong>hight-weight&lt;/strong> data points for adaboost)&lt;/li>
&lt;li>Both high-weight data points and gradients tell us how to improve our model.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>AdaBoost &amp;amp;&amp;amp; Gradient Boosting&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Invent Adaboost, the rst successful boosting algorithm
[Freund et al., 1996, Freund and Schapire, 1997]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Formulate Adaboost as gradient descent with a special loss
function[Breiman et al., 1998, Breiman, 1999]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Generalize Adaboost to Gradient Boosting in order to handle
a variety of loss functions
[Friedman et al., 2000, Friedman, 2001]&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="gradient-boosting-for-regression" >
&lt;div>
&lt;a href="#gradient-boosting-for-regression">
###
&lt;/a>
Gradient Boosting for Regression
&lt;/div>
&lt;/h4>
&lt;p>Given $D = {(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)}$, and the task is to fit a model $F(x)$ to minimize square loss.&lt;/p>
&lt;p>Suppose your friend wants to help you and gives you a model $F$. You check his model and find that the model is good but not perfect. There are some mistakes: $F(x_1)=0.8$, while $y_1=0.9$, $F(x_2)=1.8$, while $y_2=1.9$, and so on. How can you improve this model? With following rules:&lt;/p>
&lt;ul>
&lt;li>You are not allowed to remove anything from $F$ or change any parameter in $F$.&lt;/li>
&lt;li>You can add an additional model (regression tree) $h$ to $F$, so the new prediction will be $F(x) + h(x)$.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Simple solution:&lt;/strong>&lt;/p>
&lt;p>You wish to improve the model such that
$$
\begin{eqnarray}
F(x_1) + h(x_1) &amp;amp;=&amp;amp; y_1 \
F(x_2) + h(x_2) &amp;amp;=&amp;amp; y_2 \
\cdots \
F(x_n) + h(x_n) &amp;amp;=&amp;amp; y_n \
\end{eqnarray}
$$&lt;/p>
&lt;p>Or equivalently, you wish
$$
\begin{eqnarray}
h(x_1) &amp;amp;=&amp;amp; y_1 - F(x_1) \
h(x_2) &amp;amp;=&amp;amp; y_2 - F(x_2) \
\cdots \
h(x_n) &amp;amp;=&amp;amp; y_n - F(x_n) \
\end{eqnarray}
$$
Can any regression tree $h$ achieve this goal prefectly? Maybe not.&lt;/p>
&lt;p>But some regression tree might be able to do this approximately. But how?&lt;/p>
&lt;p>Just fit a regression tree $h$ to the &lt;strong>residuals&lt;/strong>&lt;sup id="fnref:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup> data :&lt;/p>
&lt;p>$(x_1,y_1 - F(x_1)), (x_2,y_2 - F(x_2)), \ldots, (x_n,y_n - F(x_n)),$&lt;/p>
&lt;p>which are the parts that existing model $F$ connot do well.&lt;/p>
&lt;p>The role of $h$ is to compensate the shortcoming of existing model $F$.&lt;/p>
&lt;p>If the the new model $F+h$ is still not satisfactory, we can add another regression tree $g$ to fit data:&lt;/p>
&lt;p>$(x_1,y_1 - F(x_1) - h(x_1)), (x_2,y_2 - F(x_2) - h(x_2)), \ldots, (x_n,y_n - F(x_n) - h(x_n)),$&lt;/p>
&lt;p>which are the parts that existing model $F+h$ connot do well.&lt;/p>
&lt;p>Repeat this process utill we are satisfied.&lt;/p>
&lt;p>Q: We are improving the predictions of training data, is the procedure also useful for test data?&lt;/p>
&lt;p>A: Yes! Because we are building a model, and the model can be applied to test data as well.&lt;/p>
&lt;p>Q: &lt;strong>How is this related to gradient descent?&lt;/strong>&lt;/p>
&lt;h4 id="relationship-to-the-gradient-descent" >
&lt;div>
&lt;a href="#relationship-to-the-gradient-descent">
###
&lt;/a>
Relationship to the Gradient Descent
&lt;/div>
&lt;/h4>
&lt;p>Minimize a function by moving in the opposite direction of the gradient.
$$
\theta_i := \theta_i - \rho{\partial{J} \over \partial{\theta_i}}
$$
&lt;img alt="gd" src="./images/gradient_descent.png">&lt;/p>
&lt;p>Figure: Gradient Descent. Source: &lt;a href="http://en.wikipedia.org/wiki/Gradient_descent">http://en.wikipedia.org/wiki/Gradient_descent&lt;/a>&lt;/p>
&lt;p>Recall that the task is to minimize the square loss, the loss function
$$
L(y, F(x)) = {1 \over 2} (y - F(x))^2
$$
And we want to minimize
$$
J = \sum_i L(y_i, F(x_i))
$$
by adjusting $F(x_i), F(x_2), \ldots, F(x_n)$.&lt;/p>
&lt;p>Notice that $F(x_i), F(x_2), \ldots, F(x_n)$ are just some numbers. We can treat $F(x_i)$ as parameters and take derivatives
$$
\frac{\partial{J}}{\partial{F(x_i)}}
= \frac{\partial{\sum_i L(y_i, F(x_i))}}{\partial{F(x_i)}}
= \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}}
= F(x_i) - y_i
$$
So we can interpret residuals as negative gradients:
$$
y_i - F(x_i) = - \frac{\partial{J}}{\partial{F(x_i)}}.
$$
And we get:
$$
\begin{eqnarray}
F(x_i) &amp;amp;:=&amp;amp; F(x_i) + h(x_i) \
F(x_i) &amp;amp;:=&amp;amp; F(x_i) + y_i - F(x_i) \
F(x_i) &amp;amp;:=&amp;amp; F(x_i) - 1 \frac{\partial{J}}{\partial{F(x_i)}} \
\end{eqnarray}
$$
This is exactly how the gradients update iteratively when $\rho=1$:
$$
\theta_i := \theta_i - \rho{\partial{J} \over \partial{\theta_i}}
$$
For regression with &lt;strong>square loss&lt;/strong>,
$$
\begin{eqnarray}
\text{residual} &amp;amp;\Leftrightarrow&amp;amp; \text{negative gradient} \
\text{fit h to residual} &amp;amp;\Leftrightarrow&amp;amp; \text{fit h to negative gradient} \
\text{update F based on residual} &amp;amp;\Leftrightarrow&amp;amp; \text{update F based on negative gradient} \
\end{eqnarray}
$$
So we are actually updating our model using &lt;strong>gradient descent&lt;/strong>!&lt;/p>
&lt;h4 id="loss-functions-for-regression-problem" >
&lt;div>
&lt;a href="#loss-functions-for-regression-problem">
###
&lt;/a>
Loss Functions for Regression Problem
&lt;/div>
&lt;/h4>
&lt;p>Square loss is:&lt;/p>
&lt;ol>
&lt;li>Easy to deal with mathematically, while&lt;/li>
&lt;li>Not robust to outliers.&lt;/li>
&lt;/ol>
&lt;p>The consequence is that it pay too much attention to outliers, and try hard to incorporate outliers into the model, leads to degrade the overall performance.&lt;/p>
&lt;p>Other commomly use loss functions are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Absolute loss (more robust to outliers):
$$
L(y, F) = |y - F|
$$
Negative gradient:
$$&lt;/p>
&lt;ul>
&lt;li>g(x_i)
= - \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}}
= \text{sign}(y_i - F(x_i))
$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Huber loss (more robust to outliers):
$$
L(y, F) = \left{
\begin{array}{ll}
{1 \over 2}(y - F)^2, &amp;amp;|y - F| \le \delta;&amp;amp; \
\delta(|y - F| - {\delta \over 2}), &amp;amp;|y - F| &amp;gt; \delta;&amp;amp; \
\end{array} \right.
$$
Negative gradient:
$$
\begin{eqnarray}&lt;/p>
&lt;ul>
&lt;li>g(x_i)
&amp;amp;=&amp;amp; - \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}} \
\
&amp;amp;=&amp;amp; \left{
\begin{array}{ll}
y_i - F(x_i), &amp;amp;|y_i - F(x_i)| \le \delta;&amp;amp; \
\delta\ \text{sign}(y_i - F(x_i)), &amp;amp;|y_i - F(x_i)| &amp;gt; \delta;&amp;amp; \
\end{array} \right.
\end{eqnarray}
$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>example:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: center">$y_i$&lt;/th>
&lt;th style="text-align: center">0.5&lt;/th>
&lt;th style="text-align: center">1.2&lt;/th>
&lt;th style="text-align: center">2&lt;/th>
&lt;th style="text-align: center">$5^*$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: center">$F(x_i)$&lt;/td>
&lt;td style="text-align: center">0.6&lt;/td>
&lt;td style="text-align: center">1.4&lt;/td>
&lt;td style="text-align: center">1.5&lt;/td>
&lt;td style="text-align: center">1.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">Square loss&lt;/td>
&lt;td style="text-align: center">0.005&lt;/td>
&lt;td style="text-align: center">0.02&lt;/td>
&lt;td style="text-align: center">0.125&lt;/td>
&lt;td style="text-align: center">5.445&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">Absolute loss&lt;/td>
&lt;td style="text-align: center">0.1&lt;/td>
&lt;td style="text-align: center">0.2&lt;/td>
&lt;td style="text-align: center">0.5&lt;/td>
&lt;td style="text-align: center">3.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">Huber loss ($\delta=0.5$)&lt;/td>
&lt;td style="text-align: center">0.005&lt;/td>
&lt;td style="text-align: center">0.02&lt;/td>
&lt;td style="text-align: center">0.125&lt;/td>
&lt;td style="text-align: center">1.525&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="regression-with-loss-function-l-general-procedure" >
&lt;div>
&lt;a href="#regression-with-loss-function-l-general-procedure">
###
&lt;/a>
Regression with loss function $L$: general procedure
&lt;/div>
&lt;/h4>
&lt;hr>
&lt;p>Give any differentiable loss function $L$,&lt;/p>
&lt;p>start with an initial model, say $F(x) = \frac{\sum^n_{i=1} y_i}{n}$,&lt;/p>
&lt;p>iterate until converge:&lt;/p>
&lt;p>‚Äã calculate negative gradients $- g(x_i) = - \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}}$,&lt;/p>
&lt;p>‚Äã fit a regression tree $h$ to negative gradients $-g(x_i)$,&lt;/p>
&lt;p>‚Äã $F := F + \rho h$&lt;/p>
&lt;hr>
&lt;p>In general, &lt;em>negative gradients&lt;/em> not equal to &lt;em>residuals&lt;/em>, and we should follow negative gradients rather than residuals because &lt;em>negative gradient&lt;/em> pays less attention to outliers.&lt;/p>
&lt;h4 id="summary-of-the-section" >
&lt;div>
&lt;a href="#summary-of-the-section">
###
&lt;/a>
Summary of the Section
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>Fit an additive model $F=\sum_t {\rho_t h_t}$ in a forward stage-wise manner.&lt;/li>
&lt;li>In each stage, introduce a new regression tree $h$ to compensate the shortcomings of existing model.&lt;/li>
&lt;li>The &lt;em>shortcomings&lt;/em> are identified by negative gradients.&lt;/li>
&lt;li>For any loss function, we can derive a gradient boosting algorithm.&lt;/li>
&lt;li>Absolute loss and Huber loss are more robust to outliers than square loss.&lt;/li>
&lt;/ul>
&lt;p>NOTE that the things not covered:&lt;/p>
&lt;p>How to choose a proper learning rate for each gradient boosting algorithm. See [Friedman, 2001]&lt;/p>
&lt;hr>
&lt;h4 id="gradient-boosting-for-classification" >
&lt;div>
&lt;a href="#gradient-boosting-for-classification">
###
&lt;/a>
Gradient Boosting for Classification
&lt;/div>
&lt;/h4>
&lt;p>&lt;strong>Problem:&lt;/strong>&lt;/p>
&lt;p>Recognize the given hand written capital letter. &lt;a href="http://archive.ics.uci.edu/ml/datasets/Letter+Recognition">dataset size: 20000 x 16&lt;/a>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Multi-class classification&lt;/p>
&lt;/li>
&lt;li>
&lt;p>26 classes. A, B, &amp;hellip;, Z&lt;/p>
&lt;/li>
&lt;li>
&lt;p>features:&lt;/p>
&lt;p>1 horizontal position of box;
2 vertical position of box;
3 width of box;
4 height of box;
5 total number on pixels;
6 mean x of on pixels in box;
7 mean y of on pixels in box;
8 mean x variance;&lt;/p>
&lt;p>9 mean y varianceÔºõ&lt;/p>
&lt;p>10 mean x y correlation;&lt;/p>
&lt;p>11 mean of x * x * y;&lt;/p>
&lt;p>12 mean of x * y * y;&lt;/p>
&lt;p>13 mean edge count left to right;&lt;/p>
&lt;p>14 correlation of x-ege with y;&lt;/p>
&lt;p>15 mean edge count bottom to top;&lt;/p>
&lt;p>16 correlation of y-ege with x.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Feature Vector= (2; 1; 3; 1; 1; 8; 6; 6; 6; 6; 5; 9; 1; 7; 5; 10)
Label = G&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Model:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>26 score functions (our models): $F_A, F_B, \ldots, F_Z$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$F_A(x)$ assigns a score for class A.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>scores are used to calculate probabilities:
$$
\begin{eqnarray}
P_A(x) &amp;amp;=&amp;amp; \frac{\exp(F_A(x))}{\sum^Z_{c=A}\exp(F_c(x))} \
\
P_B(x) &amp;amp;=&amp;amp; \frac{\exp(F_B(x))}{\sum^Z_{c=A}\exp(F_c(x))} \
\
\cdots \
\
P_Z(x) &amp;amp;=&amp;amp; \frac{\exp(F_Z(x))}{\sum^Z_{c=A}\exp(F_c(x))} \
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>predicted label = class that has the highest probability.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Loss function for each data point:&lt;/strong> step by step&lt;/p>
&lt;ol>
&lt;li>
&lt;p>turn the label $y_i$ into a (true) probability distribution $Y_c(x_i)$,&lt;/p>
&lt;p>For example: $y_5=\text{G}$,&lt;/p>
&lt;p>$Y_A(x_5)=0, Y_B(x_5)=0, \ldots, Y_G(x_5)=1, \ldots, Y_Z(x_5)=0$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>calculate the predicted probability distribution $P_c(x_i)$ based on the current model $F_A, F_B, \ldots, F_Z$.&lt;/p>
&lt;p>$P_A(x_5)=0.03, P_B(x_5)=0.05, \ldots, P_G(x_5)=0.3, \ldots, P_Z(x_5)=0.05$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>calculate the difference between the true probability distribution and the predicted distribution. One of the ways is to use KL-divergence to measure the difference.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>The Goal:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>minimize the total loss (KL-divergence)&lt;/li>
&lt;li>for each data point, we wish the predicted probability distribution to match the true probability distribution as closely as possible.&lt;/li>
&lt;li>we achieve this goal by adjusting our models $F_A, F_B, \ldots, F_Z$.&lt;/li>
&lt;/ul>
&lt;h4 id="gradient-boosting-for-classification-general-procedure" >
&lt;div>
&lt;a href="#gradient-boosting-for-classification-general-procedure">
###
&lt;/a>
Gradient Boosting for classification: general procedure
&lt;/div>
&lt;/h4>
&lt;hr>
&lt;p>start with initial models $F_A, F_B, \ldots, F_Z$.&lt;/p>
&lt;p>iterate until converge:&lt;/p>
&lt;p>‚Äã calculate negative gradients for class A: $-g_A(x_i)= - \frac{\partial{L}}{\partial{F_A(x_i)}}$&lt;/p>
&lt;p>‚Äã &amp;hellip;&lt;/p>
&lt;p>‚Äã calculate negative gradients for class Z: $-g_Z(x_i)= - \frac{\partial{L}}{\partial{F_Z(x_i)}}$&lt;/p>
&lt;p>‚Äã fit a regression tree $h_A$ to negative gradients $-g_A(x_i)$&lt;/p>
&lt;p>‚Äã &amp;hellip;&lt;/p>
&lt;p>‚Äã fit a regression tree $h_Z$ to negative gradients $-g_Z(x_i)$&lt;/p>
&lt;p>‚Äã $F_A := F_A + \rho_A h_A$&lt;/p>
&lt;p>‚Äã &amp;hellip;&lt;/p>
&lt;p>‚Äã $F_Z := F_Z + \rho_Z h_Z$&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Classification VS Regression&lt;/strong>: The Differences&lt;/p>
&lt;ul>
&lt;li>$F_A, F_B, \ldots, F_Z$ &lt;strong>vs&lt;/strong> $F$&lt;/li>
&lt;li>a matrix of parameters to optimize &lt;strong>vs&lt;/strong> a column of parameters to optimize&lt;/li>
&lt;li>a matrix of gradients &lt;strong>vs&lt;/strong> a column of gradients&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;/blockquote>
&lt;h2 id="part-ii-engineering-views" >
&lt;div>
&lt;a href="#part-ii-engineering-views">
#
&lt;/a>
Part-II: Engineering views
&lt;/div>
&lt;/h2>
&lt;h2 id="sklearn-random-forest-model" >
&lt;div>
&lt;a href="#sklearn-random-forest-model">
#
&lt;/a>
Sklearn Random-forest model
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>sklearn&lt;span style="color:#ff6ac1">.&lt;/span>ensemble&lt;span style="color:#ff6ac1">.&lt;/span>RandomForestClassifier(n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">100&lt;/span>, &lt;span style="color:#ff6ac1">*&lt;/span>, criterion&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#39;gini&amp;#39;&lt;/span>, max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, min_samples_split&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>, min_samples_leaf&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, min_weight_fraction_leaf&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.0&lt;/span>, max_features&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#39;auto&amp;#39;&lt;/span>, max_leaf_nodes&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, min_impurity_decrease&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.0&lt;/span>, min_impurity_split&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, bootstrap&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>, oob_score&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">False&lt;/span>, n_jobs&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, random_state&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, verbose&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0&lt;/span>, warm_start&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">False&lt;/span>, class_weight&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, ccp_alpha&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.0&lt;/span>, max_samples&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A random forest classifier.&lt;/p>
&lt;p>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code>max_samples&lt;/code> parameter if &lt;code>bootstrap=True&lt;/code> (default), otherwise the whole dataset is used to build each tree.&lt;/p>
&lt;h3 id="feature_importances_" >
&lt;div>
&lt;a href="#feature_importances_">
##
&lt;/a>
feature_importances_
&lt;/div>
&lt;/h3>
&lt;p>Sklearn measures a feature&amp;rsquo;s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node&amp;rsquo;s weight is equal to the number of training samples that are associated with it.&lt;/p>
&lt;p>&lt;code>feature_importances_&lt;/code> is a impurity-based feature importances.&lt;/p>
&lt;p>The higher, the more importance the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.&lt;/p>
&lt;p>Warning: impurity-based feature importance can be misleading for high cardinality features (many unique values). see [permutation_importance](###permutation importance) as an alternative.&lt;/p>
&lt;p>Tree-based models measure the feature importances based on the &lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/#MDI">mean decrease in impurity&lt;/a>. Impurity is quantified by the splitting criterion of the decision trees (Gini, Entropy(i.e., imformation gain) or Mean Square Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data (hold-out set, validation set, etc).&lt;/p>
&lt;p>Furthermore, impurity-based feature importance for trees are &lt;strong>strongly biased&lt;/strong> and &lt;strong>favor high cardinality features&lt;/strong> (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories. &lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/####Â¢ûÁõäÁéá">(see this explanation)&lt;/a>.&lt;/p>
&lt;p>The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance:&lt;/p>
&lt;p>&lt;a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py">Permutation importance vs Random Forest Feature Importance (MDI)&lt;/a>&lt;/p>
&lt;h3 id="permutation-importance" >
&lt;div>
&lt;a href="#permutation-importance">
##
&lt;/a>
permutation importance
&lt;/div>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>sklearn&lt;span style="color:#ff6ac1">.&lt;/span>inspection&lt;span style="color:#ff6ac1">.&lt;/span>permutation_importance(estimator, X, y, &lt;span style="color:#ff6ac1">*&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scoring&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_repeats&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">5&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_jobs&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> random_state&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Permutation importance for feature evaluation.&lt;/p>
&lt;p>The &lt;code>estimator&lt;/code> is required to be a fitted estimator. &lt;code>X&lt;/code> can be the data set used to train the estimator or a hold-out set. The permutation importance of a feature is calculated as follows.&lt;/p>
&lt;p>First, a baseline metric, defined by [scoring](#Scoring Parameter), is evaluated on a (potentially different) dataset defined by &lt;code>X&lt;/code>. Next, a feature column from the validation set is permuted(&lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/####How-it-work">How&lt;/a>) and the metric is evaluated again.&lt;/p>
&lt;p>The permutation importance is defined to be difference between the baseline metric and metric from permutating the feature column.&lt;/p>
&lt;hr>
&lt;p>Algorithm 1. Permutation importance&lt;/p>
&lt;hr>
&lt;p>Inputs: fitted predictive model $m$, tabular dataset (training or validation) $D$.&lt;/p>
&lt;p>Compute the reference score $s$ of the model $m$ on data $D$ (for instance the accuracy for a classifier or the $R^2$ for a regressor).&lt;/p>
&lt;p>&lt;strong>For&lt;/strong> each feature $j$ (column of $D$):&lt;/p>
&lt;p>‚Äã &lt;strong>For&lt;/strong> each repetition $k$ in $1, \ldots, K$:&lt;/p>
&lt;p>‚Äã Randomly shuffle column $j$ of dataset $D$ to generate a corrupted version of the data named $\tilde{D}_{k,j}$.&lt;/p>
&lt;p>‚Äã Compute the score $s_{k,j}$ of model $m$ on corrupted data $\tilde{D}_{k,j}$.&lt;/p>
&lt;p>‚Äã Compute importance $i_j$ for feature $f_j$ defined as:
$$
i_j = s - {1 \over K} \sum^K_{k=1}s_{k,j}.
$$&lt;/p>
&lt;hr>
&lt;h4 id="how-it-work" >
&lt;div>
&lt;a href="#how-it-work">
###
&lt;/a>
How it work
&lt;/div>
&lt;/h4>
&lt;p>Consider this: We want to predict a person&amp;rsquo;s height when they become 20 years old, using data that is available at age 10. Our data includes useful features (height at age 10), features with little predictive power (socks owned), as well as some other features we won&amp;rsquo;t focus on this explanation.&lt;/p>
&lt;p>&lt;strong>Permutation importance is calculated after a model has been fitted.&lt;/strong> so we won&amp;rsquo;t change the model or change what predictions we&amp;rsquo;d get for a given value of height, sock-count, etc.&lt;/p>
&lt;p>Instead we will ask the following question:&lt;/p>
&lt;p style="text-align:left;color:blue;">
"if I randomly shuffle a single column of the validation set data, leaving the target (or lable) and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?"
&lt;/p>
&lt;p>Randomly re-ordering a single column should cause less accuracy predictions, since the resulting data no longer corresponds to anything observed in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling &lt;code>height at age 10&lt;/code> would cause terrible predictions while shuffling &lt;code>socks-owned&lt;/code> wouldn&amp;rsquo;t sufffer nearly as much.&lt;/p>
&lt;h3 id="misleading-values-on-strongly-correlated-features" >
&lt;div>
&lt;a href="#misleading-values-on-strongly-correlated-features">
##
&lt;/a>
Misleading values on strongly correlated features
&lt;/div>
&lt;/h3>
&lt;p>When two features are correlated and one of the feature is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important.&lt;/p>
&lt;p>One way to handle this is to cluster features that are correlated and only keep one feature from each cluster. This strategy is explored in the following example:&lt;/p>
&lt;p>&lt;a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py">Permutation Importance with Multicollinear or Correlated Features&lt;/a>.&lt;/p>
&lt;h3 id="does-modeling-with-random-forests-require-cross-validation" >
&lt;div>
&lt;a href="#does-modeling-with-random-forests-require-cross-validation">
##
&lt;/a>
Does modeling with Random Forests require cross-validation?
&lt;/div>
&lt;/h3>
&lt;p style="text-align:center;color:blue;">
"Random forests provide free cross-validation."
&lt;/p>
&lt;p>The &lt;code>RandomForestClassifier&lt;/code> is trained using &lt;em>bootstrap aggregation&lt;/em>, where each new tree is fit from a bootstrap sample of the training observations $z_i = (x_i, y_i)$. The out-of-bag (OOB) error is the average error for each $z_i$ calculated using predictions from the tress that do not contain $z_i$ in their respective bootstrap sample. This allows the &lt;code>RandomForestClassifier&lt;/code> to be fit and validated whilst being trained.&lt;/p>
&lt;blockquote>
&lt;p>By principle since it randomizes the variable selection during each tree split, it&amp;rsquo;s not prone to overfit unlike other models. However if you want to use CV using nfolds in sklearn you can still use the concept of hold-out set such as &lt;code>oob_score=True&lt;/code> which shows model performance with or without using CV.&lt;/p>
&lt;/blockquote>
&lt;h3 id="sklearn-boosting" >
&lt;div>
&lt;a href="#sklearn-boosting">
##
&lt;/a>
Sklearn Boosting
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>from book „ÄäHands-onML„Äã.&lt;/p>
&lt;p>Boosting (original called &lt;em>hypothesis boosting&lt;/em>) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea most boosting methods is to train predictors sequentially, each trying to correct its predecessor. The most popular boosting methods by far are&lt;/p>
&lt;ul>
&lt;li>AdaBoost (short for Adaptive Boosting) and&lt;/li>
&lt;li>Gradient Boosting.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>1. AdaBoost&lt;/strong>&lt;/p>
&lt;p>One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard case. This technique used by AdaBoost.&lt;/p>
&lt;p>For example, when training an AdaBoost classifier, the algorithm first train a base classifier (such as a Decision Tree) and uses it to make predictions on the train set. The algorithm then increase the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on (see Figure 7-7).&lt;/p>
&lt;p>&lt;img alt="adaboost" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_adaboost.png">&lt;/p>
&lt;p>Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on their corresponding weighted training set.&lt;/p>
&lt;p>There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has ben trained and evaluated. As a result, it does not scale as well as bagging or pasting.&lt;/p>
&lt;p>&lt;strong>2. Gradient Boosting&lt;/strong>&lt;/p>
&lt;p>Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the &lt;em>residual errors&lt;/em> made by the previous predictor.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Gradient Tree Boosting for regression task,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># a.k.a., Gradient Boosted Regression Trees (GBRT)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.tree &lt;span style="color:#ff6ac1">import&lt;/span> DecisionTreeRegressor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg1 &lt;span style="color:#ff6ac1">=&lt;/span> DecisionTreeRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg1&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># train a second regressor on the residual errors made by the previous predictor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y2 &lt;span style="color:#ff6ac1">=&lt;/span> y &lt;span style="color:#ff6ac1">-&lt;/span> dtree_reg1&lt;span style="color:#ff6ac1">.&lt;/span>predict(X)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg2 &lt;span style="color:#ff6ac1">=&lt;/span> DecisionTreeRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg2&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># train a third regressor on the residual errors made by the previous predictor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y3 &lt;span style="color:#ff6ac1">=&lt;/span> y2 &lt;span style="color:#ff6ac1">-&lt;/span> dtree_reg2&lt;span style="color:#ff6ac1">.&lt;/span>predict(X)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg3 &lt;span style="color:#ff6ac1">=&lt;/span> DecisionTreeRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg3&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y3)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ensemble contains three trees which makes predictions on a new instance simply by adding up the predictions of all the trees&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">sum&lt;/span>(tree&lt;span style="color:#ff6ac1">.&lt;/span>predict(X_new) &lt;span style="color:#ff6ac1">for&lt;/span> tree &lt;span style="color:#ff6ac1">in&lt;/span> (dtree_reg1, dtree_reg2, dtree_reg3))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A simpler way to train GBRT ensembles is to use sklearn &lt;code>GradientBoostingRegressor&lt;/code> class. Much like the &lt;code>RandomForestRegressor&lt;/code> class, it has hyperparameters to control the growth of Decision Trees.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.ensemble &lt;span style="color:#ff6ac1">import&lt;/span> GradientBoostingRegressor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt &lt;span style="color:#ff6ac1">=&lt;/span> GradientBoostingRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">3&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> learning_rate&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img alt="gbrt model fitting" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_gradient_boosting.png">&lt;/p>
&lt;p>The &lt;code>learning_rate&lt;/code> hyperparameter scales the contribution of each tree.&lt;/p>
&lt;p>If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually better. This is a regularization technique called &lt;strong>shrinkage&lt;/strong>. In order to find the optimal number of trees, you can use early stopping which can simply implemented by setting&lt;code>warm_start=True&lt;/code> :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.model_selection &lt;span style="color:#ff6ac1">import&lt;/span> train_test_split
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.metrics &lt;span style="color:#ff6ac1">import&lt;/span> mean_square_error
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X_train, y_train, X_test, y_test &lt;span style="color:#ff6ac1">=&lt;/span> train_test_split(X, y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt &lt;span style="color:#ff6ac1">=&lt;/span> GradientBoostingRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> warm_start&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">early_stop_gbrt&lt;/span>(model, n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">200&lt;/span>, n_rounds&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">5&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> train_data&lt;span style="color:#ff6ac1">=&lt;/span>(X_train, y_train),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> val_data&lt;span style="color:#ff6ac1">=&lt;/span>(X_test, y_test),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> min_val_error &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">float&lt;/span>(&lt;span style="color:#5af78e">&amp;#34;inf&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> error_going_up &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">for&lt;/span> n &lt;span style="color:#ff6ac1">in&lt;/span> &lt;span style="color:#ff5c57">range&lt;/span>(&lt;span style="color:#ff9f43">1&lt;/span>, n_estimators):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model&lt;span style="color:#ff6ac1">.&lt;/span>n_estimators &lt;span style="color:#ff6ac1">=&lt;/span> n
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model&lt;span style="color:#ff6ac1">.&lt;/span>fit(train_data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> model&lt;span style="color:#ff6ac1">.&lt;/span>predict(val_data[&lt;span style="color:#ff9f43">0&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> val_error &lt;span style="color:#ff6ac1">=&lt;/span> mean_square_error(val_data[&lt;span style="color:#ff9f43">1&lt;/span>], y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> val_error &lt;span style="color:#ff6ac1">&amp;lt;&lt;/span> min_val_error:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> min_val_error &lt;span style="color:#ff6ac1">=&lt;/span> val_error
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> error_going_up &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> error_going_up &lt;span style="color:#ff6ac1">+=&lt;/span> &lt;span style="color:#ff9f43">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> error_going_up &lt;span style="color:#ff6ac1">==&lt;/span> n_round:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">break&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt &lt;span style="color:#ff6ac1">=&lt;/span> early_stop_gbrt(gbrt)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that there is an optimized implementation of Gradient Boosting out there called &amp;ldquo;XGBoost&amp;rdquo; which stands for Extreme Gradient Boosting, it is a popular Python library aimed to be extremely fast, scalable, and portable.&lt;/p>
&lt;/blockquote>
&lt;h3 id="plot-the-decision-tree" >
&lt;div>
&lt;a href="#plot-the-decision-tree">
##
&lt;/a>
Plot the decision tree
&lt;/div>
&lt;/h3>
&lt;p>To be continue &amp;hellip;&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Áî± $d$ ‰∏™Â±ûÊÄßÂº†ÊàêÁöÑ $d$ Áª¥Á©∫Èó¥‰∏≠ÔºåÊØè‰∏™Á§∫‰æãÈÉΩÂèØ‰ª•Âú®Ëøô‰∏™Á©∫Èó¥‰∏≠ÊâæÂà∞Ëá™Â∑±ÁöÑÂùêÊ†á‰ΩçÁΩÆÔºåÊØè‰∏™Á©∫Èó¥‰∏≠ÁöÑÁÇπÂØπÂ∫î‰∏Ä‰∏™ÂùêÊ†áÂêëÈáèÔºåÂõ†Ê≠§Ôºö‰∏Ä‰∏™Á§∫‰æãÂ∞±ÊòØ‰∏Ä‰∏™‚ÄúÁâπÂæÅÂêëÈáè‚ÄùÔºàfeature vectorÔºâ„ÄÇ&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>‰∏ä‰∏ÄÊ≠•ÁöÑÂàíÂàÜÂ±ûÊÄß‚ÄúÁ∫πÁêÜ‚ÄùÔºå‰∏çÂÜç‰Ωú‰∏∫ÂÄôÈÄâÂàíÂàÜÂ±ûÊÄß„ÄÇ&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Which is mainly due to the nature of how decision tree growed using greedy algorithm and easily overfiting.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>ÂèØÂ∞ÜÂàíÂàÜÁÇπËÆæ‰∏∫ËØ•Â±ûÊÄßÂú®ËÆ≠ÁªÉÈõÜ‰∏≠Âá∫Áé∞ÁöÑ‰∏çÂ§ß‰∫é‰∏≠‰ΩçÁÇπÁöÑÊúÄÂ§ßÂÄº„ÄÇÁî±Ê≠§ÔºåÂÜ≥Á≠ñÊ†ë‰ΩøÁî®ÁöÑÂàíÂàÜÁÇπÈÉΩÂá∫Áé∞Âú®ËÆ≠ÁªÉÈõÜ‰∏≠„ÄÇ&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>ÂæÖË°•ÂÖÖ„ÄÇ&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>[Mingers, 1989b]&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>[Raileanu and Stoffel, 2004]&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>[Mingers, 1989a]&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>[Mruthy et al., 1994]&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10">
&lt;p>[Brodley and Utgoff, 1995]&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11">
&lt;p>[Utgoff, 1989b]&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12">
&lt;p>[Guo and Gelfand, 1992]&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13">
&lt;p>[Schlimmer and Fisher, 1986]&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14">
&lt;p>[Utgoff, 1989a]&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15">
&lt;p>[Utgoff et al., 1997]&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16">
&lt;p>ÈõÜÊàêÂ≠¶‰π†(ensemble learningÔºåa.k.a, multi-classifier system, committee-based learning)ÈÄöËøáÊûÑÂª∫Âπ∂ÁªìÂêàÂ§ö‰∏™Â≠¶‰π†Âô®Êù•ÂÆåÊàêÂ≠¶‰π†‰ªªÂä°„ÄÇÈõÜÊàêÂ≠¶‰π†‰∏ÄËà¨ÁªìÊûÑÊòØÔºåÂÖà‰∫ßÁîü‰∏ÄÁªÑ ‚Äú‰∏™‰ΩìÂ≠¶‰π†Âô®(individual learner)‚ÄùÔºåÂÜçÁî®ÊüêÁßçÁ≠ñÁï•Â∞ÜÂÆÉ‰ª¨ÁªìÂêàËµ∑Êù•„ÄÇ‰∏™‰ΩìÂ≠¶‰π†Âô®ÈÄöÂ∏∏Áî±‰∏Ä‰∏™Áé∞ÊúâÂ≠¶‰π†ÁÆóÊ≥ï‰ªéËÆ≠ÁªÉÊï∞ÊçÆ‰∫ßÁîüÔºå‰æãÂ¶Ç C4.5ÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ïÊàñBPÁ•ûÁªèÁΩëÁªúÁÆóÊ≥ïÁ≠âÔºåÊ≠§Êó∂ÔºåÂ¶ÇÊûúÈõÜÊàê‰∏≠Âè™ÂåÖÂê´ÂêåÁßçÁ±ªÂûãÁöÑ‰∏™‰ΩìÂ≠¶‰π†Âô®ÔºåÂ¶Ç ‚ÄúÂÜ≥Á≠ñÊ†ëÈõÜÊàê‚Äù„ÄÅ‚ÄúÁ•ûÁªèÁΩëÁªúÈõÜÊàê‚ÄùÁ≠âÔºåÂàôËøôÊ†∑ÁöÑÈõÜÊàêÊòØ ‚ÄúÂêåË¥®‚Äù ÁöÑ(homogeneous)ÈõÜÊàêÔºåÂêåË¥®ÈõÜÊàê‰∏≠ÁöÑ‰∏™‰ΩìÂ≠¶‰π†Âô®‰πüÁß∞‰∏∫ ‚ÄúÂü∫Â≠¶‰π†Âô®(base learner)‚ÄùÔºåÁõ∏Â∫îÁöÑÂ≠¶‰π†ÁÆóÊ≥ïÁß∞‰∏∫ ‚ÄúÂü∫Â≠¶‰π†ÁÆóÊ≥ï(base learning algorithm)‚ÄùÔºõÂèç‰πãÔºåÂàôÊòØ ‚ÄúÂºÇË¥®‚Äù ÁöÑ(heterogenous)ÈõÜÊàêÔºåËøôÊó∂‰∏™‰ΩìÂ≠¶‰π†Âô®Â∏∏Áß∞‰∏∫ &amp;ldquo;ÁªÑ‰ª∂Â≠¶‰π†Âô®(component learner)&amp;rdquo; ÊàñÁõ¥Êé•Áß∞‰∏∫‰∏™‰ΩìÂ≠¶‰π†Âô®„ÄÇ&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17">
&lt;p>Boosting ÊòØ‰∏ÄÊóèÂèØÂ∞ÜÂº±Â≠¶‰π†Âô®ÊèêÂçá‰∏∫Âº∫Â≠¶‰π†Âô®ÁöÑÁÆóÊ≥ï„ÄÇËøôÊóèÁÆóÊ≥ïÁöÑÂ∑•‰ΩúÊú∫Âà∂Á±ª‰ººÔºöÂÖà‰ªéÂàùÂßãËÆ≠ÁªÉÈõÜËÆ≠ÁªÉÂá∫‰∏Ä‰∏™Êú∫Â≠¶‰π†Âô®ÔºåÂÜçÊ†πÊçÆÂü∫Â≠¶‰π†Âô®ÁöÑË°®Áé∞ÂØπËÆ≠ÁªÉÊ†∑Êú¨ÂàÜÂ∏ÉËøõË°åË∞ÉÊï¥Ôºå‰ΩøÂæóÂÖàÂâçÂü∫Â≠¶‰π†Âô®ÂÅöÈîôÁöÑËÆ≠ÁªÉÊ†∑Êú¨Âú®ÂêéÁª≠ÂèóÂà∞Êõ¥Â§öÂÖ≥Ê≥®ÔºåÁÑ∂ÂêéÂü∫‰∫éË∞ÉÊï¥ÂêéÁöÑÊ†∑Êú¨ÂàÜÂ∏ÉÊù•ËÆ≠ÁªÉ‰∏ã‰∏Ä‰∏™Âü∫Â≠¶‰π†Âô®ÔºõÂ¶ÇÊ≠§ÈáçÂ§çËøõË°åÔºåÁõ¥Ëá≥Â≠¶‰π†Âô®Êï∞ÁõÆËææÂà∞‰∫ãÂÖàÊåáÂÆöÁöÑÂÄº$T$ÔºåÊúÄÁªàÂ∞ÜËøô$T$‰∏™Âü∫Â≠¶‰π†Âô®ËøõË°åÂä†ÊùÉÁªìÂêà„ÄÇ&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18">
&lt;p>bootstrapping ÈááÊ†∑ÊòØÊúâÊîæÂõûÁöÑÈöèÊú∫ÈáçÂ§çÈááÊ†∑ÔºåÊ†∑Êú¨Âú®$m$Ê¨°ÈááÊ†∑‰∏≠ÂßãÁªà‰∏çË¢´ÈááÂà∞ÁöÑÊ¶ÇÁéáÊòØ$(1 - {1 \over m})^m$ÔºåÂèñÊûÅÈôêÂæóÂà∞$lim_{(m \rightarrow \infin)} (1 - {1 \over m})^m \rightarrow {1 \over e} \approx 0.368$ÔºåÂç≥ÈÄöËøáËá™Âä©ÈááÊ†∑ÔºåÂàùÂßãÊï∞ÊçÆÈõÜ‰∏≠Á∫¶Êúâ36.8%ÁöÑÊ†∑Êú¨Êú™Âá∫Áé∞Âú®Ëá™Âä©ÈááÊ†∑ÈõÜ‰∏≠„ÄÇËá™Âä©Ê≥ïËÉΩ‰ªéÂàùÂßãÊï∞ÊçÆÈõÜ‰∏≠‰∫ßÁîüÂ§ö‰∏™‰∏çÂêåËÆ≠ÁªÉÈõÜÔºåËøôÂØπÈõÜÊàêÂ≠¶‰π†ÊúâÂæàÂ§ßÂ•ΩÂ§Ñ„ÄÇÁÑ∂ËÄåÔºåËá™Âä©Ê≥ï‰∫ßÁîüÁöÑÊï∞ÊçÆÈõÜÊîπÂèò‰∫ÜÂàùÂßãÊï∞ÊçÆÈõÜÁöÑÂàÜÂ∏ÉËÄåÂºïÂÖ•‰º∞ËÆ°ÂÅèÂ∑ÆÔºåÂõ†Ê≠§ÔºåÂú®ÂàùÂßãÊï∞ÊçÆÈáèË∂≥Â§üÊó∂ÔºåÁïôÂá∫Ê≥ïÂíå‰∫§ÂèâÈ™åËØÅÊ≥ïÊõ¥Â∏∏Áî®‰∏Ä‰∫õ„ÄÇ&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19">
&lt;p>statistical mode: ‰ºóÊï∞„ÄÇ‰πüÂ∞±ÊòØÈ¢ëÁéáÊúÄÈ´òÁöÑÈ¢ÑÊµãÁ±ªÂà´Ôºå‰∏é hard voting classfifer Á±ª‰ºº„ÄÇ&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20">
&lt;p>The &lt;code>BaggingClassifier&lt;/code> class remains useful if you want a bag of something other than Decision Trees.&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21">
&lt;p>Finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.&amp;#160;&lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:22">
&lt;p>residual is the difference between the predicted and the ground true.&amp;#160;&lt;a href="#fnref:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title/><link>/posts/neuralnetworks/xgs_backprop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/neuralnetworks/xgs_backprop/</guid><description>&lt;h2 id="ËØØÂ∑ÆÈÄÜ‰º†Êí≠ÁÆóÊ≥ïerror-backpropagation-bp" >
&lt;div>
&lt;a href="#%e8%af%af%e5%b7%ae%e9%80%86%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95error-backpropagation-bp">
#
&lt;/a>
ËØØÂ∑ÆÈÄÜ‰º†Êí≠ÁÆóÊ≥ï(error BackPropagation, BP)
&lt;/div>
&lt;/h2>
&lt;p>BP ÁÆóÊ≥ïÊòØËøÑ‰ªäÊúÄÊàêÂäüÁöÑÁ•ûÁªèÁΩëÁªúÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇÁé∞ÂÆû‰ªªÂä°‰∏≠‰ΩøÁî®Á•ûÁªèÁΩëÁªúÊó∂ÔºåÂ§ßÂ§öÊòØÂú®‰ΩøÁî® BP ÁÆóÊ≥ïËøõË°åËÆ≠ÁªÉ„ÄÇBP ÁÆóÊ≥ï‰∏ç‰ªÖÂèØÁî®‰∫éÂ§öÂ±ÇÂâçÈ¶àÁ•ûÁªèÁΩëÁªú(multi-layer feedforward neural networks)&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> ÔºåËøòÂèØ‰ª•Áî®‰∫éÂÖ∂‰ªñÁ±ªÂûãÁöÑÁ•ûÁªèÁΩëÁªúÔºå‰æãÂ¶ÇËÆ≠ÁªÉÈÄíÂΩíÁ•ûÁªèÁΩëÁªú„ÄÇ‰ΩÜÈÄöÂ∏∏ËØ¥ ‚ÄúBPÁΩëÁªú‚Äù Êó∂Ôºå‰∏ÄËà¨ÊåáÁî® BP ÁÆóÊ≥ïËÆ≠ÁªÉÁöÑÂ§öÈáçÂâçÈ¶àÁ•ûÁªèÁΩëÁªú&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>„ÄÇ&lt;/p>
&lt;h3 id="bpÁΩëÁªúÂèäÂèòÈáèÁ¨¶Âè∑" >
&lt;div>
&lt;a href="#bp%e7%bd%91%e7%bb%9c%e5%8f%8a%e5%8f%98%e9%87%8f%e7%ac%a6%e5%8f%b7">
##
&lt;/a>
BPÁΩëÁªúÂèäÂèòÈáèÁ¨¶Âè∑
&lt;/div>
&lt;/h3>
&lt;p>ÁªôÂÆöËÆ≠ÁªÉÈõÜ $D = {(x_1, y_1), \ldots, (x_m, y_m)}, x_i \in \R^d, y_i \in \R^l$ ÔºåÂç≥ËæìÂÖ•Á§∫‰æãÁî± $d$ ‰∏™Â±ûÊÄßÊèèËø∞ÔºåËæìÂá∫ $l$ Áª¥ÂÆûÂÄºÂêëÈáè„ÄÇ‰∏∫‰æø‰∫éËÆ®ËÆ∫ÔºåÂõæ5.7 ÁªôÂá∫‰∫Ü‰∏Ä‰∏™Êã•Êúâ $d$ ‰∏™ËæìÂÖ•Á•ûÁªèÂÖÉ„ÄÅ$l$ ‰∏™ËæìÂá∫Á•ûÁªèÂÖÉ„ÄÅ$q$ ‰∏™ÈöêÂ±ÇÁ•ûÁªèÂÖÉÁöÑÂ§öÂ±ÇÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÁªìÊûÑÔºåÂÖ∂‰∏≠&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ËæìÂá∫Â±ÇÁ¨¨ $j$ ‰∏™Á•ûÁªèÂÖÉÁöÑÈòàÂÄºÁî® $\theta_j$ Ë°®Á§∫ÔºåÈöêÂ±ÇÁ¨¨ $h$ ‰∏™Á•ûÁªèÂÖÉÁöÑÈòàÂÄºÁî® $\gamma_h$ Ë°®Á§∫Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ËæìÂÖ•Â±ÇÁ¨¨ $i$ ‰∏™Á•ûÁªèÂÖÉ‰∏éÈöêÂ±ÇÁ¨¨ $h$ ‰∏™Á•ûÁªèÂÖÉ‰πãÈó¥ÁöÑËøûÊé•ÊùÉ‰∏∫ $v_{ih}$ Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÈöêÂ±ÇÁ¨¨ $h$ ‰∏™Á•ûÁªèÂÖÉ‰∏éËæìÂá∫Â±ÇÁ¨¨ $j$ ‰∏™Á•ûÁªèÂÖÉ‰πãÈó¥ÁöÑËøûÊé•ÊùÉ‰∏∫ $w_hj$ Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ËÆ∞ÈöêÂ±ÇÁ¨¨ $h$ ‰∏™Á•ûÁªèÂÖÉÊé•Êî∂Âà∞ÁöÑËæìÂÖ•‰∏∫ $\alpha_h = \sum^d_{i=1} v_{ih} x_i$ Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ËÆ∞ËæìÂá∫Â±ÇÁ¨¨ $j$ ‰∏™Á•ûÁªèÂÖÉÊé•Êî∂Âà∞ÁöÑËæìÂÖ•‰∏∫ $\beta_j = \sum^q_{h=1} w_{hj} b_h $ ÔºåÂÖ∂‰∏≠Ôºå$b_h$ ‰∏∫ÈöêÂ±ÇÁ¨¨ $h$ ‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂá∫Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÂÅáËÆæÈöêÂ±ÇÂíåËæìÂá∫Â±ÇÁ•ûÁªèÂÖÉÈÉΩ‰ΩøÁî®sigmoidÂáΩÊï∞&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> ‰Ωú‰∏∫ÊøÄÊ¥ªÂáΩÊï∞&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> „ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="BP network and notations" src="./images/xgs_BP_notations.png">&lt;/p>
&lt;p>ÂØπËÆ≠ÁªÉ‰æã $(x_k, y_k)$ ÔºåÂÅáÂÆöÁ•ûÁªèÁΩëÁªúÁöÑËæìÂá∫‰∏∫ $\hat{y}&lt;em>k = (\hat{y}^k_1, \ldots, \hat{y}^k_l)$ ÔºåÂç≥
$$
\tag{5.3} \label{eq_y_hat}
\hat{y}^k_j = f(\beta_j - \theta_j),
$$
ÂàôÁΩëÁªúÂú® $(x_k, y_k)$ ‰∏äÂùáÊñπËØØÂ∑Æ‰∏∫
$$
\tag{5.4}
E_k = {1 \over 2} \sum^l&lt;/em>{j=1} (\hat{y}^k_j - {y}^k_j)^2 .
$$
Âõæ5.7ÁöÑÁΩëÁªú‰∏≠Êúâ $(d + l + 1)q + l$ ‰∏™ÂèÇÊï∞ÈúÄÁ°ÆÂÆöÔºö&lt;/p>
&lt;ul>
&lt;li>ËæìÂÖ•Â±ÇÂà∞ÈöêÂ±ÇÁöÑ $d \times q$ ‰∏™ÊùÉÂÄºÔºõ&lt;/li>
&lt;li>ÈöêÂ±ÇÂà∞ËæìÂá∫Â±ÇÁöÑ $q \times l$ ‰∏™ÊùÉÂÄº;&lt;/li>
&lt;li>$q$ ‰∏™ÈöêÂ±ÇÁ•ûÁªèÂÖÉÁöÑÈòàÂÄº„ÄÅ$l$ ‰∏™ËæìÂá∫Â±ÇÁ•ûÁªèÂÖÉÁöÑÈòàÂÄº„ÄÇ&lt;/li>
&lt;/ul>
&lt;p>BP ÊòØ‰∏Ä‰∏™Ëø≠‰ª£Â≠¶‰π†ÁÆóÊ≥ïÔºåÂú®Ëø≠‰ª£ÁöÑÊØè‰∏ÄËΩÆ‰∏≠ÈááÁî®Âπø‰πâÁöÑÊÑüÁü•Êú∫Â≠¶‰π†ËßÑÂàô&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> ÂØπÂèÇÊï∞ËøõË°åÊõ¥Êñ∞‰º∞ËÆ°ÔºåÂç≥‰ªªÊÑèÂèÇÊï∞ $v$ ÁöÑÊõ¥Êñ∞‰º∞ËÆ°Âºè‰∏∫
$$
\tag{5.5}
v \leftarrow v + \Delta v .
$$&lt;/p>
&lt;h3 id="ÂèÇÊï∞ÁöÑÊõ¥Êñ∞" >
&lt;div>
&lt;a href="#%e5%8f%82%e6%95%b0%e7%9a%84%e6%9b%b4%e6%96%b0">
##
&lt;/a>
ÂèÇÊï∞ÁöÑÊõ¥Êñ∞
&lt;/div>
&lt;/h3>
&lt;p>‰ª•‰∏ãÊàë‰ª¨‰ª•Âõæ5.7‰∏≠ÈöêÂ±ÇÂà∞ËæìÂá∫Â±ÇÁöÑËøûÊé•ÊùÉ $w_{hj}$ ‰∏∫‰æãÊù•ËøõË°åÊé®ÂØº„ÄÇ&lt;/p>
&lt;p>BP ÁÆóÊ≥ïÂü∫‰∫éÊ¢ØÂ∫¶‰∏ãÈôç(gradient descent)Á≠ñÁï•Ôºå‰ª•ÁõÆÊ†áÁöÑÁöÑË¥üÊ¢ØÂ∫¶ÊñπÂêëÂØπÂèÇÊï∞ËøõË°åË∞ÉÊï¥„ÄÇÂØπÂºè(5.4)ÁöÑËØØÂ∑Æ $E_k$ ÔºåÁªôÂÆöÂ≠¶‰π†Áéá $\eta$ ÔºåÊúâ
$$
\tag{5.6}
\Delta w_{hj} = - \eta {\partial E_k \over \partial w_{hj}}.
$$
Ê≥®ÊÑè&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>Âà∞ $w_{hj}$ ÂÖàÂΩ±ÂìçÂà∞Á¨¨ $j$ ‰∏™ËæìÂá∫Â±ÇÁ•ûÁªèÂÖÉÁöÑËæìÂÖ•ÂÄº $\beta_j$ ÔºåÂÜçÂΩ±ÂìçÂà∞ÂÖ∂ËæìÂá∫ÂÄº $\hat{y}^k_j$ ÔºåÁÑ∂ÂêéÂΩ±ÂìçÂà∞ $E_k$ ÔºåÊúâ
$$
\tag{5.7}
\frac{\partial E_k}{\partial w_{hj}} =
\frac{\partial E_k}{\partial \hat{y}^k_j}
\cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j}
\cdot \frac{\partial \beta_j}{\partial w_{hj}} .
$$
Ê†πÊçÆ $\beta_j$ ÁöÑ&lt;a href="https://fgg100y.github.io/posts/neuralnetworks/xgs_backprop/###BPÁΩëÁªúÂèäÂèòÈáèÁ¨¶Âè∑">ÂÆö‰πâ&lt;/a>ÔºåÊòæÁÑ∂Êúâ
$$
\tag{5.8}
\frac{\partial \beta_j}{\partial w_{hj}} = b_h .
$$
sigmoidÂáΩÊï∞Êúâ‰∏Ä‰∏™ÂæàÂ•ΩÁöÑÊÄßË¥®Ôºö
$$
\tag{5.9}
f&amp;rsquo;(x) = f(x)(1 - f(x)),
$$
‰∫éÊòØÔºåÊ†πÊçÆÂºè(5.4)ÂíåÂºè(5.3)ÔºåÊúâ
$$
\begin{eqnarray}
g_j
&amp;amp;=&amp;amp; - \frac{\partial E_k}{\partial \hat{y}^k_j} \cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j} \
&amp;amp;=&amp;amp; - (\hat{y}^k_j - {y}^k_j) f&amp;rsquo;(\beta_j - \theta_j) \
\tag{5.10} \label{eq_output_gradient}
&amp;amp;=&amp;amp; \hat{y}^k_j(1 - \hat{y}^k_j) (\hat{y}^k_j - {y}^k_j) .
\end{eqnarray}
$$
Â∞ÜÂºè(5.10)Âíå(5.8)‰ª£ÂÖ•Âºè(5.7)ÔºåÂÜç‰ª£ÂÖ•Âºè(5.6)ÔºåÂ∞±ÂæóÂà∞‰∫Ü BP ÁÆóÊ≥ï‰∏≠ÂÖ≥‰∫é $w_{hj}$ ÁöÑÊõ¥Êñ∞ÂÖ¨Âºè
$$
\tag{5.11} \label{eq_weights_hj}
\Delta w_{hj} = \eta g_j b_h .
$$
Á±ª‰ººÂèØÂæó
$$
\begin{eqnarray}
\tag{5.12}
\Delta \theta_j &amp;amp;=&amp;amp; - \eta g_j, \
\tag{5.13}
\Delta v_{ih} &amp;amp;=&amp;amp; \eta e_h x_i, \
\tag{5.14} \label{eq_threshold_hidden}
\Delta \gamma_h &amp;amp;=&amp;amp; - \eta e_h, \
\end{eqnarray}
$$
ÂÖ∂‰∏≠
$$
\begin{eqnarray}
e_h
&amp;amp;=&amp;amp; - \frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha_h} \
&amp;amp;=&amp;amp; - \sum^l_{j=1} \frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} f&amp;rsquo;(\alpha_h - \gamma_h) \
&amp;amp;=&amp;amp; \sum^l_{j=1} w_{hj} g_j f&amp;rsquo;(\alpha_h - \gamma_h) \
\tag{5.15} \label{eq_hidden_gradient}
&amp;amp;=&amp;amp; b_h (1 - b_h) \sum^l_{j=1} w_{hj} g_j .
\end{eqnarray}
$$
Â≠¶‰π†Áéá $\eta \in (0, 1)$ ÊéßÂà∂ÁùÄÁÆóÊ≥ïÊØè‰∏ÄËΩÆËø≠‰ª£‰∏≠ÁöÑÊõ¥Êñ∞Ê≠•ÈïøÔºåËã•Â§™Â§ßÂàôÂÆπÊòìÊåØËç°ÔºåÂ§™Â∞èÂàôÊî∂ÊïõÈÄüÂ∫¶Âèà‰ºöËøáÊÖ¢„ÄÇ&lt;/p>
&lt;p>ÊúâÊó∂‰∏∫‰∫ÜÂÅöÁ≤æÁªÜË∞ÉËäÇÔºåÂèØ‰ª§Âºè(5.11)‰∏é(5.12)‰ΩøÁî® $\eta_1$ ÔºåÂºè(5.13)‰∏é(5.14)‰ΩøÁî® $\eta_2$ Ôºå‰∏§ËÄÖÊú™ÂøÖÁõ∏Á≠â„ÄÇ&lt;/p>
&lt;h3 id="bp-ÁÆóÊ≥ïÁöÑÂ∑•‰ΩúÊµÅÁ®ã" >
&lt;div>
&lt;a href="#bp-%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b">
##
&lt;/a>
BP ÁÆóÊ≥ïÁöÑÂ∑•‰ΩúÊµÅÁ®ã
&lt;/div>
&lt;/h3>
&lt;p>ÂØπÊØè‰∏™ËÆ≠ÁªÉÊ†∑‰æãÔºåBP ÁÆóÊ≥ïÊâßË°å‰ª•‰∏ãÊìç‰ΩúÔºöÂÖàÂ∞ÜËæìÂÖ•Á§∫‰æãÊèê‰æõÁªôËæìÂÖ•Â±ÇÁ•ûÁªèÂÖÉÔºåÁÑ∂ÂêéÈÄêÂ±ÇÂ∞Ü‰ø°Âè∑Ââç‰º†ÔºåÁõ¥Âà∞‰∫ßÁîüËæìÂá∫Â±ÇÁöÑÁªìÊûúÔºõÁÑ∂ÂêéËÆ°ÁÆóËæìÂá∫Â±ÇÁöÑËØØÂ∑Æ(Á¨¨4-5Ë°å)ÔºåÂÜçÂ∞ÜËØØÂ∑ÆÈÄÜÂêë‰º†Êí≠Ëá≥ÈöêÂ±ÇÁ•ûÁªèÂÖÉ(Á¨¨6Ë°å)ÔºåÊúÄÂêéÊ†πÊçÆÈöêÂ±ÇÁ•ûÁªèÂÖÉÁöÑËØØÂ∑ÆÊù•ÂØπËøûÊé•ÊùÉÂíåÈòàÂÄºËøõË°åË∞ÉÊï¥(Á¨¨7Ë°å)„ÄÇËØ•Ëø≠‰ª£ËøáÁ®ãÂæ™ÁéØËøõË°åÔºåÁõ¥Âà∞ËææÂà∞Êüê‰∫õÂÅúÊ≠¢Êù°‰ª∂‰∏∫Ê≠¢„ÄÇ&lt;/p>
&lt;hr>
&lt;p>&lt;code>BP ÁÆóÊ≥ïÂ∑•‰ΩúÊµÅÁ®ã&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ËæìÂÖ•&lt;/strong>ÔºöËÆ≠ÁªÉÈõÜ $D = {(x_k, y_k)}^m_{k=1}$ Ôºõ&lt;/p>
&lt;p>‚Äã Â≠¶‰π†Áéá $\eta$ .&lt;/p>
&lt;p>&lt;strong>ËøáÁ®ã&lt;/strong>Ôºö&lt;/p>
&lt;p>1ÔºöÂú® $(0,1)$ ËåÉÂõ¥ÂÜÖÈöèÊú∫ÂàùÂßãÂåñÁΩëÁªú‰∏≠ÊâÄÊúâËøûÊé•ÊùÉÂíåÈòàÂÄº&lt;/p>
&lt;p>2Ôºö&lt;strong>repeat&lt;/strong>&lt;/p>
&lt;p>3: &lt;strong>for all&lt;/strong> $(x_k, y_k) \in D$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>4: Ê†πÊçÆÂΩìÂâçÂèÇÊï∞ÂíåÂºè($\ref{eq_y_hat}$)ËÆ°ÁÆóÂΩìÂâçÊ†∑Êú¨ÁöÑËæìÂá∫ $\hat{y}_k$ Ôºõ&lt;/p>
&lt;p>5: Ê†πÊçÆÂºè($\ref{eq_output_gradient}$)ËÆ°ÁÆóËæìÂá∫Â±ÇÁ•ûÁªèÂÖÉÁöÑÊ¢ØÂ∫¶È°π $g_j$ Ôºõ&lt;/p>
&lt;p>6: Ê†πÊçÆÂºè($\ref{eq_hidden_gradient}$)ËÆ°ÁÆóÈöêÂ±ÇÁ•ûÁªèÂÖÉÁöÑÊ¢ØÂ∫¶È°π $e_h$ Ôºõ&lt;/p>
&lt;p>7: Ê†πÊçÆÂºè($\ref{eq_weights_hj}$)-($\ref{eq_threshold_hidden}$)Êõ¥Êñ∞ËøûÊé•ÊùÉ $w_{hj}, v_{ih}$ ‰∏éÈòàÂÄº $\theta_j, \gamma_h$&lt;/p>
&lt;p>8: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>9: &lt;strong>unitl&lt;/strong> ËææÂà∞ÂÅúÊ≠¢Êù°‰ª∂&lt;/p>
&lt;p>&lt;strong>ËæìÂá∫&lt;/strong>ÔºöËøûÊé•ÊùÉ‰∏éÈòàÂÄºÁ°ÆÂÆöÁöÑÂ§öÂ±ÇÂâçÈ¶àÁ•ûÁªèÁΩëÁªú&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØ&lt;/strong>ÔºåBP ÁÆóÊ≥ïÁöÑÁõÆÊ†áÊòØË¶ÅÊúÄÂ∞èÂåñËÆ≠ÁªÉÈõÜ $D$ ‰∏äÁöÑÁ¥ØÁßØËØØÂ∑Æ
$$
\tag{5.16}
E = {1 \over m} \sum^m_{k=1} E_k ,
$$
‰ΩÜÊàë‰ª¨‰∏äÈù¢‰ªãÁªçÁöÑ ‚ÄúÊ†áÂáÜBPÁÆóÊ≥ï‚Äù ÊØèÊ¨°‰ªÖÈíàÂØπ‰∏Ä‰∏™ËÆ≠ÁªÉÊ†∑‰æãÊõ¥Êñ∞ËøûÊé•ÊùÉÂíåÈòàÂÄºÔºå‰πüÂ∞±ÊòØËØ¥ &lt;code>BP ÁÆóÊ≥ïÂ∑•‰ΩúÊµÅÁ®ã&lt;/code> ‰∏≠ÁÆóÊ≥ïÁöÑÊõ¥Êñ∞ËßÑÂàôÊòØÂü∫‰∫éÂçï‰∏™ÁöÑ $E_k$ Êé®ÂØºËÄåÂæó„ÄÇÂ¶ÇÊûúÁ±ª‰ººÂú∞Êé®ÂØºÂá∫Âü∫‰∫éÁ¥ØÁßØËØØÂ∑ÆÊúÄÂ∞èÂåñÁöÑÊõ¥Êñ∞ËßÑÂàôÔºåÂ∞±ÂæóÂà∞‰∫Ü ‚ÄúÁ¥ØÁßØËØØÂ∑ÆÈÄÜ‰º†Êí≠(accumulated error backpropagation)‚Äù ÁÆóÊ≥ï„ÄÇÁ¥ØÁßØBPÁÆóÊ≥ï‰∏éÊ†áÂáÜBPÁÆóÊ≥ïÈÉΩÂæàÂ∏∏Áî®„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÊ†áÂáÜBPÁÆóÊ≥ïÊØèÊ¨°Êõ¥Êñ∞Âè™ÈíàÂØπÂçï‰∏™Ê†∑‰æãÔºåÂèÇÊï∞Êõ¥Êñ∞ÂæóÈùûÂ∏∏È¢ëÁπÅÔºåËÄå‰∏îÂØπ‰∏çÂêåÊ†∑‰æãËøõË°åÊõ¥Êñ∞ÁöÑÊïàÊûúÂèØËÉΩÂá∫Áé∞ ‚ÄúÊäµÊ∂à‚Äù Áé∞Ë±°„ÄÇÂõ†Ê≠§Ôºå‰∏∫‰∫ÜËææÂà∞ÂêåÊ†∑ÁöÑÁ¥ØÁßØËØØÂ∑ÆÊûÅÂ∞èÁÇπÔºåÊ†áÂáÜBPÁÆóÊ≥ïÂæÄÂæÄÈúÄË¶ÅËøõË°åÊõ¥Â§öÊ¨°Êï∞ÁöÑËø≠‰ª£„ÄÇÁ¥ØÁßØBPÁÆóÊ≥ïÁõ¥Êé•ÈíàÂØπÁ¥ØÁßØËØØÂ∑ÆÊúÄÂ∞èÂåñÔºåÂÆÉÂú®ËØªÂèñÊï¥‰∏™ËÆ≠ÁªÉÈõÜ $D$ ‰∏ÄÈÅçÂêéÊâçÂØπÂèÇÊï∞ËøõË°åÊõ¥Êñ∞ÔºåÂÖ∂ÂèÇÊï∞Êõ¥Êñ∞È¢ëÁéá‰ΩéÂæóÂ§ö„ÄÇ‰ΩÜÂú®ÂæàÂ§ö‰ªªÂä°‰∏≠ÔºåÁ¥ØÁßØËØØÂ∑Æ‰∏ãÈôçÂà∞‰∏ÄÂÆöÁ®ãÂ∫¶‰πãÂêéÔºåËøõ‰∏ÄÊ≠•‰∏ãÈôç‰ºöÈùûÂ∏∏ÁºìÊÖ¢ÔºåËøôÊó∂Ê†áÂáÜBPÁÆóÊ≥ïÂæÄÂæÄ‰ºöÊõ¥Âø´Ëé∑ÂæóËæÉÂ•ΩÁöÑËß£ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÆ≠ÁªÉÈõÜ $D$ ÈùûÂ∏∏Â§ßÊó∂Êõ¥‰∏∫ÊòéÊòæ&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>„ÄÇ&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Á•ûÁªèÁΩëÁªúÂ±ÇÁ∫ßÁªìÊûÑÁöÑ‰∏ÄÁßçÔºåÊØèÂ±ÇÁ•ûÁªèÂÖÉ‰∏é‰∏ã‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉÂÖ®‰∫íËøûÔºåÁ•ûÁªèÂÖÉ‰πãÈó¥‰∏çÂ≠òÂú®ÂêåÂ±ÇËøûÊé•Ôºå‰πü‰∏çÂ≠òÂú®Ë∑®Â±ÇËøûÊé•„ÄÇ‚ÄúÂâçÈ¶à‚ÄùÂπ∂‰∏çÊÑèÂë≥ÁùÄÁΩëÁªú‰∏≠‰ø°Âè∑‰∏çËÉΩÂêëÂêé‰º†ÔºåËÄåÊòØÊåáÁΩëÁªúÊãìÊâëÁªìÊûÑ‰∏ä‰∏çÂ≠òÂú®ÁéØÊàñÂõûË∑Ø„ÄÇ&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The term back-propagation (BP) is often misunderstood as meaning the whole learning algorithm for multi layer neural networks. Actually, back-propagation refers only to the method for computing the gradient, while another algorithm, such SGD, is used to perform learning using this gradient.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>$\text{sigmoid}(x) = {1 \over 1 + e^{-x}}$ ÔºåÂØπÊï∞Âá†ÁéáÂáΩÊï∞ÊòØÂÖ∏ÂûãÁöÑsigmoidÂáΩÊï∞ÔºåÂÆÉÊääÂèØËÉΩÂú®ËæÉÂ§ßËåÉÂõ¥ÂÜÖÂèòÂåñÁöÑËæìÂÖ•ÂÄºÊå§ÂéãÂà∞ $(0,1)$ ËæìÂá∫ÂÄºËåÉÂõ¥ÂÜÖÔºåÂõ†Ê≠§ÊúâÊó∂‰πüÁß∞‰∏∫ ‚ÄúÊå§ÂéãÂáΩÊï∞(squashing function)‚Äù„ÄÇ&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>ÊøÄÊ¥ªÂáΩÊï∞‰πüÁß∞‰∏∫ ‚ÄúÂìçÂ∫îÂáΩÊï∞‚ÄùÔºåÁêÜÊÉ≥ÁöÑÊøÄÊ¥ªÂáΩÊï∞ÊòØÈò∂Ë∑ÉÂáΩÊï∞ $\text{sgn}(x)&lt;em>{x |x &amp;lt; 0} = 0; \text{sgn}(x)&lt;/em>{x |x \ge 0} =1$ÔºåÂÆÉÂ∞ÜËæìÂÖ•ÂÄºÊò†Â∞Ñ‰∏∫ËæìÂá∫ÂÄº &amp;ldquo;0&amp;rdquo; Êàñ ‚Äú1‚ÄùÔºåÊòæÁÑ∂ ‚Äú1‚Äù ÂØπÂ∫î‰∫éÁ•ûÁªèÂÖÉÂÖ¥Â•ãÔºå‚Äú0‚Äù ÂØπÂ∫î‰∫éÁ•ûÁªèÂÖÉÂÖ¥Â•ã„ÄÇÁÑ∂ËÄåÔºåÈò∂Ë∑ÉÂáΩÊï∞ÂÖ∑Êúâ‰∏çËøûÁª≠(Âú®Èõ∂ÁÇπÂ§Ñ)„ÄÅ‰∏çÂÖâÊªëÁ≠â‰∏çÂ§™ÂèãÂ•ΩÁöÑÊÄßË¥®ÔºåÂõ†Ê≠§ÂÆûÈôÖÂ∏∏Áî®sigmoidÂáΩÊï∞‰Ωú‰∏∫ÊøÄÊ¥ªÂáΩÊï∞„ÄÇ&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>ÊÑüÁü•Êú∫(Perceptron)Áî±‰∏§Â±ÇÁ•ûÁªèÂÖÉÁªÑÊàêÔºåËæìÂÖ•Â±ÇÊé•Êî∂Â§ñÁïåËæìÂÖ•‰ø°Âè∑Âêé‰º†ÈÄíÁªôËæìÂá∫Â±ÇÔºåËæìÂá∫Â±ÇÊòØ M-P Á•ûÁªèÂÖÉÔºå‰πüÁß∞‰∏∫ ‚ÄúÈòàÂÄºÈÄªËæëÂçïÂÖÉ(threshold logic unit)‚Äù„ÄÇÊÑüÁü•Êú∫ÁöÑÂ≠¶‰π†ËßÑÂàôÈùûÂ∏∏ÁÆÄÂçïÔºåÂØπËÆ≠ÁªÉÊ†∑‰æã $(x, y)$ ÔºåËã•ÂΩìÂâçÊÑüÁü•Êú∫ÁöÑËæìÂá∫‰∏∫ $\hat{y}$ ÔºåÂàôÊÑüÁü•Êú∫ÊùÉÈáçÂ∞ÜËøôÊ†∑Ë∞ÉÊï¥Ôºö$w_i \leftarrow w_i + \Delta w_i$ ÔºåÂÖ∂‰∏≠ $\Delta w_i = \eta (y - \hat{y}) x_i$ ÔºåÂÖ∂‰∏≠ $\eta \in (0,1)$ ÊòØÂ≠¶‰π†Áéá(learning rate)„ÄÇÂèØ‰ª•ÁúãÂá∫ÔºåËã•ÊÑüÁü•Êú∫ÂØπËÆ≠ÁªÉÊ†∑‰æã $(x, y)$ È¢ÑÊµãÊ≠£Á°ÆÔºåÂç≥ $\hat{y} = y$ ÔºåÂàôÊÑüÁü•Êú∫‰∏çÂèëÁîüÂèòÂåñÔºåÂê¶ÂàôÂ∞ÜÊ†πÊçÆÈîôËØØÁöÑÁ®ãÂ∫¶ËøõË°åÊùÉÂÄºË∞ÉÊï¥„ÄÇ&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>ËøôÂ∞±ÊòØ ‚ÄúÈìæÂºèÊ≥ïÂàô‚Äù„ÄÇ&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>Ê†áÂáÜBPÁÆóÊ≥ïÂíåÁ¥ØÁßØBPÁÆóÊ≥ïÁöÑÂå∫Âà´Á±ª‰ºº‰∫éÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç(SGD)‰∏éÊ†áÂáÜÊ¢ØÂ∫¶‰∏ãÈôç‰πãÈó¥ÁöÑÂå∫Âà´„ÄÇ&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>ÊÑüÁü•Êú∫ÁöÑÂèÇÊï∞Êõ¥Êñ∞ËßÑÂàôÂíåBPÁÆóÊ≥ïÁöÑÂèÇÊï∞Êõ¥Êñ∞ËßÑÂàôÂºè(5.11)-(5.14) ÈÉΩÊòØÂü∫‰∫éÊ¢ØÂ∫¶‰∏ãÈôç„ÄÇ&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>