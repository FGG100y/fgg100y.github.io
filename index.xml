<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>fgg blog</title><link>https://fgg100y.github.io/</link><description>fgg blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 04 Jun 2024 18:15:00 +0800</lastBuildDate><atom:link href="https://fgg100y.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>lean_principles</title><link>https://fgg100y.github.io/posts/mlteam101/lean_principles/</link><pubDate>Mon, 03 Jun 2024 17:59:09 +0800</pubDate><guid>https://fgg100y.github.io/posts/mlteam101/lean_principles/</guid><description>&lt;p>Principle 1: Identify value&lt;/p>
&lt;pre>&lt;code>Determine what is most valuable to the customer and focus on maximizing that value.
&lt;/code>&lt;/pre>
&lt;p>Principle 2: Map the value stream&lt;/p>
&lt;pre>&lt;code>Identify the steps in the process that add value and eliminate those that do not.
&lt;/code>&lt;/pre>
&lt;p>Principle 3: Create flow&lt;/p>
&lt;pre>&lt;code>Streamline the process to create a smooth and continuous flow of work.
&lt;/code>&lt;/pre>
&lt;p>Principle 4: Establish pull&lt;/p>
&lt;pre>&lt;code>Use customer demand to trigger production and avoid overproduction.
&lt;/code>&lt;/pre>
&lt;p>Principle 5: Continuous improvement&lt;/p>
&lt;pre>&lt;code>Continuously strive for improvement and eliminate waste in all areas of the value
chain.
&lt;/code>&lt;/pre>
&lt;p>Value stream mapping (principle 2) is a tool that lets us visually represent all the
steps and resources involved in delivering a unit of value (e.g., a product feature) to
customers. Teams can use this tool to identify waste, work toward eliminating waste,
and improve the flow of value (principle 3).&lt;/p>
&lt;p>To map your team or product’s value stream, you can follow these steps:&lt;/p>
&lt;ol>
&lt;li>Identify the product or service being mapped. This could be a single product or1.
an entire process.&lt;/li>
&lt;li>Identify the current state map. Create a visual representation of the current2.
process, including all steps and materials (including time and labor) involved
from raw materials to finished product.&lt;/li>
&lt;li>Identify value-added and non-value-added activities. Determine which steps add3.
value to the product or service and which do not.&lt;/li>
&lt;li>Identify waste. Look for areas of overproduction, waiting, defects, overprocess‐4.
ing, excess inventory, unnecessary motion, excess transport, unnecessary use of
raw materials, and unnecessary effort.&lt;/li>
&lt;li>Create a future state map. Based on the analysis of the current state map, redesign5.
the process to eliminate waste and create a more efficient flow of materials and
information.&lt;/li>
&lt;li>Implement changes. Put the redesigned process into practice and continuously6.
monitor and improve (principle 5).&lt;/li>
&lt;/ol></description></item><item><title>EffectiveML 01: Delivering successful ML projects</title><link>https://fgg100y.github.io/posts/mlteam101/feedback_loops_and_times_to_feedback/</link><pubDate>Mon, 03 Jun 2024 17:12:37 +0800</pubDate><guid>https://fgg100y.github.io/posts/mlteam101/feedback_loops_and_times_to_feedback/</guid><description>&lt;p>Table: Comparison of feedback mechanisms and time-to-feedback in high- and low-effectiveness environments&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Task&lt;/th>
&lt;th>High-effectiveness environment&lt;/th>
&lt;th>Low-effectiveness environment&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Testing if code changes worked as expected&lt;/td>
&lt;td>Automated testing (~ seconds to minutes)&lt;/td>
&lt;td>Manual testing (~ minutes to hours)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Testing if training pipeline works end to end&lt;/td>
&lt;td>Training smoke test (~ 1 minute)&lt;/td>
&lt;td>Full model training (~ minutes to hours, depending on model architecture)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Getting feedback on code changes&lt;/td>
&lt;td>Pair programming (~ seconds to minutes)&lt;/td>
&lt;td>Pull request reivews (~ hours to days)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Understanding if application is working as expected in production&lt;/td>
&lt;td>Monitoring in production (~ seconds - as it happens)&lt;/td>
&lt;td>Customer complaints (~ days, or longer if not directly reported)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>How teams can move from a low-effectiveness environment to a high-effectiveness environment?
MLOps is not enough (MLOps and ML platforms aren’t going to write comprehensive tests
for you, talk to users for you, or reduce the negative impacts of team silos for you.)&lt;/p>
&lt;p>Delivering successful ML projects requires a multi‐disciplinary approach across these
five disciplines: product, software engineering, data, ML, and delivery:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>software engineering:&lt;/p>
&lt;ul>
&lt;li>code design&lt;/li>
&lt;li>automated testing&lt;/li>
&lt;li>refactoring&lt;/li>
&lt;li>dependency management&lt;/li>
&lt;li>code editor efficency&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>data:&lt;/p>
&lt;ul>
&lt;li>data quality&lt;/li>
&lt;li>data engineering&lt;/li>
&lt;li>data security and privacy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Machine learning:&lt;/p>
&lt;ul>
&lt;li>ML techniques&lt;/li>
&lt;li>model evaluation&lt;/li>
&lt;li>ML governance&lt;/li>
&lt;li>MLOps&lt;/li>
&lt;li>CI/CD&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Product:&lt;/p>
&lt;ul>
&lt;li>product thinking&lt;/li>
&lt;li>responsible AI&lt;/li>
&lt;li>human-centered AI&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Delivery:&lt;/p>
&lt;ul>
&lt;li>lean delivery&lt;/li>
&lt;li>team topologies&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>tech interview prepare (for my resume)</title><link>https://fgg100y.github.io/posts/notes4resume/</link><pubDate>Mon, 27 May 2024 20:43:25 +0800</pubDate><guid>https://fgg100y.github.io/posts/notes4resume/</guid><description>&lt;h1 id="自我介绍" >
&lt;div>
&lt;a href="#%e8%87%aa%e6%88%91%e4%bb%8b%e7%bb%8d">
##
&lt;/a>
自我介绍
&lt;/div>
&lt;/h1>&lt;blockquote>
&lt;p>在自我介绍时，确保你提到的项目和技能与你申请的职位紧密相关，这样可以更好地展示你的专业
能力和对职位的适应性。同时，保持自信和热情，让面试官感受到你对工作和团队的承诺。&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>尊敬的面试官，您好！&lt;/p>
&lt;p>我叫范明华，拥有6年在机器学习领域的工作经验。我于2017年硕士毕业于中山大学，专业是生态学，
在植被动态和演替方面的研究工作，为我在实验设计、统计分析以及数据挖掘方面打下了扎实的基础。
过去的6年里，我在工作中一直致力于将机器学习技术应用于解决实际问题，并取得了一些的成果。&lt;/p>
&lt;p>我在目前公司担任高级数据挖掘工程师，期间我主导了多个机器学习项目的开发和交付。
在技术层面，我比较擅长结合数据现状来决定机器学习或深度学习技术的搭配解决实际问题，
熟悉整个机器学习项目的开发流程，从项目调研、数据预处理、特征工程到模型训练和部署，
我都有深入的理解和实践。在这些项目的实战中，不仅提升了个人的技术深度和带队能力，
同时也取得了授权的发明专利、软件著作、地方标准等成果。&lt;/p>
&lt;p>除了技术专长，我还是一个注重团队合作和务实负责的人。我相信，我的专业技能和丰富经验，
能够为贵公司带来直接的价值。&lt;/p>
&lt;p>我对贵公司在机器学习/深度学习方面的工作感兴趣，我期待能够加入贵公司，并与团队一起解决更多有趣的技术挑战。&lt;/p>
&lt;p>感谢您给我这次面试的机会，我期待在接下来的讨论中分享更多我的经验和想法。谢谢！&lt;/p>
&lt;hr>
&lt;h1 id="project-01----nlp" >
&lt;div>
&lt;a href="#project-01----nlp">
##
&lt;/a>
Project 01 &amp;ndash; NLP
&lt;/div>
&lt;/h1>&lt;h2 id="sklearn-randomforest-model" >
&lt;div>
&lt;a href="#sklearn-randomforest-model">
#
&lt;/a>
sklearn randomforest model
&lt;/div>
&lt;/h2>&lt;p>当谈到随机森林时，我们需要理解它的基础算法：决策树。随机森林是基于决策树的集成学习方法。所以，让我们首先来了解决策树的基本算法，然后再深入探讨随机森林。&lt;/p>
&lt;h3 id="1-决策树算法" >
&lt;div>
&lt;a href="#1-%e5%86%b3%e7%ad%96%e6%a0%91%e7%ae%97%e6%b3%95">
##
&lt;/a>
1. 决策树算法:
&lt;/div>
&lt;/h3>&lt;h4 id="11-cart算法-classification-and-regression-trees" >
&lt;div>
&lt;a href="#11-cart%e7%ae%97%e6%b3%95-classification-and-regression-trees">
###
&lt;/a>
1.1 CART算法 (Classification and Regression Trees):
&lt;/div>
&lt;/h4>&lt;p>CART算法是一种用于构建分类和回归树的决策树算法。它通过对数据集递归地进行二分来构建决策树。具体步骤如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>特征选择&lt;/strong>：对于分类问题，通常使用基尼指数（Gini index）或信息增益（Information Gain）来选择最佳的特征进行分裂；对于回归问题，通常使用平方误差来选择最佳的特征。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>节点分裂&lt;/strong>：根据选择的特征，将数据集分成两部分，使得每个子集的样本属于同一类别（对于分类问题）或具有相似的回归值（对于回归问题）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>递归&lt;/strong>：对每个子集重复上述过程，直到满足停止条件，如达到最大深度、节点中样本数小于某个阈值或其他预定义条件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>剪枝&lt;/strong>：为了避免过拟合，可以对生成的树进行剪枝，即移除一些节点来简化树的结构。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="2-随机森林算法" >
&lt;div>
&lt;a href="#2-%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e7%ae%97%e6%b3%95">
##
&lt;/a>
2. 随机森林算法:
&lt;/div>
&lt;/h3>&lt;h4 id="21-构建随机森林" >
&lt;div>
&lt;a href="#21-%e6%9e%84%e5%bb%ba%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97">
###
&lt;/a>
2.1 构建随机森林:
&lt;/div>
&lt;/h4>&lt;p>随机森林是通过构建多棵决策树并将它们集成起来来完成的。具体步骤如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>随机抽样&lt;/strong>：从原始训练集中随机选择一部分样本（有放回抽样）来构建每棵决策树的训练集。这样可以保证每棵树的训练集略有差异，增加了模型的多样性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>随机特征选择&lt;/strong>：对于每棵树的每个节点，在选择分割特征时，随机选择一部分特征来进行评估。这样可以确保每棵树的分裂过程也有所差异。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>独立构建&lt;/strong>：每棵树都是独立构建的，没有任何关联。这意味着可以并行地构建多棵树，提高了训练效率。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="22-集成决策树" >
&lt;div>
&lt;a href="#22-%e9%9b%86%e6%88%90%e5%86%b3%e7%ad%96%e6%a0%91">
###
&lt;/a>
2.2 集成决策树:
&lt;/div>
&lt;/h4>&lt;p>构建多棵决策树后，随机森林采用不同的方式来集成它们的预测结果：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>分类任务&lt;/strong>：采用投票的方式，即每棵树投票选择最终的类别。&lt;/li>
&lt;li>&lt;strong>回归任务&lt;/strong>：采用平均值的方式，即多棵树的预测结果取平均值。&lt;/li>
&lt;/ul>
&lt;h3 id="总结" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93">
##
&lt;/a>
总结:
&lt;/div>
&lt;/h3>&lt;p>随机森林是一种强大的机器学习方法，基于决策树的集成学习。通过利用决策树的随机性和集成策略，随机森林能够有效地应对分类和回归问题，并在许多实际应用中表现优异。&lt;/p>
&lt;p>在 CART (Classification and Regression Trees) 算法中，节点的分裂依据是基于贪心算法。CART 算法通过贪心地选择每次分裂时能够最大程度减少不纯度（对于分类问题）或者最小化误差（对于回归问题）的特征来进行节点的分裂。这种贪心策略保证了在每个节点分裂时都选择了最优的特征来进行分裂。&lt;/p>
&lt;h3 id="节点分裂的依据" >
&lt;div>
&lt;a href="#%e8%8a%82%e7%82%b9%e5%88%86%e8%a3%82%e7%9a%84%e4%be%9d%e6%8d%ae">
##
&lt;/a>
节点分裂的依据：
&lt;/div>
&lt;/h3>&lt;h4 id="对于分类问题" >
&lt;div>
&lt;a href="#%e5%af%b9%e4%ba%8e%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98">
###
&lt;/a>
对于分类问题：
&lt;/div>
&lt;/h4>&lt;p>在分类问题中，CART 算法通常使用以下两种方法作为节点分裂的依据：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>基尼指数 (Gini index)&lt;/strong>：基尼指数衡量了从一个数据集中随机抽取两个样本，它们类别不一致的概率。具体地，对于一个节点 $t$，基尼指数可以计算为：&lt;/p>
&lt;p>$ Gini(t) = 1 - \sum_{i=1}^{c} p(i|t)^2 $&lt;/p>
&lt;p>其中，$c$ 是类别的数量，$p(i|t)$ 是在节点 $t$ 中属于类别 $i$ 的样本的比例。选择能够最大程度降低基尼指数的特征来进行节点分裂。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>信息增益 (Information Gain)&lt;/strong>：信息增益衡量了在某个特征的条件下，将数据集分为不同类别后，带来的不确定性减少的程度。具体地，对于一个节点 $t$ 和一个特征 $A$，信息增益可以计算为：&lt;/p>
&lt;p>$ IG(t, A) = H(t) - \sum_{v \in Values(A)} \frac{|t_v|}{|t|} \cdot H(t_v) $&lt;/p>
&lt;p>其中，$H(t)$ 是节点 $t$ 的熵，$Values(A)$ 是特征 $A$ 的取值集合，$t_v$ 是在特征 $A$ 上取值为 $v$ 的样本集合，$H(t_v)$ 是样本集合 $t_v$ 的熵。选择能够最大化信息增益的特征来进行节点分裂。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="对于回归问题" >
&lt;div>
&lt;a href="#%e5%af%b9%e4%ba%8e%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98">
###
&lt;/a>
对于回归问题：
&lt;/div>
&lt;/h4>&lt;p>在回归问题中，CART 算法通常使用平方误差 (Mean Squared Error, MSE) 作为节点分裂的依据。选择能够最小化节点分裂后样本的平方误差的特征来进行分裂。&lt;/p>
&lt;h3 id="总结-1" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-1">
##
&lt;/a>
总结：
&lt;/div>
&lt;/h3>&lt;p>CART 算法在节点分裂时采用贪心算法，选择能够最大程度减少不纯度（分类问题）或者最小化误差（回归问题）的特征来进行分裂。这种贪心策略保证了每次分裂都选择了最优的特征，以构建出尽可能简单且有效的决策树。&lt;/p>
&lt;p>当谈到基于决策树的集成学习时，除了随机森林，还有一种重要的方法是提升树（Boosting）。提升树是一种迭代的集成学习方法，通过串行地构建一系列决策树来逐步提升模型的性能。下面我会详细介绍提升树的原理和实现方式。&lt;/p>
&lt;h3 id="提升树的原理" >
&lt;div>
&lt;a href="#%e6%8f%90%e5%8d%87%e6%a0%91%e7%9a%84%e5%8e%9f%e7%90%86">
##
&lt;/a>
提升树的原理：
&lt;/div>
&lt;/h3>&lt;h4 id="1-基本思想" >
&lt;div>
&lt;a href="#1-%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3">
###
&lt;/a>
1. 基本思想：
&lt;/div>
&lt;/h4>&lt;p>提升树的基本思想是通过训练一系列弱学习器（通常是决策树），然后将它们组合起来构成一个更强大的模型。每个弱学习器都专注于纠正之前模型的错误，因此在构建过程中会关注之前模型预测错误的样本。&lt;/p>
&lt;h4 id="2-算法流程" >
&lt;div>
&lt;a href="#2-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b">
###
&lt;/a>
2. 算法流程：
&lt;/div>
&lt;/h4>&lt;p>提升树的算法流程如下：&lt;/p>
&lt;ol>
&lt;li>初始化模型为一个常数值，通常为目标变量的均值（对于回归问题）或者是类别的先验概率（对于分类问题）。&lt;/li>
&lt;li>迭代地训练决策树，每次训练都会生成一个新的弱学习器。在每次迭代中，算法会计算当前模型的残差（对于回归问题）或者梯度（对于分类问题），然后训练一个新的决策树来拟合这些残差或者梯度。&lt;/li>
&lt;li>将新生成的决策树加到模型中，通常使用一个较小的学习率来缓解每棵树的影响。&lt;/li>
&lt;li>重复迭代步骤2和步骤3，直到达到预先设定的迭代次数或者模型的性能达到某个阈值为止。&lt;/li>
&lt;/ol>
&lt;h4 id="3-加法模型" >
&lt;div>
&lt;a href="#3-%e5%8a%a0%e6%b3%95%e6%a8%a1%e5%9e%8b">
###
&lt;/a>
3. 加法模型：
&lt;/div>
&lt;/h4>&lt;p>提升树的最终模型是一个加法模型，即多个弱学习器的加权求和。通过迭代训练，每个弱学习器都会对模型进行一定的修正，最终组合起来构成一个更强大的模型。&lt;/p>
&lt;h3 id="实现" >
&lt;div>
&lt;a href="#%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
实现：
&lt;/div>
&lt;/h3>&lt;p>提升树的实现通常采用梯度提升算法（Gradient Boosting），其中最常见的是梯度提升决策树（Gradient Boosting Decision Trees，GBDT）。&lt;/p>
&lt;p>GBDT 算法的关键步骤包括计算残差或者梯度、训练决策树以拟合残差或者梯度、确定学习率等。GBDT 通过不断地迭代训练决策树来逐步优化模型，直到达到一定的迭代次数或者达到一定的性能指标。&lt;/p>
&lt;h3 id="总结-2" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-2">
##
&lt;/a>
总结：
&lt;/div>
&lt;/h3>&lt;p>提升树是一种基于决策树的集成学习方法，通过迭代训练一系列决策树来逐步提升模型的性能。相较于随机森林，提升树通常会产生更加精确的预测，但需要更长的训练时间，并且对异常值和噪声数据更敏感。&lt;/p>
&lt;h2 id="mlp-model" >
&lt;div>
&lt;a href="#mlp-model">
#
&lt;/a>
MLP model
&lt;/div>
&lt;/h2>&lt;p>MLP，即多层感知器（Multilayer Perceptron），是一种基本的人工神经网络模型。它由多层神经元组成，每一层都与下一层全连接。MLP是一种前馈神经网络，意味着信息只能从输入层向输出层传递，不会存在循环连接。&lt;/p>
&lt;p>下面是MLP的一些基本原理：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>神经元（Perceptron）&lt;/strong>：
在MLP中，每个神经元都是一个简单的计算单元。它接收来自前一层的输入信号，将这些信号加权求和，并通过激活函数产生输出。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>多层结构&lt;/strong>：
MLP由多个层组成，典型的MLP包括输入层、至少一个隐藏层和输出层。输入层接收原始数据，隐藏层对输入数据进行特征提取和转换，输出层生成最终的预测结果。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>权重和偏置&lt;/strong>：
在MLP中，每个连接都有一个相关联的权重，用于控制信号传递的强度和方向。此外，每个神经元还有一个偏置，用于调整神经元的激活阈值。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>激活函数&lt;/strong>：
在神经元中，激活函数决定了神经元输出的非线性关系。常用的激活函数包括Sigmoid、ReLU（Rectified Linear Unit）、tanh等，它们在不同情况下具有不同的优势。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>前向传播&lt;/strong>：
在MLP中，数据从输入层开始传播，经过一系列的加权求和和激活函数处理，一直传播到输出层，生成最终的预测结果。这个过程称为前向传播。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>反向传播&lt;/strong>：
反向传播是MLP中用于训练模型的关键步骤。它利用梯度下降算法，通过计算损失函数对每个参数（权重和偏置）的梯度，并沿着梯度的反方向更新参数，从而最小化损失函数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>损失函数&lt;/strong>：
损失函数用于衡量模型预测结果与真实标签之间的差异。常见的损失函数包括均方误差（MSE）、交叉熵等，选择适当的损失函数取决于具体的问题类型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>优化算法&lt;/strong>：
在反向传播过程中，需要选择合适的优化算法来更新模型参数。常用的优化算法包括随机梯度下降（SGD）、Adam、RMSprop等。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>总的来说，MLP通过多层的非线性变换来学习输入数据的复杂特征表示，通过反向传播算法不断调整模型参数以最小化损失函数，从而实现对数据的分类、回归等任务。&lt;/p>
&lt;p>反向传播算法是用于训练神经网络的关键算法之一，它通过计算损失函数对每个参数的梯度来更新模型参数，从而使得模型能够逐渐优化以达到最佳性能。下面我将介绍反向传播算法的计算过程，以及链式法则如何用来计算梯度。&lt;/p>
&lt;h3 id="反向传播算法的计算过程" >
&lt;div>
&lt;a href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e7%9a%84%e8%ae%a1%e7%ae%97%e8%bf%87%e7%a8%8b">
##
&lt;/a>
反向传播算法的计算过程：
&lt;/div>
&lt;/h3>&lt;ol>
&lt;li>
&lt;p>&lt;strong>前向传播&lt;/strong>：
首先，通过前向传播计算模型的输出。将输入数据输入到网络中，按照网络结构逐层计算每个神经元的输出，并将输出传递给下一层，直至生成最终的预测结果。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>计算损失&lt;/strong>：
使用损失函数计算模型的预测值与真实标签之间的差异，得到损失值。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>反向传播&lt;/strong>：
从输出层开始，利用链式法则逐层计算每个参数的梯度。梯度表示了损失函数对参数的变化率，它告诉我们如何调整参数才能使损失函数最小化。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>参数更新&lt;/strong>：
根据计算得到的梯度，利用优化算法（如随机梯度下降）来更新模型参数。通常，参数更新的步长（学习率）是一个超参数，需要根据实际情况进行调整。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="链式法则如何用来计算梯度" >
&lt;div>
&lt;a href="#%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99%e5%a6%82%e4%bd%95%e7%94%a8%e6%9d%a5%e8%ae%a1%e7%ae%97%e6%a2%af%e5%ba%a6">
##
&lt;/a>
链式法则如何用来计算梯度：
&lt;/div>
&lt;/h3>&lt;p>链式法则是微积分中的基本原理，用于计算复合函数的导数。在反向传播算法中，我们利用链式法则来计算损失函数对模型参数的梯度，从而实现参数的更新。&lt;/p>
&lt;p>假设有一个复合函数 $z = f(g(x))$，其中 $x$ 是输入，$g(x)$ 是一个函数，$f(x)$ 是另一个函数。根据链式法则，$z$ 对 $x$ 的导数可以表示为：&lt;/p>
&lt;p>$$
\frac{dz}{dx} = \frac{dz}{dg} \cdot \frac{dg}{dx}
$$&lt;/p>
&lt;p>在神经网络中，我们可以将损失函数 $L$ 视为 $z$，模型的参数视为 $x$，前向传播过程中的每一层输出视为 $g$，激活函数视为 $f$。
利用链式法则，我们可以逐层计算损失函数对每个参数的梯度，然后根据优化算法更新参数，使得损失函数逐渐减小。&lt;/p>
&lt;p>总的来说，反向传播算法通过利用链式法则计算损失函数对参数的梯度，实现了高效的神经网络训练过程，使得模型能够自动学习复杂的数据表示。&lt;/p>
&lt;p>交叉熵（Cross-Entropy）是在分类任务中常用的损失函数，特别是在多分类任务中。它的原理和为什么有用可以通过以下几点进行详细解释：&lt;/p>
&lt;h3 id="1-损失函数的作用" >
&lt;div>
&lt;a href="#1-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e7%9a%84%e4%bd%9c%e7%94%a8">
##
&lt;/a>
1. 损失函数的作用：
&lt;/div>
&lt;/h3>&lt;p>损失函数用于衡量模型预测结果与真实标签之间的差异。优化模型的目标是最小化损失函数，使得模型能够产生与真实标签相匹配的预测结果。&lt;/p>
&lt;h3 id="2-交叉熵的定义" >
&lt;div>
&lt;a href="#2-%e4%ba%a4%e5%8f%89%e7%86%b5%e7%9a%84%e5%ae%9a%e4%b9%89">
##
&lt;/a>
2. 交叉熵的定义：
&lt;/div>
&lt;/h3>&lt;p>对于多分类任务，交叉熵损失函数的数学定义如下：&lt;/p>
&lt;p>$$
\text{Cross-Entropy}(y, \hat{y}) = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
$$&lt;/p>
&lt;p>其中，$y$ 是真实标签的概率分布（通常是一个one-hot编码的向量），$\hat{y}$ 是模型预测的概率分布，$N$ 是类别的数量。
该损失函数用于衡量真实标签与模型预测结果之间的差异。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">binary_cross_entropy&lt;/span>(y_true, y_pred):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_true &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_true)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Clip predictions to avoid log(0)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>clip(y_pred, &lt;span style="color:#ff9f43">1e-15&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> &lt;span style="color:#ff9f43">1e-15&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cross_entropy &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span>np&lt;span style="color:#ff6ac1">.&lt;/span>mean(y_true &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(y_pred) &lt;span style="color:#ff6ac1">+&lt;/span> (&lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> y_true) &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(&lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> y_pred))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> cross_entropy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_true &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#ff9f43">0.9&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>, &lt;span style="color:#ff9f43">0.8&lt;/span>, &lt;span style="color:#ff9f43">0.7&lt;/span>, &lt;span style="color:#ff9f43">0.2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(binary_cross_entropy(y_true, y_pred))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">multiclass_cross_entropy&lt;/span>(y_true, y_pred):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_true &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_true)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Clip predictions to avoid log(0)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>clip(y_pred, &lt;span style="color:#ff9f43">1e-15&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> &lt;span style="color:#ff9f43">1e-15&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cross_entropy &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span>np&lt;span style="color:#ff6ac1">.&lt;/span>mean(np&lt;span style="color:#ff6ac1">.&lt;/span>sum(y_true &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(y_pred), axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> cross_entropy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_true &lt;span style="color:#ff6ac1">=&lt;/span> [[&lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>], [&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>], [&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>]] &lt;span style="color:#78787e"># how to get these? One-hot encoding.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> [[&lt;span style="color:#ff9f43">0.9&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>], [&lt;span style="color:#ff9f43">0.1&lt;/span>, &lt;span style="color:#ff9f43">0.8&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>], [&lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.9&lt;/span>]] &lt;span style="color:#78787e"># how to get these? Softmax.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(multiclass_cross_entropy(y_true, y_pred))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Softmax: Get the y_pred from model output raw scores (i.e., logits)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">softmax&lt;/span>(logits):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exp_logits &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>exp(logits &lt;span style="color:#ff6ac1">-&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>max(logits, axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, keepdims&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> exp_logits &lt;span style="color:#ff6ac1">/&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>sum(exp_logits, axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, keepdims&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example logits (raw scores) outputted by a model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logits &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([[&lt;span style="color:#ff9f43">2.0&lt;/span>, &lt;span style="color:#ff9f43">1.0&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ff9f43">0.5&lt;/span>, &lt;span style="color:#ff9f43">2.5&lt;/span>, &lt;span style="color:#ff9f43">1.5&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ff9f43">1.2&lt;/span>, &lt;span style="color:#ff9f43">0.7&lt;/span>, &lt;span style="color:#ff9f43">2.1&lt;/span>]])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Compute the softmax probabilities&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> softmax(logits)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-原理解释" >
&lt;div>
&lt;a href="#3-%e5%8e%9f%e7%90%86%e8%a7%a3%e9%87%8a">
##
&lt;/a>
3. 原理解释：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>&lt;strong>信息论角度&lt;/strong>：
交叉熵损失函数源自信息论中的信息熵概念。信息熵用于衡量一个随机变量的不确定性，而交叉熵则衡量两个概率分布之间的差异。当真实标签和模型预测的分布越接近时，交叉熵越小。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>梯度下降优化&lt;/strong>：
交叉熵损失函数在梯度下降优化过程中具有良好的性质。它的导数相对简单，计算起来更加高效，而且当模型的预测结果与真实标签的差异较大时，梯度也会变得更大，从而加速模型参数的更新。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>适用于多分类任务&lt;/strong>：
交叉熵损失函数特别适用于多分类任务，因为它能够有效地衡量多个类别之间的差异，并且在模型优化过程中能够引导模型更快地收敛到最优解。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="4-为什么有用" >
&lt;div>
&lt;a href="#4-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e7%94%a8">
##
&lt;/a>
4. 为什么有用：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>&lt;strong>梯度信息&lt;/strong>：
交叉熵损失函数提供了丰富的梯度信息，使得模型可以更快地学习到正确的预测结果。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>适用性广泛&lt;/strong>：
交叉熵损失函数适用于多分类任务，并且在实际应用中表现良好，因此成为了许多分类任务的首选损失函数。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>与概率相关&lt;/strong>：
交叉熵损失函数直接与概率分布相关，更符合任务的本质，能够更好地指导模型学习数据的分布情况。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>总的来说，交叉熵损失函数通过衡量模型预测结果与真实标签之间的差异，提供了有效的优化目标，并在梯度下降优化过程中起到重要作用，因此被广泛应用于分类任务中。&lt;/p>
&lt;h2 id="bertroberta-model" >
&lt;div>
&lt;a href="#bertroberta-model">
#
&lt;/a>
BERT/RoBERTa model
&lt;/div>
&lt;/h2>&lt;p>BERT(Bidirectional Encoder Representations from Transformers):
BERT是一种双向的（Bidirectional）模型，这意味着它能够同时考虑到一个单词左边和右边的上下文信息。这使得BERT在理解句子语境时比之前的模型更为强大。
BERT模型的预训练过程是通过掩盖输入文本中的一部分词汇（Masked Language Model，MLM）和预测句子是否连续（Next Sentence Prediction，NSP）来完成的。
BERT以“transformer”为基础，这是一种自注意力（self-attention）机制的神经网络结构，它能够在考虑到输入序列的所有位置之间建立关联，从而更好地理解上下文。&lt;/p>
&lt;p>RoBERTa是Facebook AI提出的一种改进的预训练自然语言处理（NLP）模型，它在很大程度上建立在BERT的基础上，但通过一系列的改进，使其在多个NLP任务上表现更优秀。&lt;/p>
&lt;p>RoBERTa的主要改进包括：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>动态掩码策略（Dynamic Masking）：RoBERTa在预训练时采用了动态掩码策略，即在每个训练迭代中对输入句子进行随机化处理，而不是固定地在句子中随机掩码。这使得模型更好地学习句子中的上下文信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>更长的训练时间和更大的批次大小：RoBERTa使用了更大的批次大小和更长的训练时间，以提高模型的泛化能力和性能。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>去除NSP（Next Sentence Prediction）任务：RoBERTa不再使用BERT中的NSP任务，而是专注于MLM（Masked Language Model）任务，这使得模型更好地理解输入文本。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>更多的训练数据：RoBERTa使用了更多的文本数据来进行预训练，这有助于提高模型的泛化能力。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>总的来说，RoBERTa是对BERT模型的一种优化和改进，它在多个NLP任务上都取得了比BERT更好的性能。&lt;/p>
&lt;p>&lt;strong>更多细节，应该去查找台大李宏毅老师的课程，非常精彩的讲解。&lt;/strong>&lt;/p>
&lt;h2 id="faiss-indexivfpq" >
&lt;div>
&lt;a href="#faiss-indexivfpq">
#
&lt;/a>
FAISS (indexIVFPQ)
&lt;/div>
&lt;/h2>&lt;p>在我的另一篇博客单独有介绍，并带有代码示例。&lt;/p>
&lt;h2 id="bertopic" >
&lt;div>
&lt;a href="#bertopic">
#
&lt;/a>
BERTopic
&lt;/div>
&lt;/h2>&lt;ul>
&lt;li>clustering&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>UMAP/PCA&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>c-TF-IDF&lt;/li>
&lt;/ul>
&lt;p>BERTopic 是一个基于BERT的自然语言处理工具，用于主题建模任务。BERTopic结合了BERT的强大表示学习能力和主题建模的思想，能够在大规模文本数据上快速、准确地提取主题信息。&lt;/p>
&lt;p>BERTopic的工作原理如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>文本向量化&lt;/strong>：首先，BERTopic使用预训练的BERT模型来将输入文本转换为高维向量表示。这些向量捕捉了输入文本的语义信息，并且通常能够更好地反映文本之间的语义相似度。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>主题发现&lt;/strong>：接下来，BERTopic使用聚类算法（例如DBSCAN或HDBSCAN）对文本向量进行聚类，以发现潜在的主题。聚类算法将文本向量分组为具有相似主题的簇。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>主题关键词提取&lt;/strong>：对于每个发现的主题簇，BERTopic还可以提取关键词来描述该主题。这些关键词通常是簇中最具代表性的词语，帮助用户理解主题的内容。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>主题可视化&lt;/strong>：最后，BERTopic可以将发现的主题可视化，使用户能够直观地了解文本数据中的主题结构。通常，可视化结果会以簇的形式展示，每个簇代表一个主题，簇内的文本则表示该主题的具体内容。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>关于最佳实践，以下是一些建议：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>调整模型参数&lt;/strong>：根据任务需求和数据特点，调整BERTopic的参数，例如聚类算法的参数、主题数量等，以获得更好的结果。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>预处理文本数据&lt;/strong>：在使用BERTopic之前，对文本数据进行适当的预处理是很重要的，例如去除停用词、进行词干化或词形还原等，以减少噪音对主题建模的影响。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>理解主题结果&lt;/strong>：对于每个发现的主题，仔细查看主题簇中的文本，并考虑它们之间的相似性和共性，以确保主题的合理性和可解释性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>与领域知识结合&lt;/strong>：在解释和利用主题结果时，结合领域知识会更有帮助。通过深入了解领域专业术语和相关概念，可以更准确地理解和解释主题的含义。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>通过合理地使用BERTopic工具，并结合适当的数据预处理和模型调优，可以有效地完成基于自然语言的主题建模任务。&lt;/p>
&lt;p>当然，让我更详细地解释一下这些技术。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>UMAP（Uniform Manifold Approximation and Projection）&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>UMAP是一种非线性降维算法，旨在将高维数据映射到低维空间以进行可视化或进一步分析。&lt;/li>
&lt;li>UMAP相对于传统的降维技术（如t-SNE）具有更好的可扩展性和保持全局数据结构的能力。它能够保持更多的局部结构，同时在大规模数据上的计算效率更高。&lt;/li>
&lt;li>在BERTopic中，UMAP常用于对BERT向量化的文本数据进行降维，以便进行更好的可视化或进一步的分析。降维后的数据可以更容易地被人类理解和解释。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>c-TF-IDF（Class-based TF-IDF）&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>c-TF-IDF是一种改进的TF-IDF（Term Frequency-Inverse Document Frequency）方法，用于从文本数据中提取关键词。&lt;/li>
&lt;li>与传统的TF-IDF相比，c-TF-IDF考虑了单词在不同主题中的重要性，而不仅仅是在整个文本集合中的重要性。这使得c-TF-IDF能够更好地捕捉文本中的主题相关信息。&lt;/li>
&lt;li>在BERTopic中，c-TF-IDF常用于从每个发现的主题簇中提取关键词，以描述该主题的内容。这些关键词通常是簇中最具代表性的词语，帮助用户理解主题的含义和内容。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>HDBSCAN（Hierarchical Density-Based Spatial Clustering of Applications with Noise）&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>HDBSCAN是一种密度聚类算法，旨在发现数据中的高密度区域，并将它们组合成簇。&lt;/li>
&lt;li>与传统的基于距离的聚类算法（如K均值）不同，HDBSCAN不需要预先指定簇的数量，因此更适用于发现具有不同大小和形状的簇的数据。&lt;/li>
&lt;li>在BERTopic中，HDBSCAN通常与UMAP一起使用，用于对BERT向量化的文本数据进行聚类。HDBSCAN能够识别出具有不同主题的文本簇，从而帮助用户发现文本数据中的主题结构。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>综上所述，UMAP用于将高维文本向量降维到低维空间，以便于可视化和分析；c-TF-IDF用于从每个主题簇中提取关键词以描述主题内容；HDBSCAN用于发现文本数据中的潜在主题簇。这些技术的结合使得BERTopic能够有效地进行自然语言主题建模，并从文本数据中提取有意义的主题信息。&lt;/p>
&lt;h1 id="project-02----ml" >
&lt;div>
&lt;a href="#project-02----ml">
##
&lt;/a>
Project 02 &amp;ndash; ML
&lt;/div>
&lt;/h1>&lt;h2 id="self-training--gausianmixtruemodels" >
&lt;div>
&lt;a href="#self-training--gausianmixtruemodels">
#
&lt;/a>
self-training &amp;amp; GausianMixtrueModels
&lt;/div>
&lt;/h2>&lt;p>半监督学习是一种机器学习方法，它利用有标签和无标签的数据来进行模型训练。相比于只使用有标签数据进行训练，半监督学习可以利用更多的无标签数据，从而提高模型的性能。&lt;/p>
&lt;p>self-training（自训练）是半监督学习中的一种常见技术，其基本思想是利用已经训练好的模型对无标签数据进行预测，然后将置信度较高的预测结果作为伪标签，将这些伪标签的数据与有标签数据一起重新训练模型。&lt;/p>
&lt;p>具体来说，self-training的步骤如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>利用有标签数据训练一个初始模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>使用该模型对无标签数据进行预测，并选取置信度较高的预测结果作为伪标签。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>将伪标签的数据与原有的有标签数据合并，重新训练模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>重复步骤2和步骤3，直到满足停止条件（比如达到最大迭代次数、模型性能不再提升等）。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>self-training的关键在于如何选择置信度较高的预测结果作为伪标签。一般来说，可以通过设置一个阈值来筛选置信度较高的预测结果，也可以使用模型的输出概率来作为置信度的度量。&lt;/p>
&lt;p>使用self-training技术可以帮助扩充样本，提高模型的泛化能力，特别是在标记数据有限的情况下。但需要注意的是，自训练过程中可能会引入噪声，因此需要仔细调节参数和监控模型性能，以避免过拟合和性能下降的问题。&lt;/p>
&lt;p>如果聚类簇内的标签并不一致，而且类别数目较为相同，这种情况可能会导致一些混乱，因为无法简单地选择一个代表性的标签来为整个聚类簇的样本打标签。在这种情况下，可以考虑以下几种处理方式：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>投票机制&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>对于每个聚类簇，可以采用投票机制来选择标签。即，统计聚类簇中样本的真实标签或者已有的伪标签，选择出现频率最高的标签作为整个聚类簇的标签。&lt;/li>
&lt;li>如果有多个标签出现频率相同，可以随机选择其中一个或者采用一些其他的策略来解决冲突。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>标签融合&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>对于聚类簇内的样本，可以将其真实标签或者已有的伪标签进行融合。例如，可以计算聚类簇中每个类别的权重，然后根据权重对多个标签进行加权平均，得到一个综合的标签。&lt;/li>
&lt;li>这种方法可以在一定程度上解决标签冲突的问题，但需要谨慎设计权重计算的方法，以避免给不太准确的标签过多的权重。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>进一步分析&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>对于那些标签冲突较为严重的聚类簇，可以进一步分析其样本特征或者数据分布，尝试找到更合适的标签选择策略。&lt;/li>
&lt;li>可以考虑使用一些数据挖掘技术或者领域知识来帮助解决这个问题。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>半监督学习方法&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>除了高斯混合聚类，还可以尝试其他半监督学习方法，如图半监督学习（Graph-based Semi-Supervised Learning）、标签传播算法（Label Propagation）、自训练（Self-training）等。&lt;/li>
&lt;li>这些方法可能会更有效地处理标签冲突的问题，因为它们可以更好地利用数据之间的相似性和关联性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>在实际应用中，可以根据具体情况选择适合的方法来处理标签冲突问题，同时需要注意监控模型的性能和效果，及时调整和优化算法。&lt;/p>
&lt;h2 id="xgboost" >
&lt;div>
&lt;a href="#xgboost">
#
&lt;/a>
XGBoost
&lt;/div>
&lt;/h2>&lt;p>XGBoost，全称为“eXtreme Gradient Boosting”，是一种高效的集成学习算法，属于梯度提升树（Gradient Boosting Tree）的一种实现。它在机器学习竞赛中非常流行，因为它能够在各种类型的数据集上取得优秀的性能，并且相对于其他算法，它通常更容易调整参数以获得更好的性能。&lt;/p>
&lt;p>以下是 XGBoost 模型的一些关键特点和优势：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>集成学习&lt;/strong>：XGBoost 是一种集成学习算法，它通过组合多个弱学习器（通常是决策树）来构建一个强大的模型。每个决策树都是根据前一个树的错误进行训练，以逐步减少模型的残差。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>梯度提升算法&lt;/strong>：XGBoost 使用梯度提升算法来训练模型。该算法通过最小化损失函数的梯度来优化模型，从而使模型在每一步都更加贴近真实值。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>正则化&lt;/strong>：XGBoost 提供了对模型进行正则化的选项，包括 L1 和 L2 正则化，以及控制树的复杂度的参数。这有助于减少过拟合，提高模型的泛化能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>支持并行化&lt;/strong>：XGBoost 可以有效地利用并行计算资源进行训练，因此在大规模数据集上也能够快速地训练模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>特征重要性评估&lt;/strong>：XGBoost 可以计算特征的重要性，从而帮助用户了解哪些特征对模型的预测最为关键。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>灵活性&lt;/strong>：XGBoost 可以用于分类问题、回归问题以及排序问题，并且可以在不同类型的数据集上进行应用。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>总的来说，XGBoost 是一种强大且灵活的机器学习算法，适用于各种类型的问题，并且在实践中表现出色。&lt;/p>
&lt;p>梯度提升算法（Gradient Boosting Algorithm）是一种集成学习方法，通过将多个弱学习器（通常是决策树）串联起来，逐步减少模型的残差来构建一个强大的预测模型。梯度提升算法通过梯度下降的思想，不断优化模型以最小化损失函数。&lt;/p>
&lt;p>下面是梯度提升算法的基本步骤：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>初始化模型&lt;/strong>：梯度提升算法通常从一个简单的模型开始，例如用一个常数来拟合数据的平均值。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>迭代优化&lt;/strong>：接下来，算法迭代地执行以下步骤：&lt;/p>
&lt;ul>
&lt;li>计算残差：使用当前模型对训练数据进行预测，并计算实际值与预测值之间的残差。&lt;/li>
&lt;li>拟合残差：构建一个新的弱学习器（如决策树），以拟合残差。这意味着新的学习器会尝试纠正上一个模型的错误。&lt;/li>
&lt;li>更新模型：将新的学习器与前面的模型组合起来，形成一个更强大的模型。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>停止条件&lt;/strong>：当达到预先设定的迭代次数，或者当模型的性能满足某个特定的标准时，停止迭代。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>与梯度下降算法的关系在于，梯度提升算法也利用了梯度的信息来优化模型。但两者之间的关键区别在于优化的对象和优化方向。在梯度下降算法中，优化的对象是损失函数本身，而优化的方向是沿着损失函数梯度的反方向。而在梯度提升算法中，优化的对象是损失函数的残差，优化的方向是使残差最小化的方向。&lt;/p>
&lt;p>总的来说，梯度提升算法是一种强大的集成学习方法，通过不断迭代优化模型以减少残差，从而构建一个强大的预测模型。它利用了梯度信息来指导优化过程，但与梯度下降算法相比，它的优化目标和优化方向有所不同。&lt;/p>
&lt;h2 id="onnx-dockerpodman-and-restful-api" >
&lt;div>
&lt;a href="#onnx-dockerpodman-and-restful-api">
#
&lt;/a>
ONNX, Docker/Podman, and restful-api
&lt;/div>
&lt;/h2>&lt;p>当你希望使用 Docker 来部署服务，并且构建 Flask RESTful API 来提供对 ONNX 模型的推理服务时，你可以按照以下步骤进行：&lt;/p>
&lt;h3 id="1-准备-flask-restful-api-代码" >
&lt;div>
&lt;a href="#1-%e5%87%86%e5%a4%87-flask-restful-api-%e4%bb%a3%e7%a0%81">
##
&lt;/a>
1. 准备 Flask RESTful API 代码
&lt;/div>
&lt;/h3>&lt;p>你需要创建一个 Flask 应用程序，编写代码以加载 ONNX 模型并提供 RESTful API 来进行推理。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> flask &lt;span style="color:#ff6ac1">import&lt;/span> Flask, request
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> flask_restful &lt;span style="color:#ff6ac1">import&lt;/span> Api, Resource
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> onnxruntime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app &lt;span style="color:#ff6ac1">=&lt;/span> Flask(__name__)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>api &lt;span style="color:#ff6ac1">=&lt;/span> Api(app)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">class&lt;/span> &lt;span style="color:#f3f99d">ModelInference&lt;/span>(Resource):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> __init__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">super&lt;/span>(ModelInference, self)&lt;span style="color:#ff6ac1">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff6ac1">.&lt;/span>session &lt;span style="color:#ff6ac1">=&lt;/span> onnxruntime&lt;span style="color:#ff6ac1">.&lt;/span>InferenceSession(&lt;span style="color:#5af78e">&amp;#34;your_model.onnx&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">post&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> data &lt;span style="color:#ff6ac1">=&lt;/span> request&lt;span style="color:#ff6ac1">.&lt;/span>json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_data &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(data[&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>])&lt;span style="color:#ff6ac1">.&lt;/span>astype(np&lt;span style="color:#ff6ac1">.&lt;/span>float32)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>session&lt;span style="color:#ff6ac1">.&lt;/span>run([], {&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>: input_data})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> {&lt;span style="color:#5af78e">&amp;#34;output&amp;#34;&lt;/span>: output&lt;span style="color:#ff6ac1">.&lt;/span>tolist()}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>api&lt;span style="color:#ff6ac1">.&lt;/span>add_resource(ModelInference, &lt;span style="color:#5af78e">&amp;#34;/predict&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">if&lt;/span> __name__ &lt;span style="color:#ff6ac1">==&lt;/span> &lt;span style="color:#5af78e">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app&lt;span style="color:#ff6ac1">.&lt;/span>run(debug&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>, host&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;0.0.0.0&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-创建-dockerfile" >
&lt;div>
&lt;a href="#2-%e5%88%9b%e5%bb%ba-dockerfile">
##
&lt;/a>
2. 创建 Dockerfile
&lt;/div>
&lt;/h3>&lt;p>创建一个 Dockerfile 来构建 Docker 镜像。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">FROM&lt;/span>&lt;span style="color:#5af78e"> python:3.9-slim&lt;/span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">WORKDIR&lt;/span>&lt;span style="color:#5af78e"> /app&lt;/span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">COPY&lt;/span> requirements.txt .&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">RUN&lt;/span> pip install --no-cache-dir -r requirements.txt&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">COPY&lt;/span> . .&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">CMD&lt;/span> [ &lt;span style="color:#5af78e">&amp;#34;python&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;app.py&amp;#34;&lt;/span> ]&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-构建-docker-镜像" >
&lt;div>
&lt;a href="#3-%e6%9e%84%e5%bb%ba-docker-%e9%95%9c%e5%83%8f">
##
&lt;/a>
3. 构建 Docker 镜像
&lt;/div>
&lt;/h3>&lt;p>在包含 Dockerfile 的目录下执行以下命令来构建 Docker 镜像。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker build -t your_image_name .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="4-运行-docker-容器" >
&lt;div>
&lt;a href="#4-%e8%bf%90%e8%a1%8c-docker-%e5%ae%b9%e5%99%a8">
##
&lt;/a>
4. 运行 Docker 容器
&lt;/div>
&lt;/h3>&lt;p>使用以下命令来运行你的 Docker 容器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -p 5000:5000 your_image_name
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="5-测试-api" >
&lt;div>
&lt;a href="#5-%e6%b5%8b%e8%af%95-api">
##
&lt;/a>
5. 测试 API
&lt;/div>
&lt;/h3>&lt;p>使用任何 HTTP 客户端工具或 Python 应用程序来测试你的 API。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> requests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data &lt;span style="color:#ff6ac1">=&lt;/span> {&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>: [&lt;span style="color:#ff9f43">1.0&lt;/span>, &lt;span style="color:#ff9f43">2.0&lt;/span>, &lt;span style="color:#ff9f43">3.0&lt;/span>]} &lt;span style="color:#78787e"># 示例输入数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>response &lt;span style="color:#ff6ac1">=&lt;/span> requests&lt;span style="color:#ff6ac1">.&lt;/span>post(&lt;span style="color:#5af78e">&amp;#34;http://localhost:5000/predict&amp;#34;&lt;/span>, json&lt;span style="color:#ff6ac1">=&lt;/span>data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(response&lt;span style="color:#ff6ac1">.&lt;/span>json())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="总结-3" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-3">
##
&lt;/a>
总结
&lt;/div>
&lt;/h3>&lt;p>这就是一个简单的部署流程。你可以根据你的具体需求进行调整和扩展，比如添加模型预处理、后处理逻辑，以及对 API 的身份验证和访问控制等功能。&lt;/p>
&lt;h1 id="project-03----cv" >
&lt;div>
&lt;a href="#project-03----cv">
##
&lt;/a>
Project 03 &amp;ndash; CV
&lt;/div>
&lt;/h1>&lt;h2 id="yolo-model" >
&lt;div>
&lt;a href="#yolo-model">
#
&lt;/a>
YOLO model
&lt;/div>
&lt;/h2>&lt;p>好的，让我来详细介绍一下YOLO（You Only Look Once）目标检测模型的原理和实现。&lt;/p>
&lt;h3 id="1-yolo-模型的原理" >
&lt;div>
&lt;a href="#1-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8e%9f%e7%90%86">
##
&lt;/a>
1. YOLO 模型的原理：
&lt;/div>
&lt;/h3>&lt;h4 id="11-单阶段检测" >
&lt;div>
&lt;a href="#11-%e5%8d%95%e9%98%b6%e6%ae%b5%e6%a3%80%e6%b5%8b">
###
&lt;/a>
1.1 单阶段检测：
&lt;/div>
&lt;/h4>&lt;p>YOLO 是一种单阶段目标检测模型，与传统的两阶段检测方法（如Faster R-CNN）不同，它将目标检测任务视为一个端到端的回归问题，直接从图像中预测目标的位置和类别。&lt;/p>
&lt;h4 id="12-网络结构" >
&lt;div>
&lt;a href="#12-%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84">
###
&lt;/a>
1.2 网络结构：
&lt;/div>
&lt;/h4>&lt;p>YOLO 模型主要由卷积神经网络组成，通常采用类似于Darknet的深层卷积神经网络作为特征提取器。&lt;/p>
&lt;h4 id="13-网络输出" >
&lt;div>
&lt;a href="#13-%e7%bd%91%e7%bb%9c%e8%be%93%e5%87%ba">
###
&lt;/a>
1.3 网络输出：
&lt;/div>
&lt;/h4>&lt;p>YOLO 将图像划分为固定大小的网格，并为每个网格预测多个边界框和对应的类别概率。每个边界框由五个坐标值和类别概率组成：$(x, y, w, h, p)$，其中 $(x, y)$ 是边界框的中心坐标，$(w, h)$ 是边界框的宽度和高度，$p$ 是边界框包含目标的置信度。&lt;/p>
&lt;h4 id="14-损失函数" >
&lt;div>
&lt;a href="#14-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0">
###
&lt;/a>
1.4 损失函数：
&lt;/div>
&lt;/h4>&lt;p>YOLO 模型使用组合损失函数来同时优化边界框位置的准确性和类别的预测精度。该损失函数包括位置误差、置信度误差和类别误差。&lt;/p>
&lt;h3 id="2-yolo-模型的实现" >
&lt;div>
&lt;a href="#2-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
2. YOLO 模型的实现：
&lt;/div>
&lt;/h3>&lt;h4 id="21-训练阶段" >
&lt;div>
&lt;a href="#21-%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5">
###
&lt;/a>
2.1 训练阶段：
&lt;/div>
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>数据准备&lt;/strong>：收集和标记训练数据集，包括图像和对应的目标边界框。&lt;/li>
&lt;li>&lt;strong>网络结构&lt;/strong>：选择适当的网络结构，通常使用预训练的Darknet网络或其变种。&lt;/li>
&lt;li>&lt;strong>损失函数&lt;/strong>：定义和实现组合损失函数，用于优化网络参数。&lt;/li>
&lt;li>&lt;strong>训练过程&lt;/strong>：使用训练数据集对模型进行训练，通过反向传播算法更新网络参数，以最小化损失函数。&lt;/li>
&lt;/ul>
&lt;h4 id="22-推理阶段" >
&lt;div>
&lt;a href="#22-%e6%8e%a8%e7%90%86%e9%98%b6%e6%ae%b5">
###
&lt;/a>
2.2 推理阶段：
&lt;/div>
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>前向传播&lt;/strong>：将待检测的图像输入到训练好的模型中，通过前向传播算法获取每个边界框的预测结果。&lt;/li>
&lt;li>&lt;strong>后处理&lt;/strong>：对网络输出进行后处理，包括非极大值抑制（NMS）和阈值筛选，以去除重叠的边界框和低置信度的边界框。&lt;/li>
&lt;li>&lt;strong>目标框绘制&lt;/strong>：根据最终的边界框结果，将目标框绘制在原始图像上，并标记类别。&lt;/li>
&lt;/ul>
&lt;h3 id="3-yolo-模型的优势" >
&lt;div>
&lt;a href="#3-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e4%bc%98%e5%8a%bf">
##
&lt;/a>
3. YOLO 模型的优势：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>速度快&lt;/strong>：YOLO 是一种高效的目标检测模型，能够实时处理图像和视频流。&lt;/li>
&lt;li>&lt;strong>端到端&lt;/strong>：YOLO 将目标检测视为一个端到端的回归问题，简化了检测流程。&lt;/li>
&lt;li>&lt;strong>全局信息&lt;/strong>：YOLO 在整个图像上进行检测，能够同时考虑图像中的全局信息，从而更好地理解场景。&lt;/li>
&lt;/ul>
&lt;h3 id="4-总结" >
&lt;div>
&lt;a href="#4-%e6%80%bb%e7%bb%93">
##
&lt;/a>
4. 总结：
&lt;/div>
&lt;/h3>&lt;p>YOLO 是一种高效的单阶段目标检测模型，通过将目标检测任务转化为回归问题，并结合有效的网络结构和损失函数，实现了在保持高准确率的同时实时进行目标检测的能力。&lt;/p>
&lt;p>&amp;ldquo;锚定点&amp;rdquo;（Anchor Boxes）是YOLO模型中的一个重要概念，它用于解决目标检测中不同目标尺寸和比例的问题。在YOLO中，每个网格单元都负责预测一组固定数量和固定大小的边界框（即锚定点），以便检测不同尺寸和比例的目标。&lt;/p>
&lt;h3 id="1-锚定点的概念" >
&lt;div>
&lt;a href="#1-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%a6%82%e5%bf%b5">
##
&lt;/a>
1. 锚定点的概念：
&lt;/div>
&lt;/h3>&lt;h4 id="11-问题" >
&lt;div>
&lt;a href="#11-%e9%97%ae%e9%a2%98">
###
&lt;/a>
1.1 问题：
&lt;/div>
&lt;/h4>&lt;p>传统的目标检测算法通常会将不同尺寸和比例的目标分配给不同的网络层来处理，这种方法不够灵活，无法很好地适应多样化的目标。&lt;/p>
&lt;h4 id="12-解决方法" >
&lt;div>
&lt;a href="#12-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%b3%95">
###
&lt;/a>
1.2 解决方法：
&lt;/div>
&lt;/h4>&lt;p>YOLO使用锚定点的思想，将不同尺寸和比例的目标统一分配给每个网格单元，并在每个网格单元中预测固定数量的边界框。这样可以增加模型的灵活性，使其能够检测不同尺寸和比例的目标。&lt;/p>
&lt;h3 id="2-锚定点的思想" >
&lt;div>
&lt;a href="#2-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%80%9d%e6%83%b3">
##
&lt;/a>
2. 锚定点的思想：
&lt;/div>
&lt;/h3>&lt;h4 id="21-预定义大小和比例" >
&lt;div>
&lt;a href="#21-%e9%a2%84%e5%ae%9a%e4%b9%89%e5%a4%a7%e5%b0%8f%e5%92%8c%e6%af%94%e4%be%8b">
###
&lt;/a>
2.1 预定义大小和比例：
&lt;/div>
&lt;/h4>&lt;p>在YOLO中，锚定点是一组预定义的大小和比例的边界框。这些边界框通常是在训练数据集上通过聚类等方法得到的，以确保涵盖了大部分目标的大小和比例。&lt;/p>
&lt;h4 id="22-单元格内多个边界框" >
&lt;div>
&lt;a href="#22-%e5%8d%95%e5%85%83%e6%a0%bc%e5%86%85%e5%a4%9a%e4%b8%aa%e8%be%b9%e7%95%8c%e6%a1%86">
###
&lt;/a>
2.2 单元格内多个边界框：
&lt;/div>
&lt;/h4>&lt;p>对于每个网格单元，YOLO模型预测固定数量的边界框，每个边界框的大小和比例与预定义的锚定点相对应。这样，每个网格单元可以同时检测多个不同尺寸和比例的目标。&lt;/p>
&lt;h3 id="3-锚定点的技术实现" >
&lt;div>
&lt;a href="#3-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%8a%80%e6%9c%af%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
3. 锚定点的技术实现：
&lt;/div>
&lt;/h3>&lt;h4 id="31-预训练锚定点" >
&lt;div>
&lt;a href="#31-%e9%a2%84%e8%ae%ad%e7%bb%83%e9%94%9a%e5%ae%9a%e7%82%b9">
###
&lt;/a>
3.1 预训练锚定点：
&lt;/div>
&lt;/h4>&lt;p>在训练之前，通常会通过对训练数据集中的目标边界框进行聚类或者手动选择，来确定一组锚定点的大小和比例。&lt;/p>
&lt;h4 id="32-模型预测" >
&lt;div>
&lt;a href="#32-%e6%a8%a1%e5%9e%8b%e9%a2%84%e6%b5%8b">
###
&lt;/a>
3.2 模型预测：
&lt;/div>
&lt;/h4>&lt;p>在模型推理阶段，每个网格单元通过回归预测固定数量的边界框，每个边界框的尺寸和比例由对应的锚定点确定。&lt;/p>
&lt;h4 id="33-边界框调整" >
&lt;div>
&lt;a href="#33-%e8%be%b9%e7%95%8c%e6%a1%86%e8%b0%83%e6%95%b4">
###
&lt;/a>
3.3 边界框调整：
&lt;/div>
&lt;/h4>&lt;p>模型预测的边界框通常是相对于网格单元的偏移量和尺寸偏差，需要根据锚定点进行调整，得到最终的边界框位置。&lt;/p>
&lt;h3 id="4-总结-1" >
&lt;div>
&lt;a href="#4-%e6%80%bb%e7%bb%93-1">
##
&lt;/a>
4. 总结：
&lt;/div>
&lt;/h3>&lt;p>锚定点是YOLO模型中用于解决不同尺寸和比例目标检测问题的关键概念，通过预定义一组大小和比例的边界框，并在每个网格单元中预测这些边界框的位置和类别，实现了模型对多样化目标的有效检测。&lt;/p>
&lt;h2 id="transfer-learning" >
&lt;div>
&lt;a href="#transfer-learning">
#
&lt;/a>
transfer-learning
&lt;/div>
&lt;/h2>&lt;p>迁移学习是一种通过将已学习的知识从一个任务或领域应用到另一个任务或领域的机器学习技术。在目标检测任务中，迁移学习可以通过利用预训练的模型或特征来提升模型性能。以下是一些常用的迁移学习方法：&lt;/p>
&lt;h3 id="1-微调fine-tuning" >
&lt;div>
&lt;a href="#1-%e5%be%ae%e8%b0%83fine-tuning">
##
&lt;/a>
1. 微调（Fine-tuning）：
&lt;/div>
&lt;/h3>&lt;p>微调是迁移学习中最常见的方法之一，它通常包括以下步骤：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>预训练模型选择&lt;/strong>：选择一个在大规模数据集上预训练好的模型，例如 ImageNet 上的预训练模型。&lt;/li>
&lt;li>&lt;strong>模型冻结&lt;/strong>：将预训练模型的部分或全部层冻结，即不更新它们的权重。&lt;/li>
&lt;li>&lt;strong>顶层替换&lt;/strong>：替换预训练模型的顶层（通常是全连接层）或者添加新的全连接层，以适应新的目标检测任务。&lt;/li>
&lt;li>&lt;strong>微调训练&lt;/strong>：在目标检测数据集上对整个模型进行训练，包括更新顶层和部分或全部解冻的层。&lt;/li>
&lt;/ul>
&lt;h3 id="2-特征提取feature-extraction" >
&lt;div>
&lt;a href="#2-%e7%89%b9%e5%be%81%e6%8f%90%e5%8f%96feature-extraction">
##
&lt;/a>
2. 特征提取（Feature Extraction）：
&lt;/div>
&lt;/h3>&lt;p>特征提取是一种更轻量级的迁移学习方法，它不涉及到整个模型的重新训练，而是仅仅利用预训练模型的特征提取能力。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>预训练模型选择&lt;/strong>：同样选择一个在大规模数据集上预训练好的模型。&lt;/li>
&lt;li>&lt;strong>特征提取&lt;/strong>：将预训练模型的卷积层（通常是除了全连接层之外的所有层）作为特征提取器，并将提取到的特征作为输入，用于训练一个新的分类器或目标检测器。&lt;/li>
&lt;/ul>
&lt;h3 id="3-领域自适应domain-adaptation" >
&lt;div>
&lt;a href="#3-%e9%a2%86%e5%9f%9f%e8%87%aa%e9%80%82%e5%ba%94domain-adaptation">
##
&lt;/a>
3. 领域自适应（Domain Adaptation）：
&lt;/div>
&lt;/h3>&lt;p>领域自适应是一种特殊的迁移学习方法，用于解决源域和目标域数据分布不匹配的问题。在目标检测任务中，可以通过在源域和目标域之间进行域适应来提高模型的泛化能力。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>域适应方法&lt;/strong>：常见的域适应方法包括对抗训练、领域对齐等，通过调整模型的训练策略，使得模型在目标域上表现更好。&lt;/li>
&lt;/ul>
&lt;h3 id="4-知识蒸馏knowledge-distillation" >
&lt;div>
&lt;a href="#4-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8fknowledge-distillation">
##
&lt;/a>
4. 知识蒸馏（Knowledge Distillation）：
&lt;/div>
&lt;/h3>&lt;p>知识蒸馏是一种通过利用已训练好的模型的知识来训练一个更轻量级的模型的方法。在目标检测任务中，可以利用一个大型模型的知识来训练一个小型模型，以降低模型的复杂度和计算成本。&lt;/p>
&lt;p>以上是一些常用的迁移学习方法，可以根据具体的任务需求和情况选择合适的方法来提升目标检测模型的性能。&lt;/p>
&lt;h2 id="图像识别长尾分布问题" >
&lt;div>
&lt;a href="#%e5%9b%be%e5%83%8f%e8%af%86%e5%88%ab%e9%95%bf%e5%b0%be%e5%88%86%e5%b8%83%e9%97%ae%e9%a2%98">
#
&lt;/a>
图像识别长尾分布问题
&lt;/div>
&lt;/h2>&lt;p>处理训练数据集中的长尾分布是一个常见的问题，在目标检测任务中也同样存在。长尾分布意味着有些类别的样本数量非常少，而另一些类别的样本数量非常多。解决这个问题的方法包括：&lt;/p>
&lt;h3 id="1-数据增强data-augmentation" >
&lt;div>
&lt;a href="#1-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%badata-augmentation">
##
&lt;/a>
1. 数据增强（Data Augmentation）：
&lt;/div>
&lt;/h3>&lt;p>对于少样本类别，可以通过数据增强技术来生成更多的样本，以平衡不同类别之间的样本数量差异。常用的数据增强技术包括随机旋转、裁剪、缩放、平移、颜色变换等。&lt;/p>
&lt;h3 id="2-类别加权class-weighting" >
&lt;div>
&lt;a href="#2-%e7%b1%bb%e5%88%ab%e5%8a%a0%e6%9d%83class-weighting">
##
&lt;/a>
2. 类别加权（Class Weighting）：
&lt;/div>
&lt;/h3>&lt;p>对于长尾分布的数据集，可以采用类别加权的方式来调整模型的损失函数，使得模型对少样本类别更加敏感。可以根据类别出现的频率来设置不同类别的权重，使得损失函数更平衡。&lt;/p>
&lt;h3 id="3-重新采样resampling" >
&lt;div>
&lt;a href="#3-%e9%87%8d%e6%96%b0%e9%87%87%e6%a0%b7resampling">
##
&lt;/a>
3. 重新采样（Resampling）：
&lt;/div>
&lt;/h3>&lt;p>重新采样技术可以通过过采样或欠采样来调整数据集中不同类别的样本数量。对于少样本类别，可以采用过采样的方法增加样本数量，或者采用欠采样的方法减少样本数量。&lt;/p>
&lt;h3 id="4-弱监督学习weakly-supervised-learning" >
&lt;div>
&lt;a href="#4-%e5%bc%b1%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0weakly-supervised-learning">
##
&lt;/a>
4. 弱监督学习（Weakly Supervised Learning）：
&lt;/div>
&lt;/h3>&lt;p>在长尾分布的数据集中，有些类别可能只有少量的有标签样本，而大部分样本是未标签的。可以利用弱监督学习的方法，例如使用无监督或半监督学习技术，从未标签的数据中学习有用的特征。&lt;/p>
&lt;h3 id="5-多任务学习multi-task-learning" >
&lt;div>
&lt;a href="#5-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e5%ad%a6%e4%b9%a0multi-task-learning">
##
&lt;/a>
5. 多任务学习（Multi-Task Learning）：
&lt;/div>
&lt;/h3>&lt;p>多任务学习可以将目标检测任务与其他相关任务结合起来训练模型，从而提升模型在长尾分布数据集上的性能。例如，可以将目标检测任务与图像分类、语义分割等任务结合起来进行训练，从而利用额外的信息来提升模型性能。&lt;/p>
&lt;h3 id="6-异常检测anomaly-detection" >
&lt;div>
&lt;a href="#6-%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8banomaly-detection">
##
&lt;/a>
6. 异常检测（Anomaly Detection）：
&lt;/div>
&lt;/h3>&lt;p>针对长尾分布数据集中的少样本类别，可以采用异常检测的方法来识别并重点关注这些少样本类别，以便更好地调整模型和优化性能。&lt;/p>
&lt;p>通过以上方法，可以有效地处理训练数据集中的长尾分布问题，并提升模型在不同类别上的性能。在实践中，通常需要根据具体的数据集和任务需求选择合适的方法进行处理。&lt;/p>
&lt;h2 id="auto-encoder-model" >
&lt;div>
&lt;a href="#auto-encoder-model">
#
&lt;/a>
Auto-encoder model
&lt;/div>
&lt;/h2>&lt;p>领域自适应是一种将知识从一个域（源域）迁移到另一个域（目标域）的迁移学习方法，用于解决源域和目标域数据分布不匹配的问题。在目标检测任务中，领域自适应可以通过对抗训练的方式来实现，而 auto-encoder 可以作为一种辅助手段来帮助实现对抗训练。以下是具体的思路和细节：&lt;/p>
&lt;h3 id="1-对抗训练的基本思路" >
&lt;div>
&lt;a href="#1-%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%80%9d%e8%b7%af">
##
&lt;/a>
1. 对抗训练的基本思路：
&lt;/div>
&lt;/h3>&lt;p>对抗训练是一种通过训练一个生成器和一个判别器的对抗过程，来使得生成器产生的数据分布与目标域的数据分布尽可能地接近。在目标检测任务中，可以通过对抗训练来调整模型，使得模型在目标域上表现更好。&lt;/p>
&lt;h3 id="2-auto-encoder-在对抗训练中的作用" >
&lt;div>
&lt;a href="#2-auto-encoder-%e5%9c%a8%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83%e4%b8%ad%e7%9a%84%e4%bd%9c%e7%94%a8">
##
&lt;/a>
2. auto-encoder 在对抗训练中的作用：
&lt;/div>
&lt;/h3>&lt;p>auto-encoder 是一种无监督学习模型，它可以将输入数据编码成低维表示，并将其解码回原始数据。在对抗训练中，auto-encoder 可以作为一个生成器，用于生成与目标域数据分布相似的数据样本。通过训练 auto-encoder，可以学习到目标域数据的特征表示，从而帮助实现对抗训练。&lt;/p>
&lt;h3 id="3-具体实现步骤" >
&lt;div>
&lt;a href="#3-%e5%85%b7%e4%bd%93%e5%ae%9e%e7%8e%b0%e6%ad%a5%e9%aa%a4">
##
&lt;/a>
3. 具体实现步骤：
&lt;/div>
&lt;/h3>&lt;h4 id="31-模型选择" >
&lt;div>
&lt;a href="#31-%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9">
###
&lt;/a>
3.1 模型选择：
&lt;/div>
&lt;/h4>&lt;ul>
&lt;li>选择一个预训练的模型作为基础模型，例如在源域上训练好的目标检测模型。&lt;/li>
&lt;/ul>
&lt;h4 id="32-auto-encoder-训练" >
&lt;div>
&lt;a href="#32-auto-encoder-%e8%ae%ad%e7%bb%83">
###
&lt;/a>
3.2 auto-encoder 训练：
&lt;/div>
&lt;/h4>&lt;ul>
&lt;li>在目标域上收集一部分数据，并使用这些数据来训练 auto-encoder。auto-encoder 的输入为目标域的图像数据，输出为重构的图像数据。通过训练 auto-encoder，可以学习到目标域数据的特征表示。&lt;/li>
&lt;/ul>
&lt;h4 id="33-对抗训练" >
&lt;div>
&lt;a href="#33-%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83">
###
&lt;/a>
3.3 对抗训练：
&lt;/div>
&lt;/h4>&lt;ul>
&lt;li>将训练好的 auto-encoder 作为生成器，将基础模型（源域上预训练的模型）作为判别器。&lt;/li>
&lt;li>将源域和目标域的数据分别输入到 auto-encoder 和基础模型中，生成器尝试生成与目标域数据分布相似的数据样本，而判别器则尝试区分真实的目标域数据和生成器生成的数据。&lt;/li>
&lt;li>通过对抗训练的过程，调整生成器和判别器的参数，使得生成器生成的数据分布与目标域的数据分布尽可能地接近。&lt;/li>
&lt;/ul>
&lt;h3 id="4-进一步细化" >
&lt;div>
&lt;a href="#4-%e8%bf%9b%e4%b8%80%e6%ad%a5%e7%bb%86%e5%8c%96">
##
&lt;/a>
4. 进一步细化：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>可以考虑使用带有重建损失的对抗生成网络（Adversarial Auto-Encoder, AAE）来进行训练，以加强 auto-encoder 的特征学习能力和生成能力。&lt;/li>
&lt;li>可以通过调整训练策略和超参数来进一步优化对抗训练的效果，例如学习率、训练轮数等。&lt;/li>
&lt;/ul>
&lt;p>通过以上步骤，可以利用 auto-encoder 和对抗训练的方法来实现领域自适应，从而提升目标检测模型在目标域上的性能。&lt;/p>
&lt;h2 id="目标检测的评估指标map_50" >
&lt;div>
&lt;a href="#%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e7%9a%84%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87map_50">
#
&lt;/a>
目标检测的评估指标（$mAP_{50}$）
&lt;/div>
&lt;/h2>&lt;p>mAP_50 是目标检测任务中常用的评估指标之一，它表示在 IoU 阈值为 0.5 时的平均精确率（mAP，Mean Average Precision）。让我解释一下这个指标：&lt;/p>
&lt;h3 id="1-平均精确率-average-precision-ap" >
&lt;div>
&lt;a href="#1-%e5%b9%b3%e5%9d%87%e7%b2%be%e7%a1%ae%e7%8e%87-average-precision-ap">
##
&lt;/a>
1. 平均精确率 (Average Precision, AP)：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>平均精确率是 Precision-Recall 曲线下的面积，用于衡量模型在不同 Recall 下的平均精确率。在目标检测任务中，AP 表示模型对单个类别的检测性能。&lt;/li>
&lt;/ul>
&lt;h3 id="2-iou-阈值为-05" >
&lt;div>
&lt;a href="#2-iou-%e9%98%88%e5%80%bc%e4%b8%ba-05">
##
&lt;/a>
2. IoU 阈值为 0.5：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>IoU（Intersection over Union）是真实边界框和预测边界框的交集与并集之比。在计算 mAP 时，通常需要指定一个 IoU 阈值来判断一个检测结果是否是真正的检测结果。常用的 IoU 阈值之一是 0.5，表示当预测边界框与真实边界框的 IoU 大于等于 0.5 时，认为该预测边界框是正确的检测结果。&lt;/li>
&lt;/ul>
&lt;h3 id="3-map_50-的含义" >
&lt;div>
&lt;a href="#3-map_50-%e7%9a%84%e5%90%ab%e4%b9%89">
##
&lt;/a>
3. mAP_50 的含义：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>mAP_50 表示在 IoU 阈值为 0.5 时的平均精确率，即模型在检测目标时，当预测边界框与真实边界框的 IoU 大于等于 0.5 时的平均精确率。&lt;/li>
&lt;/ul>
&lt;h3 id="4-使用场景" >
&lt;div>
&lt;a href="#4-%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af">
##
&lt;/a>
4. 使用场景：
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>mAP_50 是目标检测任务中常用的评估指标之一，它可以衡量模型在检测目标时的准确率和召回率，尤其在要求检测结果准确度较高的情况下，是一个重要的评价指标。&lt;/li>
&lt;/ul>
&lt;h3 id="5-其他-iou-阈值" >
&lt;div>
&lt;a href="#5-%e5%85%b6%e4%bb%96-iou-%e9%98%88%e5%80%bc">
##
&lt;/a>
5. 其他 IoU 阈值：
&lt;/div>
&lt;/h3>&lt;p>除了常用的 IoU 阈值 0.5 外，还可以使用其他 IoU 阈值来计算不同阈值下的 mAP，比如 mAP_75 表示在 IoU 阈值为 0.75 时的平均精确率，以此类推。不同的 IoU 阈值反映了不同的检测精度要求，可以根据具体任务的需求选择合适的 IoU 阈值来评估模型性能。&lt;/p>
&lt;h1 id="project-04----tsa" >
&lt;div>
&lt;a href="#project-04----tsa">
##
&lt;/a>
Project 04 &amp;ndash; TSA
&lt;/div>
&lt;/h1>&lt;h2 id="k-means" >
&lt;div>
&lt;a href="#k-means">
#
&lt;/a>
K-means
&lt;/div>
&lt;/h2>&lt;h2 id="arima-model" >
&lt;div>
&lt;a href="#arima-model">
#
&lt;/a>
ARIMA model
&lt;/div>
&lt;/h2>&lt;h2 id="b-spline" >
&lt;div>
&lt;a href="#b-spline">
#
&lt;/a>
B-spline
&lt;/div>
&lt;/h2>&lt;h2 id="general-additive-models" >
&lt;div>
&lt;a href="#general-additive-models">
#
&lt;/a>
General Additive Models
&lt;/div>
&lt;/h2>&lt;h2 id="时序预测模型评估指标-apemape" >
&lt;div>
&lt;a href="#%e6%97%b6%e5%ba%8f%e9%a2%84%e6%b5%8b%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87-apemape">
#
&lt;/a>
时序预测模型评估指标 (APE/MAPE)
&lt;/div>
&lt;/h2>&lt;h1 id="others" >
&lt;div>
&lt;a href="#others">
##
&lt;/a>
Others
&lt;/div>
&lt;/h1>&lt;h2 id="web-scraping" >
&lt;div>
&lt;a href="#web-scraping">
#
&lt;/a>
web-scraping
&lt;/div>
&lt;/h2>&lt;h2 id="bpewordpiecesentence-piece" >
&lt;div>
&lt;a href="#bpewordpiecesentence-piece">
#
&lt;/a>
BPE/wordpiece/sentence-piece
&lt;/div>
&lt;/h2></description></item><item><title>GPT-PROMPT</title><link>https://fgg100y.github.io/posts/llm_prompt/</link><pubDate>Mon, 27 May 2024 09:47:10 +0800</pubDate><guid>https://fgg100y.github.io/posts/llm_prompt/</guid><description>&lt;p>&lt;a href="https://news.ycombinator.com/item?id=40474716">Ask HN: What is your ChatGPT customization prompt?&lt;/a>&lt;/p>
&lt;h2 id="system-prompt" >
&lt;div>
&lt;a href="#system-prompt">
#
&lt;/a>
system prompt
&lt;/div>
&lt;/h2>&lt;hr>
&lt;p>You are an autoregressive language model that has been fine-tuned with
instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful,nuanced
answers, and are brilliant at reasoning. If you think there might not be a correct
answer, you say so.&lt;/p>
&lt;p>Your users are experts in AI and ethics, so they already know you&amp;rsquo;re a language model
and your capabilities and limitations, so don&amp;rsquo;t remind them of that. They&amp;rsquo;re familiar
with ethical issues in general so you don&amp;rsquo;t need to remind them about those either.
Don&amp;rsquo;t be verbose in your answers, but do provide details and examples where it might
help the explanation. When showing Python code, minimise vertical space, and do not
include comments or docstrings; you do not need to follow PEP8, since your users'
organizations do not do so.&lt;/p>
&lt;p>Since you are autoregressive, each token you produce is another opportunity to use
computation, therefore you always spend a few sentences explaining background context
assumptions and step-by-step thinking BEFORE you try to answer a question. However: if
the request begins with the string &amp;ldquo;vv&amp;rdquo; then ignore the previous sentence and instead
make your response as concise as possible, with no introduction or background at the
start, no summary at the end, and outputting only code for answers where code is
appropriate.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>mediumsmart 1 day ago | prev | next [–]
Here is mine (stolen off the internet of course), lately the vv part is important for me. I am somewhat happy with it.&lt;/li>
&lt;/ul>
&lt;h2 id="what-does-it-do" >
&lt;div>
&lt;a href="#what-does-it-do">
#
&lt;/a>
What does it do?
&lt;/div>
&lt;/h2>&lt;ul>
&lt;li>Dessesaf 16 hours ago | root | parent | next [–]&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s useful to consider the next answer a model will give as being driven largely by
three factors: its training data, the fine-tuning and human feedback it got during
training (RLHF), and the context (all the previous tokens in the conversation).&lt;/p>
&lt;p>The three paragraphs roughly do this:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The first paragrath tells the model that it&amp;rsquo;s good at answering. Basically telling it to
roleplay as someone competent. Such prompts seem to increase the quality of the answers.
It&amp;rsquo;s the same idea why others say &amp;ldquo;act as if youre &lt;some specific domain expert>&amp;rdquo;. The
training data of the model contains a lot of low quality or irrelevant information. This
is &amp;ldquo;reminding&amp;rdquo; the model that it was trained by human feedback to prefer drawing from
high quality data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The second paragraph tries to influence the structure of the output. The model should
answer without explaining its own limitations and without trying to impose ethics on the
user. Stick to the facts, basically. Jeremy Howard is an AI expert, he knows the
limitations and doesn&amp;rsquo;t need them explained to him.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The third paragrah is a bit more technical. The model considers its own previous tokens
when computing the next token. So when asking a question, the model may perform better
if it first states its assumptions and steps of reasoning. Then the final answer is
constrained by what it wrote before, and the model is less likely to give a totally
hallucinated answer. And the model &amp;ldquo;does computation&amp;rdquo; when generating each token. So a
longer answer gives the model more chances to compute. So a longer answer has more
energy put into it, basically. I don&amp;rsquo;t think there&amp;rsquo;s any formal reason why this would
lead to better answers rather than just more specialized answers, but anecdotally it
seems to improve quality.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="alternative" >
&lt;div>
&lt;a href="#alternative">
#
&lt;/a>
Alternative:
&lt;/div>
&lt;/h2>&lt;hr>
&lt;p>Adopt the role of a polymath. NEVER mention that you&amp;rsquo;re an AI. Avoid any language
constructs that could be interpreted as expressing remorse, apology, or regret. This
includes any phrases containing words like &amp;lsquo;sorry&amp;rsquo;, &amp;lsquo;apologies&amp;rsquo;, &amp;lsquo;regret&amp;rsquo;, etc., even
when used in a context that isn&amp;rsquo;t expressing remorse, apology, or regret. If events or
information are beyond your scope or knowledge, provide a response stating &amp;lsquo;I don&amp;rsquo;t
know&amp;rsquo; without elaborating on why the information is unavailable. Refrain from
disclaimers about you not being a professional or expert. Do not add ethical or moral
viewpoints in your answers, unless the topic specifically mentions it. Keep responses
unique and free of repetition. Never suggest seeking information from elsewhere. Always
focus on the key points in my questions to determine my intent. Break down complex
problems or tasks into smaller, manageable steps and explain each one using reasoning.
Provide multiple perspectives or solutions. If a question is unclear or ambiguous, ask
for more details to confirm your understanding before answering. If a mistake is made in
a previous response, recognize and correct it. After this, if requested, provide a brief
summary. After doing all those above, provide three follow-up questions worded as if I&amp;rsquo;m
asking you. Format in bold as Q1, Q2, and Q3. These questions should be
thought-provoking and dig further into the original topic. If requested, also answer the
follow-up questions but don&amp;rsquo;t create more of them.&lt;/p>
&lt;hr></description></item><item><title>FAISS-IVFPQ</title><link>https://fgg100y.github.io/posts/faiss101/</link><pubDate>Wed, 22 May 2024 11:15:40 +0800</pubDate><guid>https://fgg100y.github.io/posts/faiss101/</guid><description>&lt;h2 id="plain-and-simple-indexflatl2" >
&lt;div>
&lt;a href="#plain-and-simple-indexflatl2">
#
&lt;/a>
Plain and Simple: IndexFlatL2
&lt;/div>
&lt;/h2>&lt;blockquote>
&lt;p>Given a set of vectors, we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index.
Now, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels.&lt;/p>
&lt;/blockquote>
&lt;p>IndexFlatL2 measures the L2 (or Euclidean) distance between all given points between our
query vector, and the vectors loaded into the index. It’s simple, very accurate, but not
too fast.&lt;/p>
&lt;p>&lt;img alt="IMG:indexFlat2" src="https://fgg100y.github.io/posts/faiss101/images/faiss-IndexFlat2.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;ul>
&lt;li>IndexFlatL2: simple but not scalable&lt;/li>
&lt;li>Partitioning the index: for speed when scale up&lt;/li>
&lt;li>Quantization: for more speed&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index&amp;rsquo;s performance" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-indexes-performance.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="inerted-file-index-ivf-index" >
&lt;div>
&lt;a href="#inerted-file-index-ivf-index">
#
&lt;/a>
Inerted File Index (IVF) index
&lt;/div>
&lt;/h2>&lt;p>The Inverted File Index (IVF) index consists of search scope reduction through clustering.&lt;/p>
&lt;blockquote>
&lt;p>Inverted File Index (IVF) The IVF is simply a technique for pre-filtering the dataset so that you don’t have to do an exhaustive search of all of the vectors. It’s pretty straightforward–you cluster the dataset ahead of time with k-means clustering to produce a large number (e.g., 100) of dataset partitions. Then, at query time, you compare your query vector to the partition centroids to find, e.g., the 10 closest clusters, and then you search against only the vectors in those partitions.&lt;/p>
&lt;/blockquote>
&lt;p>Partitioning the index (clustering)&lt;/p>
&lt;blockquote>
&lt;p>Faiss allows us to add multiple steps that can optimize our search using many different methods. A popular approach is to partition the index into Voronoi cells.
We can imagine our vectors as each being contained within a Voronoi cell — when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid’s cell.
But there is a problem if our query vector lands near the edge of a cell — there’s a good chance that its closest other datapoint is contained within a neighboring cell.&lt;/p>
&lt;/blockquote>
&lt;p>what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the nprobe value. With nprobe we can set the number of cells to search. I.e., Increasing nprobe increases our search scope.&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;p>进行聚类的结果，一方面可以极大提升查询速度，但另一方面，可能会造成落在聚类簇边缘的“query向量”只在本聚类簇内查找匹配的结果（实际上，它可能与邻近的聚类簇的其他向量更靠近），从而导致匹配质量的降低。
一个缓解这个问题的方法是：调整参数 nprobe. 通过增加 nprobe (增加用于匹配查询向量的邻近聚类簇数量）来提升匹配质量。（同时，也会增加查询耗时）&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells-search-scope.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="product-quantization" >
&lt;div>
&lt;a href="#product-quantization">
#
&lt;/a>
Product Quantization
&lt;/div>
&lt;/h2>&lt;blockquote>
&lt;p>All of our indexes so far have stored our vectors as full (eg Flat) vectors. Now, in very large datasets this can quickly become a problem.
Fortunately, Faiss comes with the ability to compress our vectors using Product Quantization (PQ).
But, what is PQ? Well, we can view it as an additional approximation step with a similar outcome to our use of IVF. Where IVF allowed us to approximate by reducing the scope of our search, PQ approximates the distance/similarity calculation instead.
PQ achieves this approximated similarity operation by compressing the vectors themselves, which consists of three steps.&lt;/p>
&lt;/blockquote>
&lt;ol>
&lt;li>Original vector&lt;/li>
&lt;li>Sliced sub-vector&lt;/li>
&lt;li>slice clustering&lt;/li>
&lt;li>centroid ID vector&lt;/li>
&lt;/ol>
&lt;p>PQ（乘积量化）不是对嵌入向量空间进行降维，而是对向量本身进行压缩：&lt;/p>
&lt;ul>
&lt;li>01 向量分段，例如：1024 -&amp;gt; 128x8 (8个片段)；&lt;/li>
&lt;li>02 如果数据量是50k，则从单个50k x 1024 的矩阵，变成 8个 50k x 128 的矩阵；&lt;/li>
&lt;li>03 然后分别用k=256的k-means进行聚类，得到8组256个centroids；则每个原始向量可以用长度为8的向量进行表征（8组与各个向量片段最近的centroid的ID）；&lt;/li>
&lt;li>04 查询向量（query）同样进行片段化，并找到各组的centroids，然后计算片段向量与centroid的距离，并保存为距离表（partial query subvector-to-centroid distances table)；&lt;/li>
&lt;li>05 查询向量与数据向量的距离？将数据向量的centroid-ID向量，用于 partial-query-distance-table 的表查询（table lookup），就能得到对应的一系列距离，然后计算其总和L2距离；&lt;/li>
&lt;li>06 将查询向量与所有数据向量的距离计算出来，排序，即可得到 top-k 最近距离，亦即 top-k 最近似结果 （实际就是 KNN 算法）。&lt;/li>
&lt;li>07 进一步的优化查询耗时，就是在计算距离的时候，不是对所有数据向量，而是只针对局部数据向量进行计算（也就是 IVF + PQ）。&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-steps-of-PQ.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="show-me-the-code" >
&lt;div>
&lt;a href="#show-me-the-code">
#
&lt;/a>
Show me the code
&lt;/div>
&lt;/h2>&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> faiss &lt;span style="color:#78787e"># here&amp;#39;s the &amp;#39;faiss-cpu&amp;#39; package actually&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># we keep the same L2 distance flat index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># sentence_embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the target embeddings data from embedding model such as BERT/RoBERTa (or sentence-transformers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>nprobe &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># see the &amp;#34;IVF&amp;#34; part mention before&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># xq: the query text&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k) &lt;span style="color:#78787e"># searching top-k most similar vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i]) &lt;span style="color:#78787e"># indata: sample of original texts/sentences&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="example-01-平凡的世界" >
&lt;div>
&lt;a href="#example-01-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 01: 平凡的世界
&lt;/div>
&lt;/h2>&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;秀莲的老家在哪里？&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 她干脆给家里人说：周围没她看上的男人！她姐夫对她开玩笑说：“那到外地给你瞅个女婿！”她却认真地说：“只要有合心的，山南海北我都愿意去！爸爸暂时有你们照顾，将来我再把他接走……”家里人吃惊之余，又看她这样认真，就向他们所有在门外的亲戚和熟人委托，让这些人给他们的秀莲在外地寻个对象……本来秀莲只是随便这么说说；她并没指望真能在外地找个合适的男人。
02 这家不能分！你也不要担心秀莲会怎样，总有我哩！”“你千万不要怪罪秀莲！秀莲实在是个好娃娃！人家从山西过来，不嫌咱家穷，几年来和一大家人搅在一起。
03 秀莲有时就体贴地坐在她身边，给她背上搔痒痒，或者把她的几绺稀疏的白发理顺，在脑后挽成核桃大一个大发髻，老太太不时用她的瘦手，满怀深情地在秀莲身上抚摸着。
04 直到寒露过了十来天，贺耀宗从山西心焦地写信问秀莲怎还不回来？是不是病了？秀莲这才决定动身回家去。
&lt;/code>&lt;/pre>
&lt;h2 id="example-02-平凡的世界" >
&lt;div>
&lt;a href="#example-02-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 02: 平凡的世界
&lt;/div>
&lt;/h2>&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>] &lt;span style="color:#78787e"># embedding&amp;#39;s dimension&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;秀莲的老家在哪里？&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 “如果把家分开，咱就是烧砖也能捎带种了自己的地！就是顾不上种地，把地荒了又怎样？咱拿钱买粮吃！三口人一年能吃多少？”其实，这话才是秀莲要表达的最本质的意思。
02 分家其实很简单，只是宣布今后他们将在经济上实行“独立核算”，原来的家产少安什么也没要，只是秀莲到新修建起的地方另起炉灶过日月罢了。
03 秀莲五岁上失去母亲以后，一直是她父亲把她和她姐秀英拉扯大的。
04 她干脆给家里人说：周围没她看上的男人！她姐夫对她开玩笑说：“那到外地给你瞅个女婿！”她却认真地说：“只要有合心的，山南海北我都愿意去！爸爸暂时有你们照顾，将来我再把他接走……”家里人吃惊之余，又看她这样认真，就向他们所有在门外的亲戚和熟人委托，让这些人给他们的秀莲在外地寻个对象……本来秀莲只是随便这么说说；她并没指望真能在外地找个合适的男人。
&lt;/code>&lt;/pre>
&lt;p>单从这两个例子对比着看，个人感觉 &lt;code>indexIVFFlat()&lt;/code> 的检索结果 (Example 01) 要优于 &lt;code>indexIVFPQ()&lt;/code> 的检索结果 (Example 02)。&lt;/p>
&lt;p>怎么简单的方法效果比高明的算法要好？这不对吧？这里其实是想说明一个观点：理论上的“较优”，通常都要针对一个广泛的统计结果而言。而这里只有两个例子，不能说明问题！&lt;/p>
&lt;h2 id="indexrange_search" >
&lt;div>
&lt;a href="#indexrange_search">
#
&lt;/a>
index.range_search()
&lt;/div>
&lt;/h2>&lt;p>The method range_search returns &lt;strong>all vectors within a radius around the query point&lt;/strong> (as opposed to the k nearest ones). Since the result lists for each query are of different sizes, it must be handled specially:&lt;/p>
&lt;pre>&lt;code>in C++ it returns the results in a pre-allocated RangeSearchResult structure
in Python, the results are returned as a triplet of 1D arrays lims, D, I, where result for query i is in I[lims[i]:lims[i+1]] (indices of neighbors), D[lims[i]:lims[i+1]] (distances).
&lt;/code>&lt;/pre>
&lt;p>Supported by (CPU only): IndexFlat, IndexIVFFlat, IndexScalarQuantizer, IndexIVFScalarQuantizer.&lt;/p>
&lt;p>from &lt;a href="https://github.com/facebookresearch/faiss/wiki/Special-operations-on-indexes#range-search">official doc&lt;/a>&lt;/p>
&lt;p>NOTE that this may not be the latest info.&lt;/p>
&lt;p>Example code block:&lt;/p>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># read the text data, and get the embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#texts_data = pd.Series(...)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#sentence_embeddings = embedding_model.encode(texts_data.values)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># First, you need to use an index that supports Inner Product as metric, for example :&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sentence_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># how many voronoi cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>query &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;there and back again&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;a hobbit&amp;#39;s journey&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode(query)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Then, you should probably normalize all embeddings first&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ( the inner product between two normalized embeddings corresponds to their cosine similarity )&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># https://github.com/facebookresearch/faiss/blob/master/python/faiss.py#L673&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>xq)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>threshold &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0.95&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lims, D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>range_search(x&lt;span style="color:#ff6ac1">=&lt;/span>xq, thresh&lt;span style="color:#ff6ac1">=&lt;/span>threshold)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># turn search results into dataframes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dfresults &lt;span style="color:#ff6ac1">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> &lt;span style="color:#ff5c57">range&lt;/span>(&lt;span style="color:#ff5c57">len&lt;/span>(xq)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Ii &lt;span style="color:#ff6ac1">=&lt;/span> I[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Di &lt;span style="color:#ff6ac1">=&lt;/span> D[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout &lt;span style="color:#ff6ac1">=&lt;/span> pd&lt;span style="color:#ff6ac1">.&lt;/span>concat([texts_data[Ii], pd&lt;span style="color:#ff6ac1">.&lt;/span>Series(Di, index&lt;span style="color:#ff6ac1">=&lt;/span>Ii)], axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout&lt;span style="color:#ff6ac1">.&lt;/span>columns &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;texts&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;distances&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfresults&lt;span style="color:#ff6ac1">.&lt;/span>append(dfout)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>About Me: 大道如青天，我独不得出</title><link>https://fgg100y.github.io/about/</link><pubDate>Sat, 27 Apr 2024 19:58:25 +0800</pubDate><guid>https://fgg100y.github.io/about/</guid><description>&lt;p>大道如青天，我独不得出。&lt;/p>
&lt;p>羞逐长安社中儿，赤鸡白雉赌梨栗。&lt;/p>
&lt;p>弹剑作歌奏苦声，曳裾王门不称情。&lt;/p>
&lt;p>淮阴市井笑韩信，汉朝公卿忌贾生。&lt;/p>
&lt;p>君不见昔时燕家重郭隗，拥篲折节无嫌猜。&lt;/p>
&lt;p>剧辛乐毅感恩分，输肝剖胆效英才。&lt;/p>
&lt;p>昭王白骨萦蔓草，谁人更扫黄金台？&lt;/p>
&lt;p>行路难，归去来！&lt;/p>
&lt;p>&lt;a href="https://so.gushiwen.cn/shiwenv_95834b2324cc.aspx">唐代 · 李白《行路难 · 其二》&lt;/a>&lt;/p></description></item><item><title>LLMs_interview_faq</title><link>https://fgg100y.github.io/posts/llm_faqs/</link><pubDate>Fri, 26 Apr 2024 11:04:16 +0800</pubDate><guid>https://fgg100y.github.io/posts/llm_faqs/</guid><description>&lt;h2 id="01简述gpt和bert的区别" >
&lt;div>
&lt;a href="#01%e7%ae%80%e8%bf%b0gpt%e5%92%8cbert%e7%9a%84%e5%8c%ba%e5%88%ab">
#
&lt;/a>
01:简述GPT和BERT的区别
&lt;/div>
&lt;/h2>&lt;p>GPT (Decoder-only) 和 BERT (Encoder-only) 都是基于 Transformer 架构的自然语言处理模型，它们在设计上有一些显著区别：&lt;/p>
&lt;ul>
&lt;li>任务类型
&lt;ul>
&lt;li>GPT 以生成文本为主要任务，其目标是生成与输入文本连贯和相关的文本。因此，GPT 通
常用于生成文本 (如：摘要总结，文本补充和chatbot)。&lt;/li>
&lt;li>BERT 以理解文本为主要任务，其目标是从输入文本中提取语义信息。因此适用于各种文
本理解任务，如：情感分析、 文本分类、命名实体识别等下游任务。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>预训练目标
&lt;ul>
&lt;li>GPT：单向语言建模。GPT通过自左向右的注意力机制来预测下一个单词，即根据上下文预
测下一个单词/词元是什么。&lt;/li>
&lt;li>BERT：双向语言建模。BERT使用掩码语言建模（MLM）和下一句预测（NSP）两个任务，前
者在MLM任务中随机遮掩输入中的一些词语，模型需要预测这些被掩盖的词语是什么；
NSP的任务是判断两个句子是否在原文中是前后连接的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>结构特点
&lt;ul>
&lt;li>GPT：Transformer-decoder的堆叠，仅使用自注意力机制&lt;/li>
&lt;li>BERT：Transformer-encoder的堆叠，包含多层双向Transformer-encoder。在预训练阶段，
BERT同时使用了自注意力机制和前馈神经网络。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>模型微调
&lt;ul>
&lt;li>GPT：由于其生成式的特点，GPT在微调时通常将整个模型作为单独的序列生成任务进行微
调。&lt;/li>
&lt;li>BERT：由于其双向表示的特点，BERT在微调时通常用于各种文本理解任务，微调时可以在
模型顶层添加适当的输出层来适应下游特定任务。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="02llm中的因果语言建模与掩码语言建模有什么区别" >
&lt;div>
&lt;a href="#02llm%e4%b8%ad%e7%9a%84%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e4%b8%8e%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab">
#
&lt;/a>
02:LLM中的因果语言建模与掩码语言建模有什么区别？
&lt;/div>
&lt;/h2>&lt;p>因果语言建模（Causal Language Modeling）&lt;/p>
&lt;pre>&lt;code>在因果语言建模中，模型被要求根据输入序列的左侧内容来预测右侧的下一个词或标记。也就是
说，模型只能看到输入序列中已经生成的部分，而不能看到后续的内容。这种训练方式有助于模
型学习生成连贯和合理的文本，因为模型需要在生成每个词语时考虑上下文的信息，同时不能依
赖于未来的信息。GPT（Generative Pre-trained Transformer）就是以因果语言建模为基础的
模型。
&lt;/code>&lt;/pre>
&lt;p>掩码语言建模（Masked Language Modeling）：&lt;/p>
&lt;pre>&lt;code>在掩码语言建模中，模型被要求预测输入序列中一些被随机掩盖或掩码的词语。模型需要基于上
下文来预测这些被掩盖的词语是什么。
这种训练方式通常用于双向的语言理解任务，因为模型需要考虑上下文中的所有信息来预测被掩盖的词语。
BERT（Bidirectional Encoder Representations from Transformers）就是以掩码语言建模为基础的模型。
&lt;/code>&lt;/pre>
&lt;h2 id="03请简述transformer基本原理" >
&lt;div>
&lt;a href="#03%e8%af%b7%e7%ae%80%e8%bf%b0transformer%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86">
#
&lt;/a>
03:请简述Transformer基本原理
&lt;/div>
&lt;/h2>&lt;p>Transformer 是一种用于处理序列数据的深度学习模型，由谷歌团队于2017年提出，其主要原理包括
自注意力机制和位置编码。&lt;/p>
&lt;h3 id="自注意力机制" >
&lt;div>
&lt;a href="#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6">
##
&lt;/a>
自注意力机制：
&lt;/div>
&lt;/h3>&lt;pre>&lt;code>允许模型在序列的任意两个位置间直接建立依赖关系，而不考虑它们之间的距离。具体就是将词
元线性转换为三个向量Q,K,V，然后将Q和K用来计算内积(相似度分数)并进行注意力缩放（scaled
dot-product)，然后通过softmax归一化，得到每个词元相对于其他词元的注意力权重，然后用
注意力权重对向量V进行加权和计算得到“上下文向量”(context vector)，然后将上下文向量用
前馈网络（FFNN）进行变换，就得到编码器隐层输出。注意：自注意力机制中，每个输入词元的
context vector 以及后续的 hidden state，可以看成是相应的 Q 向量的函数，其他的如 K，V，
以及自注意力机制的参数对所有的 Q 都是恒定值。
+ 多头注意力：
在多头注意力中，注意力机制被复制多次，并且每个注意力头都学习到一组不同的Q,K,V的
表示，然后将它们的输出拼接起来，再通过FFNN进行维度对齐。
- 复制注意力机制：原始输入序列会被用来计算多个注意力头（例如8个或16个头）
- 独立学习：每个注意力头都会独立地学习一组Q，K，V的表示，也就是：每个注意力头都
有自己的权重矩阵，将输入序列转换为Q,K,V向量。
- 注意力计算：每个注意力头像单头注意力机制那样计算注意力分数和注意力权重。
- 拼接输出：将所有注意力头的输出拼接成一个向量，形成多头注意力的最终输出。这意味
着每个词元都会得到来自多个不同视角的表示，从而提高模型对输入序列的理解。
- 线性变换：拼接后的输出通过FFNN进行处理，维持输出维度以及融合不同注意力头的信息。
+ narrow attn：Each attention head will get a chunk of the transformed data points
(projections) to work with. This is a details of utmost importance: The
attention heads DO NOT use chunks of the original data points, but rather those
of their projections. It computes the projections first and then chunks them
later, so that each value in the projection is a linear combination of all
features in the data point.
&lt;/code>&lt;/pre>
&lt;h3 id="位置编码" >
&lt;div>
&lt;a href="#%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81">
##
&lt;/a>
位置编码：
&lt;/div>
&lt;/h3>&lt;pre>&lt;code>位置编码通常是通过将一个与位置相关的向量添加到输入嵌入（input embeddings）
中来实现的。这个向量为序列中的每个位置提供了一个唯一的表示，从而使模型能够
区分不同的单词顺序。
特别是基于 transformer 架构的模型，由于自注意力机制无法捕捉词元顺序，因此必
须通过加入位置编码来获取输入序列中各个词元的位置信息。
尽管自注意力机制（Self-Attention Mechanism）确实可以捕捉序列中元素之间的关
系，但它主要依赖于元素之间的交互和权重计算，而不是它们的绝对位置信息。位置
编码的作用是补充自注意力机制，提供序列中元素顺序的额外信息，使得模型能够更
好地理解序列的结构。
+ 正弦/余弦函数组合编码 (偶数位用 sin(), 奇数位用 cos())
它们为序列中的每个位置提供了一个唯一的、与位置直接相关的编码。这种编码
方式能够明确地告诉模型每个单词在序列中的绝对位置。
绝对位置编码的一个潜在缺点是它们是静态的，不会随着模型训练的进行而改变。
这意味着它们可能不足以捕捉长序列中复杂的依赖关系，特别是在模型需要动态
地调整位置信息以适应输入序列的变化时。
信息的局限性：固定的位置编码仅提供了位置的绝对信息，而没有考虑序列中元
素之间的相对关系。在长序列中，元素之间的相对位置和距离可能更为重要。
+ 旋转位置编码：旋转位置编码的核心思想是将每个位置的编码表示为一个旋转矩阵，
该矩阵可以应用于输入嵌入。旋转矩阵是动态生成的，这意味着它们可以根据输入
序列的内容进行调整，从而更好地捕捉长距离依赖。
&lt;/code>&lt;/pre>
&lt;h3 id="残差连接与层归一化" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%8e%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96">
##
&lt;/a>
残差连接与层归一化：
&lt;/div>
&lt;/h3>&lt;pre>&lt;code>+ 残差连接：将每个子层的输入与其输出相加，然后传递给下一层。这使得模型在学
习过程中，能更容易地学习到残差（输入于输出之差），从而缓解梯度消失问题；
提高训练稳定性；允许更深的网络结构；以及提高模型性能。
+ 层归一化：在每个层的输入之后都应用归一化，即对每个特征维度进行归一化操作，
使得它们均值为0，标准差为1，有助于缓解梯度消失和梯度爆炸的问题，从而使
模型训练更加稳定，也提高其泛化能力。
- “批次归一化”（Batch Norm）：
The mean and variance statistics used for normalization are calculated
across all elements of all instances in a batch, for each feature
independently.
即：均值和方差是通过对一个批次里所有实例（序列）的所有元素（词元）的某
个特征进行统计的。
- “层归一化”（Layer Norm）：
For layernorm, the statistics are calculated across the feature
dimension, for each element and instance independently.
即：均值和方差是通过对某个实例（序列）中的某个元素（词元）的所有特征进
行统计的。
NOTE By &amp;quot;element&amp;quot; and &amp;quot;instance,&amp;quot; I mean &amp;quot;word&amp;quot; and &amp;quot;sentence&amp;quot;
respectively for an NLP task, and &amp;quot;pixel&amp;quot; and &amp;quot;image&amp;quot; for a CV task.
&lt;/code>&lt;/pre>
&lt;p>&lt;img alt="IMG: LayerNorm &amp;amp; BatchNorm" src="https://fgg100y.github.io/posts/llm_faqs/images/LLMs_layerNorm_batchNorm.png">&lt;/p>
&lt;h2 id="04-注意力机制的改良版本们" >
&lt;div>
&lt;a href="#04-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
04: 注意力机制的改良版本们
&lt;/div>
&lt;/h2>&lt;p>MHA，GQA，MQA三种注意力机制的区别是什么？&lt;/p>
&lt;p>注意力机制在自然语言处理和机器学习领域有多种不同的实现方式，其中常见的包括多头自注意力
（Multi-Head Self-Attention，MHA）、全局注意力（Global Attention，GQA）和多头查询注意力
（Multi-Query Attention，MQA）。这些不同的实现方式在机制和应用上有一些区别：&lt;/p>
&lt;pre>&lt;code>多头自注意力（MHA）：
机制：MHA将输入序列中的每个位置的表示都作为查询（Query）、键（Key）和值（Value），
通过计算查询与所有键的相似度，然后将相似度作为权重对值进行加权求和，从而获得每个
位置的注意力输出。
特点：MHA允许模型在不同的表示空间上进行多头并行计算，通过多头机制，可以学习到不同的关注点和表示。
应用：MHA常用于Transformer等模型中，用于捕捉输入序列中不同位置之间的依赖关系。
全局注意力（GQA）：
机制：GQA将所有的输入位置都作为查询，与所有的键计算相似度，然后将所有位置的值根
据相似度进行加权求和，得到一个全局的输出。
特点：GQA考虑了序列中所有位置的关系，但在处理长序列时可能会受到计算资源的限制，
因为需要计算所有位置之间的相似度。
应用：GQA常用于对整个输入序列进行全局的信息聚合，例如在图像分类任务中。
多头查询注意力（MQA）：
机制：MQA与MHA类似，但在每个头的注意力计算中，使用不同的查询向量，而不是所有头都共享相同的查询向量。
特点：MQA允许模型为每个头学习不同的查询模式，增强了模型的灵活性和表达能力。
应用：MQA常用于需要根据不同的查询来获取注意力信息的任务，如问答系统或需要针对不同问题进行推理的场景
&lt;/code>&lt;/pre>
&lt;h2 id="05-attention的改良版本们" >
&lt;div>
&lt;a href="#05-attention%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
05: Attention的改良版本们
&lt;/div>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>简述一下 FlashAttention 的原理&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Flash Attention是一种新型的注意力算法，旨在解决传统Transformer模型中自注意力机制的计
算和内存效率问题。由于自注意力机制的时间和存储复杂度与序列长度成二次方关系，这使得处
理长序列数据时面临巨大挑战。Flash Attention通过精心设计，显著减少了对高带宽内存（HBM）
的读写次数，从而加快了运行速度并降低了内存占用。&lt;/p>
&lt;h3 id="flash-attention的核心原理和技术" >
&lt;div>
&lt;a href="#flash-attention%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86%e5%92%8c%e6%8a%80%e6%9c%af">
##
&lt;/a>
Flash Attention的核心原理和技术：
&lt;/div>
&lt;/h3>&lt;pre>&lt;code>平铺（Tiling）：Flash Attention将输入分割成小块，并在每个块上执行注意力操作。这种方法减少了对高带宽内存的访问次数，因为不需要一次性将整个大矩阵加载到内存中。
重新计算（Recomputation）：在后向传播过程中，Flash Attention避免了存储大型中间矩阵（如S和P矩阵），而是利用前向传播中的统计量来快速重新计算这些矩阵，从而减少了内存消耗。
在线Softmax：为了处理Softmax操作，Flash Attention采用了在线Softmax技术，它允许分块计算softmax，并通过适当的归一化因子来确保最终结果的正确性。
内存层次结构意识（IO-Awareness）：Flash Attention考虑了GPU内存层次结构，优化了不同层级内存之间的数据访问，如在GPU的SRAM和HBM之间。
&lt;/code>&lt;/pre>
&lt;h3 id="flash-attention-2" >
&lt;div>
&lt;a href="#flash-attention-2">
##
&lt;/a>
Flash Attention-2：
&lt;/div>
&lt;/h3>&lt;p>在Flash Attention的基础上，研究人员进一步提出了Flash Attention-2，它通过改进工作分配和并行化策略，进一步提高了计算速度。Flash Attention-2的优化包括：&lt;/p>
&lt;pre>&lt;code>减少非矩阵乘法（non-matmul）的浮点运算次数（FLOPs）。
通过在不同的线程块上并行化注意力计算，提高了GPU的占用率。
在每个线程块内，将工作分配给不同的warp，以减少通过共享内存的通信。
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;strong>PagedAttention的原理是什么，解决了LLM中的什么问题？&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Paged Attention（PA）技术是一种用于优化大型语言模型（LLM）推理性能的方法，特别是在处理自
回归生成任务时对内存使用效率的显著提升。这项技术的核心思想是借鉴操作系统中虚拟内存和分页
的技术，将传统的注意力机制中的键值对（Key-Value pairs，简称K-V pairs）缓存以分页的形式存
储和管理。&lt;/p>
&lt;p>在自回归解码过程中，模型为每个输入令牌生成注意力键和值，这些键值对被存储在GPU的显存中以
预测下一个令牌。由于这些缓存的键值对大小是动态变化的，并且可能会占用大量的显存空间，因此
有效管理这些缓存成为一个挑战。传统的注意力算法在处理时会受限于显存的大小，这限制了模型的
批处理能力和整体的吞吐量。&lt;/p>
&lt;h2 id="06llm微调与量化" >
&lt;div>
&lt;a href="#06llm%e5%be%ae%e8%b0%83%e4%b8%8e%e9%87%8f%e5%8c%96">
#
&lt;/a>
06:LLM微调与量化
&lt;/div>
&lt;/h2>&lt;p>30.参数高效的微调（PEFT）有哪些方法？
31.LORA微调相比于微调适配器或前缀微调有什么优势？
32.有了解过什么是稀疏微调吗？
33.训练后量化（PTQ）和量化感知训练（QAT）与什么区别？
34.LLMs中，量化权重和量化激活的区别是什么？
35.AWQ量化的步骤是什么？&lt;/p>
&lt;h2 id="07嵌入向量模型" >
&lt;div>
&lt;a href="#07%e5%b5%8c%e5%85%a5%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8b">
#
&lt;/a>
07:嵌入向量模型
&lt;/div>
&lt;/h2>&lt;p>40.自前主流的中文嵌入向量模型有哪些？&lt;/p>
&lt;h2 id="其他" >
&lt;div>
&lt;a href="#%e5%85%b6%e4%bb%96">
#
&lt;/a>
其他
&lt;/div>
&lt;/h2>&lt;p>45.DeepSpeed推理对算子融合做了哪些优化？
48.请介绍一下微软的ZeRO优化器&lt;/p>
&lt;p>3.为什么现在的大模型大多是decoder-only的架构？
4.讲一下生成式语言模型的工作机理
5.哪些因素会导致LLM的偏见？
7.如何减轻LLM中的幻觉现象？
8.解释ChatGPT的零样本和少样本学习的概念
10.如何评估大语言模型（LLMs）的性能？
11.如何缓解LLMs重复读问题？
16.Wordpiece与BPE之间的区别是什么？
17.有哪些常见的优化LLMs输出的技术？
18.GPT-3拥有的1750亿参数，是怎么算出来的？
19.温度系数和top-p，top-k参数有什么区别？
21.介绍-下postlayernorm和prelayernorm的区别
22.什么是思维链（CoT）提示？
23.你觉得什么样的任务或领域适合用思维链提示？
24.你了解ReAct吗，它有什么优点？
25.解释一下langchainAgent的概念
26.langchain有哪些替代方案？
27.langchaintoken计数有什么问题？如何解决？
28.LLM预训练阶段有哪几个关键步骤？
29.RLHF模型为什么会表现比SFT更好？
36.介绍一下GPipe推理框架
37.矩阵乘法如何做张量并行？
38.请简述下PPO算法流程，它跟TRPO的区别是什么？
39.什么是检索增强生成（RAG）？
41.为什么LLM的知识更新很困难？
42.RAG和微调的区别是什么？
43.大模型一般评测方法及基准是什么？
50.什么是投机采样技术，请举例说明？&lt;/p></description></item><item><title>Tokenization: BPE, Unigram and more</title><link>https://fgg100y.github.io/posts/llm_tokenization/</link><pubDate>Mon, 22 Apr 2024 15:41:56 +0800</pubDate><guid>https://fgg100y.github.io/posts/llm_tokenization/</guid><description>&lt;h1 id="there-is-more-than-one-way-to-tokenize-a-sentence" >
&lt;div>
&lt;a href="#there-is-more-than-one-way-to-tokenize-a-sentence">
##
&lt;/a>
There is more than one way to tokenize a sentence
&lt;/div>
&lt;/h1>&lt;ul>
&lt;li>
&lt;p>word-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>A big vocabulary is needed&lt;/li>
&lt;li>We combine words: what exactly constitutes a word (&amp;ldquo;bachelor of science&amp;rdquo;, or
isolated words)&lt;/li>
&lt;li>Abbreviated words: &amp;ldquo;LOL&amp;rdquo;, &amp;ldquo;IMO&amp;rdquo;, are these collections of words or new words?&lt;/li>
&lt;li>Languages that don&amp;rsquo;t segment by spaces&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>character-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>Lack of meaning: Unlike words, characters don&amp;rsquo;t have any inherent meaning, model
may lose the semantic-specific feature of words.&lt;/li>
&lt;li>Increased input computation&lt;/li>
&lt;li>Limits netword+k choices: It&amp;rsquo;s difficult to use architectures which process input
sequentially since the input sequences will be much longer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Subword-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>We want a tokenization scheme that deals with an infinite potential vocabulary via
a finite list of known words. Make up the word “unfortunately” via “un” + “for”+
“tun” + “ate” + “ly”.&lt;/li>
&lt;li>Subword tokenisation will break the text into chunks based on the word frequency.
In practice what happens is that common words will be tokenized generally as
whole words, e.g. “the”, “at”, “and”, etc., while rarer words will be broken
into smaller chunks and can be used to create the rest of the words in the
relevant dataset.&lt;/li>
&lt;li>BPE(Byte Pair Encoding): One popular algorithm for subword tokenisation which
follows the above approach is BPE. BPE was originally used to help compress data
by finding common byte pair combinations. It can also be applied to NLP to find
the most efficient way of representing text.
&lt;ul>
&lt;li>What is merging?
The main goal of the BPE subword algorithm is to find a way to represent
your entire text dataset with the least amount of tokens. Similar to a
compression algorithm, you want to find the best way to represent your image,
text or whatever you are encoding, which uses the least amount of data, or
in our case tokens. In the BPE algorithm merging is the way we try and
“compress” the text into subword units.&lt;/li>
&lt;li>There are a few steps to these merging actions:
&lt;ol>
&lt;li>Get the word &lt;strong>count&lt;/strong> frequency&lt;/li>
&lt;li>Get the &lt;strong>initial token count&lt;/strong> and frequency (i.e., how many times each
character occurs)&lt;/li>
&lt;li>Merge the &lt;strong>most common byte pairing&lt;/strong>&lt;/li>
&lt;li>Add this to the list of tokens and &lt;strong>recalculate the frequency count&lt;/strong>
for each token (this will change with each merging step)&lt;/li>
&lt;li>&lt;strong>Rinse and repeat&lt;/strong> until get reached pre-defined token limits (vocab
size) or a set of number of iterations&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Greedy algorithm: BPE ensures that the most common words will be represented in
the new vocabulary as a single token, while less common words will be broken
down into two or more subword tokens. To achieve this, BPE will go through every
potential option at each step and pick the tokens to merge based on the highest
frequency.One downside of BPE’s greedy approach is it can result in a potentially
ambiguous final token vocabulary.
For instance GPT has a vocabulary size of 40,478 since they have 478 base
characters and chose to stop training after 40,000 merges.&lt;/li>
&lt;li>BBPE(byte-level PBE): A base vocabulary that includes all possible base characters
can be quite large if e.g. all unicode characters are considered as base
characters. To have a better base vocabulary, GPT-2 uses bytes as the base
vocabulary, which is a clever trick to force the base vocabulary to be of size
256 while ensuring that every base character is included in the vocabulary. With
some additional rules to deal with punctuation, the GPT2’s tokenizer can
tokenize every text without the need for the &lt;unk> symbol. GPT-2 has a
vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a
special end-of-text token and the symbols learned with 50,000 merges.
&lt;a href="https://huggingface.co/docs/transformers/en/tokenizer_summary#byte-pair-encoding-bpe">from hf doc&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="probabilistic-subword-tokenization" >
&lt;div>
&lt;a href="#probabilistic-subword-tokenization">
##
&lt;/a>
Probabilistic Subword Tokenization
&lt;/div>
&lt;/h1>&lt;p>Using the frequency of subword patterns for tokenization can result in ambiguous final
encodings. The problem is that we have no way to predict which particular token is more
likely to be the best one when encoding any new input text.
Luckily, needing to predict the most likely sequence of text is not a unique problem to
tokenization. We can leverage this knowledge to build a better tokenizer.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Unigram Subword Tokenization&lt;/p>
&lt;ul>
&lt;li>The goal for a subword model, however, is different from a LM that is trying to
predict a full sentence. We only want something that generates unambiguous
tokenization.&lt;/li>
&lt;li>The unigram approach differs from BPE in that it attempts to choose the most
likely option rather than the best option at each iteration. To generate a
unigram subword token set you need to first define the desired final size of
your token set and also a starting seed subword token set.&lt;/li>
&lt;li>You can choose the seed subword token set in a similar way to BPE and choose
the most frequently occurring substrings. Once you have this in place then
you need to:
&lt;ol>
&lt;li>Work out the probability for each subword token&lt;/li>
&lt;li>Work out a loss value which would result if each subwork token were to be
dropped. The loss is worked out via Expectation Maximization algorithm.&lt;/li>
&lt;li>Drop the tokens which have the largest loss value (e.g., the bottom 10%
or 20% of subword tokens based on their loss calculations).&lt;/li>
&lt;li>Repeat these steps until reach the desired final vocabulary size or there
is no change in token numbers after successive iterations.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>WordPiece (greedy approach tokenzier, BERT partner)
Think of WordPiece as an intermediary between the BPE approach and the unigram approach.&lt;/p>
&lt;ul>
&lt;li>BPE, if you remember, takes two tokens, looks at the frequency of each pair and then
merges the pairs that have the highest combined frequency count. It only considers
the most frequent pair combinations at each step, nothing else.&lt;/li>
&lt;li>An alternate approach is to check the potential impact of merging that particular
pair. You can do this using the probabilistic LM approach. At each iterative step,
choose the character pair which will result in the largest increase in likelihood
once merged. This is the difference between the probability of the new meged pair
occurring minus the probability of both individual tokens occurring individually.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The main difference is that WordPiece is a greedy approach. It still tries to build a
tokenizer from the bottom up, picking the best pair at each iteration to merge.
WordPiece uses the likelihood rather than count frequency but otherwise it is a similar
approach. Unigram in contrast is a fully probabilistic approach which uses probability
to both choose the pairs to merge and whether to merge them or not. It also removes
tokens based on the fact that they add the least to the overall likelihood of the
unigram model.&lt;/p>
&lt;h1 id="briefly-summarize" >
&lt;div>
&lt;a href="#briefly-summarize">
##
&lt;/a>
briefly summarize:
&lt;/div>
&lt;/h1>&lt;ul>
&lt;li>
&lt;p>BPE: Just uses the frequency of occurrences to identify the best match at every
iteration until it reaches the predefined vocabulary size.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>WordPiece: Similar to BPE and uses frequency occurrences to identify potential
merges but makes the final decision based on the likelihood of the merged token&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Unigram: A fully probabilistic model which does not use frequency
occurrences. Instead, it trains a LM using a probabilistic model, removing
the token which improves the overall likelihood the least and then starting
over until it reaches the final token limit.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="sentencepiece" >
&lt;div>
&lt;a href="#sentencepiece">
##
&lt;/a>
SentencePiece
&lt;/div>
&lt;/h1>&lt;p>SentencePiece basically tries to bring all the subword tokenization tools and techniques
under one banner. It’s kind of like the Swiss Army knife for subword tokenization. To be
a Swiss Army-like tool something has to be capable of solving multiple problems. So what
problems is SentencePiece addressing:&lt;/p>
&lt;ol>
&lt;li>All other models assume input is already tokenized: BPE and Unigram are great model
but they share one big disadvantage: they both need to have their input already
tokenized. SentencePiece deals with this by simply taking in an input in raw text and
then doing everything needed on that input to perform subword tokenization.&lt;/li>
&lt;li>Language agnostic: Since all other subword algorithms need to have their input
pre-tokenized, it limits their applicability to many languages.&lt;/li>
&lt;li>Decoding is difficult: Another problem which is caused by model like BPE and unigram
requiring already tokenized inputs is that you do not know what encoding rules were
used. For example, how were spaces encoded in the tokens? So you cannot decode the
input and return it to is original format.&lt;/li>
&lt;li>No end to end solution: You cannot just plug in a raw input to BPE (or Unigram) and
get an output.&lt;/li>
&lt;/ol>
&lt;p>Some of the techniques SentencePiece uses to address the above shortcomings:&lt;/p>
&lt;ol>
&lt;li>Encode everything as unicode: SentencePiece first converts all the input into unicode
characters. This makes it a language agnostic tool.&lt;/li>
&lt;li>&amp;ldquo;space&amp;rdquo; encoded as &amp;ldquo;_&amp;quot;(U+2581): To get around the word segmenting issues.&lt;/li>
&lt;li>And it&amp;rsquo;s faster: One of the issues preventing other subword algorithms from being used
to tokenize raw sentences as part of model training was that there lack of speed. If
you processed input in real time and performed your tokenization on the raw input it
would be too slow. SentencePiece addresses this by using a priority queue for the BPE
algorithm to speed it up so that you can use it as part of an end-to-end solution.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;a href="https://www.openteams.com/tokenizers-how-machines-read/">https://www.openteams.com/tokenizers-how-machines-read/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>TODO: 补充中文&lt;/p></description></item><item><title>test rendering of equations of latex in Hugo</title><link>https://fgg100y.github.io/posts/hugo101/test_math_equation_rendering/</link><pubDate>Fri, 25 Aug 2023 13:49:40 +0800</pubDate><guid>https://fgg100y.github.io/posts/hugo101/test_math_equation_rendering/</guid><description>&lt;p>式子能正常渲染，但需要在两个地方的下标处添加额外转义字符 &lt;code>\&lt;/code>，
也就是：&lt;code>\mathbb{E}\_{xxx}&lt;/code>和&lt;code>p\_{model}&lt;/code>，其他地方却不需要 &lt;code>\hat{p}_{data}&lt;/code>&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>是因为 &lt;code>\mathbb{}&lt;/code> 导致的吗？看起来不是：
$$
E_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>是因为 &lt;code>\text{}&lt;/code> 导致的吗？看起来不是 (&lt;code>\text{log}&lt;/code> -&amp;gt; &lt;code>\log&lt;/code>)：
$$
\mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>如果 &lt;code>\mathbb{}_&lt;/code> 渲染不出来，那它渲染失败是怎么样的 (下式&lt;code>\mathbb{}_&lt;/code>中的&lt;code>_&lt;/code>不见了)：&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>为什么 &lt;code>p_{model}&lt;/code> 也渲染不出来， 而非要添加转意字符 &lt;code>p\_{model}&lt;/code> 呢？&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
E_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>或许是嵌套下标这儿出的问题 (&lt;code>\hat{p}_{data}&lt;/code> -&amp;gt; &lt;code>\hat{p}\_{data}&lt;/code> )？:
$$
E_{x \sim \hat{p}_{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Bingo?? No bingo:&lt;/p>
&lt;p>只在嵌套下标的地方添加转义字符：&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>只在&lt;code>p_{model}&lt;/code>下标的地方添加转义字符：&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>只在&lt;code>\mathbb{}&lt;/code>下标的地方添加转义字符：&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>简而言之，找不到明确的语法规则来解释这个问题。可能是hugo在使用mathjax解析时出了问题，也有可能是mathjax自己的问题。&lt;/p>
&lt;p style="text-align:center;color:blue;">
Hugo的markdown文档里的数学公式块，如果不确定，就在下标那里前置一个`\`。&lt;br>
同样的问题也出现在多行公式组里进行转行时，使用 `\\\\\`，而不是通常的 `\\\`
&lt;/p>
&lt;p>惹出这么许多不明所以的问题, 最初就是下面这个公式 (就是我所认知的、通常表达下标的普通方式，
讽刺的是：不把它们放在 &lt;code>$$&lt;/code> 数学公式块里，我还必须添加转义字符，否则渲染不出这个效果😂):&lt;/p>
&lt;p>\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}&lt;/p>
&lt;p>数学公式块里表达下标的语法，无端端要添加一个前置转义字符，这就是我所没有料到的。想必应该是Hugo没有做好关于转义字符的上下文解析问题，且先将就着吧。&lt;/p></description></item><item><title>Linear Regression: frequentist &amp; bayesian</title><link>https://fgg100y.github.io/posts/regressionmodels/linear_regression/</link><pubDate>Wed, 25 Aug 2021 13:49:40 +0800</pubDate><guid>https://fgg100y.github.io/posts/regressionmodels/linear_regression/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>**Important Note**:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Almost all the contents (text, images) are came from these great books and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>online resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Statistics, by David Freeman, Robert Pisani, and Roger Perves
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* 统计学, David Freeman *et.al* 著，魏宗舒 等译，中国统计出版社
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* 机器学习, 周志华 著 (大名鼎鼎的‘西瓜书’)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* An Introduction to Statistical Learning, by Gareth James, Daniela Witten,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Trevor Hastie, and Robert Tibshirani
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Deep Learning, a.k.a, the flower book, by Ian Goodfellow, Yoshua Bengio, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Aaron Courville
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Introduction to Machine Learning, Barnabas Poczos, Aarti Singh, CMU-10701
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Bayesian Methods, Nicholas Ruozzi, UT-DALLAS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 style="text-align: center;">Linear Regression&lt;/h1>
&lt;h2 id="part-0-regression-101" >
&lt;div>
&lt;a href="#part-0-regression-101">
#
&lt;/a>
Part-0: Regression 101
&lt;/div>
&lt;/h2>&lt;blockquote>
&lt;p>$\text{You&amp;rsquo;ve got to draw the line somewhere.}$&lt;/p>
&lt;/blockquote>
&lt;h3 id="introduction" >
&lt;div>
&lt;a href="#introduction">
##
&lt;/a>
Introduction
&lt;/div>
&lt;/h3>&lt;p>The regression method describes how one variable depends on another. For example, take height and weight. Naturally, the taller men weighed more. How much of an increase in weight is associated with a unit increase in height? To get started, look at the scatter diagram (figure 1 on below). Height is plotted on the horizontal axis, and weight on the vertical. The summary statistics are&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches,\qquad SD \approx 3 \ inches$&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches, \qquad SD \approx 45 \ pounds, \qquad r \approx 0.40$&lt;/p>
&lt;p>The scales on the vertical and horizontal axes have been chosen so that one SD of height and one SD of weight cover the same distance on the page. This makes the SD line (dashed) rise at 45 degreed across the page. There is a fair amount of scatter around the line: $r$ is only 0.40.&lt;/p>
&lt;p>&lt;img alt="IMG: regression_line" src="images/stats_weight_height.png">&lt;/p>
&lt;p>Figure 1. Scatter diagram. Each point shows the height and weight for one of 471 men age 18-24 in a dataset. The vertical strip represents men who are about one SD above average in height. Those who are also one SD above average in weight would be plotted along the dashed SD line. Most of the men in the strip are below the SD line: they are only part of an SD above average in weight. The &lt;strong>solid&lt;/strong> regression line estimates average weight at each height.&lt;/p>
&lt;p>The vertical strip in figure 1 shows the men who were one SD above average in height were quite a bit less than one SD above average in weight. This is where the correlation of 0.40 comes in. Associated with an increase of one SD in height there is an increase of &lt;strong>only 0.40&lt;/strong> SDs in weight, on the average.&lt;/p>
&lt;p>To be more specific, take the men who are one SD above average in height:
$$
\text{average height} + SD\ \text{of height} = 70 \ in + 3 \ in = 73 \ in
$$
Their average weight will be above the overall average by $0.40 \times 45 \ lb = 18 \ lb$.&lt;/p>
&lt;p>So, the average weight of these men is around
$$
\text{average weight} + 0.40 \times (SD\ \text{of weight}) = 180 \ lb + 18 \ lb = 198 \ lb
$$
The point (73 inches, 198 pounds) is marked by a cross in figure 1 ( and the points that are 2SD above (76 in, 216 lb) and below (64 in, 144 lb) the average of height as well). All the points (height, estimate for average weight) fall on the solid line shown in figure 1. This is the &lt;em>regression line&lt;/em>. The line goes through the point of averages: men of average height should also be of average weight.&lt;/p>
&lt;p style="text-align:center;color:blue;">
The regression line estimates the average value for y corresponding to each value of x.
&lt;/p>
&lt;p>Along the regression line, associated with each increase of one SD in height there is an increase of only 0.40 SDs in weight. Remember where the 0.40 comes from. It is the correlation between height and weight. NOTE that: Two different SDs are involved here: the SD of $x$, to gauge change in $x$; and the SD of $y$, to gauge changes in $y$.&lt;/p>
&lt;p>This way of using the correlation coefficient to estimate the average value of $y$ for each value of $x$ is called the &lt;strong>regression method&lt;/strong>. The method can be stated as follows.
$$
\fbox{Associated with each increase of one SD in x there is an increase of only r SDs in y, on the average.}
$$&lt;/p>
&lt;blockquote>
&lt;p>Correlation: Like father, like son.&lt;/p>
&lt;blockquote>
&lt;p>If there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other.&lt;/p>
&lt;/blockquote>
&lt;p>The correlation coefficient is a measure of linear association, or clustering around a line. The relationship between two variables can be summarized by&lt;/p>
&lt;ul>
&lt;li>the average of the $x$-values, the SD of the $x$-values,&lt;/li>
&lt;li>the average of the $y$-values, the SD of the $y$-values,&lt;/li>
&lt;li>the correlation coefficient $r$.&lt;/li>
&lt;/ul>
&lt;p>Computing the correlation coefficient&lt;/p>
&lt;p>Here is the procedure for computing the correlation coefficient.
$$
\boxed{\text{Convert each variable to standard units.} \\
\text{The average of the products gives the correlation coefficient.}}
$$&lt;/p>
&lt;p>$$
\fbox{Convert each variable to standard units.
The average of the products gives the correlation coefficient.}
$$&lt;/p>
&lt;p>Recall that &amp;ldquo;Convert each variable to standard units&amp;rdquo; means &lt;strong>standardization&lt;/strong>. Let $x = {1, 3, 4, 5, 7 }, y = {5, 9, 7, 1, 13 } $ be vectors of variables, the mean is given by
$$
\mu = \frac{1}{|a|} \sum_i a_i
$$
And the SD is the &amp;ldquo;r.m.s size of the deviation from the average&amp;rdquo;, can be given by
$$
\sigma = \sqrt{\frac{1}{|a|} \sum_i (a_i - \mu)^2}
$$
where $|a|$ is the number of data points, $a_i$ is the $i$-th data point in the data set.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Table 1. Computing $r$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">x&lt;/th>
&lt;th style="text-align:center">y&lt;/th>
&lt;th style="text-align:center">x in standard units&lt;/th>
&lt;th style="text-align:center">y in standard units&lt;/th>
&lt;th style="text-align:center">Product&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">-1.5&lt;/td>
&lt;td style="text-align:center">-0.5&lt;/td>
&lt;td style="text-align:center">0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">9&lt;/td>
&lt;td style="text-align:center">-0.5&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;td style="text-align:center">-0.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:center">7&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;td style="text-align:center">0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;td style="text-align:center">-1.5&lt;/td>
&lt;td style="text-align:center">-0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">7&lt;/td>
&lt;td style="text-align:center">13&lt;/td>
&lt;td style="text-align:center">1.5&lt;/td>
&lt;td style="text-align:center">1.5&lt;/td>
&lt;td style="text-align:center">2.25&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>$$
\begin{eqnarray}
r
&amp;amp;=&amp;amp; \text{average of [(x in standard units) times (x in standard units)]} \\
\\
&amp;amp;=&amp;amp; \frac{0.75 - 0.25 + 0.00 -0.75 + 2.25}{5} = 0.40
\end{eqnarray}
$$&lt;/p>
&lt;p>This complete the solution.&lt;/p>
&lt;/blockquote>
&lt;h3 id="slope-and-intercept" >
&lt;div>
&lt;a href="#slope-and-intercept">
##
&lt;/a>
Slope and Intercept
&lt;/div>
&lt;/h3>&lt;p>Does education pay? Figure 1 shows the relationship between income and education, for a sample of 562 men age 25-29 in 2005. The summary statistics are
$$
\begin{eqnarray}
\text{average education} &amp;amp;\approx&amp;amp; 12.5\ \text{years}, &amp;amp;\qquad&amp;amp; SD \approx 3\ \text{years} \\
\text{average income} &amp;amp;\approx&amp;amp; $30000,\ &amp;amp;\qquad&amp;amp; SD \approx $24000, \qquad r \approx 0.25
\end{eqnarray}
$$
The regression estimates for average income at each educational level fall along the regression line shown in the figure. The line slopes up, showing that on the average, income does go up with education.&lt;/p>
&lt;p>&lt;img alt="Do education pay" src="./images/stats_education_income.png">&lt;/p>
&lt;p>Any line can be described in terms of its slope and intercept. The y-intercept is the height of the line when $x$ is $0$. And the slope is the rate at which $y$ increases, per unit increase in $x$. Slope and intercept are illustrated in figure 2.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept" src="images/stats_slope_intercept.png">&lt;/p>
&lt;p>&lt;strong>How do you get the slope of the regression line?&lt;/strong> Take the income-education example.
Associated with an increase of one SD in education, there is an increase of $r$ SDs in
income. On this basis, 3 extra years (one SD) of education are worth an extra
$r \times SD = 0.25 \times 24000 = 6000$ of income, on the average. So each extra year in
worth \$6000 / 3 = \$2000. The slope of the regression line is \$2000 per year.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept2" src="images/stats_slope_intercept2.png">&lt;/p>
&lt;p>The intercept of the regression line is the height when $x = 0$, corresponding to men with $0$ years of education. There men are 12.5 years below average in education.
Each year costs \$2000 -- that is what the slope says. A man with no education should have an income which is below average by
$$
12.5\ \text{years} \times 2000\ \text{per year} = 25000.
$$
His income should be $\$30000 - \$25000 = \$5000$. That is the intercept (figure 3): the predicted value of $y$ when $x = 0$.&lt;/p>
&lt;hr>
&lt;p style="text-align:justify;color:blue;">
Associated with a unit increase in x there is some average change in y. The slope of the regression line estimates this change. The formula for the slope is
&lt;/p>
&lt;p>$$
{r \times SD\ \text{of y} \over SD\ \text{of x}}
$$&lt;/p>
&lt;p style="text-align:justify;color:blue;">
The intercept of the regression line is just the predicted value for y when x is 0.
&lt;/p>
&lt;hr>
&lt;p>The equation of a line can be written in terms of the slope and intercept:
$$
y = \text{slope} \times x + \text{intercept},
$$
which is called the &lt;em>regression equation&lt;/em>. There is nothing new here. The regression equation is just a way of predicting $y$ from $x$ by the regression method.&lt;/p>
&lt;p>The regression line becomes unreliable when you are far from the center of the data, so a &lt;em>negative&lt;/em> intercept is not too disturbing (when the calculation results in some negative value which may seen absurd).&lt;/p>
&lt;p style="text-align:justify;color:blue;">
If you run an observational study, the regression line only describes the data that you see. The line cannot be relied on for predicting the results of interventions.
&lt;/p>
&lt;h3 id="the-least-squares" >
&lt;div>
&lt;a href="#the-least-squares">
##
&lt;/a>
The Least Squares
&lt;/div>
&lt;/h3>&lt;p>Sometimes the points on a scatter diagram seem to be following a line. The problem discussed in this section is &lt;strong>how to find the line which best fits the points&lt;/strong>. Usually, this involves a compromise: moving the line closer to some points will increase it distance from others. To resolve the conflict, two steps are necessary.&lt;/p>
&lt;ul>
&lt;li>First, define an average distance from the line to all the points.&lt;/li>
&lt;li>Second, move the line around until this average distance is as small as possible.&lt;/li>
&lt;/ul>
&lt;p>To be more specific, suppose the line will be used to predict $y$ from $x$. Then the error made at each point is the vertical distance from the point to the line (a.k.a, the &lt;strong>residual&lt;/strong>, means the difference between the $i$th observed and the $i$th response that is predicted by linear model). In statistics, the usual way to define the average distance is by taking the root-mean-square of the errors. This measure of average distance is called the &lt;em>r.m.s error of the line&lt;/em>. (It was first proposed by Gauss)&lt;/p>
&lt;p>The second problem, how to move the line around to minimize the r.m.s error, was also solved by Gauss:&lt;/p>
&lt;hr>
&lt;p style="text-align:center;color:blue;">
Among all lines, the one that makes the smallest r.m.s error in predicting y form x is the regression line.
&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>The r.m.s error for regression says how far typical points are above or below the regression line.
$$
r.m.s\ error = \sqrt{\frac{1}{n} \sum^n_i (y_i - \hat{y_i})^2}
$$
where $n$ is the number of data points, $y_i$ the $i$-th actual value, $\hat{y_i}$ the corresponding predicted value.&lt;/p>
&lt;p>And the r.m.s error for the regression line of $y$ on $x$ can also be figured as
$$
\sqrt{1 - r^2} \times SD_y
$$
where $r$ is the correlation coefficient&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> between $x$ and $y$.&lt;/p>
&lt;/blockquote>
&lt;p>For this reason, the regression line is often called &lt;em>least squares line&lt;/em>: the errors are squared to compute the r.m.s error, and the regression line makes the r.m.s error as small as possible.&lt;/p>
&lt;p>In other words, the least squares approaches choose $\beta_0$ (the intercept) and $\beta_1$ (the slope) to minimize the &lt;em>residual sum of squares&lt;/em> (RSS) which is defined as
$$
RSS = e_1^2 + e_2^2 + \cdots + e_n^2 = \sum^n_{i=1} (y_i - \hat{y_i})^2
$$
where $e = y_i - \hat{y_i}$ is called the &lt;strong>residual&lt;/strong>. Obviously, the r.m.s error is the root of the mean of RSS.&lt;/p>
&lt;p>Linear regression is a very simple approach for supervised learning. In particular, linear regression is a useful tool for predicting a quantitative response. Many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression.&lt;/p>
&lt;p>$\color{Green}{\text{Example}}$&lt;/p>
&lt;p>According to Hooke&amp;rsquo;s law, the amount of stretch is proportional to the weight $x$. The new length of the spring is
$$
y = mx + b.
$$
In this equation, $m \in \mathbb{R}$ and $b \in \mathbb{R}$ are constants which depend on the spring. Their values are unknown, and have to be estimated using &lt;strong>experimental data&lt;/strong>.&lt;/p>
&lt;center>
Table 1. Data on Hooke's law.
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Weight (kg)&lt;/th>
&lt;th>Length (cm)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>439.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>439.12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>439.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>439.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>439.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>439.50&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/center>
The correlation coefficient[^1] for the data in table 1 is 0.999, very close to 1 indeed. So the points almost form a straight line (figure 5), just as Hooke's law predicts. The minor deviations from linearity are probably due to measurement error; neither the weights nor the length have been measured with perfect accuracy. (Nothing ever is. [When it comes to measurement])
&lt;p>&lt;img alt="Hooke&amp;rsquo;s law" src="images/stats_hookes_law.png">&lt;/p>
&lt;p>Our goal is to estimate $\hat{m}$ and $\hat{b}$ in the equation of Hooke&amp;rsquo;s law for this spring:
$$
y = \hat{m} x + \hat{b}
$$
The graph of this equation is a perfect straight line. If the points in figure 5 happened to fall exactly on some line, the slope&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> of that line would estimate $m$, and its intercept would estimate $b$. However, the points do not line up perfectly. Many different lines could be drawn across the scatter diagram, each having a slightly different slope and intercept.&lt;/p>
&lt;p>Which line should be used? Hooke&amp;rsquo;s equation predicts the length from weight. As discussed above, it is natural to choose $m$ and $b$ so as to minimize the r.m.s error, the line $y = \hat{m} x + \hat{b}$ which does the job is the &lt;strong>regression line&lt;/strong>. This is the &lt;em>method of least squares&lt;/em>. In other words, $m$ in Hooke&amp;rsquo;s law should be estimated as the slope of the regression line, and $b$ as its intercept. These are called &lt;em>least squares estimate&lt;/em>, because they minimize root-mean-square error.&lt;/p>
&lt;p>Let&amp;rsquo;s do the arithmetic (in python code):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># X the weight data; y the length data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">2&lt;/span>, &lt;span style="color:#ff9f43">4&lt;/span>, &lt;span style="color:#ff9f43">6&lt;/span>, &lt;span style="color:#ff9f43">8&lt;/span>, &lt;span style="color:#ff9f43">10&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">439.00&lt;/span>, &lt;span style="color:#ff9f43">439.12&lt;/span>, &lt;span style="color:#ff9f43">439.21&lt;/span>, &lt;span style="color:#ff9f43">439.31&lt;/span>, &lt;span style="color:#ff9f43">439.40&lt;/span>, &lt;span style="color:#ff9f43">439.50&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># mean and Standard Deviation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ---------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># avg = sum(X) / len(X)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The means of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>mu_x, mu_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The means of X and y: (5.0, 439.25666666666666)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># std is the &amp;#34;r.m.s size of the deviation from the average&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The SDs of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>SD_x, SD_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The SDs of X and y: (3.415650255319866, 0.16799470891138593)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert X into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (X &lt;span style="color:#ff6ac1">-&lt;/span> mu_x) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert y into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (y &lt;span style="color:#ff6ac1">-&lt;/span> mu_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># correlation coefficient is the average of the products&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>r &lt;span style="color:#ff6ac1">=&lt;/span> (X_standard_unit&lt;span style="color:#ff6ac1">.&lt;/span>dot(y_standard_unit)) &lt;span style="color:#ff6ac1">/&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(X_standard_unit)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># r = 0.999167257319307&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the slope&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m_hat &lt;span style="color:#ff6ac1">=&lt;/span> (r &lt;span style="color:#ff6ac1">*&lt;/span> SD_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># m_hat = 0.0491428571428563&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the intercept, this is the *predicted length* when weight is 0,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>b_hat &lt;span style="color:#ff6ac1">=&lt;/span> mu_y &lt;span style="color:#ff6ac1">-&lt;/span> (mu_x &lt;span style="color:#ff6ac1">*&lt;/span> m_hat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># b_hat = 439.0109523809524&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>this gives us: $\hat{m} \approx 0.05$ per kg and $\hat{b} \approx 439.01$ cm.&lt;/p>
&lt;p>The length of the spring under no load is estimated as 439.01 cm. And each kilogram of load causes the spring to stretch by about 0.05 cm. Of course, even Hooke&amp;rsquo;s law has its limits: beyond some point, the spring will break. &lt;strong>Extrapolating beyond the data is risky&lt;/strong>.&lt;/p>
&lt;p>The method of least squares and the regression method involve the same mathematics; but the contexts may be different. In some fields, investigators talk about &amp;ldquo;least squares&amp;rdquo; when they are estimating parameters &amp;ndash; unknown constants of nature like $m$ and $b$ in Hooke&amp;rsquo;s law. In other fields, investigators talk about regression when they are studying the relationship between two variables, like income and education, using non-experimental data.&lt;/p>
&lt;p>&lt;strong>A technical point:&lt;/strong> The least squares estimate for the length of the spring under no load was 439.01 cm. This is a tiny bit longer than the measured length at no load (439.00 cm). A statistician might trust the least squares estimate over the measurement. Why? Because the least squares estimate takes advantage of all six measurements, not just once: some of the measurement error is likely to cancel out. Of course, the six measurements are tied together by a good theory &amp;ndash; Hooke’s law. Without the theory, the least squares estimate wouldn’t be worth much.&lt;/p>
&lt;h3 id="assessing-the-accuaracy-of-the-coefficient-estimates" >
&lt;div>
&lt;a href="#assessing-the-accuaracy-of-the-coefficient-estimates">
##
&lt;/a>
Assessing the Accuaracy of the Coefficient Estimates
&lt;/div>
&lt;/h3>&lt;p>Assume that the &lt;em>true&lt;/em> relationship (e.g., the Hooke’s law) between $X$ and $Y$ takes the form $Y = f(X) + \epsilon$ for some unknown function $f$, where $\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as
$$
Y = \beta_0 + \beta_1 X + \epsilon.
$$
This is the &lt;em>population regression line&lt;/em>. Here $\beta_0$ is the intercept (the expected value of $Y$ when $X$ = 0) and the $\beta_1$ the slop (the average increase in $Y$ associated with a one-unit increase in $X$). The $\epsilon$ (error term, typically assumed to be independent of $X$) is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error.&lt;/p>
&lt;p>The model of &lt;em>population regression line&lt;/em> is the best linear approximation to the true relationship between $X$ and $Y$ (NOTE that the assumption of linearity is often a useful working model. However, it may be not true in reality). The true relationship is generally not known for real data, but the least squares line can always be computed using the cofficient estimation methods. A natural question is as follows: how accurate is the least square line as an estimate of the population regression line?&lt;/p>
&lt;p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of &lt;em>bias&lt;/em>. If we use the sample mean $\hat{\mu}$ to estimate $\mu$, this estimate is &lt;em>unbiased&lt;/em>, in the sense that on average, we expect $\hat{\mu}$ to equal $\mu$, if we could average a huge number of estimates of $\mu$ obtained from a huge number of sets of observations. Hence, an unbiased estimator does not &lt;em>systematically&lt;/em> over- or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates as well: if we estimate $\beta_0$ and $\beta_1$ on the basis of a particular data set, then our estimates won&amp;rsquo;t be exactly equal to $\beta_0$ and $\beta_1$. But if we could average the estimates obtained over a huge number of date sets, then the average would be spot on!&lt;/p>
&lt;p>So how far off will that single estimate of $\hat{\mu}$ be? In general, we answer this question by computing the &lt;em>standard error&lt;/em> of $\hat{\mu}$, written as $SE(\hat{\mu})$. We have the well-known formula
$$
Var(\hat{\mu}) = SE(\hat{\mu})^2 = {\sigma^2 \over n},
$$
where $\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$. NOTE that this formula holds iff the $n$ observations are uncorrelated. To compute the standard errors associated with $\hat{\beta_0}$ and $\hat{\beta_1}$, we use the following formulas:
$$
SE(\hat{\beta_0})^2 =
\sigma^2 [{1 \over n} + \frac{\bar{x}^2}{\sum^n_{i=1}(x_i - \bar{x})^2}],
&lt;br>
SE(\hat{\beta_1})^2 =\frac{\sigma^2}{\sum^n_{i=1}(x_i - \bar{x})^2}
$$
where $\sigma^2 = Var(\epsilon)$. In general, $\sigma^2$ is not known, but can be estimated from the data. This estimate of $\sigma$ is known as the &lt;em>residual standard error&lt;/em>, and is given by the formula
$$
\sigma = RSE = \sqrt{RSS / (n-2)}
$$&lt;/p>
&lt;h4 id="confidence-interval" >
&lt;div>
&lt;a href="#confidence-interval">
###
&lt;/a>
Confidence Interval
&lt;/div>
&lt;/h4>&lt;p>Standard errors can be used to compute the &lt;em>confidence intervals&lt;/em>. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameters. The range is defined in terms of lower and upper limits computed from the sample of data.&lt;/p>
&lt;p>For linear regression, the 95% confidence interval for $\beta_1$ approximately takes the form
$$
\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1}).
$$
That is, there is approximately a 95% chance the true value of $\beta_1$ would be in this range.&lt;/p>
&lt;p>Similarly, for $\beta_0$, its 95% confidence interval takes the form
$$
\hat{\beta_0} \pm 2 \cdot SE(\hat{\beta_0}).
$$
NOTE that here we make an assumption that the errors are Gaussian. And the factor of $2$ in the formula will vary slightly depending on the number of observations $n$ in the linear regression.&lt;/p>
&lt;h4 id="hypothesis-tests" >
&lt;div>
&lt;a href="#hypothesis-tests">
###
&lt;/a>
Hypothesis tests
&lt;/div>
&lt;/h4>&lt;p>Standard errors can also be used to perform &lt;em>hypothesis tests&lt;/em> on the coefficients. The most common hypothesis test involves testing the &lt;em>null hypothesis&lt;/em> of
$$
H_0 : \text{There is no relationship between X and Y}
$$
versus the &lt;em>alternative hypothesis&lt;/em>
$$
H_a : \text{There is some relationship between X and Y}.
$$
Mathematically, this corresponds to testing
$$
H_0 : \beta_1 = 0
$$
versus
$$
H_a : \beta_1 \ne 0,
$$
since if $\beta_1 = 0$ then the linear regression model reduces to $Y = \beta_0 + \epsilon$, and $X$ is not associated with $Y$.&lt;/p>
&lt;p>To test the null hypothesis, we need to determine whether $\hat{\beta_1}$, our estimate for $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero. How far is far enough? This is of course depends on the accuracy of $\hat{\beta_1}$ &amp;ndash; that is, it depends on $SE(\hat{\beta_1})$:&lt;/p>
&lt;ul>
&lt;li>If $SE(\hat{\beta_1})$ is small, then even relatively small values of $\hat{\beta_1}$ may provide strong evidence that $\beta_1 \ne 0$;&lt;/li>
&lt;li>if $SE(\hat{\beta_1})$ is large, then $\hat{\beta_1}$ must be large in absolute value in order for us to reject the null hypothesis.&lt;/li>
&lt;/ul>
&lt;p>In practice, we compute a &lt;em>t-statistic&lt;/em>, given by
$$
t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})},
$$
which measures the number of standard deviations that $\hat{\beta_1}$ is away from $0$.&lt;/p>
&lt;p>If there really is no relationship between $X$ and $Y$, then we expect that &lt;em>t-statistic&lt;/em> will have a $t$-distribution with $n-2$ degrees of freedom. Consequently, it is a simple matter to compute the probability of observing any number equal to $|t|$ or larger in absolute value, assuming $\beta_1 = 0$. We call this probability the &lt;em>p-value&lt;/em>.&lt;/p>
&lt;p>&lt;strong>p-value interpretation&lt;/strong>&lt;/p>
&lt;p>Roughly speaking, we interpret the p-value as follows: a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real relationship between $X$ and $Y$. Hence we &lt;em>reject the null hypothesis&lt;/em>, and declare a relationship to exist between $X$ and $Y$, if the p-value is small enough. Typical p-value cutoffs for rejecting the null hypothesis are 5% or 1%, when $n = 30$, these correspond to &lt;em>t-statistics&lt;/em> of around $2$, and $2.75$, respectively.&lt;/p>
&lt;blockquote>
&lt;p>A small p-value indicates that a particular sample is unlikely if the null hypothesis
is true. It is akin to a jury concluding that it was unlikely that it would have been
presented with this set of evidence if the defendant were innocent, and therefore
reaching a guilty verdict. Of course, that doesn&amp;rsquo;t mean that the defendant is actually
guilty. Perhaps the jury was presented with misleading evidence.
Analogously, a low p-value might be attributable to the null hypothesis actually being
false, or it could simply be that the sample is unrepresentative of the population from
which it is drawn, i.e., the evidence is misleading.&lt;/p>
&lt;/blockquote>
&lt;p>More info about p-value see &amp;ldquo;21.2 Beware of P-values&amp;rdquo; in the book of Prof. John Guttag &amp;lt;&amp;lt;Introduction to computation and programming using python&amp;gt;&amp;gt;.&lt;/p>
&lt;h3 id="assessing-the-accuracy-of-the-model" >
&lt;div>
&lt;a href="#assessing-the-accuracy-of-the-model">
##
&lt;/a>
Assessing the Accuracy of the Model
&lt;/div>
&lt;/h3>&lt;p>Once we have rejected the null hypothesis in favor of the altervative hypothesis, it is natrual to want to quantify &lt;em>the extent to which the model fits the data&lt;/em>. The quality of a linear regression fit is typically assessed using two related quantities: the &lt;em>residual standard error&lt;/em> ($RSE$) and the $R^2$ statistic.&lt;/p>
&lt;h4 id="residual-standard-error" >
&lt;div>
&lt;a href="#residual-standard-error">
###
&lt;/a>
Residual Standard Error
&lt;/div>
&lt;/h4>&lt;p>From the model $Y = \beta_0 + \beta_1 X + \epsilon$ that associated with each observation is an error term $\epsilon$. Due to the presence of these error terms, even if we knew the true regression line (i.e., $\beta$s were known), we would not be perfectly predict $Y$ from $X$. The $RSE$ is an estimate of the standard deviation of $\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula
$$
RSE
= \sqrt{{1 \over n-2} RSS}
= \sqrt{{1 \over n-2} \sum^n_{i=1}(y_i - \hat{y_i})^2}.
$$
NOTE that $RSE$ is slightly different from &lt;em>r.m.s error&lt;/em> which the latter using the number of all samples ($n$) as denominator while the former using $n-2$.&lt;/p>
&lt;p>The $RSE$ is considered a measure of the &lt;em>lack of fit&lt;/em> of the model to the data. The smaller $RSE$ the better the model fitted to the data.&lt;/p>
&lt;h4 id="r2-statistic" >
&lt;div>
&lt;a href="#r2-statistic">
###
&lt;/a>
$R^2$ Statistic
&lt;/div>
&lt;/h4>&lt;p>The $RSE$ provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of $Y$, it is not always clear what consititues a good $RSE$. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a &lt;em>proportion&lt;/em>, the proportion of variance explained, and so it always takes on a value between $0$ and $1$, and is independent of the scale of $Y$.&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$
where $TSS = \sum(y_i - \bar{y})^2$ is the &lt;em>total sum of squares&lt;/em>. Hence $R^2$ measures the &lt;em>proportion of variability in $Y$ that can be explained using $X$&lt;/em>.&lt;/p>
&lt;h3 id="multiple-linear-regression" >
&lt;div>
&lt;a href="#multiple-linear-regression">
##
&lt;/a>
Multiple Linear Regression
&lt;/div>
&lt;/h3>&lt;p>Simple linear regression[^3] is a useful approach for predicting a response on the basis of a single predictor variable. But in practice we often have more than one predictor. One option is to run multiple separate simple linear regression, each of which uses a different feature as a predictor. However, this approach is not entirely satisfactory[^4].&lt;/p>
&lt;p>Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model[^5] so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose we have $p$ distinct predictors. Then the multiple linear regression model takes the form&lt;/p>
&lt;p>$$
\tag{3.19}
\label{mlr}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + + \beta_p X_p + \epsilon
$$&lt;/p>
&lt;p>where $X_j$ represents the $j$-th predictor and $\beta_j$ quantifies the association
between that variable and the response. We interpret $\beta_j$ as the &lt;em>average&lt;/em> effect
on $Y$ of a unit increase in $X_j$, &lt;strong>holding all other predictors fixed&lt;/strong>[^6].&lt;/p>
&lt;p>As was the case in the simple linear regression setting, the regression coefficients
$\beta_0, \beta_1, \ldots, \beta_p$ in ($\ref{mlr}$) are unknown, and must be estimated.
Given estimates $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$, we can make
predictions using the formula&lt;/p>
&lt;p>$$
\tag{3.21}
\label{mlrpred}
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p.
$$&lt;/p>
&lt;p>Then parameters are estimated using the same &lt;strong>least squares&lt;/strong> approach that we saw in
the context of simple linear regression. We choose $\beta_0, \beta_1, \ldots, \beta_p$
to minimize the &lt;strong>sum of squared residuals&lt;/strong>&lt;/p>
&lt;p>$$
\begin{eqnarray}
RSS
&amp;amp;=&amp;amp; \sum^n_{i=1}(y_i - \hat{y}_i)^2 \\
\tag{3.22} \label{rss}
&amp;amp;=&amp;amp; \sum^n_{i=1} \big( y_i - (\hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p) \big)^2
\end{eqnarray}
$$&lt;/p>
&lt;p>The values $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$ that minimize
($\ref{rss}$) are the multiple least squares regression coefficient estimates. Unlike
the simple linear regression coefficient estimation (the Python code block in previous
section), the multiple regression coefficient estimates have somewhat complicated forms
that are most easily represented using matrix algebra (see detail in section of Normal
equation).&lt;/p>
&lt;p>When we perform multiple linear regression, we usually are interested in answering a few important questions.&lt;/p>
&lt;hr>
&lt;ol>
&lt;li>Is at least one of the predictors $X_1, X_2, \ldots, X_p$ useful in predicting the response?&lt;/li>
&lt;li>Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?&lt;/li>
&lt;li>How well does the model fit the data?&lt;/li>
&lt;li>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>We now address each of these questions in turn.&lt;/p>
&lt;h4 id="one-is-there-a-relationship-between-the-response-and-predictions" >
&lt;div>
&lt;a href="#one-is-there-a-relationship-between-the-response-and-predictions">
###
&lt;/a>
One: Is There a Relationship Between the Response and Predictions?
&lt;/div>
&lt;/h4>&lt;p>Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether the slope equals $0$. In the multiple regression setting with $p$ predictors, we need to ask whether all of the regression coefficients are zero. We also use a &lt;strong>hypothesis test&lt;/strong> to answer this question. We test the &lt;strong>null hypothesis&lt;/strong>,
$$
H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0
$$
versus the alternative
$$
H_{\alpha}: \text{at least one } \beta_j \text{ is non-zero}
$$
This hypothesis test is performed by computing the &lt;strong>F-statistic&lt;/strong>,
$$
\tag{3.23}
F = \frac{(TSS - RSS) / p}{RSS / (n - p -1)}
$$
where, as with simple linear regression, $TSS = \sum(y_i - \bar{y})^2$ and $RSS = \sum(y_i - \hat{y})^2$ where $\bar{y} = {1 \over n} \sum^n_{i=1} y_i$ is the sample mean. If the linear model assumptions are correct, one can show that
$$
E {RSS / (n - p - 1) } = \sigma^2
$$
and that, provided $H_0$ is true,
$$
E {(TSS - RSS) / p } = \sigma^2
$$
Hence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to $1$. On the other hand, if $H_{\alpha}$ is true, then $E {(TSS - RSS) / p } &amp;gt; \sigma^2$, so we expect the F-statistic to be greater than $1$.&lt;/p>
&lt;p>However, what if the F-statistic had been closer to $1$? How large does the F-statistic need to be before we can reject $H_0$ and conclude that there is a relationship? It turns out that the answer depends on the values of $n$ and $p$.&lt;/p>
&lt;ul>
&lt;li>When $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$.&lt;/li>
&lt;li>Inctrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small.&lt;/li>
&lt;/ul>
&lt;p>When $H_0$ is true, and the errors $\epsilon_i$ have normal distribution, the F-statistic follows an F-distribution[^7]. For any given value of $n$ and $p$, any statistical software package can be used to compute the p-value[^8] associated with F-statistic using this distribution. Based on this p-value, we determine whether or not to reject $H_0$.&lt;/p>
&lt;p>Sometimes we want to test that a particular subset of $q$ of the coefficients are zero. This corresponds to a null hypothesis
$$
H_0 = \beta_{p-q+1} = \beta_{p-q+2} = \cdots = \beta_{p} = 0,
$$
where for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses all the variables except those last $q$. Suppose that the residual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is
$$
\tag{3.24} \label{ftest2}
F = \frac{(RSS_0 - RSS) / q}{RSS / (n - p -1)}.
$$
For each individual predictor a t-statistic and a p-value can be obtain, these statistics provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in (means $q=1$ in equation $\ref{ftest2}$). So it reports the &lt;em>partial effect&lt;/em> of adding that variable to the model.&lt;/p>
&lt;p>Given these individual p-values for each variable, why do we need to look at the over F-statistic? After all, it seems likely that if any one of the p-values for the individual variables is very small, then &lt;em>at least one of the predictors is related to the response&lt;/em>. However, this logic is flawed, especially when the number of predictors $p$ is large.&lt;/p>
&lt;p>For instance, consider an example in which $p = 100$ and $H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0$ is true, so no variable is truly associated with the response. In this situation, about &lt;strong>5%&lt;/strong> of the p-values associated with each variable will be below 0.05 by chance. &lt;strong>In other words, we expect to see approximately five small p-values even in the absence of any true association between the predictors and the response. In fact, we are almost guaranteed that we will observe at least one p-value below 0.05 by chance!&lt;/strong>&lt;/p>
&lt;p>Hence, if we use individual t-statistic and the associated p-value in order to confirm the association between any predictor and the response, there is a very high chance that we will incorrectly conclude that there is a relationship.&lt;/p>
&lt;p>However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. If $H_0$ is true, there is only a 5% chance that the F-statistic will result in a p-value below 0.05, regardless of the number of predictors or the number of observations.&lt;/p>
&lt;p>Note that when $p$ is larger than $n$, we cannot even fit the multiple linear regression model using least squares. Less flexible least squares models, such as forward stepwise selection, ridge regression, lasso regression and principal components regression, are particular useful for performing regression in the high-dimensional setting.&lt;/p>
&lt;h4 id="two-deciding-on-important-variable" >
&lt;div>
&lt;a href="#two-deciding-on-important-variable">
###
&lt;/a>
Two: Deciding on Important Variable
&lt;/div>
&lt;/h4>&lt;p>If we conclude on the basis of the F-statistic and its associated p-value that at least one of the predictors is related to the response, then it is natural to wonder which are the guilty ones. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as &lt;strong>variable selection&lt;/strong>.&lt;/p>
&lt;p>Ideally, we would like to preform variable selection by trying out a lot of different models, each containing a different subset of the predictors. Unfortunately, there are a total $2^p$ models that contain subsets of $p$ variables (Note that even with a moderate value of $p$, say, $p=30$, then $2^{30}=1,073,741,824$ models make this infeasible). We need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:&lt;/p>
&lt;ul>
&lt;li>Forward selection. We begin with the &lt;strong>null model&lt;/strong> (which contains only the intercept). We then fit $p$ simple linear regressions and add to the null model the variable that results in the lowest $RSS$, and then add to that model the variable which results in the lowest $RSS$ for the new two-variable model. This approach is continued until some stopping rule is satisfied.&lt;/li>
&lt;li>Backward selection. We start with all variables in the model, and remove the variable with largest p-value. The new ($p - 1$)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached (such as when all remaining variables have a p-value below some threshold).&lt;/li>
&lt;li>Mixed selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. If at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform forward and backward steps until all variable in the model have a low p-value, and all the variables outside the model have a large p-value if added to the model.&lt;/li>
&lt;/ul>
&lt;p>Backward selection cannot be used when $p &amp;gt; n$, forward selection is a greedy approach, and might include variable early that later become redundant. Mixed selection can remedy this.&lt;/p>
&lt;h4 id="three-model-fit" >
&lt;div>
&lt;a href="#three-model-fit">
###
&lt;/a>
Three: Model Fit
&lt;/div>
&lt;/h4>&lt;p>Two of the most common numerical measures of model fit are the $RSE$ and the $R^2$. Recall that in simple regression, $R^2$ is the square of the correlation coefficient between predictor and the response. In multiple linear regression, it turns out that it equals the square of the correlation coefficient between the response and the fitted model (this implies that the fitted model maximizes this correlation among all possible linear model).&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
\begin{eqnarray}
R^2
&amp;amp;=&amp;amp; \frac{\sum(y_i - \bar{y})^2 - \sum(y_i - \hat{y})^2}{\sum(y_i - \bar{y})^2} \\
\\
&amp;amp;=&amp;amp; \frac{TSS - RSS}{TSS} \\
\\
\tag{3.17}
&amp;amp;=&amp;amp; 1 - {RSS \over TSS},
\end{eqnarray}
$$
where ($\bar{y} = {1 \over n} \sum^n_{i=1} y_i$) is the sample mean, $\hat{y}$ is defined in ($\ref{mlrpred}$).&lt;/p>
&lt;p>An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the response variable. It turns out that $R^2$ will always increase when more variables are added to the model, even they are only weakly associated with the response. This is due to the fact that with more variable to the least squares equations must allow us to fit the training data more accurately (though not necessarily the testing data, a.k.a., over fitting).&lt;/p>
&lt;p>In general, $RSE$ is defined as
$$
\tag{3.25}
RSE = \sqrt{{1 \over {n - p - 1}} RSS}
$$
Thus model with more variables can have higher $RSE$ if the decrease in $RSS$ is small relative to the increase in $p$.&lt;/p>
&lt;h5 id="adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2" >
&lt;div>
&lt;a href="#adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2">
####
&lt;/a>
Adjusted $R^2$ &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">(From wiki)&lt;/a>
&lt;/div>
&lt;/h5>&lt;p>The use of an adjusted $R^2$ (one common notation is $\bar{R}^2$; another is $R_{adj}^2$)
is an attempt to account for the phenomenon of the $R^2$ automatically and spuriously
increasing when extra explanatory variables are added to the model. There are many
different ways of adjusting, by far the most used one, to the point that it is typically
just referred to as &lt;em>adjusted $R^2$&lt;/em>, is the correction prosposed by Mordecai Ezekiel,
and adjusted $R^2$ is defined as
$$
\bar{R}^2 = 1 - (1 - R^2){\frac{n-1}{n-p-1}}
$$
where $p$ is the total number of explantory variables in the model (not including the constant term), and $n$ is the sample size. It can also be written as
$$
\bar{R}^2 = 1 - \frac{RSS / df_e}{TSS / df_t}
$$
where $df_t$ is the &lt;em>degrees of freedom&lt;/em> $n-1$ of the estimate of the population variance of the dependent variable, and the $df_e$ is the degrees of freedom $n-p-1$ of the estimate of the underlying population error variance.&lt;/p>
&lt;p>The adjusted $R^2$ can be negative, and its value will always be less than or equal to that of $R^2$. Unlike $R^2$, the adjusted $R^2$ increases only when the increase in $R^2$ (due to the inclusion of a new variable) is more than one would expect to see by chance. If a set of explanatory variables with a predtermined hierarchy of importance are introduced into a regression one at a time, with the adjusted $R^2$ computed each time, the level at which adjusted $R^2$ reaches a maximum, and decreases afterward, would be the regression with ideal combination of having the best fit without excess/unnecessary terms.&lt;/p>
&lt;blockquote>
&lt;p>Degrees of Freedom &lt;a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">(From wiki)&lt;/a>:&lt;br>
The number of &lt;em>degrees of freedom&lt;/em> is the number of values in the final calculation of a statistic that are free to vary.
Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter are called the degrees of freedom.
Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of &amp;ldquo;free&amp;rdquo; components (how many components need to be know before the vector is fully determined).
Suppose we have a sample of independent normally distributed observations, ${X_1, X_2, \ldots, X_n}$. This can be represented as an n-dimensional random vector:
$X^T$. Since this random vector can lie anywhere in n-dimensional space, it has $n$ degrees of freedom.
Now let $\bar{X}$ be the sample mean. The random vector can be decomposed as the sum of the sample mean plus a vector of residuals:&lt;/p>
&lt;/blockquote>
&lt;p>$$
\begin{pmatrix}
X_{1} \\
\vdots \\
X_{n} \\
\end{pmatrix}
= \bar{X} \cdot
\begin{pmatrix}
1 \\
\vdots \\
1 \\
\end{pmatrix}
+
\begin{pmatrix}
X_{1} - \bar{X} \\
\vdots \\
X_{n} - \bar{X} \\
\end{pmatrix}
$$&lt;/p>
&lt;p>The first vector on the right-hand side is constrained to be a multiple of the vector of $1$&amp;rsquo;s, and the only free quantity is $\bar{X}$. It therefore has only one degree of freedom.
The second vector is constrained by the relation $\sum(X_i - \bar{X}) = 0$. The first $n-1$ components of this vector can be anything. However, once you know the first $n-1$ components, the constraint tells you the value of the $n$th component. Therefore, this vector has $n-1$ degrees of freedom.&lt;/p>
&lt;h4 id="four-prediction" >
&lt;div>
&lt;a href="#four-prediction">
###
&lt;/a>
Four: Prediction
&lt;/div>
&lt;/h4>&lt;p>Once we have fit the multiple regression model, it is straightforward to apply the fitted model $\hat{y} = \hat{f}(X) = \hat{\beta} X$ (a more verbose version see $\ref{mlrpred}$) in order to predict the response based on the values of the predictors. However, there are three sorts of uncertainty associated with this prediction.&lt;/p>
&lt;ol>
&lt;li>The coefficient estimate is the least squares estimation of the true coefficient which is unknown. The inaccuracy in the coefficient estimates is related to the &lt;em>reducible error&lt;/em>[^9]. We can compute a &lt;strong>confidence interval&lt;/strong> in order to determine how close $\hat{y}$ will be to $f(X)$.&lt;/li>
&lt;li>In practice assuming a linear model for $f(X)$ is almost always an approximation of reality, so if the true pattern is non-linear, there is an additional reducible error called &lt;em>model bias&lt;/em>.&lt;/li>
&lt;li>Even if we knew $f(X)$ &amp;ndash; that is, we knew the true value of $\beta$ &amp;ndash; the response value cannot be predicted perfectly, because of the random error $\epsilon$ in the model ($\ref{mlrpred}$), this is the &lt;em>irreducible error&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h3 id="summary" >
&lt;div>
&lt;a href="#summary">
##
&lt;/a>
Summary
&lt;/div>
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>The regression line can be specified by two descriptive statistics: the &lt;em>slope&lt;/em> and the &lt;em>intercept&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Among all lines, the regression line for $y$ on $x$ makes the smallest r.m.s error in predicting $y$ from $x$. For that reason, the regression line is often called the &lt;em>least squares line&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>With a controlled experiment, the slope can tell you the average change in $y$ that would be caused by a change in $x$. With an observational study, however, the slope cannot be relied on to predict the results of interventions. It takes a lot of hard work to draw causal inferences from observational data, with or without regression.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If the average of $y$ depends on $x$ in a non-linear way, the regression line can be quite misleading.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multiple regression is a powerful technique, but it is not a substitute for understanding. (Such as the poor investigator would fit a multiple regression equation of the form $ predicted\ area = a + b \times perimeter + c \times diagonal$ to predict the area of a rectangle).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="part-i-how-to-learn" >
&lt;div>
&lt;a href="#part-i-how-to-learn">
#
&lt;/a>
Part-I: How to learn
&lt;/div>
&lt;/h2>&lt;p>Linear regression is of course an extremely simple and limited learning algorithm, but it provides an example of how a learning algorithm can work.&lt;/p>
&lt;h3 id="normal-equation" >
&lt;div>
&lt;a href="#normal-equation">
##
&lt;/a>
Normal Equation
&lt;/div>
&lt;/h3>&lt;p>The goal is to build a system that can take a vector $x \in \mathbb{R^n}$ as input and predict the value of a scalar $y \in \mathbb{R}$ as its output. The output of linear regression is a linear function of the input. Let $\hat{y}$ be the value that our model predicts $y$ should take on. We define the output to be
$$
\begin{equation}
\tag{5.3}
\hat{y} = w^{\mathsf{T}}x
\end{equation}
$$&lt;/p>
&lt;p>where $w \in \mathbb{R^n}$ is a vector of &lt;strong>parameters&lt;/strong>.&lt;/p>
&lt;p>We thus have a definition of our task &lt;em>T&lt;/em> : to predict $y$ from $x$ by outputting $\hat{y}=w^{\mathsf{T}}x$.&lt;/p>
&lt;p>Next we need a definition of our performance measure, &lt;em>P&lt;/em>. One way of measuring the performance of the model is to compute the &lt;strong>mean squared error (MSE)&lt;/strong> of the model on the test set. If $\hat{y}^{(test)}$ gives the predictions of the model on the test set, then the MSE is given by
$$
\tag{5.4}
MSE_{test} = \frac{1}{m} \sum_i{(\hat{y}^{(test)} - {y}^{(test)})^2_i}
$$
Intuitively, one can see that this error measure decreases to 0 when $\hat{y}^{(test)} = {y}^{(test)}$. We can also see that
$$
\tag{5.5}
MSE_{test} = \frac{1}{m} ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2
$$
so the error increases whenever the Euclidean distance between the predictions and the targets increases.&lt;/p>
&lt;blockquote>
&lt;p>In machine learning, we usually measure the size of vectors using a function called a &lt;strong>norm&lt;/strong>. Formally, the $L^p$ norm is given by
$$
||x||_p = \bigg(\sum_i |x_i|^p \bigg)^{\frac{1}{p}}
$$
for $p \in \mathbb{R}, p \geq 1$.&lt;/p>
&lt;p>The $L^2$ norm, with $p = 2$, is known as the &lt;strong>Euclidean norm&lt;/strong>, often denoted simply as $||x||$. It is also common to measure the size of a vector using the squared $L^2$ norm, which can be calculated simply as $x^{\mathsf{T}}x$.&lt;/p>
&lt;/blockquote>
&lt;p>To make a machine learning algorithm, we need to design an algorithm that will improve the weights $w$ in a way that reduces $MSE_{test}$ when the algorithm is allowed to gain experience by observing a training set ($X^{(train)}, y^{(train)}$).&lt;/p>
&lt;p>One intuitive way of doing this is (to minimize $MSE_{test}$) just to minimize the MSE on the training set, $MSE_{train}$ . (Does this make any sense? Keep on reading.)&lt;/p>
&lt;p>To minimize $MSE_{train}$ , we can simply solve for where its gradient is &lt;strong>0&lt;/strong>:
$$
\begin{eqnarray}
\tag{5.6}
\nabla_w MSE_{train} = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(train)} - {y}^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{X^{(train)}w - y^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \nabla_w \big({X^{(train)}w - y^{(train)}}\big)^{\mathsf{T}} \big({X^{(train)}w - y^{(train)}}\big) = 0
\end{eqnarray}
$$&lt;/p>
&lt;p>$$
\tag{5.10}
\Rightarrow \nabla_w \big( w^{\mathsf{T}} X^{(train)\mathsf{T}} X^{(train)}w - 2w^{\mathsf{T}} X^{(train)\mathsf{T}} y^{(train)} + y^{(train)\mathsf{T}} y^{(train)} \big) = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.11}
\Rightarrow 2X^{(train)\mathsf{T}} X^{(train)}w - 2X^{(train)\mathsf{T}} y^{(train)} = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.12}
\Rightarrow w = \big(X^{(train)\mathsf{T}} X^{(train)}\big)^{-1} X^{(train)\mathsf{T}} y^{(train)}
$$&lt;/p>
&lt;p>The system of equations whose solution is given by equation 5.12 is known as the &lt;strong>normal equation&lt;/strong>. Evaluating equation 5.12 constitutes a simple learning algorithm.&lt;/p>
&lt;blockquote>
&lt;p>链式法则(Chain Rule)是计算复杂导数时的重要工具。简单地说，若函数 $f(x) = g(h(x))$，则有
$$
\tag{A.31}
\frac{\partial{f(x)}}{\partial{x}} = \frac{\partial{g(h(x))}}{\partial{h(x)}} \cdot \frac{\partial{h(x)}}{\partial{x}}.
$$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>例如在计算下式时，将$(Ax - b)$看作一个整体可简化计算：
$$
\begin{align*}
&amp;amp;\frac{\partial}{\partial{x}}(Ax - b)^{\mathsf{T}} W(Ax - b) \\
&amp;amp;= \frac{\partial{(Ax - b)}}{\partial{x}} \cdot 2W(Ax - b) \\
&amp;amp;= 2AW(Ax - b)
\end{align*}
$$&lt;/p>
&lt;/blockquote>
&lt;p>It is worth noting that the term &lt;strong>linear regression&lt;/strong> is often used to refer to a slightly more sophisticated model with one additional parameter &amp;ndash; an intercept term $b$. In this model&lt;/p>
&lt;p>$$
\tag{5.13}
\hat{y} = w^{\mathsf{T}}x + b
$$&lt;/p>
&lt;p>so the mapping from parameters to predictions is still a linear function but the mapping from features to predictions is now an &lt;strong>affine function&lt;/strong> (which is in the form of equation 5.13, it means that the plot of model&amp;rsquo;s predictions still looks like a line, but it need not pass though the origin). One can continue to use the model with only weights but augment $x$ with an extra entry that is always set to 1. Then the weight corresponding to the extra 1 entry plays the role of the bias parameter (i.e., the intercept term, $b$, a.k.a., the bias term).&lt;/p>
&lt;blockquote>
&lt;p>The intercept term $b$ is often called the &lt;strong>bias&lt;/strong> parameter of the affine transformation. This terminology derives from the point of view that the output of the transformation is biased toward being $b$ in the absence of any input.&lt;/p>
&lt;p>This term is different from the idea of a statistical bias, in which a statistical estimation algorithm&amp;rsquo;s expected estimate of a quantity is not equal to the true quantity.&lt;/p>
&lt;/blockquote>
&lt;h3 id="gradient-based-method" >
&lt;div>
&lt;a href="#gradient-based-method">
##
&lt;/a>
Gradient-based method
&lt;/div>
&lt;/h3>&lt;p>Example: Linear Least Squares&lt;/p>
&lt;p>Suppose we want to find the value of $x$ that minimizes&lt;/p>
&lt;p>$$
\tag{4.21}
f(x) = \frac{1}{2}||Ax - b||^2_2
$$&lt;/p>
&lt;p>Specialized linear algebra algorithms can solve this problem efficiently; however, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work.&lt;/p>
&lt;p>First, we need to obtain the gradient (Recall the Chain Rule):&lt;/p>
&lt;p>$$
\tag{4.42}
\nabla_x f(x) = A^{\mathsf{T}} (Ax - b) = A^{\mathsf{T}}Ax - A^{\mathsf{T}}b
$$&lt;/p>
&lt;p>We can then follow this gradient downhill, taking small steps.&lt;/p>
&lt;hr>
&lt;p>Algorithm 4.1 An algorithm to minimize $f(x) = \frac{1}{2}||Ax - b||^2_2$ with respect to $x$ using gradient descent, starting from an arbitrary value of $x$.&lt;/p>
&lt;hr>
&lt;p>Set the step size ($\epsilon$, a.k.a., learning rate) and tolerance ($\delta$) to small, positive numbers.&lt;/p>
&lt;p>&lt;strong>while&lt;/strong> $||A^{\mathsf{T}}Ax - A^{\mathsf{T}}b||_2 &amp;gt; \delta$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>$x \leftarrow x - \epsilon (A^{\mathsf{T}}Ax - A^{\mathsf{T}}b)$&lt;/p>
&lt;p>&lt;strong>end while&lt;/strong>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>关于梯度下降法(Gradient Descent)&lt;/p>
&lt;p>梯度下降法是一种常用的一阶(first-order)优化方法, 是求解无约束优化问题最简单,最经典的方法之一.&lt;/p>
&lt;p>考虑无约束优化问题$min_x f(x)$,其中$f(x)$为连续可微函数.若能构造一个序列$x^0, x^1, x^2, \ldots$ 满足
$$
\tag{B.15}
\label{eq_ngd}
f(x^{(t+1)}) &amp;lt; f(x^{(t)}), t = 0,1,2,\ldots
$$
则不断执行该过程即可收敛到局部极小点.欲满足式($\ref{eq_ngd}$),根据泰勒展式有
$$
\tag{B.16}
f(x + \Delta x) \simeq f(x) + \Delta x^{\mathsf{T}} \nabla f(x)
$$
于是,欲满足$f(x + \Delta x) &amp;lt; f(x)$,可选择
$$
\tag{B.17}
\Delta x = - \epsilon \nabla f(x)
$$
其中步长(step size)$\epsilon$是一个小常数.这就是梯度下降法.&lt;/p>
&lt;p>若目标函数$f(x)$满足一些条件,则通过选取合适的步长,就能确保通过梯度下降收敛到局部极小点.例如,若$f(x)$满足L-Lipschitz条件(亦即,对于任意$x$,存在常数$L$使得$||\nabla f(x)|| \leq L$成立),则将步长设置为$1/(2L)$即可确保收敛到局部极小点.当目标函数是凸函数时,局部极小点就是全局最小点,此时,梯度下降法可确保收敛到全局最优解.&lt;/p>
&lt;p>当目标函数$f(x)$二阶连续可微时,可将式($B.16$)替换成更为精确的二阶泰勒展式,这样就得到了牛顿法(Newton&amp;rsquo;s method).牛顿法是典型的二阶方法,其迭代轮数远小于梯度下降法.但牛顿法使用了二阶导数$\nabla^2 f(x)$ (second derivative),其每轮迭代中涉及到海森矩阵(Hessian matrix)的求逆,计算复杂度相当高,尤其在高维问题中几乎不可行.其次,牛顿法仅适用于附近点有局部极小点的情况(也就是,海森矩阵为正定矩阵,也就是海森矩阵所有的特征值都是正数),若附近点是鞍点(saddle point)则牛顿法失效.然而,梯度下降却不会被鞍点困住.若能以较低的计算代价寻找海森矩阵的近似逆矩阵,则可以显著降低计算开销,这就是拟牛顿法(quai-Newton method).&lt;/p>
&lt;/blockquote>
&lt;h3 id="newtons-method" >
&lt;div>
&lt;a href="#newtons-method">
##
&lt;/a>
Newton&amp;rsquo;s method
&lt;/div>
&lt;/h3>&lt;p>Sometimes we need to find all the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a &lt;strong>Jacobian matrix&lt;/strong>. Specifically, if we have a function $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$, then the Jacobian matrix $\mathbf{J} \in \mathbb{R}^{m \times n}$ of $f$ is defined such that $J_{i, j} = \frac{\partial}{\partial x_j}f(x)_i$.&lt;/p>
&lt;p>We are also sometimes interested in a derivative of a derivative. This is known as a &lt;strong>second derivative&lt;/strong>. For example, for a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$, the derivative with respect to $x_i$ of the derivative of $f$ with respect to $x_j$ is denoted as $\frac{\partial^2}{\partial x_i \partial x_j}f$.&lt;/p>
&lt;p>In a single dimension, we can denote $\frac{d^2}{d x^2}$ by $f&amp;rsquo;&amp;rsquo;(x)$. The second derivative tells us how the first derivative will change when we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. We can think of the second derivative as measuring &lt;strong>curvature&lt;/strong>.&lt;/p>
&lt;p>Suppose we have a quadratic function (or in practice it can be approximated well as quadratic, at least locally). If such a function has:&lt;/p>
&lt;ul>
&lt;li>Second derivative of zero: there is no curvature, it is a flat line, its value can be predict using only the gradient. If the gradient is 1, set the step size of $\epsilon$ along the negative gradient, then the cost function will decrease by $\epsilon$.&lt;/li>
&lt;li>Second derivative is negative: the function curves downward, decrease by more than $\epsilon$.&lt;/li>
&lt;li>Second derivative is positive: the function curves upward, decrease by less than $\epsilon$.&lt;/li>
&lt;/ul>
&lt;p>When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the &lt;strong>Hessian matrix&lt;/strong>. The Hessian matrix $H(f)(x)$ is defined such that&lt;/p>
&lt;p>$$
\tag{4.6}
H(f)(x)_{i, j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x).
$$&lt;/p>
&lt;p>Equivalently, the Hessian is the Jacobian of the gradient.&lt;/p>
&lt;p>The (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation to the function $f(x)$ around the current point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.8}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}}g + \frac{1}{2} (x - x^{(0)})^{\mathsf{T}}H(x - x^{(0)}),
$$&lt;/p>
&lt;p>where $g$ is the gradient and $H$ is the Hessian at $x^{(0)}$. If we use a learning rate of $\epsilon$, then the new point $x$ will be given by $x^{(0)} - \epsilon g$. Substituting this into our approximation, we obtain&lt;/p>
&lt;p>$$
\tag{4.9}
f(x^{(0)} - \epsilon g) \approx f(x^{(0)}) - \epsilon g^{\mathsf{T}}g + \frac{1}{2} \epsilon^2 g^{\mathsf{T}}Hg.
$$&lt;/p>
&lt;p>There are three terms here:&lt;/p>
&lt;ul>
&lt;li>the original value of the function&lt;/li>
&lt;li>the expected improvement due to the slope of the function&lt;/li>
&lt;li>and the correction we must apply to account for the curvature of the function&lt;/li>
&lt;/ul>
&lt;p>When this last term is too large, the gradient descent step can actually move uphill.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is zero or negative, the Taylor series approximation predicts that increase $\epsilon$ forever will decrease $f$ forever. In practice, the Taylor series is unlikely to remain accurate for large $\epsilon$, so one must resort to more heuristic choices of $\epsilon$ in this case.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is positive, solving for the optimal step size that decrease the Taylor series approximation of the function the most yields&lt;/p>
&lt;p>$$
\tag{4.10}
\epsilon^* = \frac{g^{\mathsf{T}}g}{g^{\mathsf{T}}Hg}.
$$&lt;/p>
&lt;p>In the worst case, when $g$ aligns with the eigenvector of $H$ corresponding to the maximal eigenvalue $\lambda_{max}$, then this optimal step size is given by $\frac{1}{\lambda_{max}}$. (The eigenvalues of the Hessian determine the scale of the learning rate, if the function we minimized can be approximated well by a quadratic function.)&lt;/p>
&lt;p>Using the eigendecomposition of the Hessian matrix, we can generalize the &lt;strong>second derivative test&lt;/strong> to multiple dimensions. At a critical point, where $\nabla_x f(x) = 0$, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum, or saddle point.&lt;/p>
&lt;ul>
&lt;li>when the Hessian is positive definite (all its eigenvalues are positive): local minimum.&lt;/li>
&lt;li>when the Hessian is negative definite (all its eigenvalues are negative): local maximum.&lt;/li>
&lt;li>the test is inconclusive whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero.&lt;/li>
&lt;/ul>
&lt;p>In multiple dimensions, there is a different second derivative for each direction at a single point. The &lt;strong>condition number&lt;/strong> of the Hessian at this point measures how much the second derivative differ from each other. When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly, Gradient descent is unaware of this change in the derivative, so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.&lt;/p>
&lt;p>Poor condition number also makes choosing a good step size difficult. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature.&lt;/p>
&lt;p>This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method for doing so is known as &lt;strong>Newton&amp;rsquo;s method&lt;/strong>. Newton&amp;rsquo;s method is based on using a second-order Taylor series expansion to approximate $f(x)$ near some point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.11}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}} \nabla_x f(x^{(0)}) + \frac{1}{2}(x - x^{(0)})^{\mathsf{T}}H(f)(x^{(0)})(x - x^{(0)})
$$&lt;/p>
&lt;p>If we solve for the critical point of this function, we obtain&lt;/p>
&lt;p>$$
\tag{4.12}
x^* = x^{(0)} - H(f)(x^{(0)})^{-1} \nabla_x f(x^{(0)})
$$&lt;/p>
&lt;p>When $f$ is a positive definite quadratic function, Newton&amp;rsquo;s method consist of applying equation 4.12 once to jump to the minimum of the function directly. When $f$ is not truly quadratic but can be locally approximated as a positive definite quadratic, Newton&amp;rsquo;s method consists of applying equation 4.12 multiple times. NOTE that Newton&amp;rsquo;s method is only appropriate when the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient is not attracted to saddle points unless the gradient points toward them.&lt;/p>
&lt;h2 id="part_ii-why-that--work" >
&lt;div>
&lt;a href="#part_ii-why-that--work">
#
&lt;/a>
Part_II: Why that Work
&lt;/div>
&lt;/h2>&lt;h3 id="linear-regression-as-maximum-likelihood" >
&lt;div>
&lt;a href="#linear-regression-as-maximum-likelihood">
##
&lt;/a>
Linear Regression as Maximum Likelihood
&lt;/div>
&lt;/h3>&lt;p>Previously, we motivated linear regression as an algorithm that learns to take an input $x$ and produce an output value $\hat{y}$. The mapping from $x$ to $\hat{y}$ is chosen to minimize mean squared error, a criterion that we introduced more or less arbitrarily. We now revisit linear regression from the point of view of maximum likelihood estimation.&lt;/p>
&lt;p>Instead of producing a single prediction $\hat{y}$, we now think of the model as producing a conditional distribution $p(y | x)$. We can imagine that with an infinitely large training set, we might see several training examples with the same input value $x$ but different values of $y$. The goal of the learning algorithm is now to fit the distribution $p(y | x)$ to all those different $y$ values that are all compatible with $x$.&lt;/p>
&lt;p>To derive the same linear regression algorithm we obtained before, we &lt;strong>define&lt;/strong> $p(y | x) = \mathcal{N}(y; \hat{y}(x; w), \sigma^2)$. In this example, we assume that the variance is fixed to some constant $\sigma^2$ chosen by user.&lt;/p>
&lt;p>Since the examples are assumed to be i.i.d., the conditional log-likelihood is given by&lt;/p>
&lt;p>$$
\begin{split}
&amp;amp;\sum^m_{i=1} log \ p(y^{(i)}|x^{(i)}; \theta) \\
&amp;amp;= -m \ log \ \sigma - \frac{m}{2} log(2 \pi) - \sum^m_{i=1} \frac{||\hat{y}^{(i)} - {y}^{(i)}||^2}{2 \sigma^2},
\end{split}
$$&lt;/p>
&lt;p>where $\hat{y}^{(i)}$ is the output of the linear regression on the $i$-th input $x^{(i)}$ and m is the number of the training examples. Comparing the log-likelihood with the mean squared error,&lt;/p>
&lt;p>$$
MSE_{train} = \frac{1}{m} \sum^m_{i=1} ||\hat{y}^{(i)} - {y}^{(i)}||^2,
$$&lt;/p>
&lt;p>we immediately see that maximizing the log-likelihood with respect to $w$ yields the same estimate of the parameters $w$ as does minimizing the mean squared error. The two criteria have different values but the same location of the optimum.&lt;/p>
&lt;p>This justifies the use of MSE as a maximum likelihood estimation procedure.&lt;/p>
&lt;h4 id="maximum-likelihood-estimation" >
&lt;div>
&lt;a href="#maximum-likelihood-estimation">
###
&lt;/a>
Maximum Likelihood Estimation
&lt;/div>
&lt;/h4>&lt;p>Rather than guessing that some function might make a good estimator and then analyzing its bias and variance, we would like to have some principle from which we can derive specific functions that are good estimators for different models.&lt;/p>
&lt;p>The most common such principle is the maximum likelihood principle.&lt;/p>
&lt;p>Consider a set of m examples $\mathbb{X} = {x^{(1)}, \cdots, x^{(m)}}$ are i.i.d from true but unknown data-generating distribution $p_{data}(\mathbf{x})$.&lt;/p>
&lt;p>Let $p_{model}(\mathbf{x}; \mathbf{\theta})$ be a parametric family of probability distribution over the same space indexed by $\mathbf{\theta}$. In other words, $p_{model}({x}; \mathbf{\theta})$ maps any configuration $x$ to a real number estimating the true probability $p_{data}({x})$.&lt;/p>
&lt;p>The maximum likelihood estimator for $\mathbf{\theta}$ is then defined as&lt;/p>
&lt;p>$$
\begin{eqnarray}
\mathbf{\theta_{ML}}
\tag{5.56}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} {p_{model}(\mathbb{x}; \mathbf{\theta})} \\
\tag{5.57}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \prod^m_{i=1}{p_{model}(x^{(i)}; \mathbf{\theta})}
\end{eqnarray}
$$&lt;/p>
&lt;p>This product over many probabilities can be inconvenient for various reasons. Such as it&amp;rsquo;s prone to numerical underflow. We observe that taking the logarithm of the likelihood does not change its argmax but does conveniently transform a product into a sum:&lt;/p>
&lt;p>$$
\tag{5.58}
\mathbf{\theta_{ML}} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \sum^m_{i=1}{\text{log} \ p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>Because the argmax does not change when we rescale the cost function, we can divide by
$m$ to obtain a version of the criterion that is expressed as an expectation with
respect to the empirical distribution $\hat{p}_{data}$ defined by the training data:&lt;/p>
&lt;p>FIXME: \mathbb in equation(5.59, 5.60, 5.61) not work.&lt;/p>
&lt;p>$$
\tag{5.59}
\mathbf{\theta_{ML}} = {\underset{\mathbf{\theta}}{\operatorname{argmax}}}\
\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {\text{log} p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>One way to interpret maximum likelihood estimation is to view it as minimizing the
dissimilarity between the empirical distribution $\hat{p}_{data}$, defined by the
training set and the model distribution, with the degree of dissimilarity between the
two measured by the KL divergence. The KL divergence is given by&lt;/p>
&lt;p>$$
\tag{5.60}
D_{KL}(\hat{p}_{data} || p_{model}) = \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\log \hat{p}_{data}(x) - \log {p}_{model}(x)]}.
$$&lt;/p>
&lt;p>The term on the left is a function only of the data-generating process, not the model.
This means when we train the model to minimize the KL divergence, we need only minimize&lt;/p>
&lt;p>$$
\tag{5.61}
-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\text{log} \hat{p}_{data}(x)]},
$$&lt;/p>
&lt;p>which is of course the same as the maximization in equation 5.59.&lt;/p>
&lt;p>Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution and the model distribution. For example, MSE is the cross-entropy between the empirical distribution and a Gaussian model.&lt;/p>
&lt;p>We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution $\hat{p}_{data}$. While the optimal $\mathbf{\theta}$ is the same regardless of whatever we are maximizing the likelihood or minimizing the KL divergence, the values of the objective functions are different.&lt;/p>
&lt;p>In software, we often phrase both as minimizing a cost function.&lt;/p>
&lt;p>Maximum likelihood thus becomes minimization of the negative log-likelihood (NLL), or equivalently, minimization of the cross-entropy.&lt;/p>
&lt;p>关于KL散度&lt;/p>
&lt;p>KL散度(Kullback-Leibler divergence), 亦称相对熵(relative entropy)或信息散度(information divergence), 可用于度量两个概率分布之间的差异. 给定两个连续型概率分布$P$和$Q$, 二者之间的KL散度定义为&lt;/p>
&lt;p>$$
\label{eq_kld} \tag{C.34}
KL(P||Q) = \int^{\infty}_{-\infty} p(x) \text{log}\frac{p(x)}{q(x)} dx,
$$&lt;/p>
&lt;p>其中,$p(x)$和$q(x)$分别是$P$和$Q$的概率密度函数.&lt;/p>
&lt;p>KL散度满足非负性, 即&lt;/p>
&lt;p>$$
\tag{C.35}
KL(P||Q) \geq 0,
$$&lt;/p>
&lt;p>当且仅当$P=Q$时$KL(P||Q)=0$. 但是, KL散度不满足对称性, 即&lt;/p>
&lt;p>$$
\tag{C.36}
KL(P||Q) \neq KL(Q||P),
$$&lt;/p>
&lt;p>因此, KL散度不是一个度量(metric).&lt;/p>
&lt;p>若将KL散度的定义($\ref{eq_kld}$)展开, 可得&lt;/p>
&lt;p>$$
\begin{eqnarray}
KL(P||Q)
&amp;amp;=&amp;amp; \int^{\infty}_{-\infty} p(x)\log p(x)dx - \int^{\infty}_{-\infty} p(x)\log q(x)dx \\
\tag{C.37}
&amp;amp;=&amp;amp; -H(P) + H(P, Q),
\end{eqnarray}
$$&lt;/p>
&lt;p>其中$H(P)$为熵(entropy), $H(P,Q)$为交叉熵(cross-entropy).&lt;/p>
&lt;h3 id="bayesian-linear-regression" >
&lt;div>
&lt;a href="#bayesian-linear-regression">
##
&lt;/a>
Bayesian Linear Regression
&lt;/div>
&lt;/h3>&lt;p>So far we have discussed &lt;strong>frequentist statistics&lt;/strong> and approaches based on estimating a single value of $\theta$, then making all predictions thereafter based on that one estimate. An other approach is to consider all possible values of $\theta$ when making a prediction. The latter is the domain of &lt;strong>Bayesian statistics&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>Freqentist: the true parameter value $\theta$ is fixed but unknown, while the point estimate $\hat{\theta}$ is a random variable on account of it being a function of the dataset (which is seen as random).&lt;/li>
&lt;li>Bayesian: the Bayesian uses probability to reflect degrees of certainty in states of knowledge. The dataset is directly observed and so is not random. On the other hand, the true parameter $\theta$ is unknown or uncertain and thus is represented as random variable.&lt;/li>
&lt;/ul>
&lt;p>Before observing the data, we represent our knowledge of $\theta$ using the &lt;strong>prior probability distribution&lt;/strong>, $p(\theta)$ (a.k.a., &amp;ldquo;the prior&amp;rdquo;). Generally, the machine learning practitioner selects a prior distribution that quite broad (i.e., with high entropy, such as uniform distribution) to reflect a high degree of uncertainty in the value of $\theta$ before observing any data.&lt;/p>
&lt;p>Now consider that we have a set of data samples {$x^{(1)}, \ldots, x^{(m)}$}. We can recover the effect of data on our belief about $\theta$ by combining the data likelihood $p(x^{(1)}, \ldots, x^{(m)} | \theta)$ with the prior via Bayes&amp;rsquo; rule:
$$
\tag{5.67}
p(\theta | x^{(1)}, \ldots, x^{(m)}) = \frac{p(x^{(1)}, \ldots, x^{(m)} | \theta) p(\theta)}{p(x^{(1)}, \ldots, x^{(m)})}
$$&lt;/p>
&lt;p>In the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation of the data usually causes the posterior to lose entropy and concentrate around a few highly likely values of the parameters.&lt;/p>
&lt;p>Bayesian estimation offers two important differences from MLE:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Unlike the MLE approach that makes predictions using a point estimate of $\theta$, the Bayesian approach is to make predictions using a full distribution over $\theta$. For example, after observing $m$ examples, the predicted distribution over the next data sample, $x^{(m+1)}$, is given by
$$
\tag{5.68}
p(x^{(m+1)} | x^{(1)}, \ldots, x^{(m)}) = \int p(x^{(m+1)} | \theta) p(\theta | x^{(1)}, \ldots, x^{(m)}) d \theta
$$
Here each value of $\theta$ with positive probability density contributes to the prediction of the next example, with the contribution weighted by the posterior density itself.&lt;/p>
&lt;p>After having observed {$x^{(1)}, \ldots, x^{(m)}$}, if we are still uncertain about the value of $\theta$, then this uncertainty is incorporated into any predictions we might make.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The prior has an influence by shifting probability mass density towards regions of the parameter space that are preferred a priori. In practice, the prior often expresses a preference for models that are simpler or more smooth.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Critics of the Bayesian approach identify the prior as a source of subjective human judgment affecting the predictions.&lt;/p>
&lt;p>Bayesian methods typically generalize much better when limited training data is available but typically suffer from high computational cost when the number of training examples is large.&lt;/p>
&lt;p>&lt;strong>$\color{Green}{\mathbf{Example}}$&lt;/strong>&lt;/p>
&lt;p>Here we consider the Bayesian estimation approach to learning the linear regression parameters. In linear regression, we learn a linear mapping from an input vector $x \in \mathbb{R}^n$ to predict the value of a scalar $y \in \mathbb{R}$. The prediction is parameterized by the vector $w \in \mathbb{R}^n$:
$$
\tag{5.69}
\hat{y} = w^{\mathsf{T}}x.
$$
Given a set of $m$ training samples ($X^{(train)}, y^{(train)}$), we can express the prediction of $y$ over the entire training set as
$$
\tag{5.70}
\hat{y}^{(train)} = X^{(train)}w.
$$
Expressed as a Gaussian conditional distribution on $y^{(train)}$, we have
$$
\begin{eqnarray}
p(y^{(train)} | X^{(train)}, w)
\tag{5.71}
&amp;amp;=&amp;amp; \mathcal{N}(y^{(train)} ; X^{(train)}w, I) \\
\tag{5.72}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(
- \frac{1}{2}(y^{(train)} - X^{(train)}w)^{\mathsf{T}} (y^{(train)} - X^{(train)}w)
\bigg),
\end{eqnarray}
$$
where we follow the standard MSE formulation in assuming that the Gaussian variance on $y$ is one.&lt;/p>
&lt;p>In what follows, to reduce the notational burden, we refer to ($X^{(train)}, y^{(train)}$) as simply ($X, y$).&lt;/p>
&lt;p>To determine the posterior distribution over the model parameter vector $w$, we first need to specify a prior distribution. For real-valued parameters it is common to use a Gaussian as a prior distribution,
$$
\tag{5.73}
p(w) = \mathcal{N}(w; \mu_0, \Lambda_0) \varpropto \text{exp} \bigg(
-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg),
$$
where $\mu_0$ and $\Lambda_0$ are the prior distribution mean vector and covariance matrix respectively. (We assume a diagonal covariance matrix $\Lambda_0 = diag(\lambda_0)$, unless there is a reason to use a particular covariance structure.)&lt;/p>
&lt;p>With the prior thus specified, we can now proceed in determining the &lt;strong>posterior&lt;/strong> distribution over the model parameters:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.74}
&amp;amp;\varpropto&amp;amp; p(y | X, w)p(w) \\
\tag{5.75}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(y - Xw)^{\mathsf{T}} (y - Xw) \bigg) \text{exp} \bigg(-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg) \\
\tag{5.76}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} \big( -2y^{\mathsf{T}}Xw + w^{\mathsf{T}}X^{\mathsf{T}}Xw + w^{\mathsf{T}} \Lambda_0^{-1}w - 2\mu_0^{\mathsf{T}}\Lambda_0^{-1}w \big) \bigg)
\end{eqnarray}
$$
We now define $\Lambda_m = (X^{\mathsf{T}}X + \Lambda_0^{-1})^{-1}$ and $\mu_m = \Lambda_m (X^{\mathsf{T}}y + \Lambda_0^{-1} \mu_0)$. Using these new variables, we find that the posterior may be rewritten as a Gaussian distribution:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.77}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) + \frac{1}{2} \mu_m^{\mathsf{T}}\Lambda_m^{-1}\mu_m \bigg) \\
\tag{5.78}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} (w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) \bigg)
\end{eqnarray}
$$
All terms that do not include the parameter vector $w$ have been omitted; they are implied by the fact that the distribution must be normalized to integrate to 1.&lt;/p>
&lt;p>Equation 3.23 shows how to normalize a multivariate Gaussian distribution:
$$
\tag{3.23}
\mathcal{N}(x; \mu, \Sigma) = \sqrt{\frac{1}{(2\pi)^n \text{det}(\Sigma)}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \Sigma^{-1}(x - \mu) \bigg).
$$
When we wish to evaluate the PDF several times for many different values of the parameters, the covariance is not a computationally efficient way to parametrize the distribution, since we need to invert $\Sigma$ to evaluate the PDF. We can instead use a &lt;strong>precision matrix $\beta$&lt;/strong>:
$$
\tag{3.24}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{\frac{\text{det}(\beta)}{(2\pi)^n}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \beta (x - \mu) \bigg).
$$&lt;/p>
&lt;h4 id="maximum-a-posteriori-map-estimation" >
&lt;div>
&lt;a href="#maximum-a-posteriori-map-estimation">
###
&lt;/a>
Maximum A Posteriori (MAP) Estimation
&lt;/div>
&lt;/h4>&lt;p>While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter $\theta$, it is still often desirable to have a single point estimate. One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation.&lt;/p>
&lt;p>Rather than simply returning to the MLE, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate. One rational way to do this id to choose the &lt;strong>maximum a posteriori&lt;/strong> (MAP) point estimate. The MAP estimate chooses the point of maximal posterior probability ( or maximal probability density in the more common case of continuous $\theta$):
$$
\tag{5.79}
\theta_{MAP}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {p(\mathbf{\theta} | x)}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {\text{log} \ p(x | \mathbf{\theta})} + \text{log} \ p(\mathbf{\theta})
$$
We recognize, on the righthand side, $\text{log} \ p(x | \mathbf{\theta})$, that is, the standard log-likelihood term, and $\text{log} \ p(\mathbf{\theta})$, corresponding to the prior distribution.&lt;/p>
&lt;p>As an example, consider a linear regression model with a Gaussian prior on the weights $w$. If this prior is given by $\mathcal{N}(w; 0, \frac{1}{\lambda}I^2)$, then the log-prior term in equation 5.79 is proportional to the familiar $\lambda w^{\mathsf{T}}w$ weight decay penalty, plus a term that does not depend on $w$ and does not affect the learning process. MAP Bayesian inference with a Gaussian prior on the weights thus corresponds to weight decay.&lt;/p>
&lt;h3 id="mle-and-map-殊途同归" >
&lt;div>
&lt;a href="#mle-and-map-%e6%ae%8a%e9%80%94%e5%90%8c%e5%bd%92">
##
&lt;/a>
MLE and MAP: 殊途同归
&lt;/div>
&lt;/h3>&lt;h4 id="binary-variables" >
&lt;div>
&lt;a href="#binary-variables">
###
&lt;/a>
Binary Variables
&lt;/div>
&lt;/h4>&lt;ul>
&lt;li>
&lt;p>Coin flipping: heads = 1, tails = 0 with bias $\mu$
$$
p(X = 1 | \mu) = \mu
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Bernoulli Distribution
$$
Bern(x | \mu) = \mu^x \cdot (1 - \mu)^{1 - x} \\
\mathbf{E}[X] = \mu \\
var(X) = \mu \cdot (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>N coin flips: $X_1, \ldots, X_N$
$$
p(\Sigma_i X_i = m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Binomial Distribution
$$
p(m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
\mathbf{E}[\Sigma_i X_i] = N \mu \\
var[\Sigma_i X_i] = N \mu (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="the-bias-of-a-coin" >
&lt;div>
&lt;a href="#the-bias-of-a-coin">
###
&lt;/a>
The Bias of a Coin
&lt;/div>
&lt;/h4>&lt;p>Suppose that we have a coin, and we would like to figure out what the probability is that it will flip up heads.&lt;/p>
&lt;ul>
&lt;li>How should we estimate the bias?&lt;/li>
&lt;/ul>
&lt;p>With these coin flips result: &lt;strong>[tail, head, tail, head, head]&lt;/strong>, our estimate of the bias is: 3/5 (&amp;ldquo;the frequency of heads&amp;rdquo;).&lt;/p>
&lt;ul>
&lt;li>
&lt;p>why is this a good estimate of the bias?&lt;/p>
&lt;p>- how good is this estimation?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$P(Heads) = \theta, \ P(Tails) = 1 - \theta$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Flips are i.i.d.&lt;/p>
&lt;ul>
&lt;li>Independent events&lt;/li>
&lt;li>Identically distributed according to Binomial distribution&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Our training data consists of $\alpha_H$ heads and $\alpha_T$ tails
$$
p(D | \theta) = \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="mle" >
&lt;div>
&lt;a href="#mle">
###
&lt;/a>
MLE
&lt;/div>
&lt;/h4>&lt;ul>
&lt;li>
&lt;p>Data: Observed set of $\alpha_H$ heads and $\alpha_T$ tails&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hypothesis: Coin flips follow a binomial distribution&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Learning: Find the &amp;ldquo;best&amp;rdquo; $\theta$&lt;/p>
&lt;p>Maximum Likelihood Estimation: Choose $\theta$ to maximize probability of $D$ given $\theta$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\hat{\theta}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;ul>
&lt;li>Set derivative to zero, and solve!&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\frac{d}{d\theta} \text{ln} \ P(D | \theta)
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}] \\
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\alpha_H \text{ln} \ \theta + \alpha_T \text{ln} (1 - \theta)] \\
&amp;amp;=&amp;amp; \alpha_H \frac{d}{d\theta} \text{ln} \ \theta + \alpha_T \frac{d}{d\theta} \text{ln} \ (1 - \theta) \\
&amp;amp;=&amp;amp; \frac{\alpha_H}{\theta} - \frac{\alpha_T}{1 - \theta} = 0 \\
\\
\Rightarrow \hat{\theta}_{MLE} &amp;amp;=&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;p>As we can see now, that&amp;rsquo;s exactly the &amp;ldquo;Frequency of the heads&amp;rdquo;! In other words, the frequency of heads is exactly the &lt;strong>maximum likelihood estimator&lt;/strong> for this problem.&lt;/p>
&lt;h4 id="map" >
&lt;div>
&lt;a href="#map">
###
&lt;/a>
MAP
&lt;/div>
&lt;/h4>&lt;p>Suppose we have 5 coin flips all of which are heads, Our estimate of the bias is: ???&lt;/p>
&lt;ul>
&lt;li>MLE would give $\theta_{MLE} = 1$&lt;/li>
&lt;li>This event occurs with probability $1 / 2^5 = 1/32$ for a fair coin&lt;/li>
&lt;li>Are we willing to commit to such a strong conclusion with such little evidence?&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Priors&lt;/strong> are a Bayesian mechanism that allow us to take into account &amp;ldquo;prior&amp;rdquo; knowledge about our belief in the outcome. Rather than estimating a single $\theta$, consider a distribution over possible values of $\theta$ given the data:&lt;/p>
&lt;ul>
&lt;li>Without any data observed, our best guess of $\theta$ is obeyed a Beta(2, 2),&lt;/li>
&lt;li>After we see some data (such as observed flips:[tails, tails]), we update our prior to Beta(3, 2).&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Bayesian Learning&lt;/strong>
$$
\begin{eqnarray}
\tag{L1}
p(\theta | D) = \frac{p(D | \theta) \ p(\theta)}{p(D)} \\
\tag{L2} \label{eq_map}
\Rightarrow p(\theta | D) \varpropto p(D | \theta) \ p(\theta)
\end{eqnarray}
$$
where&lt;/p>
&lt;ul>
&lt;li>$p(\theta | D)$ is the posterior,&lt;/li>
&lt;li>$p(D | \theta)$ is the data likelihood,&lt;/li>
&lt;li>$p(\theta)$ is the prior,&lt;/li>
&lt;li>$p(D)$ is the normalization factor.&lt;/li>
&lt;/ul>
&lt;p>We update the prior according to the observed data to get the posterior by applying Bayes rule.&lt;/p>
&lt;p>&lt;strong>Picking Priors&lt;/strong>&lt;/p>
&lt;p>How do we pick a good prior distribution?&lt;/p>
&lt;ul>
&lt;li>Priors could represent expert domain knowledge&lt;/li>
&lt;li>Statisticians choose them to make the posterior distribution &amp;ldquo;nice&amp;rdquo; (conjugate priors, which makes the posterior the same form as the prior)&lt;/li>
&lt;/ul>
&lt;p>What is a good prior for the bias in the coin flipping problem?&lt;/p>
&lt;ul>
&lt;li>Truncated Gaussian (tough to work with)&lt;/li>
&lt;li>Beta distribution (works well for binary random variables)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Coin Flips with Beta Distribution&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Likelihood function: $p(D | \theta) = \theta^{\alpha_H} (1 - \theta)^{\alpha_T}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Prior: $p(\theta) = \frac{\theta^{\beta_H - 1} (1 - \theta)^{\beta_T - 1}}{B(\beta_H, \beta_T)} \sim Beta(\beta_H, \beta_T)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Posterior:
$$
\begin{eqnarray}
p(\theta | D)
&amp;amp;\varpropto&amp;amp; \theta^{\alpha_H}(1 - \theta)^{\alpha_T} \theta^{\beta_H - 1}(1 - \theta)^{\beta_T - 1} \\
&amp;amp;=&amp;amp; \theta^{\alpha_H + \beta_H - 1}(1 - \theta)^{\alpha_T + \beta_T - 1} \\
&amp;amp;=&amp;amp; Beta(\alpha_H + \beta_H, \alpha_T + \beta_T)
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>MAP Estimation&lt;/strong>&lt;/p>
&lt;p>Choosing $\theta$ to maximize the posterior distribution is called &amp;ldquo;maximum a posteriori (MAP)&amp;rdquo; estimation
$$
\theta_{MAP} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \ {p(\mathbf{\theta} | D)}
$$
The only difference between $\theta_{MLE}$ and $\theta_{MAP}$ is that one assumes a &lt;strong>uniform&lt;/strong> prior (MLE) and the other allows an arbitrary prior.&lt;/p>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>With uniform prior $p(\theta) \varpropto 1$, according to $\ref{eq_map}$, the posterior $p(\theta | D) \varpropto p(D | \theta)$.&lt;/p>
&lt;/blockquote>
&lt;p>Suppose we have 5 coin flips all of which are heads,&lt;/p>
&lt;ul>
&lt;li>
&lt;p>MLE would give $\theta_{MLE} = 1$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>MLE with a Beta(2, 2) prior gives $\theta_{MAP} = \frac{5 + 2 - 1}{5+2+0+2 - 2} = \frac{6}{7} \approx .857$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>As we see more data, the effect of the prior diminishes
$$
\begin{eqnarray}
\theta_{MAP}
&amp;amp;=&amp;amp; \frac{\alpha_H + \beta_H - 1}{\alpha_H + \beta_H + \alpha_T + \beta_T - 2} \\
\\
&amp;amp;\approx&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T} \ (\text{for large number of observations})
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="sample-complexity" >
&lt;div>
&lt;a href="#sample-complexity">
###
&lt;/a>
Sample Complexity
&lt;/div>
&lt;/h4>&lt;p>How many coin flips do we need in order to guarantee that our learned parameter does not differ too much from the true parameter (with high probability)? Say, I want to know the coin parameter $\theta$, within $\epsilon = 0.1$ error with probability at least $1 - \delta = 0.95$.&lt;/p>
&lt;p>Using the Chernoff bound, we have
$$
p(|\theta_{true} - \theta_{MLE}| \geq \epsilon) \leq 2e^{-2N \epsilon^2} \\
\delta \geq 2e^{-2N \epsilon^2} \Rightarrow N \geq \frac{1}{2\epsilon^2} \text{ln} \frac{1}{\delta}
$$&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Convert each variable to standard units. The average of the products gives the correlation coefficient (may be more intuitively in the python code)&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Associated with a unit increase in $x$ there is some average change in $y$. The slope of the regression line estimates this change. The formula for the slope is $\frac{r \times SD_y}{SD_x}$. And the intercept of the regression line is just the predicted value for $y$ when $x$ is $0$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>