<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>fgg blog</title><link>/</link><description>fgg blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 01 Aug 2024 19:25:54 +0800</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>matplotlibIssue</title><link>/posts/pyplot/matplotlibissue/</link><pubDate>Thu, 01 Aug 2024 17:58:28 +0800</pubDate><guid>/posts/pyplot/matplotlibissue/</guid><description>&lt;p>matplotlibÊòØ‰∏™Â•ΩÂ∑•ÂÖ∑Ôºå‰ΩÜËøòÊ≤°Â•ΩÈÄèÔºåÂÅöÂá∫Êù•ÁöÑÂõæ‰∏≠‰∏≠ÊñáÂèòÊ°ÜÊ°Ü„ÄÇÈúÄË¶ÅÈ¢ùÂ§ñËÆæÁΩÆ„ÄÇ&lt;/p>
&lt;ol>
&lt;li>Ëé∑Âèñ matplotlibrc&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Êü•ÁúãmatplotlibÂÆâË£ÖÂú®Âì™ÈáåÔºö&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python -c &lt;span style="color:#5af78e">&amp;#34;import matplotlib;print(matplotlib.__file__)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># your-venv/lib/python3.x/site-packages/matplotlib/mpl-data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Â§çÂà∂ matplotlibrc Âà∞Áî®Êà∑ÈÖçÁΩÆÁõÆÂΩï&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv your-venv/lib/python3.x/site-packages/matplotlib/mpl-data/matplotlibrc ~/.config/matplotlib/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>‰øÆÊîπ matplotlibrc&lt;/li>
&lt;/ol>
&lt;p>ÁºñËæëÂô®‰∏≠ÊâìÂºÄ matplotlibrc Êñá‰ª∂ÔºåÊâæÂà∞Âπ∂‰øÆÊîπ‰ª•‰∏ã‰∏§‰∏™Âú∞ÊñπÔºö&lt;/p>
&lt;p>&amp;lsquo;#font.family: sans-serif&amp;rsquo; &amp;raquo; font.family: sans-serif
&amp;lsquo;#font.sans-serif: DejaVu Sans,&amp;hellip;&amp;rsquo; &amp;raquo; font.sans-serif: DejaVu Sans,&amp;hellip;&lt;/p>
&lt;p>ÁÑ∂ÂêéÊ∑ªÂä†‰∏≠ÊñáÂ≠ó‰ΩìÔºàttf Êñá‰ª∂ÁöÑÊñá‰ª∂ÂêçÔºåÂ¶Ç‰∏ãÊñáÁöÑ &lt;code>simfang&lt;/code> ‰ªøÂÆãÂ≠ó‰ΩìÔºâ
font.sans-serif: &lt;code>simfang&lt;/code>, DejaVu Sans,&amp;hellip;&lt;/p>
&lt;p>ÊúÄÂêéÂ∞Ü &lt;code>simfang.ttf&lt;/code> Â≠ó‰ΩìÊñá‰ª∂Âä†ÂÖ•Âà∞ &lt;code>mpl-data/fonts/ttf/&lt;/code> ÁõÆÂΩï‰∏≠„ÄÇ&lt;/p>
&lt;p>Ê≥®ÔºöÊõ¥ÂÆπÊòìÁª¥Êä§ÁöÑ‰∏ÄÁßçÈÖçÁΩÆÊñπÂºèÂ∫îËØ•ÊòØËøôÊ†∑Ôºö
‰ΩøÁî® &lt;code>ln -s&lt;/code> ÂëΩ‰ª§Â∞Ü‰∏™‰∫∫Â≠ó‰ΩìÂ∫ì (&lt;code>.local/share/fonts/&lt;/code>) ‰∏≠ÁöÑÊüê‰∏™Â≠ó‰ΩìÊò†Â∞ÑÂà∞‰∏äËø∞ÁõÆÂΩï&lt;/p>
&lt;p>Ê≥®2ÔºöÂèØËÉΩÈúÄË¶ÅÂà†Èô§matplotlibÁöÑÁºìÂ≠ò&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>rm -rf /home/user/.cache/matplotlib
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>&lt;code>mpl.rcParams&lt;/code> ‰ª£Á†ÅÂùó&lt;/li>
&lt;/ol>
&lt;p>Âú®‰ΩúÂõæÁöÑÊ®°Âùó‰∏≠Ê∑ªÂä†‰ª•‰∏ã‰ª£Á†ÅÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> matplotlib &lt;span style="color:#ff6ac1">as&lt;/span> mpl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> matplotlib.pyplot &lt;span style="color:#ff6ac1">as&lt;/span> plt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ‰∏≠ÊñáÂáèÂè∑ÊòæÁ§∫ÈóÆÈ¢ò&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mpl&lt;span style="color:#ff6ac1">.&lt;/span>rcParams[&lt;span style="color:#5af78e">&amp;#39;axes.unicode_minus&amp;#39;&lt;/span>] &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ‰∏≠ÊñáÂ≠óÁ¨¶ÊòæÁ§∫ÈóÆÈ¢ò&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ÂêçÂ≠ó‰∏éttfÊñá‰ª∂ÁöÑÂêçÂ≠ó‰∏çÂêå: simfang.ttf &amp;gt;&amp;gt; &amp;#39;FangSong&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mpl&lt;span style="color:#ff6ac1">.&lt;/span>rcParams[&lt;span style="color:#5af78e">&amp;#39;font.sans-serif&amp;#39;&lt;/span>] &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#39;FangSong&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÊúÄÂêéÔºåÂ¶ÇÊûúÊúâÂ§ö‰∏™ venv ÁéØÂ¢ÉÔºåÈöæÈÅìË¶Å‰∏Ä‰∏™‰∏Ä‰∏™Âéª‰øÆÊîπÂêóÔºü
&lt;code>pipx&lt;/code> Â§ßÊ≥ïÊòØËß£ËçØ„ÄÇ&lt;/p></description></item><item><title>inspect_substrings_in_file_using_sed_awk_jq</title><link>/posts/cs101/awk_sed_cat_head_etc/awk_use_cases/</link><pubDate>Wed, 17 Jul 2024 19:12:45 +0800</pubDate><guid>/posts/cs101/awk_sed_cat_head_etc/awk_use_cases/</guid><description>&lt;p>Âú®ÂëΩ‰ª§Ë°åÂ∑•‰ΩúËøúÈùûÂÆåÁæéÔºå‰ΩÜÂÖ∑ÊúâÊûÅÈ´òÁöÑËá™Áî±Â∫¶„ÄÇÊØîÂ¶ÇÔºå‰Ω†ÊÉ≥Ë¶ÅÊü•ÁúãÊüê‰∏™Êñá‰ª∂ÂÜÖ(ËøôÈáåÁªùÂØπ‰∏çÊòØÊåá
doc/docx ËøôÁ±ªÊÄ™ËÉéÔºåËÄåÊòØ text/csv/tsv/json Á≠âÁ≠âÊ≠£Â∏∏ÁöÑÊñá‰ª∂) ÁöÑÊüê‰∏ÄË°åÁöÑÂÜÖÂÆπÔºåÂ¶ÇÊûúÈÇ£‰∏ÄË°åÊúâÂæà
Â§öÂàóÔºåËÄåÂÖ∂‰∏≠Ë¶ÅÊòØÊüê‰∏ÄÂàóÁöÑÂÜÖÂÆπÁâπÂà´ÈïøÔºàÂ∞±ÊòØ&lt;code>sed&lt;/code>Âá∫Êù•Âç†ÊçÆ‰∫ÜÊï¥‰∏™Â±èÂπïËøôÁßçÈïøÂ∫¶ÔºâÔºåÈÇ£‰πàÔºå‰Ω†ÊÉ≥Ë¶ÅÂØπ
Ëøô‰∏™Êñá‰ª∂ÁöÑÂÜÖÂÆπÊúâÊâÄ‰∫ÜËß£ÔºåÂèØËÉΩÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËΩØ‰ª∂Êù•ÊâìÂºÄÊ≠§Êñá‰ª∂ÔºåÂÜçÂ•ΩÂ•ΩÊ¨£Ëµè‰πã„ÄÇÊàñËÄÖÔºåÂÖ∂ÂÆûÂú®ÂëΩ‰ª§Ë°å
Â∞±ÂèØ‰ª•Âë¢Ôºü&amp;ndash; &lt;code>awk&lt;/code> ‰∫ÜËß£‰∏Ä‰∏ã„ÄÇ&lt;/p>
&lt;p>&lt;img alt="is_value_missing" src="https://fgg100y.github.io/posts/cs101/awk_sed_cat_head_etc/awk_use_cases/images/awk-to-the-rescue.png">&lt;/p>
&lt;p>ÂÉèÊàëÂ∞±ÊòØÁ¢∞‰∏äËøô‰πà‰∏™ÊÉÖÂÜµÔºåÂ∞ÜÂõæÁâáËΩ¨Êç¢Êàê base64 ÁºñÁ†ÅÁöÑ byte-data ÂêéÔºåÂú® Jupyterlab ÊâìÂºÄÂèëÁé∞
Êúâ‰∫õË°åÊÄé‰πàÊòØÁ©∫Ë°åÔºüÔºàÈ¶ñÂÖàÊàë‰∏çÊòØÂéªË¥®Áñë‰ª£Á†ÅÔºåËÄåÊòØË¥®Áñë‚ÄúÁúºËßÅ‰∏∫ÂÆû‚ÄùÔºâ&lt;/p>
&lt;p>ÊâÄ‰ª•ÔºåÂøÖÈ°ªË¶ÅÊü•ÁúãÊüê‰∏ÄË°åÁöÑÂÜÖÂÆπÔºåÂπ∂‰∏îË¶ÅÊà™Êñ≠ËøáÈïøÁöÑÂ≠ó‰∏≤ÔºåËÄÅ‰ºôËÆ° GPT4o ÂæàÂø´ÁªôÂá∫Á≠îÊ°àÔºö&lt;/p>
&lt;h2 id="tsv" >
&lt;div>
&lt;a href="#tsv">
#
&lt;/a>
TSV
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and truncate the 3rd column to 50 characters&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#5af78e">&amp;#39;NR==10 { $3=substr($3, 1, 50) &amp;#34;...&amp;#34;; print }&amp;#39;&lt;/span> &lt;span style="color:#ff5c57">FS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> &lt;span style="color:#ff5c57">OFS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> file.tsv
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ËøôËøòÊ≤°ÂÆåÔºåÂõ†‰∏∫ base64 ÁºñÁ†ÅÁöÑÁºòÊïÖÔºà‰ª•ÂèäËæìÂÖ•ÂõæÁâáÊØîËæÉÁõ∏‰ººÔºâÂØºËá¥ &lt;code>awk&lt;/code> Âà∞ÁöÑÁªìÊûúÈÉΩÊòØ‰∏ÄÊ®°‰∏ÄÊ†∑
ÁöÑËµ∑ÂßãÂ≠ó‰∏≤ÔºåËá™ÁÑ∂ËÄåÁÑ∂Â∞±ÊÄÄÁñëËøôÊòØÂ∑ßÂêàËøòÊòØÈîôËØØÔºåÊâÄ‰ª•ÔºåÂøÖÈ°ªÊü•ÁúãÂ≠ó‰∏≤Â∞æÈÉ®ÂÜÖÂÆπÊòØÂê¶‰πü‰∏ÄÊ†∑, ËøôÂΩìÁÑ∂
Èöæ‰∏çÂÄí‰∏ÄÁÇπËÑæÊ∞î‰πü‰∏ç‰ºöÊúâÁöÑ GPT4o ËÄÅÂì•Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># print the last 50 characters of the string in the 3rd column of the 10th row&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#5af78e">&amp;#39;NR==10 { len=length($3); print substr($3, len-50, 50) }&amp;#39;&lt;/span> &lt;span style="color:#ff5c57">FS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> file.tsv
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="jsonl" >
&lt;div>
&lt;a href="#jsonl">
#
&lt;/a>
JSONL
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and the field names of json object&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sed -n &lt;span style="color:#5af78e">&amp;#39;10p&amp;#39;&lt;/span> file.jsonl | jq &lt;span style="color:#5af78e">&amp;#39;keys&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and the first 10 chars of strings or 10 elements of array&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sed -n &lt;span style="color:#5af78e">&amp;#39;10p&amp;#39;&lt;/span> file.jsonl | jq &lt;span style="color:#5af78e">&amp;#39;.field_name[:10]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;p>ËØùÁó®Â§ö‰∏§Âè•Ôºö&lt;/p>
&lt;p>Ëá≥‰∫éÂèÇÊï∞‰ª£Ë°®Âï•ÊÑèÊÄùÔºåÊàëÈÄöÂ∏∏‰∏çËÆ© GPT4o Â§öË¥πÂîáËàå„ÄÇ&lt;/p>
&lt;p>ÔºàÊØïÁ´üÂ∞±ÁÆóËß£Èáä‰∫ÜÊàëËøòÊòØ‰ºöÂä®ÊâãÈ™åËØÅ‰∏ÄÁï™ÔºåÂÜçËÄÖÔºåÊúÄÂÖ≥ÈîÆÊòØÊàëËøòÊ≤°ÂºÄÈÄö plus ‰ºöÂëòÔºåÊâÄ‰ª•ÂæóÂê¨ÊüØÊôØËÖæ
‰ªñËÄÅÂ¶àÁöÑÈÇ£Âè•ÂäùÔºöË¶ÅÁúÅÁùÄÁÇπÁî®Âì¶ :Ôºâ&lt;/p></description></item><item><title>wsl2_and_unicode</title><link>/posts/cs101/wsl2_and_unicode/</link><pubDate>Tue, 16 Jul 2024 21:27:30 +0800</pubDate><guid>/posts/cs101/wsl2_and_unicode/</guid><description>&lt;h2 id="ÂÖ≥‰∫é-localeunicodejsondump-‰ª•ÂèäË¢´ÂÜ§ÊûâÁöÑ-wsl2" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e-localeunicodejsondump-%e4%bb%a5%e5%8f%8a%e8%a2%ab%e5%86%a4%e6%9e%89%e7%9a%84-wsl2">
#
&lt;/a>
ÂÖ≥‰∫é localeÔºåunicodeÔºåjson.dump() ‰ª•ÂèäË¢´ÂÜ§ÊûâÁöÑ WSL2
&lt;/div>
&lt;/h2>
&lt;p>WSL2ÁöÑ &lt;code>locale&lt;/code> ‰∏ÄÁõ¥ÊòØÈªòËÆ§ËÆæÁΩÆÂÄº &amp;ndash; &amp;ldquo;C.UTF-8&amp;rdquo;„ÄÇÁÑ∂Âêé‰ªäÂ§©Áî® python Ëß£ÊûêË∑ØÂæÑÊñá‰ª∂‰∏≠ÊñáÂêçÊó∂Ôºå
ÊúÄÁªà‰øùÂ≠ò‰∏ãÊù•ÁöÑÁ´üÊòØ unicode (&amp;quot;\u5c0f\u6728\u4ed4-\u7761\u7740\u4e862&amp;quot;)ÔºåËøôÂÄíÊòØÁ†¥Â§©ËçíÁ¨¨‰∏Ä
Ê¨°Á¢∞‰∏ä :&amp;rsquo;) ‰∏∫‰∫ÜÁúÅ‰∫ãÔºåÂøÖÈ°ªÂéªÊâæ GPT ÈòÅ‰∏ã„ÄÇ&lt;/p>
&lt;p>Êõ¥Êñ∞Ôºö‰∏ãÊñáÊòØÂÆåÂÖ®Áªï‰∫ÜËøúË∑Ø‰∫ÜÁöÑ„ÄÇÈÄ†Êàê‰∏äËø∞ÊÉÖÂÜµÁöÑÂÆûÈôÖÂéüÂõ†Â∫îËØ•ÊòØÔºö&lt;code>json.dump()&lt;/code> ÈªòËÆ§ÂèÇÊï∞‰∏≠ÁöÑ
&lt;code>ensure_ascii=True&lt;/code>ÔºåËøôÂ∞±ÂÜ≥ÂÆö‰∫ÜÂÜôÂÖ•Êñá‰ª∂ÁöÑÂ≠óÁ¨¶‰∏ç‰ºöË∂ÖÂá∫ASCIIÂ≠óÁ¨¶ÈõÜ„ÄÇ&lt;/p>
&lt;p>ÊâÄ‰ª•ÔºåÊ≠£Á°ÆÁöÑ‰øÆÊîπÊñπÂºè‰∏çÊòØÂ§ßË¥πÂë®Á´†ÁöÑÊîπÂÜôÁ≥ªÁªü &lt;code>locale&lt;/code> ËÄåÊòØÂ∞ÜÂèÇÊï∞Êîπ‰∏∫ &lt;code>ensure_ascii=False&lt;/code>„ÄÇ&lt;/p>
&lt;p>ÂÆûÈôÖ‰∏äÔºåjson.dump() ËøôÈÉ®ÂàÜÁöÑÂáΩÊï∞‰πüÊòØ GPT ÈòÅ‰∏ãÂÜôÁöÑÔºåÂÆÉÁêÜÊâÄÂΩìÁÑ∂Âú∞ËÆ§‰∏∫ÊòØÂú®Ëã±ÊñáËØ≠Â¢É‰∏≠Ôºå‰ΩøÁî®
ÈªòËÆ§ÁöÑÂèÇÊï∞‰∏ç‰∏∫ËøáÂêß„ÄÇüòÇ&lt;/p>
&lt;p>‰ΩÜ GPT4o ÊúÄÂÖàË¶ÅÊàëÁ°ÆËÆ§ÊòØÂê¶ÊòØÁªàÁ´ØÁöÑÁºñÁ†ÅÈóÆÈ¢òÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ËøôÂ∞±ÊòØÂéüÂéüÊú¨Êú¨ copy ÂæóÂà∞ÁöÑunicodeÂ≠óÁ¨¶‰∏≤&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">echo&lt;/span> -e &lt;span style="color:#5af78e">&amp;#34;\u5c0f\u6728\u4ed4-\u7761\u7740\u4e862&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># output: Â∞èÊú®‰ªî-Áù°ÁùÄ‰∫Ü2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÂÖ∂Ê¨°Ôºå‰ΩøÁî® &lt;code>file -bi&lt;/code> ÂëΩ‰ª§Êü•ÁúãÊñá‰ª∂ÁöÑÁºñÁ†ÅÊ†ºÂºèÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>file -bi path/to/the/suspicious_file.jsonl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#output: application/json; charset=us-ascii&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ËøôÂ∞±ÊÑèÂë≥ÁùÄÁ®ãÂ∫èÂú®ÂÜôÂÖ•Êú¨Âú∞Êñá‰ª∂Êó∂ÔºåÂè™‰ΩøÁî®‰∫Ü ascii Â≠óÁ¨¶„ÄÇ&lt;/p>
&lt;h2 id="ÂÖ≥‰∫é-escape-sequences-ÂèäÂÖ∂Ëß£Êûê" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e-escape-sequences-%e5%8f%8a%e5%85%b6%e8%a7%a3%e6%9e%90">
#
&lt;/a>
ÂÖ≥‰∫é escape sequences ÂèäÂÖ∂Ëß£Êûê
&lt;/div>
&lt;/h2>
&lt;p>ÁÑ∂Âêé GPT4o ÂæàËØ¶ÁªÜÁöÑÂëäÁü•Â∫îËØ•Â¶Ç‰ΩïÂú®ÂëΩ‰ª§Ë°åÁªàÁ´ØÁîüÊàêÂíåËÆæÁΩÆ &amp;ldquo;zh_CN.UTF-8&amp;rdquo;Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÂú® python ‰∏≠ËøõË°å
ËΩ¨‰πâÁ≥ªÂàóÁöÑÈáçÁºñÁ†ÅÂíåËß£Á†ÅÊù•ÊòæÁ§∫‰∏≠ÊñáÂ≠ó‰∏≤Ôºå‰ΩÜÂ∞ùËØï‰πãÂêéÊ≤°ÊàêÂäü„ÄÇÂéüÊù•ÊòØÊàëÁêÜËß£ÊúâËØØ„ÄÇ&lt;/p>
&lt;p>GPT4o:
It appears there may be a misunderstanding regarding the purpose and usage of the
unicode-escape encoding. The unicode-escape codec is used to interpret escape sequences
in strings, but it does not process normal Unicode escape sequences (\uXXXX) that are
already present in a Unicode string.&lt;/p>
&lt;p>If you have a string that looks like a Unicode escape sequence and you need to convert
it into actual Unicode characters, you can use the unicode-escape codec. However, your
string needs to be encoded in a way that the codec understands.&lt;/p>
&lt;p>Here&amp;rsquo;s the corrected approach to convert a string with Unicode escape sequences into
actual Chinese characters:&lt;/p>
&lt;pre>&lt;code>Given String: You have a string with Unicode escape sequences.
Decode: Use unicode-escape to decode the string.
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Given string with Unicode escape sequences&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>unicode_str &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u5c0f&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u6728&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u4ed4-&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u7761&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u7740&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u4e862&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Decode the Unicode escape sequences&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>decoded_str &lt;span style="color:#ff6ac1">=&lt;/span> unicode_str&lt;span style="color:#ff6ac1">.&lt;/span>encode(&lt;span style="color:#5af78e">&amp;#39;utf-8&amp;#39;&lt;/span>)&lt;span style="color:#ff6ac1">.&lt;/span>decode(&lt;span style="color:#5af78e">&amp;#39;unicode-escape&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(decoded_str)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Explanation&lt;/p>
&lt;pre>&lt;code>The string unicode_str contains Unicode escape sequences.
The encode('utf-8') part ensures the string is in a byte format.
The decode('unicode-escape') part interprets the byte format as Unicode escape sequences and converts them into actual characters.
&lt;/code>&lt;/pre>
&lt;p>This should correctly output the Chinese characters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>Â∞èÊú®‰ªî-Áù°ÁùÄ‰∫Ü2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note&lt;/p>
&lt;p>Ensure that your string uses double backslashes (\uXXXX) if you&amp;rsquo;re defining it directly
in the script. This is necessary because a single backslash (\uXXXX) would be
interpreted as an actual Unicode character during the string definition phase, not as an
escape sequence.&lt;/p>
&lt;hr>
&lt;p>‰∏ÄÁÇπÁâ¢È™öÔºöÂèàÂ≠¶‰∫Ü‰∏Ä‰∫õÊ≤°Áî®ÁöÑÁü•ËØÜÔºåÊØïÁ´üÔºöÈ¶ñÂÖà‰Ω†‰∏∫‰ªÄ‰πàË¶ÅÁî®WSLËøôÁ†¥ÁÉÇÁé©ÊÑèÂÑøÂë¢ÔºüÔºüÔºü&lt;/p>
&lt;p>&lt;del>Âõ†‰∏∫ËøôÊú¨Êù•ÊòØ‰π∞ÁªôÈùûÁªàÁ´ØÁà±Â•ΩËÄÖ‰∫∫Â£´‰ΩøÁî®ÁöÑÁîµËÑë&lt;/del>&lt;/p>
&lt;p>&lt;del>ÂèØ‰∫∫ÂÆ∂ÊØïÁ´ü‰πüÊ≤°ÊúâÁî®ÔºåËøòÊòØ‰Ω†Ëá™‰∏™Èú∏Âç†ÁùÄ„ÄÇ
Ââç‰∏ç‰πÖÂèëÁé∞Â∑®Á°¨ÂèàÂú®Á≥ªÁªüÊòéÁÅ´Êâß‰ªóÂú∞ÊêûÂÅ∑Á™•Ê¥ªÂä®ÔºàËÆæÁΩÆ&amp;gt;ÈöêÁßÅÂíåÂÆâÂÖ®ÊÄß&amp;gt;Ê¥ªÂä®ÂéÜÂè≤ËÆ∞ÂΩïÔºâÂ∞±‰ø°Ë™ìÊó¶Êó¶ËØ¥
Á´ãÂàªÈ©¨‰∏äÂàª‰∏çÂÆπÁºìÂ∫üÊéâËøô‰∏™ÊÅ∂ÂøÉÁé©ÊÑè&lt;/del>&lt;/p>
&lt;p>&lt;del>ÊúÄËøëÊØîËæÉÂøôÂìîÂì©ÔºåÊñ∞ÂÆâË£ÖÁöÑ2TÁõò‰∏çÂ•ΩÊ∏ÖÁêÜÂêßÂï¶ÔºåÊÄªËßâÁùÄ‰∫∫ÂÆ∂ÊúÄÁªàË¶ÅÁî®ÂíïÂôú&lt;/del>&lt;/p>
&lt;p>Êú¨Ë¥®‰∏äÊòØÂõ†‰∏∫ÊçèÁùÄÈºªÂ≠êËøòËÉΩÂãâÂº∫Áî®Áî®Ôºå‰ΩÜ‰º∞ËÆ°‰πüÊíë‰∏ç‰∫ÜÂ§ö‰πÖ‰∫Ü„ÄÇ&lt;/p>
&lt;p>ÊØïÁ´ü Linux ÊâçÊòØ&lt;ruby>Ê†π&lt;rt>root&lt;/rt>&lt;/ruby>ÔºåÊâçÊúâ&lt;ruby>ÂÆ∂&lt;rt>home/&lt;/rt>&lt;/ruby>ÁöÑÊÑüËßâ :)&lt;/p>
&lt;p>Êõ¥Êñ∞ÔºöËôΩÁÑ∂ÂØºËá¥Êú¨ÊñáËØûÁîüÁöÑÂéüÂõ†ÂèëÁîü‰∫ÜÊ†πÊú¨ÊÄßÊîπÂèòÔºàËØ¶ËßÅÊñáÁ´†ÂºÄÂ§¥ÁöÑ‚ÄúÊõ¥Êñ∞‚ÄùÂÜÖÂÆπÔºâ‰ΩÜ‰∏çÂΩ±Âìç‚Äú‰∏ÄÁÇπÁâ¢
È™ö‚Äù‰πãÂêéÂíåÊú¨Êõ¥Êñ∞‰πãÂâçÁöÑÁâ¢È™öÂÜÖÂÆπ„ÄÇ&lt;/p></description></item><item><title>poor_dad</title><link>/posts/personals/momentsoflife/poor_dad/</link><pubDate>Mon, 15 Jul 2024 21:47:50 +0800</pubDate><guid>/posts/personals/momentsoflife/poor_dad/</guid><description>&lt;p>Âú∞ÈìÅÂè£Âá∫Âè£Â§Ñ„ÄÇÁ´ãÁùÄÁöÑË°åÊùéÁÆ±‰∏äÂÑøÂ≠êÂçäË∑™ÁùÄÂÄæÂêëÁà∏Áà∏ÔºåÁà∏Áà∏‰∏ÄÂè™ÊâãÊäìÁùÄË°åÊùéÁÆ±ÊùÜÂ≠êÔºåÂè¶‰∏ÄÂè™ÊâãÊãøÁùÄÊâãÊú∫„ÄÇ
Â•≥ÂÑøÂú®ÊóÅËæπ‰πüÊÉ≥ÂæÄË°åÊùéÁÆ±‰∏äÊå§ÔºåË¢´Áà∏Áà∏Âà∂Ê≠¢„ÄÇÂ•≥ÂÑøÂºÄÂßãÊúâÂì≠ËÖîÔºåÁà∏Áà∏Ê≤°ÁúãÂ•πÔºåÊ≥®ÊÑèÂäõÂÖ®Âú®ÊâãÊú∫‰∏äÔºå
Â§ßÁ∫¶ÊòØÂú®ÊâìËΩ¶„ÄÅÁúãÂú∞Âõæ„ÄÅËÅîÁ≥ª‰∫∫ÊàñËÄÖÁé©Ê∏∏ÊàèÔºå‰∏çÁü•ÈÅì„ÄÇÂ•≥ÂÑø‰∏âÁï™‰∏§Ê¨°ÔºåËøòÊòØ‰∏çË°åÔºåË¢´Áà∏Áà∏Â§ßÂ£∞ÂëµÊñ•Âç±Èô©„ÄÇ
ÂêéÈù¢Â•πÂπ≤ËÑÜÂæÄÂú∞‰∏äÂ∞±ÊòØ‰∏ÄÂùêÔºåËµñÂì≠ÁùÄ‰π±Ë∏¢ÂèåËÖø„ÄÇËµñÂì≠Â£∞Èáå‰º†ËææÁöÑÂ§ßÁ∫¶ÊòØÂßîÂ±àÁöÑ‰ø°Âè∑ÔºàÂâç‰∏ç‰πÖÊàëÂàöÈ¢ÜÊïô
ËøáÊàëÂ§ßËàÖÂì•‰ªñÈÇ£Â∑Æ‰∏çÂ§öÂπ¥Á∫™Â§ßÁöÑÂπºÁ®öÂõ≠Â•≥ÂÑøÁöÑËµñÂú∞ÂºèÂì≠ÈóπÔºåÊâÄ‰ª•ÂØπËøô‰∏™‰ø°Âè∑È¢á‰∏∫ÁÜüÊÇâÔºâ„ÄÇ&lt;/p>
&lt;p>Á≠âÊàëËµ∞‰∏ãÂú∞ÈìÅÂè£ÔºåÂßîÂ±à‰ø°Âè∑Â∑≤ÁªèÂèòÊàêÂ§ö‰∏™ÂùáÂåÄÈó¥ÈöîÁöÑÊ±ÇÂÖ≥Ê≥®‰ø°Âè∑‰∫Ü„ÄÇ&lt;/p>
&lt;hr>
&lt;p>‰∏ÄÁÇπÁâ¢È™öÔºö
ÂêéÊù•ÊàëÂ∞±ËøõÈó∏Âè£‰∫Ü„ÄÇÂ∞±ÁÆóÊúâÔºåÊàë‰πü‰∏çÂèØËÉΩËæ®Âà´Âá∫ÂõûÂ∫î‰ø°Âè∑‰∫ÜÔºõÂ∞±ÁÆóÊúâÔºåÊàñËÆ∏ÂõûÂ∫îÁöÑ‰πüÂ§™Ëøü‰∫ÜÁÇπ„ÄÇ&lt;/p>
&lt;p>ËøôÂèØËÉΩÊòØÂ∞è‰∫ã‰∏ÄÊ°©Ôºå‰πüÂèØËÉΩÊòØÂ§ß‰∫ã‰∏Ä‰ª∂„ÄÇÂõ†‰∏∫ÊàëÊÄªÂøç‰∏ç‰ΩèÊÉ≥Ëµ∑Ââç‰∏ç‰πÖÁúãÁöÑÈÇ£ÈÉ®ÂØºÊºîÊú¨Êù•ÊÉ≥ÊãçËÄÅÁà∏ÁªìÊûú
Êãç‰∫Ü‰ªñÂÆ∂ÈæôÂá§ÂèåËÉûËÉéÁöÑÁ∫™ÂΩïÁâá„ÄäÂßêÂßê„Äã(1996Âπ¥)„ÄÇ&lt;/p></description></item><item><title>python_module</title><link>/posts/cs101/python_module/</link><pubDate>Wed, 10 Jul 2024 16:19:58 +0800</pubDate><guid>/posts/cs101/python_module/</guid><description>&lt;h2 id="layout-of-python-installations" >
&lt;div>
&lt;a href="#layout-of-python-installations">
#
&lt;/a>
Layout of Python installations
&lt;/div>
&lt;/h2>
&lt;p>All paths in the table are relative to the installation root:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Files&lt;/th>
&lt;th>Windows&lt;/th>
&lt;th>Linux and macOS&lt;/th>
&lt;th>Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Interpreter&lt;/td>
&lt;td>python.exe&lt;/td>
&lt;td>bin/python3.x&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Standard library&lt;/td>
&lt;td>Lib and DLLs&lt;/td>
&lt;td>lib/python3.x&lt;/td>
&lt;td>Extension modules are located under DLLs on Windows. Fedora places the standard library under lib64 instead of lib.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Third-party packages&lt;/td>
&lt;td>Lib\site-packages&lt;/td>
&lt;td>lib/python3.x/site-packages&lt;/td>
&lt;td>Debian and Ubuntu put packages in dist-packages. Fedora places extension modules under lib64 instead of lib.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Entry-point scriptsa&lt;/td>
&lt;td>Scripts&lt;/td>
&lt;td>bin&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="the-interpreter" >
&lt;div>
&lt;a href="#the-interpreter">
#
&lt;/a>
The interpreter
&lt;/div>
&lt;/h2>
&lt;p>The Python interpreter ties the environment to three things:&lt;/p>
&lt;ul>
&lt;li>A specific version of the Python language (e.g., 2.x.y, 3.x.y)&lt;/li>
&lt;li>A specific implementation of Python (e.g., CPython, PyPy)&lt;/li>
&lt;li>A specific build of the interpreter (e.g., 32bit, 64bit, Intel or Apple)&lt;/li>
&lt;/ul>
&lt;p>Try this command (in Bash) to print metadata complied into the interpreter:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>python3 -m sysconfig
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="the-modules" >
&lt;div>
&lt;a href="#the-modules">
#
&lt;/a>
The modules
&lt;/div>
&lt;/h2>
&lt;p>Modules are containers of Python objects that you load via the &lt;code>import&lt;/code> statement.
Modules come in various forms and shapes:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Simple modules
In the simplest case, a &lt;code>module&lt;/code> is a single file containing Python source code. The
statement &lt;code>import string&lt;/code> executes the code in &lt;em>string.py&lt;/em> and binds the result to
the name &lt;code>string&lt;/code> in the local scope.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Packages
Directories with &lt;em>&lt;strong>init&lt;/strong>.py&lt;/em> files are known as &lt;em>packages&lt;/em> which allow you to
organize modules in a hierarchy. The statement &lt;code>import email.message&lt;/code> loads the
&lt;code>message&lt;/code> module from the &lt;code>email&lt;/code> package.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Namespace packages
Directories with modules but no &lt;em>&lt;strong>init&lt;/strong>.py&lt;/em> are known as &lt;em>namespace packages&lt;/em>. One
may use them to organize modules in a common namespace such as a company name (say
&lt;code>dsta.voicedsp&lt;/code> and &lt;code>dsta.vedioprepr&lt;/code>). Unlike with regular packages, one can
distribute each module in a namespace package separately.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Extension modules
Extension modules such as &lt;code>math&lt;/code> module, contain native code compiled from low-level
language like C. They are shared libraries with a special entry-point&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> that lets
you import them as modules from Python. People write them for performance reasons or
to make existing C libraries available as Python modules. Their names end in &lt;code>.pyd&lt;/code>
on Windows, &lt;code>.dylib&lt;/code> on macOS, and &lt;code>.so&lt;/code> on Linux.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Built-in modules
Some modules from the standard library, such as &lt;code>sys&lt;/code> and &lt;code>builtins&lt;/code> modules, are
compiled into the interpreter. The variable &lt;code>sys.builtin_module_names&lt;/code> lists all of
these modules.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Frozen modules
Some modules from the standard library are written in Python but have their
bytecode&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> embedding in the interpreter. Recent versions of Python freeze every
module that&amp;rsquo;s imported during interpreter startup, such as &lt;code>os&lt;/code> and &lt;code>io&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="python-virtual-environment" >
&lt;div>
&lt;a href="#python-virtual-environment">
#
&lt;/a>
Python virtual environment
&lt;/div>
&lt;/h2>
&lt;p>Python environments consist of an interpreter and modules. Virtual environments share
the interpreter and the standard library with their parent environment.&lt;/p>
&lt;p>&lt;img alt="py-venv" src="https://fgg100y.github.io/posts/cs101/python_module/images/py_python_and_venv.png">&lt;/p>
&lt;p>A Python environment can contain only a single version of each third-party package &amp;ndash;
if two projects require different versions of the same package, they can‚Äôt be
installed side by side. That‚Äôs why it‚Äôs considered good practice to install every Python
application, and every project you work on, in a dedicated virtual environment.&lt;/p>
&lt;p>NOTE
&lt;span class="custom-en">
The term &lt;em>package&lt;/em> carries some ambiguity in the Python world. It refers both to modules
and to the artifacts used for distributing modules (aka &lt;em>distributions&lt;/em>).
&lt;/span>&lt;/p>
&lt;h2 id="the-module-path" >
&lt;div>
&lt;a href="#the-module-path">
#
&lt;/a>
The module path
&lt;/div>
&lt;/h2>
&lt;p>It&amp;rsquo;s helpful to look at entries of &lt;code>sys.path&lt;/code> to debugg the import error, naturally,
one may wonder where do the entries on &lt;code>sys.path&lt;/code> come from at first?&lt;/p>
&lt;p>When the interpreter starts up, it constructs the module path in two steps:&lt;/p>
&lt;ol>
&lt;li>It builds an initial module path which includes the standard library,&lt;/li>
&lt;li>It imports the &lt;code>site&lt;/code> module (from standard library) which extends the module path to
include the site packages from the current environment.&lt;/li>
&lt;/ol>
&lt;p>The locations on the initial module path fall into three categories, and they occur in
this order:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The current directory or the directory containing the Python scripts (if any)
The first item on &lt;code>sys.path&lt;/code> can be any of the following:&lt;/p>
&lt;ul>
&lt;li>If you ran &lt;code>python3 &amp;lt;script&amp;gt;&lt;/code>, the directory where the script is located&lt;/li>
&lt;li>If you ran &lt;code>python3 -m &amp;lt;module&amp;gt;&lt;/code>, the current directory&lt;/li>
&lt;li>Otherwise, the empty string, which also denotes the current directory
&lt;strong>Safty issue&lt;/strong>: having the working directory on &lt;code>sys.path&lt;/code> is quite unsafe, as an
attacker (or you, mistakenly) can override the standard library by placing Python
files in the victim&amp;rsquo;s directory. To avoid this,&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>Python-3.11 provides &lt;code>-P&lt;/code> option or the PYTHONSAFEPATH variable to omit the
current directory from &lt;code>sys.path&lt;/code>;&lt;/li>
&lt;li>Using a virtual environment.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>The locations in the PYTHONPATH environment variable (if set)
Avoid this mechanism for the same reasons as the current working directory and use a
virtual environment instead.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The locations of the standard library
The location of the standard library is not hardcoded in the interpreter. Rather,
Python looks for landmark files on the path to its own executable and uses them to
locate the current environment (&lt;code>sys.prefix&lt;/code>) and the Python installation
(&lt;code>sys.base_prefix&lt;/code>). One such landmark file is &lt;em>pyvenv.cfg&lt;/em>, which marks a virtual
environment and points to its parent installation via the &lt;code>home&lt;/code> key. Another
landmark file is &lt;em>os.py&lt;/em>, the file containing the standard &lt;code>os&lt;/code> module: Python uses
&lt;em>os.py&lt;/em> to discover the prefix outside a virtual environment and to locate the
standard library itself.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>For more glory details and interesting contents, go to the newest version (2024) of
&lt;em>&amp;ldquo;Hypermodern Python Tooling&amp;rdquo;&lt;/em> by Claudio Jolowicz.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>An entry-point script is an executable file in &lt;em>Scripts/&lt;/em> (Windows) or &lt;em>bin/&lt;/em>
(Linux and macOS) with a single purpose: it launches a Python application by
importing the module with its entry-point function and calling that function.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Bytecode is an intermediate representation of Python code that is
platform-independent and optimized for fast execution. The interpreter compiles pure
Python modules to bytecode when it loads them for the first time. Their names end in
&lt;code>.pyc&lt;/code> and they are cached in &lt;code>__pycache__&lt;/code> directories.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Add_and_test_custom_font</title><link>/posts/hugo101/2024-07-09-test_custom_font/</link><pubDate>Tue, 09 Jul 2024 21:53:56 +0800</pubDate><guid>/posts/hugo101/2024-07-09-test_custom_font/</guid><description>&lt;p>Linear regression:&lt;/p>
&lt;blockquote>
&lt;p>$\text{You&amp;rsquo;ve got to draw the line somewhere.}$&lt;/p>
&lt;/blockquote>
&lt;p>You&amp;rsquo;ve got to draw the line somewhere. (system-font)&lt;/p>
&lt;p>&lt;span class="custom-en">You&amp;rsquo;ve got to draw the line somewhere. (NerdFont 3270)&lt;/span>&lt;/p>
&lt;p>&lt;span class="custom-zh">‰∏≠ÊñáÔºö‰Ω†ÂøÖÈ°ªÂú®Êüê‰∏™Âú∞ÊñπÊääÈÇ£Êù°Á∫øÁªôÁîªÂá∫Êù•„ÄÇ(‰∫¨ËèØËÄÅÂÆã‰Ωì)&lt;/span>&lt;/p>
&lt;hr>
&lt;p>ÂÆû‰ΩúËøáÁ®ãÔºö&lt;/p>
&lt;p>&lt;span class="custom-en">hugo version:
v0.128.2-de36c1a95d28595d8243fd8b891665b069ed0850+extended linux/amd64
BuildDate=2024-07-04T08:13:25Z VendorInfo=gohugoio&lt;/span>&lt;/p>
&lt;hr>
&lt;ol>
&lt;li>
&lt;p>Â∞ÜÊú¨Âú∞Â≠ó‰ΩìÊñá‰ª∂Â§çÂà∂Âà∞ &lt;code>static/fonts/&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Âú® &lt;code>assets/css/&lt;/code> (Êàñ &lt;code>static/css/&lt;/code>) ÂàõÂª∫Êñá‰ª∂ &lt;code>custom.css&lt;/code>:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-css" data-lang="css">&lt;span style="display:flex;">&lt;span>@&lt;span style="color:#ff6ac1">font-face&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#5af78e">&amp;#39;3270NerdFont&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">src&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">url&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;/fonts/3270NerdFont-Regular.ttf&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#ff6ac1">format&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;truetype&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-weight&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-style&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>@&lt;span style="color:#ff6ac1">font-face&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#5af78e">&amp;#39;KingHwa_OldSong&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">src&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">url&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;/fonts/KingHwa_OldSong.ttf&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#ff6ac1">format&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;truetype&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-weight&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-style&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.&lt;span style="color:#f3f99d">custom-en&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;3270NerdFont&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.&lt;span style="color:#f3f99d">custom-zh&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;KingHwa_OldSong&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Êõ¥Â§öÂÖ≥‰∫éËá™ÂÆö‰πâÂ≠ó‰ΩìÁöÑËÆ®ËÆ∫ÔºåÂèØÂèÇËÄÉËøôÈáå: &lt;a href="https://discourse.gohugo.io/t/how-to-add-custom-fonts-and-not-depend-on-google/34267/3">https://discourse.gohugo.io/t/how-to-add-custom-fonts-and-not-depend-on-google/34267/3&lt;/a>&lt;/p>
&lt;ol start="3">
&lt;li>‰øÆÊîπ &lt;code>config.toml&lt;/code>Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[params]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> customCSS = [&lt;span style="color:#5af78e">&amp;#34;css/custom.css&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="4">
&lt;li>Â§çÂà∂ &lt;code>themes/layouts/partials/head.html&lt;/code> Âà∞Ê†πÁõÆÂΩï&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>cp themes/layouts/partials/head.html ./layouts/partials/head.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Optional:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp themes/layouts/partials/head-extra.html ./layouts/partials/head-extra.html
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Â¶ÇÊûú‰∏ªÈ¢òËÆæËÆ°ÂæóË∂≥Â§üÂ•ΩÔºåÈÄöÂ∏∏‰ºöÁªô‰∏Ä‰ªΩ &lt;code>head-extra.html&lt;/code> Áî®‰∫éÂÆöÂà∂ÂåñÂÜÖÂÆπ„ÄÇ
Ê≠§Êó∂ÔºåÂè™ÈúÄË¶ÅÂ∞ÜÁõ∏Â∫îÈÖçÁΩÆÂä†ÂÖ•Âà∞ &lt;code>head-extra.html&lt;/code> Êñá‰ª∂‰∏≠ÔºåÈÅøÂÖçÂØπ &lt;code>head.html&lt;/code> ÁöÑÊâìÊâ∞„ÄÇ&lt;/p>
&lt;p>Â¢ûÂä†ÁöÑÁõ∏Â∫îÂÜÖÂÆπÊòØÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;lt;!-- Other head content --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{- range .Site.Params.customCSS }}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;&lt;span style="color:#ff6ac1">link&lt;/span> &lt;span style="color:#57c7ff">rel&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;stylesheet&amp;#34;&lt;/span> &lt;span style="color:#57c7ff">href&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;{{ . | relURL }}&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{- end }}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="5">
&lt;li>‰ΩøÁî®Ëá™ÂÆö‰πâÂ≠ó‰Ωì&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>You&amp;#39;ve got to draw the line somewhere. (system-font)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;span class=&amp;#34;custom-en&amp;#34;&amp;gt;You&amp;#39;ve got to draw the line somewhere. (NerdFont 3270)&amp;lt;/span&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;span class=&amp;#34;custom-zh&amp;#34;&amp;gt;‰∏≠ÊñáÔºö‰Ω†ÂøÖÈ°ªÂú®Êüê‰∏™Âú∞ÊñπÊääÈÇ£Êù°Á∫øÁªôÁîªÂá∫Êù•„ÄÇ(‰∫¨ËèØËÄÅÂÆã‰Ωì)&amp;lt;/span&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>rime_trime</title><link>/posts/personals/momentsoflife/rime_trime/</link><pubDate>Sat, 06 Jul 2024 15:34:59 +0800</pubDate><guid>/posts/personals/momentsoflife/rime_trime/</guid><description>&lt;p>&lt;a href="https://github.com/iDvel/rime-ice">rime-ice&lt;/a> ÈõæÂááÊãºÈü≥ + &lt;a href="https://github.com/osfans/trime">trime&lt;/a> ÂêåÊñáËæìÂÖ•Ê≥ï&lt;/p>
&lt;p>&lt;img alt="IMG_trime_a_grimpse" src="https://fgg100y.github.io/posts/personals/momentsoflife/rime_trime/images/trime_theme_4in1.png">&lt;/p>
&lt;p>ÊÄé‰πàÂú®ÊâãÊú∫‰∏ä‰ΩøÁî®Ëøô‰∏™ËæìÂÖ•Ê≥ïÔºüÊØîËæÉÊó†ËÑëÁúÅÂäõÈÇ£Áßç&amp;hellip;
Á≠âÁ≠âÂÖàÔºåÈô§ÂºÄ‰ªÄ‰πà‚ÄúÊÑ§‰∏ñÂ´â‰øó‚ÄùËøôÁßçÂÜ†ÂÜïÂ†ÇÁöáÔºåÊàñËÄÖ‚ÄúÊãíÁªùÈõ∑Âêå‚ÄùËøôÁßçÁ¨ëÊéâÂ§ßÁâôÔºå‰ª•Âèä‚ÄúÂê¨ËØ¥ÂÆÉÂæàÈöæÊêûÊàëÈùûË¶ÅÊêûÂÆöÂÆÉ‚ÄùËøôÁßçÂÅèÊâßÁãÇÁöÑÁêÜÁî±Â§ñÔºå‰∏∫‰ªÄ‰πàÁî®Ëøô‰∏™Ôºü
üëâ ÂºÄÊ∫ê‰∏áÂ≤Å„ÄÅÂºÄÊ∫ê‰∏áÂ≤Å„ÄÅÂºÄÊ∫ê‰∏áÂ≤Å„ÄÇ&lt;/p>
&lt;p>Â∫îËØ•ËØ¥ÊòéÔºöËøôÁØáÊñáÁ´†Âü∫Êú¨ÊòØÊàëËá™Â∑±ÁöÑÂ§áÂøòÂΩïÔºåÊàëËá™Â∑±Â∑≤Áªè‰ΩøÁî®Ëøô‰∫õÈÖçÁΩÆÂ§öÂπ¥„ÄÇ
Â¶ÇÊûú‰Ω†ÊòØÁ¨¨‰∏ÄÊ¨°Êé•Ëß¶ rime/TrimeÔºåÁõ¥Êé•Êü•Áúã‰ªñ‰ª¨ÂÆòÁΩëÊàñËÄÖÊú¨ÊñáÁöÑÂèÇËÄÉÈìæÊé•ÔºåÂê¶ÂàôÂ¶ÇÊûúÊåâÁùÄÊàëÁöÑËØ¥Ê≥ïËøõË°åÊìç‰ΩúÔºåÂ§ßÊ¶ÇÁéá‰Ω†‰ºöÊêûÂæóËá™Â∑±ÁÅ´ÂÜí‰∏â‰∏à„ÄÇ&lt;/p>
&lt;hr>
&lt;p>ÊàëÁöÑÊìç‰ΩúÔºö&lt;/p>
&lt;p>1.1 ÊâãÊú∫‰∏ä‰∏ãËΩΩ &lt;a href="https://f-droid.org/">F-Droid&lt;/a>ÔºåÂú® F-Droid ÊêúÁ¥¢Âπ∂‰∏ãËΩΩ Trime ËæìÂÖ•Ê≥ï (‰∏≠ÊñáÂêçÔºöÂêåÊñáËæìÂÖ•Ê≥ï&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>)&lt;/p>
&lt;p>1.2 ÊâãÊú∫Â∫îÁî®ÊéàÊùÉÁ≠âÂÖ∂‰ªñÊìç‰Ωú&lt;/p>
&lt;p>2.1 ÁîµËÑëÁ´Ø‰∏ãËΩΩ‰ªìÂ∫ì&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">cd&lt;/span> rime_hub
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>git clone https://github.com/iDvel/rime-ice.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>2.2 Â∞ÜÊâãÊú∫Êé•ÂÖ•ÁîµËÑëÔºåÂ∞Ü‰ªìÂ∫ìÂ§çÂà∂Âà∞ÊâãÊú∫ÁõÆÂΩïË∑ØÂæÑÔºàÈÄöÂ∏∏ÊòØ‰∏éDownload,DocumentsÂêåÁ∫ßÁöÑË∑ØÂæÑÔºâÔºåÂ∞Ü
ÂÖ∂ÂêçÁß∞‰øÆÊîπ‰∏∫ &lt;code>rime&lt;/code> (rime-ice‰πüÂèØ‰ª•Ôºå‰ΩÜË¶ÅÂÅöÂá∫Áõ∏Â∫îÁöÑÈÄÇÈÖçÂíåËÆæÁΩÆ)&lt;/p>
&lt;p>3.1 ÊâãÊú∫Á´ØÊâìÂºÄ Trime ËæìÂÖ•Ê≥ïÔºåÁÇπÂáªÂè≥‰∏äËßíÈÉ®ÁΩ≤&lt;/p>
&lt;p>3.2 ÂèØËÉΩ‰ºöÊä•ÈîôÂºπÂá∫Êó•ÂøóÁ™óÂè£Ôºå‰ΩÜ‰∏ç‰∏ÄÂÆöÂΩ±Âìç‰ΩøÁî®ÔºõÂê¶ÂàôÔºåÂéªÂÆòÁΩëÁ≠âÂú∞ÊñπÂØªÊâæËß£ÂÜ≥ÊñπÊ°à&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>4.1 Â¶ÇÊûú‰πüÂú®ÁîµËÑëÁ´Ø‰ΩøÁî®ÔºåÂèØ‰ª•ÂÄüÂä© Plum ËøõË°åÂÆâË£ÖÂíåÊõ¥Êñ∞Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>git clone --depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span> https://github.com/rime/plum
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">cd&lt;/span> plum
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#ÂÖ®ÈáèÊõ¥Êñ∞Ôºö&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#Â±ÄÈÉ®Êõ¥Êñ∞Ôºö&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/all_dicts
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>5 Â§öÁ´ØÂêåÊ≠•ÔºöÁîµËÑëÁ´ØÊõ¥Êñ∞ÔºåÂ§çÂà∂Âà∞ÊâãÊú∫Á≠âÂÖ∂‰ªñÁªàÁ´Ø„ÄÇ&lt;/p>
&lt;hr>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>ÂêåÊñáËæìÂÖ•Ê≥ïÔºöÊää RIME Ë£ÖËøõ Android ÊâãÊú∫ &lt;a href="https://sspai.com/post/77499">https://sspai.com/post/77499&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Rime ÈÖçÁΩÆÔºöÈõæÂááÊãºÈü≥ &lt;a href="https://dvel.me/posts/rime-ice/">https://dvel.me/posts/rime-ice/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>ËÅåÁß∞ËØÑÂÆ°</title><link>/posts/personals/engineering_career_ladder/</link><pubDate>Fri, 05 Jul 2024 22:40:23 +0800</pubDate><guid>/posts/personals/engineering_career_ladder/</guid><description>&lt;p>ËôΩÁÑ∂‰∫∫ÂÆ∂Âè™ËØÑ‰º∞Ëøë‰∫îÂπ¥ÁöÑÂ∑•‰ΩúÔºå‰ΩÜÁ≤óÁï•ÁÆóËµ∑Êù•Ëá™Â∑±Â∑•‰ΩúÈÉΩÂø´ÂÖ´Âπ¥‰∫ÜÔºåÊâçÁà¨‰∫ÜÁ¨¨‰∫å‰∏™Èò∂Ê¢Ø„ÄÇ&lt;/p>
&lt;p>ÂçäË∑ØÂá∫ÂÆ∂ÁùÄÂÆûË¥πÂä≤„ÄÇ&lt;/p>
&lt;p>&lt;img alt="MidLevel" src="https://fgg100y.github.io/posts/personals/engineering_career_ladder/images/fmh_engineering_career_blur_extra_info.png">&lt;/p>
&lt;p>Á™ÅÁÑ∂ÊÉ≥Ëµ∑ÁªèÂÖ∏ÁîµÂΩ±„Ää‰∏âÊØõ‰ªéÂÜõËÆ∞„ÄãÁªìÂ∞æÈÉ®ÂàÜÁöÑÈ•±Âê´Ê≤ßÊ°ëËÄåÂèàËé´ÂêçÂñúÊÑüÁöÑÈÇ£‰∏ÄÂè•Âè∞ËØçÔºöÂÖ´Âπ¥‰∫Ü&amp;hellip;&lt;/p>
&lt;p>ÊÄ•Ë∫ÅÊó∂‰∏çÂÖçÊöóÊöóÂ•¢ÊúõÊúâ‰∫∫Ôºö‚ÄúÁúãÂú®ÊäÄÊúØÁöÑ‰ªΩ‰∏äÔºåÊãâÂÖÑÂºüÊàë‰∏ÄÊää„ÄÇ‚Äù üòÇ&lt;/p>
&lt;p>‰ΩÜËØùËØ¥ÂõûÊù•Ôºå‚ÄúÂΩìÂπ∏Á¶èÊù•Êï≤Èó®‚ÄùÁöÑÊó∂ÂÄôÔºå‰Ω†ÊúÄÂ•ΩÊòØÊÅ∞Âú®ÂÆ∂ÔºåÂÅöÂ•ΩÂáÜÂ§á„ÄÇ&lt;/p>
&lt;p>Ë¶Å‰∏çÁÑ∂ÁúüÊúâ‰∫∫Êãâ‰Ω†‰∏ÄÊääÊó∂ÔºåÁªìÊûúÂç¥ÂºÑÊàêÁÉÇÊ≥•Êâ∂‰∏ç‰∏äÂ¢ô„ÄÇÊÅêÊÄïÊ≤°ÊúâÊØîËøôÊõ¥Á≥üÁ≥ïÁöÑ‰∫Ü„ÄÇ&lt;/p></description></item><item><title>so_long_and_no_thanks</title><link>/posts/personals/momentsoflife/so_long/</link><pubDate>Thu, 04 Jul 2024 20:03:19 +0800</pubDate><guid>/posts/personals/momentsoflife/so_long/</guid><description>&lt;p>Â¶ÇÊûú‰Ω†ÁúãËøá„ÄäÈì∂Ê≤≥Á≥ªÊº´Ê∏∏ÊåáÂçó„ÄãÔºåÂ∫îËØ•ËÆ∞ÂæóÂú∞ÁêÉÁîüÁâ©Êô∫ÊÖßÊéíÁ¨¨‰∏ÄÁöÑÊòØÊµ∑Ë±öÔºà‰∫∫Á±ªÂè™ÊéíÁ¨¨‰∏âÔºâüòÇ&lt;/p>
&lt;p>Á¶ªÂà´Êó∂Êú¨Êù•Â∫îËØ•ÊòØÊù•‰∏ÄÂè•Ôºö&lt;/p>
&lt;blockquote>
&lt;p>‚ÄúSo long! And thanks for all the fish.&amp;quot;&lt;/p>
&lt;/blockquote>
&lt;p>‰ΩÜÂÆûÈôÖÊÉÖÂÜµÂç¥ÊòØÔºö&lt;/p>
&lt;p>&lt;img alt="no_thanks" src="https://fgg100y.github.io/posts/personals/momentsoflife/so_long/images/dophine.jpg">&lt;/p></description></item><item><title>Short_time_Fourier_Transform</title><link>/posts/dsp101/2024-07-03-short_time_fourier_transform/</link><pubDate>Wed, 03 Jul 2024 11:25:37 +0800</pubDate><guid>/posts/dsp101/2024-07-03-short_time_fourier_transform/</guid><description>&lt;p>Sine Wave Signal&lt;/p>
&lt;p>An audio signal, y(t), composed of exactly one sine wave, can be completely described by
the parameters $t, A, f$ and $\phi$,
$$
y(t) = A \sin(2 \pi f t + \phi)
$$
where $t$ represents time in seconds, $A$ is the wave&amp;rsquo;s amplitude (unit-less), $f$ is
its frequency in Hz, and $\phi$ is its phase offset in radians (i.e., where in the cycle
the wave is at $t=0$). If $t \ne 0$, then the sine wave appears shifted in time by
$\frac{\phi}{2 \pi f}$, where negative values mean &amp;ldquo;delay&amp;rdquo; and positive &amp;ldquo;advance&amp;rdquo; it.&lt;/p>
&lt;p>Fourier Series&lt;/p>
&lt;blockquote>
&lt;p>Our old pal Fourier told us that any sound can be represented as an infinite summation
of sine waves each with their own amplitudes, frequencies, and phase offsets. This means
that any sound we hear can be represented as many, many tuples of $t, A, f, \phi$.&lt;/p>
&lt;/blockquote>
&lt;p>Time-Frequency representation&lt;/p>
&lt;p>A Time-Frequency representation is a 2 dimensional matrix that represents the frequency
contents of an audio signal over time.&lt;/p>
&lt;p>We can visualize a TF Representation using a heatmap, which has time along the x-axis
and frequency along the y-axis. Each &lt;em>TF bin&lt;/em> (entry in heatmap) in the heatmap
represents the &lt;strong>amplitude&lt;/strong> of the signal at that particular time and frequency.
If there is no color bar, it is usually safe to assume that brighter colors indicate
higher amplitudes than darker colors.&lt;/p>
&lt;p>&lt;img alt="TFrepr" src="images/dsp_tf_representation.png">&lt;/p>
&lt;p>Short-time Fourier Transform (STFT)&lt;/p>
&lt;p>An STFT is calculated from a waveform representation by computing a discrete Fourier
transform (DFT) of a small, moving window across the duration of the window. The
location of each entry in an STFT determines its time (x-axis) and frequency (y-axis).
The absolute value of a TF bin |$X(t,f)$| at time t and frequency f determines the amount
of energy heard from frequency $f$ at time $t$.&lt;/p>
&lt;p>Importantly, each bin in our STFT is complex, meaning each entry contains both a
magnitude component and a phase component. Both components are needed to convert an STFT
matrix back to a waveform by &lt;em>inverse STFT&lt;/em> so that we may hear it.&lt;/p>
&lt;p>&lt;img alt="STFT" src="images/dsp_stft_process.png">&lt;/p>
&lt;p>Window Types&lt;/p>
&lt;p>The window type determines the shape of the short-time window that will segment the
audio into short segments before applying the DFT. The shape of this window will affect
which frequencies get emphasized or attenuated in the DFT. There are many types of
&lt;a href="https://docs.scipy.org/doc/scipy/reference/signal.windows.html">window functions&lt;/a>.&lt;/p>
&lt;p>&lt;img alt="windows" src="images/dsp_window_types.png">&lt;/p></description></item><item><title>Discrete_Fourier_Transform</title><link>/posts/dsp101/2024-07-03-discrete_fourier_transform/</link><pubDate>Wed, 03 Jul 2024 09:46:45 +0800</pubDate><guid>/posts/dsp101/2024-07-03-discrete_fourier_transform/</guid><description>&lt;p>An analysis problem, which is equivalent to the &lt;strong>DFT&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>Given a signal, how to find the amplitude and phase offset of its frequency
components?
&lt;/code>&lt;/pre>
&lt;p>A synthesis problem, which is equivalent to the &lt;strong>inverse DFT&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>Given a set of frequency components and their amplitudes, how can we construct a
signal?
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>DFT&lt;/strong> and &lt;strong>Inverse DFT&lt;/strong> in code&lt;/p>
&lt;p>The &lt;code>analyze()&lt;/code> computes something very close to the DFT, with one difference: The
conventional definition of DFT does not divide by &lt;code>N&lt;/code> (highlighted line below):&lt;/p>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">analyze&lt;/span>(ys):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> amps &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>conj()&lt;span style="color:#ff6ac1">.&lt;/span>transpose()&lt;span style="color:#ff6ac1">.&lt;/span>dot(ys) &lt;span style="color:#ff6ac1">/&lt;/span> N
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> amps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">dft&lt;/span>(ys): &lt;span style="color:#78787e"># return same result as np.fft.fft()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> amps &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>conj()&lt;span style="color:#ff6ac1">.&lt;/span>transpose()&lt;span style="color:#ff6ac1">.&lt;/span>dot(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> amps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">idft&lt;/span>(amps): &lt;span style="color:#78787e"># inverse DFT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(amps)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ys &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>dot(amps) &lt;span style="color:#ff6ac1">/&lt;/span> N
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> ys &lt;span style="color:#78787e"># ys: value of the signal at each time step&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">synthesis_matrix&lt;/span>(N):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ts &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(N) &lt;span style="color:#ff6ac1">/&lt;/span> N &lt;span style="color:#78787e"># ts: the sequence of times where the signal should be evaluated&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fs &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(N) &lt;span style="color:#78787e"># fs: the list of frequencies&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>outer(ts, fs) &lt;span style="color:#78787e"># args: outer product of ts and fs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>exp(&lt;span style="color:#ff9f43">1&lt;/span>j &lt;span style="color:#ff6ac1">*&lt;/span> PI2 &lt;span style="color:#ff6ac1">*&lt;/span> args) &lt;span style="color:#78787e"># PI2 = np.pi * 2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>ResNet</title><link>/posts/resnet/</link><pubDate>Sat, 29 Jun 2024 09:17:15 +0800</pubDate><guid>/posts/resnet/</guid><description>&lt;p>ÊÄùÁª¥ÂÆûÈ™åÔºö
Âú®ÊµÖÂ±ÇÁΩëÁªúÁªìÊûÑÁöÑÂü∫Á°Ä‰∏äÔºàÊØîÂ¶Ç20Â±ÇÔºâÔºåÂæÄÂêéÈù¢Áõ¥Êé•Ê∑ªÂä†Êõ¥Â§öÁöÑÂêåÊò†Â∞ÑÈöêÂ±ÇÔºàidentity layersÔºâÔºå
ÂæóÂà∞ÁöÑÊ∑±Â±ÇÁΩëÁªúÔºàÊØîÂ¶Ç50Â±ÇÔºâÁêÜËÆ∫‰∏äÊïàÊûúÂ∫îËØ•‰∏ç‰ºöÂèòÂ∑Æ„ÄÇ‰ΩÜÂÆûÈ™åÁªìÊûúËØ¥ÊòéÔºåÂÆÉÁúü‰ºöÂèòÂ∑Æ„ÄÇÊÑèÂë≥ÁùÄÔºö
SGDÁÆóÊ≥ïÊó†Ê≥ïÊâæÂà∞‰ΩøÂæóÊõ¥Ê∑±Â±ÇÁΩëÁªúÊÄßËÉΩ‰∏çÂèòÂ∑ÆÁöÑÂèÇÊï∞„ÄÇ&lt;/p>
&lt;p>ÊÆãÂ∑ÆÁΩëÁªúÊû∂ÊûÑÂèØ‰ª•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;h2 id="why-what-and-how" >
&lt;div>
&lt;a href="#why-what-and-how">
#
&lt;/a>
Why, What, and How
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>Deeper neural networks are more difficult to train. We present a residual learning
framework to ease the training of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learning residual functions with
reference to the layer inputs, instead of learning unreferenced functions.&lt;/p>
&lt;/blockquote>
&lt;p>ËÆ≠ÁªÉ‰∏Ä‰∏™Ê∑±Â∫¶ÔºàË∂≥Â§üÊ∑±ÁöÑÔºâÁ•ûÁªèÁΩëÁªúÊòØ‰∏Ä‰ª∂ÂæàÈöæÁöÑ‰∫ãÊÉÖÔºà2015Âπ¥Ôºâ„ÄÇ
‰ΩøÁî®‚ÄúÊÆãÂ∑Æ‚ÄùÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÂèØ‰ª•Êõ¥ÂÆπÊòìÂú∞ËÆ≠ÁªÉË∂≥Â§üÊ∑±ÁöÑÁ•ûÁªèÁΩëÁªú„ÄÇ
‚ÄúÊÆãÂ∑Æ‚ÄùÊû∂ÊûÑÂ∞±ÊòØÊääËøô‰∫õ‰∏≠Èó¥Â±Ç‰Ωú‰∏∫‰∏Ä‰∏™Â≠¶‰π†ËæìÂÖ•‰∏éËæìÂá∫ÁöÑÊÆãÂ∑ÆÁöÑÂáΩÊï∞„ÄÇ&lt;/p>
&lt;p>&lt;img alt="ResNet Block" src="images/resnet_learning_block.png">&lt;/p>
&lt;p>Â∞±ÊòØËØ¥ÔºöÂ¢ûÂä†ÁöÑÈöêÂ±ÇÂéªÂ≠¶‰π† $h(x) - x$ÔºàÊÆãÂ∑ÆÔºâËÄå‰∏çÊòØ $h(x)$ÔºåËÄåËæìÂá∫ÁöÑÊòØÂΩìÂâçÈöêÂ±ÇÁöÑËæìÂá∫Âä†‰∏äÂâç
‰∏ÄÂ±ÇÁöÑËæìÂá∫ $x$ ÔºàÂêåÊó∂‰πüÊòØÂΩìÂâçÂ±ÇÁöÑËæìÂÖ•Ôºâ„ÄÇ&lt;/p>
&lt;h2 id="ÊÆãÂ∑ÆÁΩëÁªúÁªìÊûÑ" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84">
#
&lt;/a>
ÊÆãÂ∑ÆÁΩëÁªúÁªìÊûÑ
&lt;/div>
&lt;/h2>
&lt;p>ÊÆãÂ∑ÆËøûÊé•Ôºàaka, shortcut connectionÔºâÂú®ËæìÂÖ•ÂíåËæìÂá∫ÁöÑÁª¥Â∫¶Áõ∏ÂêåÊÉÖÂÜµ‰∏ãÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®Â¶Ç‰∏ãÁªìÊûÑÂùóÔºö&lt;/p>
&lt;p>$$
y = F(x, {W_i}) + x.
$$&lt;/p>
&lt;p>ÂΩìÊÉ≥Ë¶ÅÂ¢ûÂä†Áª¥Â∫¶Êó∂ÔºåÊúâ‰∏§ÁßçÊñπÂºèÊù•ÂØπÈΩêËæìÂÖ•ÂíåËæìÂá∫ÁöÑÁª¥Â∫¶Ôºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÊÆãÂ∑ÆËøûÊé•ËøõË°åÂêåÊò†Â∞ÑÊìç‰ΩúÔºåÁÑ∂ÂêéÂØπÂ¢ûÂä†ÁöÑÁª¥Â∫¶ËøõË°åË°•Èõ∂Êìç‰ΩúÔºàpadding zeros)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊÆãÂ∑ÆËøûÊé•ËøõË°åÁ∫øÊÄßÊäïÂΩ±Êìç‰Ωú $y = F(x, {W_i}) + W_s x.$ Êù•ÂØπÈΩêÁª¥Â∫¶Ôºà‰ΩøÁî® 1x1 Âç∑ÁßØÂÆûÁé∞Ôºâ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>‰∏§ÁßçÊÉÖÂÜµ‰∏ãÔºåÂΩìÊÆãÂ∑ÆËøûÊé•ÁöÑÊòØ‰∏çÂêåÂ∞∫ÂØ∏ÁöÑÁâπÂæÅÂõæÔºàfeature mapÔºâÊó∂Ôºå‰ΩøÁî®Ê≠•ÂπÖ‰∏∫2ÁöÑÂç∑ÁßØÊìç‰Ωú„ÄÇ
ÔºàÈÄöÈÅìÊï∞ÁøªÂÄçÔºåÂàôÈ´òÂÆΩÂáèÂçäÔºåÂõ†Ê≠§‰ΩøÁî®Ê≠•ÂπÖ=2Ôºàstride=2ÔºâÁöÑÂç∑ÁßØÊù•‰øùËØÅÁª¥Êï∞ÂØπÈΩêÔºâ&lt;/p>
&lt;p>&lt;img alt="bottlenet-block" src="https://fgg100y.github.io/posts/resnet/images/resnet_bottlenet_building_block.png">&lt;/p>
&lt;p>&lt;img alt="resnet-arch" src="https://fgg100y.github.io/posts/resnet/images/resnet_arch_for_imagenet.png">&lt;/p>
&lt;h2 id="ÊÆãÂ∑ÆËøûÊé•‰∏∫‰ªÄ‰πàÊúâÁî®" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e7%94%a8">
#
&lt;/a>
ÊÆãÂ∑ÆËøûÊé•‰∏∫‰ªÄ‰πàÊúâÁî®Ôºü
&lt;/div>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Easier Gradient Flow&lt;/p>
&lt;ul>
&lt;li>ÁºìËß£Ê¢ØÂ∫¶Ê∂àÂ§±/Ê¢ØÂ∫¶ÁàÜÁÇ∏ÈóÆÈ¢ò&lt;/li>
&lt;li>ÊÆãÂ∑ÆËøûÊé•‰ΩøÂæóÊ¢ØÂ∫¶Êõ¥Á®≥ÂÆö&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Êõ¥ÂÆπÊòìÂ≠¶‰π†ÂêåÊò†Â∞ÑÂáΩÊï∞&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊèêÂçáÊúÄ‰ºòÂåñÂíåÊî∂ÊïõÊïàÊûú&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Êõ¥Â•ΩÁöÑÁâπÂæÅ‰º†Êí≠Ôºàfeature propagationÔºâ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="Ê≠§ÊÆãÂ∑ÆÈùûÂΩºÊÆãÂ∑Æresnet-vs-gdbt" >
&lt;div>
&lt;a href="#%e6%ad%a4%e6%ae%8b%e5%b7%ae%e9%9d%9e%e5%bd%bc%e6%ae%8b%e5%b7%aeresnet-vs-gdbt">
#
&lt;/a>
Ê≠§ÊÆãÂ∑ÆÈùûÂΩºÊÆãÂ∑ÆÔºàResNet VS GDBTÔºâ
&lt;/div>
&lt;/h2>
&lt;p>GBDTÔºàÊ¢ØÂ∫¶ÊèêÂçáÂÜ≥Á≠ñÊ†ëÔºâ‰∏≠ÁöÑÊÆãÂ∑ÆÔºàresidualÔºâÊòØÊåáÈ¢ÑÊµãÂÄº‰∏éÁúüÂÆûÂÄº‰πãÈó¥ÁöÑÂ∑ÆÂÄº„ÄÇ&lt;/p>
&lt;p>ResNet ‰∏≠ÁöÑÊÆãÂ∑ÆÔºàresidualÔºâÔºåÊòØÁ•ûÁªèÁΩëÁªúÂ∞ùËØïÂéªÂ≠¶‰π†ÁâπÂæÅÂõæÔºàfeature mapÔºâÁöÑÊÆãÂ∑ÆÔºàÊòØÁΩëÁªúÂùóËæìÂá∫‰∏éËæìÂÖ•‰πãÈó¥ÁöÑÂ∑ÆÂÄºÔºâ„ÄÇ&lt;/p>
&lt;p>Paper: &lt;a href="https://arxiv.org/pdf/1512.03385">https://arxiv.org/pdf/1512.03385&lt;/a>&lt;/p></description></item><item><title>knowledge_distillation2</title><link>/posts/2024-06-28-knowledge_distillation2/</link><pubDate>Fri, 28 Jun 2024 11:25:43 +0800</pubDate><guid>/posts/2024-06-28-knowledge_distillation2/</guid><description>&lt;h2 id="generalized-knowledge-distillation-gkd" >
&lt;div>
&lt;a href="#generalized-knowledge-distillation-gkd">
#
&lt;/a>
Generalized Knowledge Distillation (GKD)
&lt;/div>
&lt;/h2>
&lt;p>Ê≥õÂåñÁü•ËØÜËí∏È¶èÊòØ‰∏ÄÁßçÊîπËøõÁöÑÁü•ËØÜËí∏È¶èÊäÄÊúØÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüÁü•ËØÜËí∏È¶èÊñπÊ≥ïÂú®Ëá™ÂõûÂΩíÂ∫èÂàóÊ®°Âûã‰∏≠ÈÅáÂà∞ÁöÑÂàÜ
Â∏É‰∏çÂåπÈÖçÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®ËÆ≠ÁªÉÂíåÊé®ÁêÜÈò∂ÊÆµ‰πãÈó¥„ÄÇ‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÈÄöÂ∏∏Âü∫‰∫éÂõ∫ÂÆöÁöÑËæìÂá∫Â∫èÂàóÈõÜËøõË°åÔºå
Ëøô‰∫õÂ∫èÂàóÊàñËÄÖÊòØÊïôÂ∏àÊ®°ÂûãÁîüÊàêÁöÑÔºåÊàñËÄÖÊòØÂü∫‰∫éÁúüÂÆûÊï∞ÊçÆÁöÑÊ†áÁ≠æ„ÄÇÁÑ∂ËÄåÔºåËøôÂØºËá¥Â≠¶ÁîüÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ÁîüÊàê
ÁöÑÂ∫èÂàóÂèØËÉΩ‰∏éËÆ≠ÁªÉÊó∂ËßÅÂà∞ÁöÑÂ∫èÂàóÂàÜÂ∏É‰∏çÂêåÔºå‰ªéËÄåÂΩ±Âìç‰∫ÜÂ≠¶ÁîüÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;p>GKD ‰∏çÂÜçÂ±ÄÈôê‰∫éÂõ∫ÂÆöËæìÂá∫Â∫èÂàóÁöÑËÆ≠ÁªÉÔºåËÄåÊòØÂÖÅËÆ∏Â≠¶ÁîüÊ®°ÂûãÂú®ÂÖ∂Ëá™ÊàëÁîüÊàêÁöÑÂ∫èÂàó‰∏äËøõË°åÂ≠¶‰π†ÔºåÂêåÊó∂Âà©Áî®
ÊïôÂ∏àÊ®°ÂûãÊèê‰æõÁöÑÂèçÈ¶à„ÄÇ&lt;/p>
&lt;h2 id="gkd-ÊäÄÊúØË¶ÅÁÇπ" >
&lt;div>
&lt;a href="#gkd-%e6%8a%80%e6%9c%af%e8%a6%81%e7%82%b9">
#
&lt;/a>
GKD ÊäÄÊúØË¶ÅÁÇπ
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Âä®ÊÄÅÂ∫èÂàóÁîüÊàê‰∏éÂèçÈ¶àÔºöGKD Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰ºöÊ†πÊçÆÂ≠¶ÁîüÊ®°ÂûãÁöÑËá™ÊàëÁîüÊàêÂ∫èÂàóËøõË°åÂ≠¶‰π†„ÄÇËøôÊÑèÂë≥ÁùÄÂ≠¶
ÁîüÊ®°ÂûãÂú®Â∞ùËØïÁîüÊàêÂ∫èÂàóÊó∂ÔºåÂ¶ÇÊûúÁîüÊàêÈîôËØØÊàñ‰∏çÁêÜÊÉ≥ÁöÑÂ∫èÂàóÔºåÂÆÉÂèØ‰ª•‰ªéÊïôÂ∏àÊ®°ÂûãÈÇ£ÈáåËé∑ÂæóÁõ¥Êé•ÂèçÈ¶àÔºå
Â≠¶‰π†Â¶Ç‰ΩïÁ∫†Ê≠£Ëøô‰∫õÈîôËØØ„ÄÇËøôÁßçÊú∫Âà∂ËÉΩÂ§üÂáèÂ∞ëËÆ≠ÁªÉÂíåÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÊèêÈ´òÂ≠¶ÁîüÊ®°ÂûãÂØπÂêÑÁßçËæì
ÂÖ•Â∫èÂàóÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÁÅµÊ¥ªÁöÑÊçüÂ§±ÂáΩÊï∞ÈÄâÊã©Ôºö‰∏é‰ªÖÈááÁî®ÂâçÂêëKLÊï£Â∫¶ÁöÑ‰º†ÁªüÁõëÁù£ÂºèÁü•ËØÜËí∏È¶è‰∏çÂêåÔºåGKD Êèê‰æõ‰∫ÜÁÅµÊ¥ªÊÄßÔºåÂèØ‰ª•
Âú®Â≠¶ÁîüÂíåÊïôÂ∏àÊ®°Âûã‰πãÈó¥‰ΩøÁî®Êõø‰ª£ÁöÑÊçüÂ§±ÂáΩÊï∞„ÄÇÂΩìÂ≠¶ÁîüÊ®°ÂûãÈöæ‰ª•Á≤æÁ°ÆÊ®°ÊãüÊïôÂ∏àÊ®°ÂûãÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÊó∂ÔºåËøô
‰∏ÄÁÇπÂ∞§‰∏∫ÈáçË¶Å„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•ÈááÁî®Jensen-ShannonÊï£Â∫¶Á≠âÊåáÊ†áÊù•Ë°°ÈáèÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåËøôÊúâÂä©‰∫éÂ≠¶Áîü
Ê®°ÂûãÂú®Ë°®ËææÂäõÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÊõ¥Â•ΩÂú∞Â≠¶‰π†„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÈõÜÊàêÂº∫ÂåñÂ≠¶‰π†Á≤æË∞ÉÔºöGKD Ëøò‰øÉËøõ‰∫ÜÁü•ËØÜËí∏È¶è‰∏éÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËØ≠Ë®ÄÊ®°ÂûãÂæÆË∞ÉÁöÑÊó†ÁºùÊï¥Âêà„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="Âä®ÊÄÅÂ∫èÂàóÁîüÊàê" >
&lt;div>
&lt;a href="#%e5%8a%a8%e6%80%81%e5%ba%8f%e5%88%97%e7%94%9f%e6%88%90">
##
&lt;/a>
Âä®ÊÄÅÂ∫èÂàóÁîüÊàê
&lt;/div>
&lt;/h3>
&lt;p>Âú®‰º†ÁªüÁöÑÁü•ËØÜËí∏È¶è‰∏≠ÔºåÂ≠¶ÁîüÊ®°ÂûãÈÄöÂ∏∏Âü∫‰∫éÈ¢ÑÂÖàÂÆö‰πâÂ•ΩÁöÑÂõ∫ÂÆöÊï∞ÊçÆÈõÜÊàñÁî±ÊïôÂ∏àÊ®°ÂûãÁîüÊàêÁöÑËæìÂá∫Â∫èÂàóËøõË°å
ËÆ≠ÁªÉ„ÄÇËÄåÂä®ÊÄÅÂ∫èÂàóÁîüÊàêÂàôÊâìÁ†¥‰∫ÜËøô‰∏ÄÈôêÂà∂ÔºåÂÆÉËÆ©Â≠¶ÁîüÊ®°ÂûãËá™Â∑±ÁîüÊàêËæìÂá∫Â∫èÂàóÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äËøõË°åÂ≠¶‰π†„ÄÇ
Ëøô‰∏™ËøáÁ®ãÊ∂âÂèä‰ª•‰∏ãÂá†‰∏™Ê≠•È™§Ôºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÂàùÂßãÂåñÂ≠¶ÁîüÊ®°ÂûãÔºöÈ¶ñÂÖàÔºåÂ≠¶ÁîüÊ®°ÂûãÈúÄË¶ÅÂÖ∑Â§á‰∏ÄÂÆöÁöÑÁîüÊàêËÉΩÂäõÔºåÈÄöÂ∏∏ÈÄöËøáÂàùÊ≠•ÁöÑÁõëÁù£ÂºèÈ¢ÑËÆ≠ÁªÉÊàñÂæÆË∞É
ÔºàSupervised Fine-Tuning, SFTÔºâÊù•ÂÆûÁé∞ÔºåÁ°Æ‰øùÂÆÉÂèØ‰ª•ÁîüÊàêË¥®ÈáèÂ∞öÂèØÁöÑÂ∫èÂàó„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ëá™ÊàëÁîüÊàêÂ∫èÂàóÔºöÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂ≠¶ÁîüÊ®°ÂûãÊé•Êî∂Âà∞ËæìÂÖ•ÂêéÔºå‰ºöÊ†πÊçÆÂΩìÂâçÁöÑÁ≠ñÁï•ÁîüÊàêËæìÂá∫Â∫èÂàó„ÄÇËøô‰∫õÂ∫è
ÂàóÂèçÊò†‰∫ÜÂ≠¶ÁîüÊ®°ÂûãÂΩìÂâçÁöÑËÉΩÂäõÂíåÁêÜËß£ÔºåÂèØËÉΩÂåÖÂê´ÈîôËØØÊàñ‰∏çÂÆåÁæéÁöÑÈÉ®ÂàÜ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="ÂèçÈ¶àÊú∫Âà∂" >
&lt;div>
&lt;a href="#%e5%8f%8d%e9%a6%88%e6%9c%ba%e5%88%b6">
##
&lt;/a>
ÂèçÈ¶àÊú∫Âà∂
&lt;/div>
&lt;/h3>
&lt;p>‰∏ÄÊó¶Â≠¶ÁîüÊ®°ÂûãÁîüÊàê‰∫ÜËæìÂá∫Â∫èÂàóÔºåÊé•‰∏ãÊù•ÁöÑÂÖ≥ÈîÆÊ≠•È™§ÊòØËé∑ÂèñÊïôÂ∏àÊ®°ÂûãÁöÑÂèçÈ¶àÔºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÊïôÂ∏àÊ®°ÂûãËØÑ‰º∞ÔºöÊïôÂ∏àÊ®°Âûã‰ºöÂØπÂ≠¶ÁîüÁîüÊàêÁöÑÂ∫èÂàóËøõË°åËØÑ‰º∞ÔºåÈÄöÂ∏∏ÈÄöËøáÊØîËæÉÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÂú®ÊØè‰∏™ÁîüÊàê
Ê≠•È™§‰∏äÂØπ‰∏ã‰∏Ä‰∏™ËØçÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇËøôÊ∂âÂèäÂà∞ËÆ°ÁÆó‰∏§ËÄÖÁöÑÊ¶ÇÁéáÂàÜÂ∏É‰πãÈó¥ÁöÑÊüêÁßçÂ∑ÆÂºÇÂ∫¶ÈáèÔºåÂ¶ÇKLÊï£Â∫¶„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊåáÂØºÊÄßÂ≠¶‰π†ÔºöÊ†πÊçÆÊïôÂ∏àÊ®°ÂûãÁöÑÂèçÈ¶àÔºåÂ≠¶ÁîüÊ®°ÂûãË∞ÉÊï¥ÂÖ∂ÂèÇÊï∞ÔºåÁõÆÁöÑÊòØ‰ΩøËá™Â∑±Âú®ÁîüÊàêÁ±ª‰ººÂ∫èÂàóÊó∂ËÉΩÊõ¥Êé•
ËøëÊïôÂ∏àÊ®°ÂûãÁöÑË°®Áé∞„ÄÇGKD ÂÖÅËÆ∏ÈÄâÊã©‰∏çÂêåÁöÑÂ∑ÆÂºÇÂ∫¶ÈáèÔºàdivergenceÔºâÊù•‰ºòÂåñÔºå‰ª•ÈÄÇÂ∫î‰∏çÂêå‰ªªÂä°ÁöÑÈúÄÊ±Ç„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ëø≠‰ª£‰ºòÂåñÔºöÈöèÁùÄÂ≠¶ÁîüÊ®°Âûã‰∏çÊñ≠‰ªéËá™ÊàëÁîüÊàêÁöÑÈîôËØØ‰∏≠Â≠¶‰π†ÔºåÂÆÉÁöÑÁîüÊàêËÉΩÂäõÈÄêÊ≠•ÊèêÂçáÔºåÁîüÊàêÁöÑÂ∫èÂàóË¥®Èáè
‰πüÈöè‰πãÊèêÈ´òÔºåÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™Ê≠£ÂêëÂæ™ÁéØ„ÄÇÊ≠§ËøáÁ®ãÂèØ‰ª•ËßÜ‰∏∫‰∏ÄÁßçÊ®°‰ªøÂ≠¶‰π†ÔºàImitation LearningÔºâ‰∏éÂº∫Âåñ
Â≠¶‰π†ÔºàReinforcement LearningÔºâÁöÑÁªìÂêàÔºåÂÖ∂‰∏≠Â≠¶ÁîüÊ®°ÂûãÈÄöËøáËø≠‰ª£Âú∞Êé¢Á¥¢Âíå‰øÆÊ≠£ÂÖ∂Ë°å‰∏∫Ôºå‰ª•ÈÄºËøëÊïô
Â∏àÊ®°ÂûãÁöÑË°®Áé∞„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ÂèÇËÄÉÊñáÁåÆÔºöhttps://arxiv.org/pdf/2306.13649&lt;/p></description></item><item><title>knowledge_distillation</title><link>/posts/2024-06-27-knowledge_distillation/</link><pubDate>Thu, 27 Jun 2024 11:34:18 +0800</pubDate><guid>/posts/2024-06-27-knowledge_distillation/</guid><description>&lt;p>Áü•ËØÜËí∏È¶èÔºàKnowledge DistillationÔºâÊòØ‰∏ÄÁßçÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÔºåÂÆÉÈÄöËøáÂ∞ÜÂ§ßÂûã„ÄÅÂ§çÊùÇÁöÑÊ®°ÂûãÔºàÁß∞‰∏∫ÊïôÂ∏à
Ê®°ÂûãÔºåTeacher ModelÔºâÁöÑÁü•ËØÜ‚ÄúËí∏È¶è‚ÄùÂà∞Â∞èÂûã„ÄÅÁÆÄÊ¥ÅÁöÑÊ®°ÂûãÔºàÁß∞‰∏∫Â≠¶ÁîüÊ®°ÂûãÔºåStudent ModelÔºâ‰∏≠Ôºå‰ªé
ËÄåÂÆûÁé∞Ê®°ÂûãÂéãÁº©ÂíåÂä†ÈÄüÔºåÂêåÊó∂Â∞ΩÂèØËÉΩ‰øùÊåÅÂéüÂßãÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËøô‰∏ÄÊäÄÊúØ‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•Âú®ËµÑÊ∫êÊúâÈôêÁöÑËÆæÂ§á
‰∏äÈ´òÊïàËøêË°åÔºåÂ¶ÇÊâãÊú∫ÊàñÂµåÂÖ•ÂºèËÆæÂ§á„ÄÇ&lt;/p>
&lt;blockquote>
&lt;p>The method works by incorporating an additional loss into the traditional cross entropy
loss, which is based on the softmax output of the teacher network. The assumption is
that the output activations of a properly trained teacher network carry additional
information that can be leveraged by a student network during training.&lt;/p>
&lt;/blockquote>
&lt;h2 id="Âü∫Êú¨ÂéüÁêÜ" >
&lt;div>
&lt;a href="#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86">
#
&lt;/a>
Âü∫Êú¨ÂéüÁêÜ
&lt;/div>
&lt;/h2>
&lt;p>Áü•ËØÜËΩ¨ÁßªÔºöÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáËÆ©Â≠¶ÁîüÊ®°ÂûãÊ®°‰ªøÊïôÂ∏àÊ®°ÂûãÁöÑËæìÂá∫Ë°å‰∏∫Ôºå‰∏ç‰ªÖÂåÖÊã¨Á°¨ÂàÜÁ±ªÊ†áÁ≠æÔºåËøòÊúâËΩØÊ¶ÇÁéá
ÂàÜÂ∏ÉÔºàsoftmaxÊ¶ÇÁéáÔºâÔºåËøôÊ†∑ÂèØ‰ª•‰º†ÈÄíÊõ¥Â§öÂÖ≥‰∫éÊï∞ÊçÆÂàÜÂ∏ÉÁöÑ‰ø°ÊÅØ„ÄÇËΩØÊ†áÁ≠æÁõ∏ÊØîÁ°¨Ê†áÁ≠æÂê´ÊúâÊõ¥Â§öÂÖ≥‰∫éÊï∞
ÊçÆ‰∏çÁ°ÆÂÆöÊÄßÂèäÁ±ªÈó¥ÂÖ≥Á≥ªÁöÑ‰ø°ÊÅØÔºåÊúâÂä©‰∫éÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†Êõ¥ÁªÜËÖªÁöÑÂÜ≥Á≠ñËæπÁïå„ÄÇ&lt;/p>
&lt;p>ÁâπÂæÅËí∏È¶èÔºöÈô§‰∫ÜËæìÂá∫Â±ÇÁöÑÁü•ËØÜÂ§ñÔºåÊ≥õÂåñÁü•ËØÜËí∏È¶èËøòÂèØ‰ª•Ê∂âÂèä‰∏≠Èó¥Â±ÇÁâπÂæÅÁöÑÂ≠¶‰π†ÔºåÂç≥Â≠¶ÁîüÊ®°ÂûãËØïÂõæÂ≠¶‰π†
ÊïôÂ∏àÊ®°ÂûãÁöÑÈ´òÂ±ÇÁâπÂæÅË°®Á§∫„ÄÇËøôÊúâÂä©‰∫éÊèêÂçáÂ≠¶ÁîüÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂõ†‰∏∫ÂÆÉÂ≠¶‰ºö‰∫ÜÂ¶Ç‰Ωï‰ªéËæìÂÖ•Êï∞ÊçÆ‰∏≠ÊèêÂèñ
Êõ¥ÊúâÁî®ÁöÑÁâπÂæÅ„ÄÇ&lt;/p>
&lt;p>ÂÖ≥Á≥ªËí∏È¶èÔºöÂº∫Ë∞É‰øùÊåÅÊïôÂ∏àÂíåÂ≠¶ÁîüÊ®°ÂûãÂØπ‰∫éËæìÂÖ•Ê†∑Êú¨Èó¥ÂÖ≥Á≥ªÁöÑÁêÜËß£‰∏ÄËá¥ÊÄß„ÄÇËøôÊÑèÂë≥ÁùÄÂ≠¶ÁîüÊ®°Âûã‰∏ç‰ªÖË¶ÅÂ≠¶
‰ºöÂçï‰∏™Ê†∑Êú¨ÁöÑÂ§ÑÁêÜÔºåËøòË¶ÅÁêÜËß£Ê†∑Êú¨‰πãÈó¥ÁöÑÁõ∏ÂØπÂÖ≥Á≥ªÔºåËøôÂØπ‰∫é‰∏Ä‰∫õÈúÄË¶ÅÁêÜËß£Â§çÊùÇ‰∏ä‰∏ãÊñáÁöÑ‰ªªÂä°Â∞§‰∏∫ÈáçË¶Å„ÄÇ&lt;/p>
&lt;p>&lt;strong>Áü•ËØÜËí∏È¶èËøáÁ®ã&lt;/strong>Ôºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ËÆ≠ÁªÉÊïôÂ∏àÊ®°Âûã&lt;/strong>ÔºöÈ¶ñÂÖàÔºå‰ΩøÁî®Â§ßÈáèÊï∞ÊçÆÂíåËÆ°ÁÆóËµÑÊ∫êËÆ≠ÁªÉ‰∏Ä‰∏™È´òÊÄßËÉΩÁöÑÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÔºàÊïôÂ∏àÊ®°
ÂûãÔºâ„ÄÇËøô‰∏™Ê®°ÂûãÂèØËÉΩÂåÖÂê´Êï∞Áôæ‰∏áÁîöËá≥Êï∞ÂçÅ‰∫ø‰∏™ÂèÇÊï∞Ôºå‰ΩÜÂÆÉÂú®ÂàÜÁ±ª‰ªªÂä°‰∏äÁöÑË°®Áé∞ÈùûÂ∏∏Âá∫Ëâ≤„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÁîüÊàêËΩØÊ†áÁ≠æ&lt;/strong>ÔºöÊïôÂ∏àÊ®°ÂûãÂú®ÂØπËæìÂÖ•Êï∞ÊçÆËøõË°åÈ¢ÑÊµãÊó∂Ôºå‰∏ç‰ªÖ‰ªÖÁªôÂá∫ÊúÄÁªàÁöÑÂàÜÁ±ªÁªìÊûúÔºåËøò‰ºöÁªôÂá∫ÂêÑ
Á±ªÂà´ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºàÈÄöÂ∏∏ÈÄöËøásoftmaxÂ±ÇËé∑ÂæóÔºâ„ÄÇËøô‰∫õÊ¶ÇÁéáÂàÜÂ∏ÉË¢´Áß∞‰∏∫‚ÄúËΩØÊ†áÁ≠æ‚ÄùÔºåÂÆÉ‰ª¨ÂåÖÂê´‰∫ÜÈ¢ùÂ§ñÁöÑ
‰ø°ÊÅØÔºåÊØîÂ¶ÇÁ±ªÂà´ÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ËÆ≠ÁªÉÂ≠¶ÁîüÊ®°Âûã&lt;/strong>ÔºöÊé•‰∏ãÊù•Ôºå‰ΩøÁî®ÊïôÂ∏àÊ®°ÂûãÁöÑËΩØÊ†áÁ≠æÂíåÂÆûÈôÖÁöÑÁ°¨Ê†áÁ≠æÔºàÂç≥Êï∞ÊçÆÁöÑÁúüÂÆûÁ±ªÂà´ÔºâÊù•ËÆ≠
ÁªÉÂ≠¶ÁîüÊ®°Âûã„ÄÇÂ≠¶ÁîüÊ®°ÂûãÁöÑÊû∂ÊûÑËÆæËÆ°ÂæóÊõ¥ÁÆÄÂçïÔºåÂèÇÊï∞ÈáèËøúÂ∞è‰∫éÊïôÂ∏àÊ®°Âûã„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂ≠¶ÁîüÊ®°Âûã‰∏ç
‰ªÖË¶ÅÂ≠¶‰π†Ê®°‰ªøÁ°¨Ê†áÁ≠æÔºåËøòË¶ÅÈÄöËøáÊçüÂ§±ÂáΩÊï∞ÔºàÂ¶ÇKLÊï£Â∫¶Êàñ‰∫§ÂèâÁÜµÔºâÂ∞ΩÈáèÊé•ËøëÊïôÂ∏àÊ®°ÂûãÁöÑËΩØÊ†áÁ≠æËæìÂá∫„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ê∏©Â∫¶ÂèÇÊï∞Ë∞ÉÊï¥&lt;/strong>ÔºöÂú®ÁîüÊàêËΩØÊ†áÁ≠æÊó∂ÔºåÊúâÊó∂‰ºöÂºïÂÖ•‰∏Ä‰∏™‚ÄúÊ∏©Â∫¶‚ÄùÂèÇÊï∞Êù•Ë∞ÉÊï¥Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÂπ≥ÊªëÁ®ãÂ∫¶„ÄÇ
È´òÊ∏©ÂèØ‰ª•‰ΩøËΩØÊ†áÁ≠æÊõ¥Âä†Âπ≥ÊªëÔºå‰øÉËøõÂ≠¶ÁîüÊ®°ÂûãÂ≠¶‰π†Âà∞ÊïôÂ∏àÊ®°ÂûãÁöÑÂÜ≥Á≠ñËæπÁïåÔºõ‰ΩéÊ∏©Âàô‰ΩøÂæóËΩØÊ†áÁ≠æÊé•Ëøë
Á°¨Ê†áÁ≠æÔºå‰ΩÜÂèØËÉΩ‰ºö‰∏¢Â§±ÊïôÂ∏àÊ®°ÂûãÁöÑ‰∏Ä‰∫õÁªÜÂæÆÂÜ≥Á≠ñ‰ø°ÊÅØ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="Áü•ËØÜËí∏È¶èÁöÑ‰∏âÁßç‰ª£Á†ÅÂÆûÁé∞" >
&lt;div>
&lt;a href="#%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f%e7%9a%84%e4%b8%89%e7%a7%8d%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0">
#
&lt;/a>
Áü•ËØÜËí∏È¶èÁöÑ‰∏âÁßç‰ª£Á†ÅÂÆûÁé∞Ôºö
&lt;/div>
&lt;/h2>
&lt;ol>
&lt;li>Áü•ËØÜËΩ¨ÁßªÔºöÂü∫‰∫é softmax output ËΩØÊ†áÁ≠æ&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the teacher model - do not save gradients here as we do not change the teacher&amp;#39;s weights&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> teacher_logits &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#Soften the student logits by applying softmax first and log() second&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_targets &lt;span style="color:#ff6ac1">=&lt;/span> nn&lt;span style="color:#ff6ac1">.&lt;/span>functional&lt;span style="color:#ff6ac1">.&lt;/span>softmax(teacher_logits &lt;span style="color:#ff6ac1">/&lt;/span> T, dim&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_prob &lt;span style="color:#ff6ac1">=&lt;/span> nn&lt;span style="color:#ff6ac1">.&lt;/span>functional&lt;span style="color:#ff6ac1">.&lt;/span>log_softmax(student_logits &lt;span style="color:#ff6ac1">/&lt;/span> T, dim&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper &amp;#34;Distilling the knowledge in a neural network&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_targets_loss &lt;span style="color:#ff6ac1">=&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>sum(soft_targets &lt;span style="color:#ff6ac1">*&lt;/span> (soft_targets&lt;span style="color:#ff6ac1">.&lt;/span>log() &lt;span style="color:#ff6ac1">-&lt;/span> soft_prob)) &lt;span style="color:#ff6ac1">/&lt;/span> soft_prob&lt;span style="color:#ff6ac1">.&lt;/span>size()[&lt;span style="color:#ff9f43">0&lt;/span>] &lt;span style="color:#ff6ac1">*&lt;/span> (T&lt;span style="color:#ff6ac1">**&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> soft_target_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> soft_targets_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>ÁâπÂæÅËí∏È¶èÔºöÂü∫‰∫é hidden state ‰ΩôÂº¶Áõ∏‰ººÂ∫¶&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the teacher model and keep only the hidden representation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _, teacher_hidden_representation &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits, student_hidden_representation &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hidden_rep_loss &lt;span style="color:#ff6ac1">=&lt;/span> cosine_loss(student_hidden_representation, teacher_hidden_representation, target&lt;span style="color:#ff6ac1">=&lt;/span>torch&lt;span style="color:#ff6ac1">.&lt;/span>ones(inputs&lt;span style="color:#ff6ac1">.&lt;/span>size(&lt;span style="color:#ff9f43">0&lt;/span>))&lt;span style="color:#ff6ac1">.&lt;/span>to(device))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> hidden_rep_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> hidden_rep_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>ÁâπÂæÅËí∏È¶èÔºöÂü∫‰∫é Intermediate regressor MSEÊçüÂ§±&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Again ignore teacher logits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _, teacher_feature_map &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits, regressor_feature_map &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hidden_rep_loss &lt;span style="color:#ff6ac1">=&lt;/span> mse_loss(regressor_feature_map, teacher_feature_map)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> feature_map_weight &lt;span style="color:#ff6ac1">*&lt;/span> hidden_rep_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÂÖ∑‰ΩìÂèÇËÄÉÔºöhttps://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html?highlight=distill&lt;/p></description></item><item><title>mixup_beyond_ERM</title><link>/posts/2024-06-27-mixup_beyond_erm/</link><pubDate>Thu, 27 Jun 2024 09:42:34 +0800</pubDate><guid>/posts/2024-06-27-mixup_beyond_erm/</guid><description>&lt;h1 id="empirical-risk-minimazation-erm" >
&lt;div>
&lt;a href="#empirical-risk-minimazation-erm">
##
&lt;/a>
Empirical Risk Minimazation (ERM)
&lt;/div>
&lt;/h1>
&lt;p>ÁªèÈ™åÈ£éÈô©ÊúÄÂ∞èÂåñ, Empirical Risk Minimazation principle (Vapnik, 1998)&lt;/p>
&lt;ol>
&lt;li>Âü∫‰∫éERMËÆ≠ÁªÉÊ®°ÂûãÔºö‰∫¶Âç≥Âú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏äÂ≠¶‰π†‰ª•ÊúÄÂ∞èÂåñÂÖ∂Âπ≥ÂùáËØØÂ∑Æ„ÄÇ&lt;/li>
&lt;li>ÂΩìÂâçSOTAÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÈöèÁùÄËÆ≠ÁªÉÊï∞ÊçÆÈõÜËßÑÊ®°Â¢ûÂ§ßËÄåÁ∫øÊÄßÂ¢ûÂä†„ÄÇ&lt;/li>
&lt;/ol>
&lt;p>ËÄåÁªèÂÖ∏VCÂ≠¶‰π†ÁêÜËÆ∫Ôºàlearning theory, Vapnik &amp;amp; Chervonenkis, 1971ÔºâË°®ÊòéÔºöÂè™Ë¶ÅÂ≠¶‰π†Âô®ÁöÑÂèÇÊï∞Èáè
‰∏çÈöèÁùÄËÆ≠ÁªÉÊ†∑Êú¨Êï∞ÈáèÂ¢ûÂä†ÔºåÂàôÂü∫‰∫éERMÂ≠¶‰π†‰∏ÄÂÆö‰ºöÊî∂Êïõ(convergence, i.e., good generalization
to new data)„ÄÇ‰∫¶Âç≥ÔºöÊ®°ÂûãÁöÑÂ§çÊùÇÂ∫¶ÔºàÂèÇÊï∞ÈáèËßÑÊ®°ÔºâÁõ∏ÂØπ‰∫éËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°Â∫îËØ•ÊòØÂõ∫ÂÆöÁöÑÊàñËÄÖÂèòÂä®‰∏çÂ§ß„ÄÇ&lt;/p>
&lt;p>ËøôÂ∞±ÈÄ†Êàê‰∫ÜËøôÊ†∑ÁöÑÁüõÁõæÔºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ÁªèÂÖ∏Â≠¶‰π†ÁêÜËÆ∫ËÆ§‰∏∫ÔºöÊÉ≥Ë¶ÅERMÊúâÊïàÔºåÂàôÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÂ∫îËØ•‰øùÊåÅ‰∏ÄÂÆöÂ§ßÂ∞èÔºåËÄå‰∏çÊòØÈöèÁùÄËÆ≠ÁªÉÊï∞ÊçÆÈõÜËßÑÊ®°Â¢ûÂä†ËÄåÂ¢ûÂä†Ôºõ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÁÑ∂ËÄåÂú®ÂÆûÈôÖ‰ªªÂä°‰∏≠ÔºöSOTAÊ®°ÂûãÁöÑÂèÇÊï∞ÈáèÊòØ‰∏éËÆ≠ÁªÉÊï∞ÊçÆÈáè‰øùÊåÅÁ∫øÊÄßÂ¢ûÂä†ÁöÑ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ÂÆûÈôÖ‰∏äÔºå‰∏ÄÊñπÈù¢ERMÂÖÅËÆ∏Â§ßÁöÑÁ•ûÁªèÁΩëÁªúÊ®°Âûã‚ÄúËÆ∞‰Ωè‚ÄùËÆ≠ÁªÉÊ†∑Êú¨ÔºàÂç≥‰ΩøÁî®‰∫ÜÂæàÂº∫ÁöÑÊ≠£ÂàôÂåñÁ∫¶ÊùüÔºå
strong regularization)ÔºåÂè¶‰∏ÄÊñπÈù¢ERMËÆ≠ÁªÉÂæóÂà∞ÁöÑÊ®°ÂûãÂú®ËÆ≠ÁªÉÂàÜÂ∏É‰πãÂ§ñÁöÑÊ†∑Êú¨‰∏äÁöÑÈ¢ÑÊµãÁªìÊûúÂ∑ÆÂºÇÂ∑®
Â§ßÔºàÂç≥‰ΩøË¢´È¢ÑÊµãÁöÑÊ†∑Êú¨‰ªÖ‰ªÖÂèëÁîü‰∫ÜÁõ∏ÂØπÂæÆÂ∞èÁöÑÊîπÂèòÔºåadversarial examplesÔºâ„ÄÇ&lt;/p>
&lt;p>ÈÇ£‰πàÔºåÊúâÊ≤°ÊúâÂà´ÁöÑÊ®°ÂûãËÆ≠ÁªÉÊúÄ‰ºòÂåñÂáÜÂàôÂë¢Ôºü-&amp;gt; Vicinal Risk Minimization (VRM).&lt;/p>
&lt;blockquote>
&lt;p>In VRM, human knowledge is required to describe a vicinity or neighborhood around each
example in the training data. Then, additional virtual examples can be drawn from the
vicinity distribution of the training examples to enlarge the support of the training
distribution.&lt;/p>
&lt;/blockquote>
&lt;p>‰æãÂ¶ÇÔºåÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåÂõæÂÉèÂ¢ûÂº∫ÈÄöÂ∏∏ÂåÖÊã¨ËΩªÂæÆÁöÑÊóãËΩ¨„ÄÅÁøªËΩ¨„ÄÅÁº©ÊîæÁ≠âÊìç‰ΩúÔºåËøôÂÖ∂ÂÆûÊòØÂõæÂÉèÁöÑËøëÈÇª
ÈõÜÔºàvicinity setÔºâ„ÄÇËøô‰∫õÊï∞ÊçÆÂ¢ûÂº∫ÈÄöÂ∏∏ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñÊÄßËÉΩ„ÄÇ‰ΩÜËøô‰∫õÊìç‰ΩúÈÄöÂ∏∏‰πüÊòØÊï∞ÊçÆ‰æùËµñÁöÑ„ÄÇ&lt;/p>
&lt;p>Mixup Êèê‰æõ‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÊó†ÂÖ≥Ôºàdata-agnosticÔºâÁöÑÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂÆÉÊûÑÈÄ†ËôöÊãüÊ†∑Êú¨ÁöÑÊñπÂºèÂ¶Ç‰∏ãÔºö&lt;/p>
&lt;p>$$
\begin{eqnarray}
\tilde{x} &amp;amp;=&amp;amp; \lambda x_i + (1 - \lambda) x_j, \text{where } x_i, x_j \text{ are raw input vectors} \\
\tilde{y} &amp;amp;=&amp;amp; \lambda y_i + (1 - \lambda) y_j, \text{where } y_i, y_j \text{ are one-hot label encoding}
\end{eqnarray}
$$&lt;/p>
&lt;p>$(x_i, y_i)$ Âíå $(x_j, y_j)$ ÊòØ‰ªéËÆ≠ÁªÉÈõÜ‰∏≠ÈöèÊú∫ÊäΩÂèñÁöÑ‰∏§‰∏™Ê†∑‰æã„ÄÇ$\lambda \in [0, 1]$„ÄÇ&lt;/p>
&lt;p>Âõ†Ê≠§ÔºåmixupÈÄöËøáÁªìÂêàÂÖàÈ™åÁü•ËØÜÊâ©Â±ï‰∫ÜËÆ≠ÁªÉÂàÜÂ∏ÉÔºåÂç≥ÁâπÂæÅÂêëÈáèÁöÑÁ∫øÊÄßÊèíÂÄºÂ∫îÂØºËá¥Áõ∏ÂÖ≥ÁõÆÊ†áÁöÑÁ∫øÊÄßÊèíÂÄº„ÄÇ
mixupÂèØ‰ª•Áî®Âá†Ë°å‰ª£Á†ÅÂÆûÁé∞ÔºåÂπ∂‰∏îÂè™ÂºïÂÖ•ÊúÄÂ∞èÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇ&lt;/p>
&lt;p>Mixup: beyond_ERM, &lt;a href="https://arxiv.org/pdf/1710.09412">https://arxiv.org/pdf/1710.09412&lt;/a>&lt;/p></description></item><item><title>Ë∑ØÁî±Âô®wifiËøû‰∏ç‰∏ä_‰ΩÜÊâãÊú∫ÊµÅÈáèÂèØ‰ª•</title><link>/posts/2024-06-16-wifi%E8%BF%9E%E4%B8%8D%E4%B8%8A_%E4%BD%86%E6%89%8B%E6%9C%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E4%BB%A5/</link><pubDate>Sun, 16 Jun 2024 10:07:12 +0800</pubDate><guid>/posts/2024-06-16-wifi%E8%BF%9E%E4%B8%8D%E4%B8%8A_%E4%BD%86%E6%89%8B%E6%9C%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E4%BB%A5/</guid><description>&lt;p>QÔºöËØ∑ÈóÆWifiÊó†Ê≥ïÁøªÂ¢ôÊàñËÆøÈóÆÊüê‰∫õÁΩëÁ´ôÔºå‰ΩÜÁî®ÊâãÊú∫ÊµÅÈáèÂç¥ÂèØ‰ª•ÊòØ‰ªÄ‰πàÂéüÂõ†Ôºü&lt;/p>
&lt;p>A:
ÂÖàËØ¥ËØ¥‰Ω†ÁöÑwifiÂíå5gÁéØÂ¢É‰∏ãÁøªÂ¢ôÁöÑÈóÆÈ¢òÔºåÊàëÁåúÊµã‰∏ã‰Ω†ÂÆ∂ÈáåÁöÑÂÆΩÂ∏¶ÊòØ‰∏çÊòØÁßªÂä®ÁöÑÔºåÊâãÊú∫ÊµÅÈáèÁî®ÁöÑÊòØËÅîÈÄö
ÊàñËÄÖÁîµ‰ø°ÔºåÊÄª‰πãÔºåÂÆ∂Â∫≠ÂÆΩÂ∏¶Âíå5g‰∏çÊòØ‰∏ÄÂÆ∂isp„ÄÇ Â¶ÇÊûúÊòØÁöÑËØùÔºåËøô‰∏™ÈóÆÈ¢òÂ∞±ÊØîËæÉÂ•ΩÁêÜËß£Ôºå‰∏çÂêåisp‰∏çÂêå
ÁöÑÂú∞Âå∫ÈÉΩ‰ºöÊúâËá™Â∑±ÁöÑ‰∏ÄÂ•óÈªëÂêçÂçïÊú∫Âà∂„ÄÇ ÊâÄ‰ª•ÂèØËÉΩ‰Ω†ÁöÑÂÆ∂ÂÆΩispÁõëÊµã‰Ω†ÁöÑ‰∏äÁΩëÊµÅÈáèÊúâ‰∏çÊ≠£Â∏∏ÔºåÊàñËÄÖÂ§ßÊï∞
ÊçÆËÆ§‰∏∫‰Ω†ÁöÑÁøªÂ¢ôÊúçÂä°Âô®ÂæàÂèØÁñëÔºàÊú™ÂøÖÊòØ‰Ω†ÈÄ†ÊàêÁöÑÔºåÂõ†‰∏∫ÊúâÂæàÂ§ö‰∫∫Âú®Áî®ÔºâÔºå‰∫éÊòØÂºÄÂêØ‰∫ÜÂ±èËîΩÔºå‰ΩÜÊòØÂè¶‰∏Ä
ÂÆ∂ÊâãÊú∫ispÂπ∂Ê≤°ÊúâËß¶ÂèëÈªëÂêçÂçïÂà§ÂÆöÔºåÊâÄ‰ª•Êú™Â±èËîΩ„ÄÇ ÁßªÂä®ÁöÑÈªëÂêçÂçï‰∏ÄËà¨ÊòØÊúÄÊøÄËøõÁöÑÔºåÂè∑Áß∞Â¢ô‰∏≠Â¢ôÔºå‰∏çËøá
‰∏çÂêåÂú∞Âå∫ÁöÑÂêå‰∏ÄÊúçÂä°ÂïÜÊîøÁ≠ñ‰πü‰ºöÊúâÂæàÂ§ßÂ∑ÆÂà´Ôºå‰∏çÂ•ΩËØ¥Ë∞ÅÂ∞±‰∏ÄÂÆöÂ•ΩÔºåË∞ÅÂ∞±‰∏ÄÂÆöÂ∑Æ„ÄÇÊâÄ‰ª•Ëß£ÂÜ≥ÊñπÊ≥ïÂ∞±ÊòØÊç¢
‰∏™ÁøªÂ¢ôÊúçÂä°Âô®Â∞±ÂèØ‰ª•‰∫Ü„ÄÇ&lt;/p>
&lt;p>&lt;del>ÂÜçËØ¥ËØ¥Êú∫Âú∫ÈóÆÈ¢òÔºåÊú∫Âú∫Â¶ÇÊûúË¶ÅÊâæÁöÑËØùÂçÉÂçÉ‰∏áÔºå‰∏çÂ§∏Âº†ÁöÑËØ¥Â∞èÂ≠¶ÁîüÈÉΩËÉΩÂºÄÊú∫Âú∫ÔºåÊâÄ‰ª•ÔºåÂ¶ÇÊûúÁúü
Ë¶ÅÊòØÈÄâÁöÑËØùÔºåÂ¶ÇÊûú‰Ω†‰∏çÂ§™ÊáÇÊäÄÊúØÔºåÂ∞±ÈÄâÂ§ßÊú∫Âú∫Â•Ω‰∫ÜÔºåÂèØËÉΩË¥µ‰∏Ä‰∫õÔºå‰ΩÜÊòØÁ®≥ÂÆöÊÄßÊòØÊúâ‰øùÈöúÁöÑ„ÄÇ
Â¶ÇÊûúÊáÇÊäÄÊúØÔºåÂ∞±‰∏çÂ¶ÇÊú∫Âú∫ÔºàÊàñËΩ¨ÂèëÊúçÂä°Ôºâ+Ëá™Âª∫ÊúçÂä°Âô®ÁªìÂêàÁöÑÊñπÂºèÔºåËøôÊ†∑Â∞±ËÉΩÂÖºÈ°æÈÄüÂ∫¶ÂíåÈöê
ÁßÅÂÆâÂÖ®„ÄÇ Âá†‰πéÊâÄÊúâÁöÑÊú∫Âú∫ÈÉΩ‰ºöÊúâÂÆ°Êü•Êú∫Âà∂ÔºåÊúÄËµ∑Á†Å‰ªñÈúÄË¶ÅÁõëÊéßÊµÅÈáèÊù•ËÆ°Ë¥πÔºåÂè™ÊòØÂÆ°Êü•Á®ãÂ∫¶
ÂíåÈöêÁßÅÊï∞ÊçÆÂ§ÑÁêÜ‰∏•Ë∞®Â∫¶ÊúâÂå∫Âà´ËÄåÂ∑≤ÔºåÁúãÂïÜÂÆ∂ËâØÂøÉ‰∫Ü„ÄÇÊàë‰∏™‰∫∫Ê≤°Áî®ËøáÊú∫Âú∫ÔºåÂè™ËÉΩËØ¥‰∏§‰∏™ÊàëÊâÄÁü•
ÈÅìÁöÑÊØîËæÉÂ§ßÁöÑÔºåjustmysocksÔºådler cloudÔºåÈ≠ÖÂΩ±ÊûÅÈÄü„ÄÇ Âè¶Â§ñÔºåÂà´Â§™Ëø∑‰ø°ÈÇ£‰∫õÊú∫Âú∫Êé®ËçêÔºåÂü∫
Êú¨ÈÉΩÊòØÊéêÈ•≠„ÄÇ ÂØπ‰∫ÜÔºåËøòÊúâ‰∏™ÊàëËßâÂæó‰Ω†ÂèØ‰ª•ËØï‰∏ãÔºåÂè´Shiliew„ÄÇ ÁêÜÁî±Êúâ2‰∏™ÔºåËøô‰∏™appÁöÑ‰ΩúËÄÖ
Â∞±ÊòØÂºÄÊ∫êÁøªÂ¢ôËΩØ‰ª∂brookÁöÑ‰ΩúËÄÖÔºåÂú®app storeÈáåÁúãÁùÄÊùÉÈôêÂæàÂπ≤ÂáÄÔºåÂè¶Â§ñËøô‰∏™‰ΩúËÄÖÂêçÂ≠óÂè´
txthinkingÔºåÊúâÁÇπËá¥Êï¨ÁºñÁ®ãÈöèÊÉ≥ÁöÑÊÑèÊÄùÔºåÊÑüËßâ‰ªñÂ∫îËØ•ÊúÄËµ∑Á†Å‰∏çÁ∫¢Âêß„ÄÇ„ÄÇ„ÄÇÂìàÂìàÂìà ‰ΩÜÊòØËøô
‰∏úË•øÂ•ΩÁî®‰∏çÂ•ΩÁî®ÊàëÂ∞±‰∏çÁü•ÈÅì‰∫Ü„ÄÇ&lt;/del>&lt;/p>
&lt;p>ÊúÄÂêéÊàëÂÜçËØ¥‰∏ãÂÖ≥‰∫é@han_chineseÁöÑËßÇÁÇπÔºåÁßªÂä®Êï∞ÊçÆÁöÑÂÆ°Êü•ÊòØÊúÄÂº∫ÁöÑÔºåÂõ†‰∏∫ÁßªÂä®ËúÇÁ™ùÊï∞ÊçÆÊòØÈúÄË¶ÅÁ≤æÁ°ÆËÆ°
Ë¥πÁöÑÔºå‰ºóÊâÄÂë®Áü•ÔºåÊØè‰∏™ÊúçÂä°ÂïÜÈÉΩÊúâÂæàÂ§öÂÖçÊµÅÈáèÊúçÂä°ÔºåÊâÄ‰ª•ÈúÄË¶ÅËØÜÂà´ÊµÅÈáèÂÜÖÂÆπÊù•Âà§Êñ≠ËÆ°Ë¥π‰∏éÂê¶„ÄÇÊõ¥ÁîöÁöÑ
ÊòØÔºåÁßªÂä®Êï∞ÊçÆÊµÅÈáèÈÉΩ‰ºöË¢´Ëá™Âä®Ê†áËÆ∞‰∏äÊú¨Êú∫ÁîµËØùÂè∑Á†ÅÔºå‰ªñ‰ª¨Ëøò‰∏ìÈó®Êêû‰∫ÜapiÊù•ÂçñËøôÁßçÊúçÂä°„ÄÇÁÆÄÂçïÊù•ËÆ≤Ôºå
ÊØîÂ¶Ç‰Ω†Áî®ÊµÅÈáè‰∏ä‰∫¨‰∏úÔºå‰Ω†‰∏çÈúÄË¶ÅËæìÂÖ•‰Ω†ÁöÑÁîµËØùÂè∑Á†ÅÊù•Ê≥®ÂÜåÊàñËÄÖÁôªÂΩïÔºå‰∫¨‰∏úÂ∞±Â∑≤ÁÑ∂Áü•Êôì‰∫ÜÔºåÂõ†‰∏∫‰∫¨‰∏ú‰π∞
‰∫ÜispÁöÑapiÊé•Âè£ÔºåÂè™ÈúÄË¶ÅÂú®ÁΩëÈ°µ‰∏≠Âä†ÂÖ•Âá†Ë°å‰ª£Á†ÅÊù•Ë∞ÉÁî®Ëøô‰∏™apiÔºå‰Ω†‰∏ÄÊó¶Áî®ÊâãÊú∫ÊµÅÈáèËÆøÈóÆËøô‰∏™ÁΩëÈ°µÔºå
ÁîµËØùÂè∑Á†ÅÂ∞±‰ºöËøîÂõûÁªô‰∫¨‰∏ú„ÄÇ ÊâÄ‰ª•Êó†ËÆ∫ÁøªÂ¢ô‰∏éÂê¶ÔºåÈÉΩÂª∫ËÆÆÂú®‰ΩøÁî®Êï∞ÊçÆÊµÅÈáè‰∏äÁΩëÊó∂ÂºÄÂêØvpnÊúçÂä°Êù•Âä†ÂØÜÂíå
‰∏≠ËΩ¨Êï∞ÊçÆ„ÄÇ ÂÖ≥‰∫é‰π∞ÂõΩÂ§ñÁîµËØùÂç°Êù•ÁøªÂ¢ôÔºåÂêåÊ†∑‰πüÊòØ‰∏çÈù†Ë∞±ÁöÑÔºåÂõ†‰∏∫Â§ñÂõΩÁöÑsimÂç°ÊãøÂà∞‰∏≠ÂõΩÊù•ËÉΩÁî®ÔºåÂè™ÊòØ
Êº´Ê∏∏‰∫ÜËÄåÂ∑≤ÔºåÊº´Ê∏∏ÁöÑÊÑèÊÄùÂ∞±ÊòØÔºåÂõΩÂ§ñÂíåÂõΩÂÜÖÁöÑispÁ≠æ‰∫ÜÂêàÂêåÔºåËÉΩËÆ©ÂêÑËá™ÁöÑÁî®Êà∑ÁßüÁî®Ëá™Â∑±ËøôËæπÁöÑÂü∫Á´ôÁ∫ø
Ë∑ØÁ≠âËÆæÂ§áÊù•‰øùËØÅË∑®ÂõΩ‰ΩøÁî®ÔºåÊâÄ‰ª•Âç≥‰Ωø‰π∞‰∫Ügoogle-fiÊù•‰∏≠ÂõΩ‰ΩøÁî®ÔºåÂíå‰Ω†Áî®‰∏≠ÂõΩÁöÑsimÂç°Âú®‰∏≠ÂõΩ‰ΩøÁî®ÔºåÈÉΩ
ÊòØÈÄöËøá‰∏≠ÂõΩispÁöÑÂü∫Á´ôÁ∫øË∑ØÂíåÊúçÂä°Âô®„ÄÇÂîØ‰∏ÄÂå∫Âà´Â∞±ÊòØgoogle-fiÁöÑÊâÄÊúâËÆøÈóÆÈÉΩ‰ºöË¢´ÂàóÂÖ•ÁôΩÂêçÂçïÔºå‰ΩÜÊòØÊï∞
ÊçÆÊµÅÈáèÁöÑÂÆ°ËÆ°ÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ‰Ω†Êãøgoogle-fiÊù•‰∏äpincongÔºåÁßªÂä®ÊúçÂä°ÂïÜÊòØÁü•ÈÅìÔºåÂè™ÊòØ‰∏çÈòªÊã¶ËÄåÂ∑≤ÔºåËøôÊ†∑‰∏Ä
ÂÆöÁ®ãÂ∫¶‰∏äÊõ¥Âä†Âç±Èô©„ÄÇ&lt;/p>
&lt;p>@han_chinese:
‰∏≠ÂúãÈõª‰ø°Â∏ÇÂ†¥ÁöÑÁßªÂãïÊï∏ÊìöÁ∂≤Áµ°‰ΩøÁî®ÁöÑÊòØË§áÈõúÁöÑÂ§öÈáç‰ºÅÊ•≠IPv4-NATÔºåÂä†‰πãÔºåÁßªÂãïÊï∏ÊìöÁ∂≤Áµ°Ê•≠ÂãôÂæàË§áÈõú„ÄÇÊâÄ‰ª•ÔºåGFWÂ∞çÁßªÂãïÊï∏ÊìöÁ∂≤Ë∑ØÁöÑÂÆ°Êü•ËºÉÂº±„ÄÇ
Âª∫Ë≠∞Áõ¥Êé•‰ΩøÁî®ÂúãÂ§ñÊï∏ÊìöÊ•≠ÂãôÔºåÂÉèÊòØGoogle-FiÔºå‰ΩøÁî®‰∏çÂèóGFWÂΩ±ÈüøÁöÑÁßªÂãïÊï∏ÊìöÁ∂≤Áµ°„ÄÇ&lt;/p>
&lt;p>&lt;a href="https://pincong.rocks/question/45451">ÁÇπËøôÈáåÊü•ÈòÖ‰ø°ÊÅØÊù•Ê∫ê&lt;/a>&lt;/p></description></item><item><title>calculate_gpu_vram_for_llama3-70B</title><link>/posts/2024-06-13-calculate_gpu_vram_for_llama3-70b/</link><pubDate>Thu, 13 Jun 2024 16:11:47 +0800</pubDate><guid>/posts/2024-06-13-calculate_gpu_vram_for_llama3-70b/</guid><description>&lt;p>How many GPUs do I need to be able to serve Llama 70B? In order to answer that, you need
to know how much GPU memory will be required by the Large Language Model.&lt;/p>
&lt;p>The formula is simple:&lt;/p>
&lt;p>$$
M=\frac{(P * 4B)}{(32/Q)} * 1.2
$$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Symbol&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M&lt;/td>
&lt;td>GPU memory expressed in Gigabyte&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>P&lt;/td>
&lt;td>The amount of parameters in the model. E.g. a 7B model has 7 billion parameters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>4 bytes, expressing the bytes used for each parameter&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>There are 32 bits in 4 bytes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Q&lt;/td>
&lt;td>The amount of bits that should be used for loading the model. E.g. 16 bits, 8 bits or 4 bits.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.2&lt;/td>
&lt;td>Represents a 20% overhead of loading additional things in GPU memory.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now let&amp;rsquo;s try out some examples.
GPU memory required for serving Llama 70B&lt;/p>
&lt;p>Let&amp;rsquo;s try it out for Llama 70B that we will load in 16 bit. The model has 70 billion parameters.&lt;/p>
&lt;p>$$
\frac{70 * 4bytes}{32/16} * 1.2=168GB
$$&lt;/p>
&lt;p>That&amp;rsquo;s quite a lot of memory. A single A100 80GB wouldn&amp;rsquo;t be enough, although 2x A100
80GB should be enough to serve the Llama 2 70B model in 16 bit mode.&lt;/p></description></item><item><title>lean_principles</title><link>/posts/mlteam101/lean_principles/</link><pubDate>Mon, 03 Jun 2024 17:59:09 +0800</pubDate><guid>/posts/mlteam101/lean_principles/</guid><description>&lt;p>Principle 1: Identify value&lt;/p>
&lt;pre>&lt;code>Determine what is most valuable to the customer and focus on maximizing that value.
&lt;/code>&lt;/pre>
&lt;p>Principle 2: Map the value stream&lt;/p>
&lt;pre>&lt;code>Identify the steps in the process that add value and eliminate those that do not.
&lt;/code>&lt;/pre>
&lt;p>Principle 3: Create flow&lt;/p>
&lt;pre>&lt;code>Streamline the process to create a smooth and continuous flow of work.
&lt;/code>&lt;/pre>
&lt;p>Principle 4: Establish pull&lt;/p>
&lt;pre>&lt;code>Use customer demand to trigger production and avoid overproduction.
&lt;/code>&lt;/pre>
&lt;p>Principle 5: Continuous improvement&lt;/p>
&lt;pre>&lt;code>Continuously strive for improvement and eliminate waste in all areas of the value
chain.
&lt;/code>&lt;/pre>
&lt;p>Value stream mapping (principle 2) is a tool that lets us visually represent all the
steps and resources involved in delivering a unit of value (e.g., a product feature) to
customers. Teams can use this tool to identify waste, work toward eliminating waste,
and improve the flow of value (principle 3).&lt;/p>
&lt;p>To map your team or product‚Äôs value stream, you can follow these steps:&lt;/p>
&lt;ol>
&lt;li>Identify the product or service being mapped. This could be a single product or1.
an entire process.&lt;/li>
&lt;li>Identify the current state map. Create a visual representation of the current2.
process, including all steps and materials (including time and labor) involved
from raw materials to finished product.&lt;/li>
&lt;li>Identify value-added and non-value-added activities. Determine which steps add3.
value to the product or service and which do not.&lt;/li>
&lt;li>Identify waste. Look for areas of overproduction, waiting, defects, overprocess‚Äê4.
ing, excess inventory, unnecessary motion, excess transport, unnecessary use of
raw materials, and unnecessary effort.&lt;/li>
&lt;li>Create a future state map. Based on the analysis of the current state map, redesign5.
the process to eliminate waste and create a more efficient flow of materials and
information.&lt;/li>
&lt;li>Implement changes. Put the redesigned process into practice and continuously6.
monitor and improve (principle 5).&lt;/li>
&lt;/ol></description></item><item><title>EffectiveML 01: Delivering successful ML projects</title><link>/posts/mlteam101/feedback_loops_and_times_to_feedback/</link><pubDate>Mon, 03 Jun 2024 17:12:37 +0800</pubDate><guid>/posts/mlteam101/feedback_loops_and_times_to_feedback/</guid><description>&lt;p>Table: Comparison of feedback mechanisms and time-to-feedback in high- and low-effectiveness environments&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Task&lt;/th>
&lt;th>High-effectiveness environment&lt;/th>
&lt;th>Low-effectiveness environment&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Testing if code changes worked as expected&lt;/td>
&lt;td>Automated testing (~ seconds to minutes)&lt;/td>
&lt;td>Manual testing (~ minutes to hours)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Testing if training pipeline works end to end&lt;/td>
&lt;td>Training smoke test (~ 1 minute)&lt;/td>
&lt;td>Full model training (~ minutes to hours, depending on model architecture)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Getting feedback on code changes&lt;/td>
&lt;td>Pair programming (~ seconds to minutes)&lt;/td>
&lt;td>Pull request reivews (~ hours to days)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Understanding if application is working as expected in production&lt;/td>
&lt;td>Monitoring in production (~ seconds - as it happens)&lt;/td>
&lt;td>Customer complaints (~ days, or longer if not directly reported)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>How teams can move from a low-effectiveness environment to a high-effectiveness environment?
MLOps is not enough (MLOps and ML platforms aren‚Äôt going to write comprehensive tests
for you, talk to users for you, or reduce the negative impacts of team silos for you.)&lt;/p>
&lt;p>Delivering successful ML projects requires a multi‚Äêdisciplinary approach across these
five disciplines: product, software engineering, data, ML, and delivery:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>software engineering:&lt;/p>
&lt;ul>
&lt;li>code design&lt;/li>
&lt;li>automated testing&lt;/li>
&lt;li>refactoring&lt;/li>
&lt;li>dependency management&lt;/li>
&lt;li>code editor efficency&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>data:&lt;/p>
&lt;ul>
&lt;li>data quality&lt;/li>
&lt;li>data engineering&lt;/li>
&lt;li>data security and privacy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Machine learning:&lt;/p>
&lt;ul>
&lt;li>ML techniques&lt;/li>
&lt;li>model evaluation&lt;/li>
&lt;li>ML governance&lt;/li>
&lt;li>MLOps&lt;/li>
&lt;li>CI/CD&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Product:&lt;/p>
&lt;ul>
&lt;li>product thinking&lt;/li>
&lt;li>responsible AI&lt;/li>
&lt;li>human-centered AI&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Delivery:&lt;/p>
&lt;ul>
&lt;li>lean delivery&lt;/li>
&lt;li>team topologies&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>tech interview prepare (for my resume)</title><link>/posts/notes4resume/</link><pubDate>Mon, 27 May 2024 20:43:25 +0800</pubDate><guid>/posts/notes4resume/</guid><description>&lt;h1 id="Ëá™Êàë‰ªãÁªç" >
&lt;div>
&lt;a href="#%e8%87%aa%e6%88%91%e4%bb%8b%e7%bb%8d">
##
&lt;/a>
Ëá™Êàë‰ªãÁªç
&lt;/div>
&lt;/h1>
&lt;blockquote>
&lt;p>Âú®Ëá™Êàë‰ªãÁªçÊó∂ÔºåÁ°Æ‰øù‰Ω†ÊèêÂà∞ÁöÑÈ°πÁõÆÂíåÊäÄËÉΩ‰∏é‰Ω†Áî≥ËØ∑ÁöÑËÅå‰ΩçÁ¥ßÂØÜÁõ∏ÂÖ≥ÔºåËøôÊ†∑ÂèØ‰ª•Êõ¥Â•ΩÂú∞Â±ïÁ§∫‰Ω†ÁöÑ‰∏ì‰∏ö
ËÉΩÂäõÂíåÂØπËÅå‰ΩçÁöÑÈÄÇÂ∫îÊÄß„ÄÇÂêåÊó∂Ôºå‰øùÊåÅËá™‰ø°ÂíåÁÉ≠ÊÉÖÔºåËÆ©Èù¢ËØïÂÆòÊÑüÂèóÂà∞‰Ω†ÂØπÂ∑•‰ΩúÂíåÂõ¢ÈòüÁöÑÊâøËØ∫„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>Â∞äÊï¨ÁöÑÈù¢ËØïÂÆòÔºåÊÇ®Â•ΩÔºÅ&lt;/p>
&lt;p>ÊàëÂè´ËåÉÊòéÂçéÔºåÊã•Êúâ6Âπ¥Âú®Êú∫Âô®Â≠¶‰π†È¢ÜÂüüÁöÑÂ∑•‰ΩúÁªèÈ™å„ÄÇÊàë‰∫é2017Âπ¥Á°ïÂ£´ÊØï‰∏ö‰∫é‰∏≠Â±±Â§ßÂ≠¶Ôºå‰∏ì‰∏öÊòØÁîüÊÄÅÂ≠¶Ôºå
Âú®Ê§çË¢´Âä®ÊÄÅÂíåÊºîÊõøÊñπÈù¢ÁöÑÁ†îÁ©∂Â∑•‰ΩúÔºå‰∏∫ÊàëÂú®ÂÆûÈ™åËÆæËÆ°„ÄÅÁªüËÆ°ÂàÜÊûê‰ª•ÂèäÊï∞ÊçÆÊåñÊéòÊñπÈù¢Êâì‰∏ã‰∫ÜÊâéÂÆûÁöÑÂü∫Á°Ä„ÄÇ
ËøáÂéªÁöÑ6Âπ¥ÈáåÔºåÊàëÂú®Â∑•‰Ωú‰∏≠‰∏ÄÁõ¥Ëá¥Âäõ‰∫éÂ∞ÜÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÂ∫îÁî®‰∫éËß£ÂÜ≥ÂÆûÈôÖÈóÆÈ¢òÔºåÂπ∂ÂèñÂæó‰∫Ü‰∏Ä‰∫õÁöÑÊàêÊûú„ÄÇ&lt;/p>
&lt;p>ÊàëÂú®ÁõÆÂâçÂÖ¨Âè∏ÊãÖ‰ªªÈ´òÁ∫ßÊï∞ÊçÆÊåñÊéòÂ∑•Á®ãÂ∏àÔºåÊúüÈó¥Êàë‰∏ªÂØº‰∫ÜÂ§ö‰∏™Êú∫Âô®Â≠¶‰π†È°πÁõÆÁöÑÂºÄÂèëÂíå‰∫§‰ªò„ÄÇ
Âú®ÊäÄÊúØÂ±ÇÈù¢ÔºåÊàëÊØîËæÉÊìÖÈïøÁªìÂêàÊï∞ÊçÆÁé∞Áä∂Êù•ÂÜ≥ÂÆöÊú∫Âô®Â≠¶‰π†ÊàñÊ∑±Â∫¶Â≠¶‰π†ÊäÄÊúØÁöÑÊê≠ÈÖçËß£ÂÜ≥ÂÆûÈôÖÈóÆÈ¢òÔºå
ÁÜüÊÇâÊï¥‰∏™Êú∫Âô®Â≠¶‰π†È°πÁõÆÁöÑÂºÄÂèëÊµÅÁ®ãÔºå‰ªéÈ°πÁõÆË∞ÉÁ†î„ÄÅÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ„ÄÅÁâπÂæÅÂ∑•Á®ãÂà∞Ê®°ÂûãËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤Ôºå
ÊàëÈÉΩÊúâÊ∑±ÂÖ•ÁöÑÁêÜËß£ÂíåÂÆûË∑µ„ÄÇÂú®Ëøô‰∫õÈ°πÁõÆÁöÑÂÆûÊàò‰∏≠Ôºå‰∏ç‰ªÖÊèêÂçá‰∫Ü‰∏™‰∫∫ÁöÑÊäÄÊúØÊ∑±Â∫¶ÂíåÂ∏¶ÈòüËÉΩÂäõÔºå
ÂêåÊó∂‰πüÂèñÂæó‰∫ÜÊéàÊùÉÁöÑÂèëÊòé‰∏ìÂà©„ÄÅËΩØ‰ª∂Ëëó‰Ωú„ÄÅÂú∞ÊñπÊ†áÂáÜÁ≠âÊàêÊûú„ÄÇ&lt;/p>
&lt;p>Èô§‰∫ÜÊäÄÊúØ‰∏ìÈïøÔºåÊàëËøòÊòØ‰∏Ä‰∏™Ê≥®ÈáçÂõ¢ÈòüÂêà‰ΩúÂíåÂä°ÂÆûË¥üË¥£ÁöÑ‰∫∫„ÄÇÊàëÁõ∏‰ø°ÔºåÊàëÁöÑ‰∏ì‰∏öÊäÄËÉΩÂíå‰∏∞ÂØåÁªèÈ™åÔºå
ËÉΩÂ§ü‰∏∫Ë¥µÂÖ¨Âè∏Â∏¶Êù•Áõ¥Êé•ÁöÑ‰ª∑ÂÄº„ÄÇ&lt;/p>
&lt;p>ÊàëÂØπË¥µÂÖ¨Âè∏Âú®Êú∫Âô®Â≠¶‰π†/Ê∑±Â∫¶Â≠¶‰π†ÊñπÈù¢ÁöÑÂ∑•‰ΩúÊÑüÂÖ¥Ë∂£ÔºåÊàëÊúüÂæÖËÉΩÂ§üÂä†ÂÖ•Ë¥µÂÖ¨Âè∏ÔºåÂπ∂‰∏éÂõ¢Èòü‰∏ÄËµ∑Ëß£ÂÜ≥Êõ¥Â§öÊúâË∂£ÁöÑÊäÄÊúØÊåëÊàò„ÄÇ&lt;/p>
&lt;p>ÊÑüË∞¢ÊÇ®ÁªôÊàëËøôÊ¨°Èù¢ËØïÁöÑÊú∫‰ºöÔºåÊàëÊúüÂæÖÂú®Êé•‰∏ãÊù•ÁöÑËÆ®ËÆ∫‰∏≠ÂàÜ‰∫´Êõ¥Â§öÊàëÁöÑÁªèÈ™åÂíåÊÉ≥Ê≥ï„ÄÇË∞¢Ë∞¢ÔºÅ&lt;/p>
&lt;hr>
&lt;h1 id="project-01----nlp" >
&lt;div>
&lt;a href="#project-01----nlp">
##
&lt;/a>
Project 01 &amp;ndash; NLP
&lt;/div>
&lt;/h1>
&lt;h2 id="sklearn-randomforest-model" >
&lt;div>
&lt;a href="#sklearn-randomforest-model">
#
&lt;/a>
sklearn randomforest model
&lt;/div>
&lt;/h2>
&lt;p>ÂΩìË∞àÂà∞ÈöèÊú∫Ê£ÆÊûóÊó∂ÔºåÊàë‰ª¨ÈúÄË¶ÅÁêÜËß£ÂÆÉÁöÑÂü∫Á°ÄÁÆóÊ≥ïÔºöÂÜ≥Á≠ñÊ†ë„ÄÇÈöèÊú∫Ê£ÆÊûóÊòØÂü∫‰∫éÂÜ≥Á≠ñÊ†ëÁöÑÈõÜÊàêÂ≠¶‰π†ÊñπÊ≥ï„ÄÇÊâÄ‰ª•ÔºåËÆ©Êàë‰ª¨È¶ñÂÖàÊù•‰∫ÜËß£ÂÜ≥Á≠ñÊ†ëÁöÑÂü∫Êú¨ÁÆóÊ≥ïÔºåÁÑ∂ÂêéÂÜçÊ∑±ÂÖ•Êé¢ËÆ®ÈöèÊú∫Ê£ÆÊûó„ÄÇ&lt;/p>
&lt;h3 id="1-ÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ï" >
&lt;div>
&lt;a href="#1-%e5%86%b3%e7%ad%96%e6%a0%91%e7%ae%97%e6%b3%95">
##
&lt;/a>
1. ÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ï:
&lt;/div>
&lt;/h3>
&lt;h4 id="11-cartÁÆóÊ≥ï-classification-and-regression-trees" >
&lt;div>
&lt;a href="#11-cart%e7%ae%97%e6%b3%95-classification-and-regression-trees">
###
&lt;/a>
1.1 CARTÁÆóÊ≥ï (Classification and Regression Trees):
&lt;/div>
&lt;/h4>
&lt;p>CARTÁÆóÊ≥ïÊòØ‰∏ÄÁßçÁî®‰∫éÊûÑÂª∫ÂàÜÁ±ªÂíåÂõûÂΩíÊ†ëÁöÑÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ï„ÄÇÂÆÉÈÄöËøáÂØπÊï∞ÊçÆÈõÜÈÄíÂΩíÂú∞ËøõË°å‰∫åÂàÜÊù•ÊûÑÂª∫ÂÜ≥Á≠ñÊ†ë„ÄÇÂÖ∑‰ΩìÊ≠•È™§Â¶Ç‰∏ãÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ÁâπÂæÅÈÄâÊã©&lt;/strong>ÔºöÂØπ‰∫éÂàÜÁ±ªÈóÆÈ¢òÔºåÈÄöÂ∏∏‰ΩøÁî®Âü∫Â∞ºÊåáÊï∞ÔºàGini indexÔºâÊàñ‰ø°ÊÅØÂ¢ûÁõäÔºàInformation GainÔºâÊù•ÈÄâÊã©ÊúÄ‰Ω≥ÁöÑÁâπÂæÅËøõË°åÂàÜË£ÇÔºõÂØπ‰∫éÂõûÂΩíÈóÆÈ¢òÔºåÈÄöÂ∏∏‰ΩøÁî®Âπ≥ÊñπËØØÂ∑ÆÊù•ÈÄâÊã©ÊúÄ‰Ω≥ÁöÑÁâπÂæÅ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ËäÇÁÇπÂàÜË£Ç&lt;/strong>ÔºöÊ†πÊçÆÈÄâÊã©ÁöÑÁâπÂæÅÔºåÂ∞ÜÊï∞ÊçÆÈõÜÂàÜÊàê‰∏§ÈÉ®ÂàÜÔºå‰ΩøÂæóÊØè‰∏™Â≠êÈõÜÁöÑÊ†∑Êú¨Â±û‰∫éÂêå‰∏ÄÁ±ªÂà´ÔºàÂØπ‰∫éÂàÜÁ±ªÈóÆÈ¢òÔºâÊàñÂÖ∑ÊúâÁõ∏‰ººÁöÑÂõûÂΩíÂÄºÔºàÂØπ‰∫éÂõûÂΩíÈóÆÈ¢òÔºâ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÈÄíÂΩí&lt;/strong>ÔºöÂØπÊØè‰∏™Â≠êÈõÜÈáçÂ§ç‰∏äËø∞ËøáÁ®ãÔºåÁõ¥Âà∞Êª°Ë∂≥ÂÅúÊ≠¢Êù°‰ª∂ÔºåÂ¶ÇËææÂà∞ÊúÄÂ§ßÊ∑±Â∫¶„ÄÅËäÇÁÇπ‰∏≠Ê†∑Êú¨Êï∞Â∞è‰∫éÊüê‰∏™ÈòàÂÄºÊàñÂÖ∂‰ªñÈ¢ÑÂÆö‰πâÊù°‰ª∂„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ââ™Êûù&lt;/strong>Ôºö‰∏∫‰∫ÜÈÅøÂÖçËøáÊãüÂêàÔºåÂèØ‰ª•ÂØπÁîüÊàêÁöÑÊ†ëËøõË°åÂâ™ÊûùÔºåÂç≥ÁßªÈô§‰∏Ä‰∫õËäÇÁÇπÊù•ÁÆÄÂåñÊ†ëÁöÑÁªìÊûÑ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="2-ÈöèÊú∫Ê£ÆÊûóÁÆóÊ≥ï" >
&lt;div>
&lt;a href="#2-%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e7%ae%97%e6%b3%95">
##
&lt;/a>
2. ÈöèÊú∫Ê£ÆÊûóÁÆóÊ≥ï:
&lt;/div>
&lt;/h3>
&lt;h4 id="21-ÊûÑÂª∫ÈöèÊú∫Ê£ÆÊûó" >
&lt;div>
&lt;a href="#21-%e6%9e%84%e5%bb%ba%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97">
###
&lt;/a>
2.1 ÊûÑÂª∫ÈöèÊú∫Ê£ÆÊûó:
&lt;/div>
&lt;/h4>
&lt;p>ÈöèÊú∫Ê£ÆÊûóÊòØÈÄöËøáÊûÑÂª∫Â§öÊ£µÂÜ≥Á≠ñÊ†ëÂπ∂Â∞ÜÂÆÉ‰ª¨ÈõÜÊàêËµ∑Êù•Êù•ÂÆåÊàêÁöÑ„ÄÇÂÖ∑‰ΩìÊ≠•È™§Â¶Ç‰∏ãÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ÈöèÊú∫ÊäΩÊ†∑&lt;/strong>Ôºö‰ªéÂéüÂßãËÆ≠ÁªÉÈõÜ‰∏≠ÈöèÊú∫ÈÄâÊã©‰∏ÄÈÉ®ÂàÜÊ†∑Êú¨ÔºàÊúâÊîæÂõûÊäΩÊ†∑ÔºâÊù•ÊûÑÂª∫ÊØèÊ£µÂÜ≥Á≠ñÊ†ëÁöÑËÆ≠ÁªÉÈõÜ„ÄÇËøôÊ†∑ÂèØ‰ª•‰øùËØÅÊØèÊ£µÊ†ëÁöÑËÆ≠ÁªÉÈõÜÁï•ÊúâÂ∑ÆÂºÇÔºåÂ¢ûÂä†‰∫ÜÊ®°ÂûãÁöÑÂ§öÊ†∑ÊÄß„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÈöèÊú∫ÁâπÂæÅÈÄâÊã©&lt;/strong>ÔºöÂØπ‰∫éÊØèÊ£µÊ†ëÁöÑÊØè‰∏™ËäÇÁÇπÔºåÂú®ÈÄâÊã©ÂàÜÂâ≤ÁâπÂæÅÊó∂ÔºåÈöèÊú∫ÈÄâÊã©‰∏ÄÈÉ®ÂàÜÁâπÂæÅÊù•ËøõË°åËØÑ‰º∞„ÄÇËøôÊ†∑ÂèØ‰ª•Á°Æ‰øùÊØèÊ£µÊ†ëÁöÑÂàÜË£ÇËøáÁ®ã‰πüÊúâÊâÄÂ∑ÆÂºÇ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Áã¨Á´ãÊûÑÂª∫&lt;/strong>ÔºöÊØèÊ£µÊ†ëÈÉΩÊòØÁã¨Á´ãÊûÑÂª∫ÁöÑÔºåÊ≤°Êúâ‰ªª‰ΩïÂÖ≥ËÅî„ÄÇËøôÊÑèÂë≥ÁùÄÂèØ‰ª•Âπ∂Ë°åÂú∞ÊûÑÂª∫Â§öÊ£µÊ†ëÔºåÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéá„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="22-ÈõÜÊàêÂÜ≥Á≠ñÊ†ë" >
&lt;div>
&lt;a href="#22-%e9%9b%86%e6%88%90%e5%86%b3%e7%ad%96%e6%a0%91">
###
&lt;/a>
2.2 ÈõÜÊàêÂÜ≥Á≠ñÊ†ë:
&lt;/div>
&lt;/h4>
&lt;p>ÊûÑÂª∫Â§öÊ£µÂÜ≥Á≠ñÊ†ëÂêéÔºåÈöèÊú∫Ê£ÆÊûóÈááÁî®‰∏çÂêåÁöÑÊñπÂºèÊù•ÈõÜÊàêÂÆÉ‰ª¨ÁöÑÈ¢ÑÊµãÁªìÊûúÔºö&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ÂàÜÁ±ª‰ªªÂä°&lt;/strong>ÔºöÈááÁî®ÊäïÁ•®ÁöÑÊñπÂºèÔºåÂç≥ÊØèÊ£µÊ†ëÊäïÁ•®ÈÄâÊã©ÊúÄÁªàÁöÑÁ±ªÂà´„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÂõûÂΩí‰ªªÂä°&lt;/strong>ÔºöÈááÁî®Âπ≥ÂùáÂÄºÁöÑÊñπÂºèÔºåÂç≥Â§öÊ£µÊ†ëÁöÑÈ¢ÑÊµãÁªìÊûúÂèñÂπ≥ÂùáÂÄº„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="ÊÄªÁªì" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93">
##
&lt;/a>
ÊÄªÁªì:
&lt;/div>
&lt;/h3>
&lt;p>ÈöèÊú∫Ê£ÆÊûóÊòØ‰∏ÄÁßçÂº∫Â§ßÁöÑÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ïÔºåÂü∫‰∫éÂÜ≥Á≠ñÊ†ëÁöÑÈõÜÊàêÂ≠¶‰π†„ÄÇÈÄöËøáÂà©Áî®ÂÜ≥Á≠ñÊ†ëÁöÑÈöèÊú∫ÊÄßÂíåÈõÜÊàêÁ≠ñÁï•ÔºåÈöèÊú∫Ê£ÆÊûóËÉΩÂ§üÊúâÊïàÂú∞Â∫îÂØπÂàÜÁ±ªÂíåÂõûÂΩíÈóÆÈ¢òÔºåÂπ∂Âú®ËÆ∏Â§öÂÆûÈôÖÂ∫îÁî®‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ&lt;/p>
&lt;p>Âú® CART (Classification and Regression Trees) ÁÆóÊ≥ï‰∏≠ÔºåËäÇÁÇπÁöÑÂàÜË£Ç‰æùÊçÆÊòØÂü∫‰∫éË¥™ÂøÉÁÆóÊ≥ï„ÄÇCART ÁÆóÊ≥ïÈÄöËøáË¥™ÂøÉÂú∞ÈÄâÊã©ÊØèÊ¨°ÂàÜË£ÇÊó∂ËÉΩÂ§üÊúÄÂ§ßÁ®ãÂ∫¶ÂáèÂ∞ë‰∏çÁ∫ØÂ∫¶ÔºàÂØπ‰∫éÂàÜÁ±ªÈóÆÈ¢òÔºâÊàñËÄÖÊúÄÂ∞èÂåñËØØÂ∑ÆÔºàÂØπ‰∫éÂõûÂΩíÈóÆÈ¢òÔºâÁöÑÁâπÂæÅÊù•ËøõË°åËäÇÁÇπÁöÑÂàÜË£Ç„ÄÇËøôÁßçË¥™ÂøÉÁ≠ñÁï•‰øùËØÅ‰∫ÜÂú®ÊØè‰∏™ËäÇÁÇπÂàÜË£ÇÊó∂ÈÉΩÈÄâÊã©‰∫ÜÊúÄ‰ºòÁöÑÁâπÂæÅÊù•ËøõË°åÂàÜË£Ç„ÄÇ&lt;/p>
&lt;h3 id="ËäÇÁÇπÂàÜË£ÇÁöÑ‰æùÊçÆ" >
&lt;div>
&lt;a href="#%e8%8a%82%e7%82%b9%e5%88%86%e8%a3%82%e7%9a%84%e4%be%9d%e6%8d%ae">
##
&lt;/a>
ËäÇÁÇπÂàÜË£ÇÁöÑ‰æùÊçÆÔºö
&lt;/div>
&lt;/h3>
&lt;h4 id="ÂØπ‰∫éÂàÜÁ±ªÈóÆÈ¢ò" >
&lt;div>
&lt;a href="#%e5%af%b9%e4%ba%8e%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98">
###
&lt;/a>
ÂØπ‰∫éÂàÜÁ±ªÈóÆÈ¢òÔºö
&lt;/div>
&lt;/h4>
&lt;p>Âú®ÂàÜÁ±ªÈóÆÈ¢ò‰∏≠ÔºåCART ÁÆóÊ≥ïÈÄöÂ∏∏‰ΩøÁî®‰ª•‰∏ã‰∏§ÁßçÊñπÊ≥ï‰Ωú‰∏∫ËäÇÁÇπÂàÜË£ÇÁöÑ‰æùÊçÆÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Âü∫Â∞ºÊåáÊï∞ (Gini index)&lt;/strong>ÔºöÂü∫Â∞ºÊåáÊï∞Ë°°Èáè‰∫Ü‰ªé‰∏Ä‰∏™Êï∞ÊçÆÈõÜ‰∏≠ÈöèÊú∫ÊäΩÂèñ‰∏§‰∏™Ê†∑Êú¨ÔºåÂÆÉ‰ª¨Á±ªÂà´‰∏ç‰∏ÄËá¥ÁöÑÊ¶ÇÁéá„ÄÇÂÖ∑‰ΩìÂú∞ÔºåÂØπ‰∫é‰∏Ä‰∏™ËäÇÁÇπ $t$ÔºåÂü∫Â∞ºÊåáÊï∞ÂèØ‰ª•ËÆ°ÁÆó‰∏∫Ôºö&lt;/p>
&lt;p>$ Gini(t) = 1 - \sum_{i=1}^{c} p(i|t)^2 $&lt;/p>
&lt;p>ÂÖ∂‰∏≠Ôºå$c$ ÊòØÁ±ªÂà´ÁöÑÊï∞ÈáèÔºå$p(i|t)$ ÊòØÂú®ËäÇÁÇπ $t$ ‰∏≠Â±û‰∫éÁ±ªÂà´ $i$ ÁöÑÊ†∑Êú¨ÁöÑÊØî‰æã„ÄÇÈÄâÊã©ËÉΩÂ§üÊúÄÂ§ßÁ®ãÂ∫¶Èôç‰ΩéÂü∫Â∞ºÊåáÊï∞ÁöÑÁâπÂæÅÊù•ËøõË°åËäÇÁÇπÂàÜË£Ç„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>‰ø°ÊÅØÂ¢ûÁõä (Information Gain)&lt;/strong>Ôºö‰ø°ÊÅØÂ¢ûÁõäË°°Èáè‰∫ÜÂú®Êüê‰∏™ÁâπÂæÅÁöÑÊù°‰ª∂‰∏ãÔºåÂ∞ÜÊï∞ÊçÆÈõÜÂàÜ‰∏∫‰∏çÂêåÁ±ªÂà´ÂêéÔºåÂ∏¶Êù•ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÂáèÂ∞ëÁöÑÁ®ãÂ∫¶„ÄÇÂÖ∑‰ΩìÂú∞ÔºåÂØπ‰∫é‰∏Ä‰∏™ËäÇÁÇπ $t$ Âíå‰∏Ä‰∏™ÁâπÂæÅ $A$Ôºå‰ø°ÊÅØÂ¢ûÁõäÂèØ‰ª•ËÆ°ÁÆó‰∏∫Ôºö&lt;/p>
&lt;p>$ IG(t, A) = H(t) - \sum_{v \in Values(A)} \frac{|t_v|}{|t|} \cdot H(t_v) $&lt;/p>
&lt;p>ÂÖ∂‰∏≠Ôºå$H(t)$ ÊòØËäÇÁÇπ $t$ ÁöÑÁÜµÔºå$Values(A)$ ÊòØÁâπÂæÅ $A$ ÁöÑÂèñÂÄºÈõÜÂêàÔºå$t_v$ ÊòØÂú®ÁâπÂæÅ $A$ ‰∏äÂèñÂÄº‰∏∫ $v$ ÁöÑÊ†∑Êú¨ÈõÜÂêàÔºå$H(t_v)$ ÊòØÊ†∑Êú¨ÈõÜÂêà $t_v$ ÁöÑÁÜµ„ÄÇÈÄâÊã©ËÉΩÂ§üÊúÄÂ§ßÂåñ‰ø°ÊÅØÂ¢ûÁõäÁöÑÁâπÂæÅÊù•ËøõË°åËäÇÁÇπÂàÜË£Ç„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="ÂØπ‰∫éÂõûÂΩíÈóÆÈ¢ò" >
&lt;div>
&lt;a href="#%e5%af%b9%e4%ba%8e%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98">
###
&lt;/a>
ÂØπ‰∫éÂõûÂΩíÈóÆÈ¢òÔºö
&lt;/div>
&lt;/h4>
&lt;p>Âú®ÂõûÂΩíÈóÆÈ¢ò‰∏≠ÔºåCART ÁÆóÊ≥ïÈÄöÂ∏∏‰ΩøÁî®Âπ≥ÊñπËØØÂ∑Æ (Mean Squared Error, MSE) ‰Ωú‰∏∫ËäÇÁÇπÂàÜË£ÇÁöÑ‰æùÊçÆ„ÄÇÈÄâÊã©ËÉΩÂ§üÊúÄÂ∞èÂåñËäÇÁÇπÂàÜË£ÇÂêéÊ†∑Êú¨ÁöÑÂπ≥ÊñπËØØÂ∑ÆÁöÑÁâπÂæÅÊù•ËøõË°åÂàÜË£Ç„ÄÇ&lt;/p>
&lt;h3 id="ÊÄªÁªì-1" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-1">
##
&lt;/a>
ÊÄªÁªìÔºö
&lt;/div>
&lt;/h3>
&lt;p>CART ÁÆóÊ≥ïÂú®ËäÇÁÇπÂàÜË£ÇÊó∂ÈááÁî®Ë¥™ÂøÉÁÆóÊ≥ïÔºåÈÄâÊã©ËÉΩÂ§üÊúÄÂ§ßÁ®ãÂ∫¶ÂáèÂ∞ë‰∏çÁ∫ØÂ∫¶ÔºàÂàÜÁ±ªÈóÆÈ¢òÔºâÊàñËÄÖÊúÄÂ∞èÂåñËØØÂ∑ÆÔºàÂõûÂΩíÈóÆÈ¢òÔºâÁöÑÁâπÂæÅÊù•ËøõË°åÂàÜË£Ç„ÄÇËøôÁßçË¥™ÂøÉÁ≠ñÁï•‰øùËØÅ‰∫ÜÊØèÊ¨°ÂàÜË£ÇÈÉΩÈÄâÊã©‰∫ÜÊúÄ‰ºòÁöÑÁâπÂæÅÔºå‰ª•ÊûÑÂª∫Âá∫Â∞ΩÂèØËÉΩÁÆÄÂçï‰∏îÊúâÊïàÁöÑÂÜ≥Á≠ñÊ†ë„ÄÇ&lt;/p>
&lt;p>ÂΩìË∞àÂà∞Âü∫‰∫éÂÜ≥Á≠ñÊ†ëÁöÑÈõÜÊàêÂ≠¶‰π†Êó∂ÔºåÈô§‰∫ÜÈöèÊú∫Ê£ÆÊûóÔºåËøòÊúâ‰∏ÄÁßçÈáçË¶ÅÁöÑÊñπÊ≥ïÊòØÊèêÂçáÊ†ëÔºàBoostingÔºâ„ÄÇÊèêÂçáÊ†ëÊòØ‰∏ÄÁßçËø≠‰ª£ÁöÑÈõÜÊàêÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøá‰∏≤Ë°åÂú∞ÊûÑÂª∫‰∏ÄÁ≥ªÂàóÂÜ≥Á≠ñÊ†ëÊù•ÈÄêÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ‰∏ãÈù¢Êàë‰ºöËØ¶ÁªÜ‰ªãÁªçÊèêÂçáÊ†ëÁöÑÂéüÁêÜÂíåÂÆûÁé∞ÊñπÂºè„ÄÇ&lt;/p>
&lt;h3 id="ÊèêÂçáÊ†ëÁöÑÂéüÁêÜ" >
&lt;div>
&lt;a href="#%e6%8f%90%e5%8d%87%e6%a0%91%e7%9a%84%e5%8e%9f%e7%90%86">
##
&lt;/a>
ÊèêÂçáÊ†ëÁöÑÂéüÁêÜÔºö
&lt;/div>
&lt;/h3>
&lt;h4 id="1-Âü∫Êú¨ÊÄùÊÉ≥" >
&lt;div>
&lt;a href="#1-%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3">
###
&lt;/a>
1. Âü∫Êú¨ÊÄùÊÉ≥Ôºö
&lt;/div>
&lt;/h4>
&lt;p>ÊèêÂçáÊ†ëÁöÑÂü∫Êú¨ÊÄùÊÉ≥ÊòØÈÄöËøáËÆ≠ÁªÉ‰∏ÄÁ≥ªÂàóÂº±Â≠¶‰π†Âô®ÔºàÈÄöÂ∏∏ÊòØÂÜ≥Á≠ñÊ†ëÔºâÔºåÁÑ∂ÂêéÂ∞ÜÂÆÉ‰ª¨ÁªÑÂêàËµ∑Êù•ÊûÑÊàê‰∏Ä‰∏™Êõ¥Âº∫Â§ßÁöÑÊ®°Âûã„ÄÇÊØè‰∏™Âº±Â≠¶‰π†Âô®ÈÉΩ‰∏ìÊ≥®‰∫éÁ∫†Ê≠£‰πãÂâçÊ®°ÂûãÁöÑÈîôËØØÔºåÂõ†Ê≠§Âú®ÊûÑÂª∫ËøáÁ®ã‰∏≠‰ºöÂÖ≥Ê≥®‰πãÂâçÊ®°ÂûãÈ¢ÑÊµãÈîôËØØÁöÑÊ†∑Êú¨„ÄÇ&lt;/p>
&lt;h4 id="2-ÁÆóÊ≥ïÊµÅÁ®ã" >
&lt;div>
&lt;a href="#2-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b">
###
&lt;/a>
2. ÁÆóÊ≥ïÊµÅÁ®ãÔºö
&lt;/div>
&lt;/h4>
&lt;p>ÊèêÂçáÊ†ëÁöÑÁÆóÊ≥ïÊµÅÁ®ãÂ¶Ç‰∏ãÔºö&lt;/p>
&lt;ol>
&lt;li>ÂàùÂßãÂåñÊ®°Âûã‰∏∫‰∏Ä‰∏™Â∏∏Êï∞ÂÄºÔºåÈÄöÂ∏∏‰∏∫ÁõÆÊ†áÂèòÈáèÁöÑÂùáÂÄºÔºàÂØπ‰∫éÂõûÂΩíÈóÆÈ¢òÔºâÊàñËÄÖÊòØÁ±ªÂà´ÁöÑÂÖàÈ™åÊ¶ÇÁéáÔºàÂØπ‰∫éÂàÜÁ±ªÈóÆÈ¢òÔºâ„ÄÇ&lt;/li>
&lt;li>Ëø≠‰ª£Âú∞ËÆ≠ÁªÉÂÜ≥Á≠ñÊ†ëÔºåÊØèÊ¨°ËÆ≠ÁªÉÈÉΩ‰ºöÁîüÊàê‰∏Ä‰∏™Êñ∞ÁöÑÂº±Â≠¶‰π†Âô®„ÄÇÂú®ÊØèÊ¨°Ëø≠‰ª£‰∏≠ÔºåÁÆóÊ≥ï‰ºöËÆ°ÁÆóÂΩìÂâçÊ®°ÂûãÁöÑÊÆãÂ∑ÆÔºàÂØπ‰∫éÂõûÂΩíÈóÆÈ¢òÔºâÊàñËÄÖÊ¢ØÂ∫¶ÔºàÂØπ‰∫éÂàÜÁ±ªÈóÆÈ¢òÔºâÔºåÁÑ∂ÂêéËÆ≠ÁªÉ‰∏Ä‰∏™Êñ∞ÁöÑÂÜ≥Á≠ñÊ†ëÊù•ÊãüÂêàËøô‰∫õÊÆãÂ∑ÆÊàñËÄÖÊ¢ØÂ∫¶„ÄÇ&lt;/li>
&lt;li>Â∞ÜÊñ∞ÁîüÊàêÁöÑÂÜ≥Á≠ñÊ†ëÂä†Âà∞Ê®°Âûã‰∏≠ÔºåÈÄöÂ∏∏‰ΩøÁî®‰∏Ä‰∏™ËæÉÂ∞èÁöÑÂ≠¶‰π†ÁéáÊù•ÁºìËß£ÊØèÊ£µÊ†ëÁöÑÂΩ±Âìç„ÄÇ&lt;/li>
&lt;li>ÈáçÂ§çËø≠‰ª£Ê≠•È™§2ÂíåÊ≠•È™§3ÔºåÁõ¥Âà∞ËææÂà∞È¢ÑÂÖàËÆæÂÆöÁöÑËø≠‰ª£Ê¨°Êï∞ÊàñËÄÖÊ®°ÂûãÁöÑÊÄßËÉΩËææÂà∞Êüê‰∏™ÈòàÂÄº‰∏∫Ê≠¢„ÄÇ&lt;/li>
&lt;/ol>
&lt;h4 id="3-Âä†Ê≥ïÊ®°Âûã" >
&lt;div>
&lt;a href="#3-%e5%8a%a0%e6%b3%95%e6%a8%a1%e5%9e%8b">
###
&lt;/a>
3. Âä†Ê≥ïÊ®°ÂûãÔºö
&lt;/div>
&lt;/h4>
&lt;p>ÊèêÂçáÊ†ëÁöÑÊúÄÁªàÊ®°ÂûãÊòØ‰∏Ä‰∏™Âä†Ê≥ïÊ®°ÂûãÔºåÂç≥Â§ö‰∏™Âº±Â≠¶‰π†Âô®ÁöÑÂä†ÊùÉÊ±ÇÂíå„ÄÇÈÄöËøáËø≠‰ª£ËÆ≠ÁªÉÔºåÊØè‰∏™Âº±Â≠¶‰π†Âô®ÈÉΩ‰ºöÂØπÊ®°ÂûãËøõË°å‰∏ÄÂÆöÁöÑ‰øÆÊ≠£ÔºåÊúÄÁªàÁªÑÂêàËµ∑Êù•ÊûÑÊàê‰∏Ä‰∏™Êõ¥Âº∫Â§ßÁöÑÊ®°Âûã„ÄÇ&lt;/p>
&lt;h3 id="ÂÆûÁé∞" >
&lt;div>
&lt;a href="#%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
ÂÆûÁé∞Ôºö
&lt;/div>
&lt;/h3>
&lt;p>ÊèêÂçáÊ†ëÁöÑÂÆûÁé∞ÈÄöÂ∏∏ÈááÁî®Ê¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÔºàGradient BoostingÔºâÔºåÂÖ∂‰∏≠ÊúÄÂ∏∏ËßÅÁöÑÊòØÊ¢ØÂ∫¶ÊèêÂçáÂÜ≥Á≠ñÊ†ëÔºàGradient Boosting Decision TreesÔºåGBDTÔºâ„ÄÇ&lt;/p>
&lt;p>GBDT ÁÆóÊ≥ïÁöÑÂÖ≥ÈîÆÊ≠•È™§ÂåÖÊã¨ËÆ°ÁÆóÊÆãÂ∑ÆÊàñËÄÖÊ¢ØÂ∫¶„ÄÅËÆ≠ÁªÉÂÜ≥Á≠ñÊ†ë‰ª•ÊãüÂêàÊÆãÂ∑ÆÊàñËÄÖÊ¢ØÂ∫¶„ÄÅÁ°ÆÂÆöÂ≠¶‰π†ÁéáÁ≠â„ÄÇGBDT ÈÄöËøá‰∏çÊñ≠Âú∞Ëø≠‰ª£ËÆ≠ÁªÉÂÜ≥Á≠ñÊ†ëÊù•ÈÄêÊ≠•‰ºòÂåñÊ®°ÂûãÔºåÁõ¥Âà∞ËææÂà∞‰∏ÄÂÆöÁöÑËø≠‰ª£Ê¨°Êï∞ÊàñËÄÖËææÂà∞‰∏ÄÂÆöÁöÑÊÄßËÉΩÊåáÊ†á„ÄÇ&lt;/p>
&lt;h3 id="ÊÄªÁªì-2" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-2">
##
&lt;/a>
ÊÄªÁªìÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÊèêÂçáÊ†ëÊòØ‰∏ÄÁßçÂü∫‰∫éÂÜ≥Á≠ñÊ†ëÁöÑÈõÜÊàêÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáËø≠‰ª£ËÆ≠ÁªÉ‰∏ÄÁ≥ªÂàóÂÜ≥Á≠ñÊ†ëÊù•ÈÄêÊ≠•ÊèêÂçáÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁõ∏ËæÉ‰∫éÈöèÊú∫Ê£ÆÊûóÔºåÊèêÂçáÊ†ëÈÄöÂ∏∏‰ºö‰∫ßÁîüÊõ¥Âä†Á≤æÁ°ÆÁöÑÈ¢ÑÊµãÔºå‰ΩÜÈúÄË¶ÅÊõ¥ÈïøÁöÑËÆ≠ÁªÉÊó∂Èó¥ÔºåÂπ∂‰∏îÂØπÂºÇÂ∏∏ÂÄºÂíåÂô™Â£∞Êï∞ÊçÆÊõ¥ÊïèÊÑü„ÄÇ&lt;/p>
&lt;h2 id="mlp-model" >
&lt;div>
&lt;a href="#mlp-model">
#
&lt;/a>
MLP model
&lt;/div>
&lt;/h2>
&lt;p>MLPÔºåÂç≥Â§öÂ±ÇÊÑüÁü•Âô®ÔºàMultilayer PerceptronÔºâÔºåÊòØ‰∏ÄÁßçÂü∫Êú¨ÁöÑ‰∫∫Â∑•Á•ûÁªèÁΩëÁªúÊ®°Âûã„ÄÇÂÆÉÁî±Â§öÂ±ÇÁ•ûÁªèÂÖÉÁªÑÊàêÔºåÊØè‰∏ÄÂ±ÇÈÉΩ‰∏é‰∏ã‰∏ÄÂ±ÇÂÖ®ËøûÊé•„ÄÇMLPÊòØ‰∏ÄÁßçÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÔºåÊÑèÂë≥ÁùÄ‰ø°ÊÅØÂè™ËÉΩ‰ªéËæìÂÖ•Â±ÇÂêëËæìÂá∫Â±Ç‰º†ÈÄíÔºå‰∏ç‰ºöÂ≠òÂú®Âæ™ÁéØËøûÊé•„ÄÇ&lt;/p>
&lt;p>‰∏ãÈù¢ÊòØMLPÁöÑ‰∏Ä‰∫õÂü∫Êú¨ÂéüÁêÜÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Á•ûÁªèÂÖÉÔºàPerceptronÔºâ&lt;/strong>Ôºö
Âú®MLP‰∏≠ÔºåÊØè‰∏™Á•ûÁªèÂÖÉÈÉΩÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑËÆ°ÁÆóÂçïÂÖÉ„ÄÇÂÆÉÊé•Êî∂Êù•Ëá™Ââç‰∏ÄÂ±ÇÁöÑËæìÂÖ•‰ø°Âè∑ÔºåÂ∞ÜËøô‰∫õ‰ø°Âè∑Âä†ÊùÉÊ±ÇÂíåÔºåÂπ∂ÈÄöËøáÊøÄÊ¥ªÂáΩÊï∞‰∫ßÁîüËæìÂá∫„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Â§öÂ±ÇÁªìÊûÑ&lt;/strong>Ôºö
MLPÁî±Â§ö‰∏™Â±ÇÁªÑÊàêÔºåÂÖ∏ÂûãÁöÑMLPÂåÖÊã¨ËæìÂÖ•Â±Ç„ÄÅËá≥Â∞ë‰∏Ä‰∏™ÈöêËóèÂ±ÇÂíåËæìÂá∫Â±Ç„ÄÇËæìÂÖ•Â±ÇÊé•Êî∂ÂéüÂßãÊï∞ÊçÆÔºåÈöêËóèÂ±ÇÂØπËæìÂÖ•Êï∞ÊçÆËøõË°åÁâπÂæÅÊèêÂèñÂíåËΩ¨Êç¢ÔºåËæìÂá∫Â±ÇÁîüÊàêÊúÄÁªàÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÊùÉÈáçÂíåÂÅèÁΩÆ&lt;/strong>Ôºö
Âú®MLP‰∏≠ÔºåÊØè‰∏™ËøûÊé•ÈÉΩÊúâ‰∏Ä‰∏™Áõ∏ÂÖ≥ËÅîÁöÑÊùÉÈáçÔºåÁî®‰∫éÊéßÂà∂‰ø°Âè∑‰º†ÈÄíÁöÑÂº∫Â∫¶ÂíåÊñπÂêë„ÄÇÊ≠§Â§ñÔºåÊØè‰∏™Á•ûÁªèÂÖÉËøòÊúâ‰∏Ä‰∏™ÂÅèÁΩÆÔºåÁî®‰∫éË∞ÉÊï¥Á•ûÁªèÂÖÉÁöÑÊøÄÊ¥ªÈòàÂÄº„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÊøÄÊ¥ªÂáΩÊï∞&lt;/strong>Ôºö
Âú®Á•ûÁªèÂÖÉ‰∏≠ÔºåÊøÄÊ¥ªÂáΩÊï∞ÂÜ≥ÂÆö‰∫ÜÁ•ûÁªèÂÖÉËæìÂá∫ÁöÑÈùûÁ∫øÊÄßÂÖ≥Á≥ª„ÄÇÂ∏∏Áî®ÁöÑÊøÄÊ¥ªÂáΩÊï∞ÂåÖÊã¨Sigmoid„ÄÅReLUÔºàRectified Linear UnitÔºâ„ÄÅtanhÁ≠âÔºåÂÆÉ‰ª¨Âú®‰∏çÂêåÊÉÖÂÜµ‰∏ãÂÖ∑Êúâ‰∏çÂêåÁöÑ‰ºòÂäø„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÂâçÂêë‰º†Êí≠&lt;/strong>Ôºö
Âú®MLP‰∏≠ÔºåÊï∞ÊçÆ‰ªéËæìÂÖ•Â±ÇÂºÄÂßã‰º†Êí≠ÔºåÁªèËøá‰∏ÄÁ≥ªÂàóÁöÑÂä†ÊùÉÊ±ÇÂíåÂíåÊøÄÊ¥ªÂáΩÊï∞Â§ÑÁêÜÔºå‰∏ÄÁõ¥‰º†Êí≠Âà∞ËæìÂá∫Â±ÇÔºåÁîüÊàêÊúÄÁªàÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÇËøô‰∏™ËøáÁ®ãÁß∞‰∏∫ÂâçÂêë‰º†Êí≠„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÂèçÂêë‰º†Êí≠&lt;/strong>Ôºö
ÂèçÂêë‰º†Êí≠ÊòØMLP‰∏≠Áî®‰∫éËÆ≠ÁªÉÊ®°ÂûãÁöÑÂÖ≥ÈîÆÊ≠•È™§„ÄÇÂÆÉÂà©Áî®Ê¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÔºåÈÄöËøáËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞ÂØπÊØè‰∏™ÂèÇÊï∞ÔºàÊùÉÈáçÂíåÂÅèÁΩÆÔºâÁöÑÊ¢ØÂ∫¶ÔºåÂπ∂Ê≤øÁùÄÊ¢ØÂ∫¶ÁöÑÂèçÊñπÂêëÊõ¥Êñ∞ÂèÇÊï∞Ôºå‰ªéËÄåÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÊçüÂ§±ÂáΩÊï∞&lt;/strong>Ôºö
ÊçüÂ§±ÂáΩÊï∞Áî®‰∫éË°°ÈáèÊ®°ÂûãÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÊ†áÁ≠æ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÂ∏∏ËßÅÁöÑÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨ÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâ„ÄÅ‰∫§ÂèâÁÜµÁ≠âÔºåÈÄâÊã©ÈÄÇÂΩìÁöÑÊçüÂ§±ÂáΩÊï∞ÂèñÂÜ≥‰∫éÂÖ∑‰ΩìÁöÑÈóÆÈ¢òÁ±ªÂûã„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>‰ºòÂåñÁÆóÊ≥ï&lt;/strong>Ôºö
Âú®ÂèçÂêë‰º†Êí≠ËøáÁ®ã‰∏≠ÔºåÈúÄË¶ÅÈÄâÊã©ÂêàÈÄÇÁöÑ‰ºòÂåñÁÆóÊ≥ïÊù•Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞„ÄÇÂ∏∏Áî®ÁöÑ‰ºòÂåñÁÆóÊ≥ïÂåÖÊã¨ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÔºàSGDÔºâ„ÄÅAdam„ÄÅRMSpropÁ≠â„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>ÊÄªÁöÑÊù•ËØ¥ÔºåMLPÈÄöËøáÂ§öÂ±ÇÁöÑÈùûÁ∫øÊÄßÂèòÊç¢Êù•Â≠¶‰π†ËæìÂÖ•Êï∞ÊçÆÁöÑÂ§çÊùÇÁâπÂæÅË°®Á§∫ÔºåÈÄöËøáÂèçÂêë‰º†Êí≠ÁÆóÊ≥ï‰∏çÊñ≠Ë∞ÉÊï¥Ê®°ÂûãÂèÇÊï∞‰ª•ÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞Ôºå‰ªéËÄåÂÆûÁé∞ÂØπÊï∞ÊçÆÁöÑÂàÜÁ±ª„ÄÅÂõûÂΩíÁ≠â‰ªªÂä°„ÄÇ&lt;/p>
&lt;p>ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÊòØÁî®‰∫éËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÁöÑÂÖ≥ÈîÆÁÆóÊ≥ï‰πã‰∏ÄÔºåÂÆÉÈÄöËøáËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞ÂØπÊØè‰∏™ÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶Êù•Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞Ôºå‰ªéËÄå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÈÄêÊ∏ê‰ºòÂåñ‰ª•ËææÂà∞ÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ‰∏ãÈù¢ÊàëÂ∞Ü‰ªãÁªçÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÁöÑËÆ°ÁÆóËøáÁ®ãÔºå‰ª•ÂèäÈìæÂºèÊ≥ïÂàôÂ¶Ç‰ΩïÁî®Êù•ËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ&lt;/p>
&lt;h3 id="ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÁöÑËÆ°ÁÆóËøáÁ®ã" >
&lt;div>
&lt;a href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e7%9a%84%e8%ae%a1%e7%ae%97%e8%bf%87%e7%a8%8b">
##
&lt;/a>
ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÁöÑËÆ°ÁÆóËøáÁ®ãÔºö
&lt;/div>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ÂâçÂêë‰º†Êí≠&lt;/strong>Ôºö
È¶ñÂÖàÔºåÈÄöËøáÂâçÂêë‰º†Êí≠ËÆ°ÁÆóÊ®°ÂûãÁöÑËæìÂá∫„ÄÇÂ∞ÜËæìÂÖ•Êï∞ÊçÆËæìÂÖ•Âà∞ÁΩëÁªú‰∏≠ÔºåÊåâÁÖßÁΩëÁªúÁªìÊûÑÈÄêÂ±ÇËÆ°ÁÆóÊØè‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂá∫ÔºåÂπ∂Â∞ÜËæìÂá∫‰º†ÈÄíÁªô‰∏ã‰∏ÄÂ±ÇÔºåÁõ¥Ëá≥ÁîüÊàêÊúÄÁªàÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ËÆ°ÁÆóÊçüÂ§±&lt;/strong>Ôºö
‰ΩøÁî®ÊçüÂ§±ÂáΩÊï∞ËÆ°ÁÆóÊ®°ÂûãÁöÑÈ¢ÑÊµãÂÄº‰∏éÁúüÂÆûÊ†áÁ≠æ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂæóÂà∞ÊçüÂ§±ÂÄº„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÂèçÂêë‰º†Êí≠&lt;/strong>Ôºö
‰ªéËæìÂá∫Â±ÇÂºÄÂßãÔºåÂà©Áî®ÈìæÂºèÊ≥ïÂàôÈÄêÂ±ÇËÆ°ÁÆóÊØè‰∏™ÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶„ÄÇÊ¢ØÂ∫¶Ë°®Á§∫‰∫ÜÊçüÂ§±ÂáΩÊï∞ÂØπÂèÇÊï∞ÁöÑÂèòÂåñÁéáÔºåÂÆÉÂëäËØâÊàë‰ª¨Â¶Ç‰ΩïË∞ÉÊï¥ÂèÇÊï∞ÊâçËÉΩ‰ΩøÊçüÂ§±ÂáΩÊï∞ÊúÄÂ∞èÂåñ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÂèÇÊï∞Êõ¥Êñ∞&lt;/strong>Ôºö
Ê†πÊçÆËÆ°ÁÆóÂæóÂà∞ÁöÑÊ¢ØÂ∫¶ÔºåÂà©Áî®‰ºòÂåñÁÆóÊ≥ïÔºàÂ¶ÇÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÔºâÊù•Êõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞„ÄÇÈÄöÂ∏∏ÔºåÂèÇÊï∞Êõ¥Êñ∞ÁöÑÊ≠•ÈïøÔºàÂ≠¶‰π†ÁéáÔºâÊòØ‰∏Ä‰∏™Ë∂ÖÂèÇÊï∞ÔºåÈúÄË¶ÅÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµËøõË°åË∞ÉÊï¥„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="ÈìæÂºèÊ≥ïÂàôÂ¶Ç‰ΩïÁî®Êù•ËÆ°ÁÆóÊ¢ØÂ∫¶" >
&lt;div>
&lt;a href="#%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99%e5%a6%82%e4%bd%95%e7%94%a8%e6%9d%a5%e8%ae%a1%e7%ae%97%e6%a2%af%e5%ba%a6">
##
&lt;/a>
ÈìæÂºèÊ≥ïÂàôÂ¶Ç‰ΩïÁî®Êù•ËÆ°ÁÆóÊ¢ØÂ∫¶Ôºö
&lt;/div>
&lt;/h3>
&lt;p>ÈìæÂºèÊ≥ïÂàôÊòØÂæÆÁßØÂàÜ‰∏≠ÁöÑÂü∫Êú¨ÂéüÁêÜÔºåÁî®‰∫éËÆ°ÁÆóÂ§çÂêàÂáΩÊï∞ÁöÑÂØºÊï∞„ÄÇÂú®ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ï‰∏≠ÔºåÊàë‰ª¨Âà©Áî®ÈìæÂºèÊ≥ïÂàôÊù•ËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞ÂØπÊ®°ÂûãÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶Ôºå‰ªéËÄåÂÆûÁé∞ÂèÇÊï∞ÁöÑÊõ¥Êñ∞„ÄÇ&lt;/p>
&lt;p>ÂÅáËÆæÊúâ‰∏Ä‰∏™Â§çÂêàÂáΩÊï∞ $z = f(g(x))$ÔºåÂÖ∂‰∏≠ $x$ ÊòØËæìÂÖ•Ôºå$g(x)$ ÊòØ‰∏Ä‰∏™ÂáΩÊï∞Ôºå$f(x)$ ÊòØÂè¶‰∏Ä‰∏™ÂáΩÊï∞„ÄÇÊ†πÊçÆÈìæÂºèÊ≥ïÂàôÔºå$z$ ÂØπ $x$ ÁöÑÂØºÊï∞ÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö&lt;/p>
&lt;p>$$
\frac{dz}{dx} = \frac{dz}{dg} \cdot \frac{dg}{dx}
$$&lt;/p>
&lt;p>Âú®Á•ûÁªèÁΩëÁªú‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜÊçüÂ§±ÂáΩÊï∞ $L$ ËßÜ‰∏∫ $z$ÔºåÊ®°ÂûãÁöÑÂèÇÊï∞ËßÜ‰∏∫ $x$ÔºåÂâçÂêë‰º†Êí≠ËøáÁ®ã‰∏≠ÁöÑÊØè‰∏ÄÂ±ÇËæìÂá∫ËßÜ‰∏∫ $g$ÔºåÊøÄÊ¥ªÂáΩÊï∞ËßÜ‰∏∫ $f$„ÄÇ
Âà©Áî®ÈìæÂºèÊ≥ïÂàôÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄêÂ±ÇËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞ÂØπÊØè‰∏™ÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶ÔºåÁÑ∂ÂêéÊ†πÊçÆ‰ºòÂåñÁÆóÊ≥ïÊõ¥Êñ∞ÂèÇÊï∞Ôºå‰ΩøÂæóÊçüÂ§±ÂáΩÊï∞ÈÄêÊ∏êÂáèÂ∞è„ÄÇ&lt;/p>
&lt;p>ÊÄªÁöÑÊù•ËØ¥ÔºåÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÈÄöËøáÂà©Áî®ÈìæÂºèÊ≥ïÂàôËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞ÂØπÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶ÔºåÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÁ•ûÁªèÁΩëÁªúËÆ≠ÁªÉËøáÁ®ãÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üËá™Âä®Â≠¶‰π†Â§çÊùÇÁöÑÊï∞ÊçÆË°®Á§∫„ÄÇ&lt;/p>
&lt;p>‰∫§ÂèâÁÜµÔºàCross-EntropyÔºâÊòØÂú®ÂàÜÁ±ª‰ªªÂä°‰∏≠Â∏∏Áî®ÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁâπÂà´ÊòØÂú®Â§öÂàÜÁ±ª‰ªªÂä°‰∏≠„ÄÇÂÆÉÁöÑÂéüÁêÜÂíå‰∏∫‰ªÄ‰πàÊúâÁî®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂá†ÁÇπËøõË°åËØ¶ÁªÜËß£ÈáäÔºö&lt;/p>
&lt;h3 id="1-ÊçüÂ§±ÂáΩÊï∞ÁöÑ‰ΩúÁî®" >
&lt;div>
&lt;a href="#1-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e7%9a%84%e4%bd%9c%e7%94%a8">
##
&lt;/a>
1. ÊçüÂ§±ÂáΩÊï∞ÁöÑ‰ΩúÁî®Ôºö
&lt;/div>
&lt;/h3>
&lt;p>ÊçüÂ§±ÂáΩÊï∞Áî®‰∫éË°°ÈáèÊ®°ÂûãÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÊ†áÁ≠æ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ‰ºòÂåñÊ®°ÂûãÁöÑÁõÆÊ†áÊòØÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§ü‰∫ßÁîü‰∏éÁúüÂÆûÊ†áÁ≠æÁõ∏ÂåπÈÖçÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÇ&lt;/p>
&lt;h3 id="2-‰∫§ÂèâÁÜµÁöÑÂÆö‰πâ" >
&lt;div>
&lt;a href="#2-%e4%ba%a4%e5%8f%89%e7%86%b5%e7%9a%84%e5%ae%9a%e4%b9%89">
##
&lt;/a>
2. ‰∫§ÂèâÁÜµÁöÑÂÆö‰πâÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÂØπ‰∫éÂ§öÂàÜÁ±ª‰ªªÂä°Ôºå‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÁöÑÊï∞Â≠¶ÂÆö‰πâÂ¶Ç‰∏ãÔºö&lt;/p>
&lt;p>$$
\text{Cross-Entropy}(y, \hat{y}) = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠Ôºå$y$ ÊòØÁúüÂÆûÊ†áÁ≠æÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºàÈÄöÂ∏∏ÊòØ‰∏Ä‰∏™one-hotÁºñÁ†ÅÁöÑÂêëÈáèÔºâÔºå$\hat{y}$ ÊòØÊ®°ÂûãÈ¢ÑÊµãÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºå$N$ ÊòØÁ±ªÂà´ÁöÑÊï∞Èáè„ÄÇ
ËØ•ÊçüÂ§±ÂáΩÊï∞Áî®‰∫éË°°ÈáèÁúüÂÆûÊ†áÁ≠æ‰∏éÊ®°ÂûãÈ¢ÑÊµãÁªìÊûú‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">binary_cross_entropy&lt;/span>(y_true, y_pred):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_true &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_true)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Clip predictions to avoid log(0)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>clip(y_pred, &lt;span style="color:#ff9f43">1e-15&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> &lt;span style="color:#ff9f43">1e-15&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cross_entropy &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span>np&lt;span style="color:#ff6ac1">.&lt;/span>mean(y_true &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(y_pred) &lt;span style="color:#ff6ac1">+&lt;/span> (&lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> y_true) &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(&lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> y_pred))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> cross_entropy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_true &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#ff9f43">0.9&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>, &lt;span style="color:#ff9f43">0.8&lt;/span>, &lt;span style="color:#ff9f43">0.7&lt;/span>, &lt;span style="color:#ff9f43">0.2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(binary_cross_entropy(y_true, y_pred))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">multiclass_cross_entropy&lt;/span>(y_true, y_pred):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_true &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_true)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Clip predictions to avoid log(0)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>clip(y_pred, &lt;span style="color:#ff9f43">1e-15&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> &lt;span style="color:#ff9f43">1e-15&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cross_entropy &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span>np&lt;span style="color:#ff6ac1">.&lt;/span>mean(np&lt;span style="color:#ff6ac1">.&lt;/span>sum(y_true &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(y_pred), axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> cross_entropy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_true &lt;span style="color:#ff6ac1">=&lt;/span> [[&lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>], [&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>], [&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>]] &lt;span style="color:#78787e"># how to get these? One-hot encoding.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> [[&lt;span style="color:#ff9f43">0.9&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>], [&lt;span style="color:#ff9f43">0.1&lt;/span>, &lt;span style="color:#ff9f43">0.8&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>], [&lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.9&lt;/span>]] &lt;span style="color:#78787e"># how to get these? Softmax.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(multiclass_cross_entropy(y_true, y_pred))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Softmax: Get the y_pred from model output raw scores (i.e., logits)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">softmax&lt;/span>(logits):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exp_logits &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>exp(logits &lt;span style="color:#ff6ac1">-&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>max(logits, axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, keepdims&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> exp_logits &lt;span style="color:#ff6ac1">/&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>sum(exp_logits, axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, keepdims&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example logits (raw scores) outputted by a model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logits &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([[&lt;span style="color:#ff9f43">2.0&lt;/span>, &lt;span style="color:#ff9f43">1.0&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ff9f43">0.5&lt;/span>, &lt;span style="color:#ff9f43">2.5&lt;/span>, &lt;span style="color:#ff9f43">1.5&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ff9f43">1.2&lt;/span>, &lt;span style="color:#ff9f43">0.7&lt;/span>, &lt;span style="color:#ff9f43">2.1&lt;/span>]])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Compute the softmax probabilities&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> softmax(logits)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-ÂéüÁêÜËß£Èáä" >
&lt;div>
&lt;a href="#3-%e5%8e%9f%e7%90%86%e8%a7%a3%e9%87%8a">
##
&lt;/a>
3. ÂéüÁêÜËß£ÈáäÔºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>‰ø°ÊÅØËÆ∫ËßíÂ∫¶&lt;/strong>Ôºö
‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Ê∫êËá™‰ø°ÊÅØËÆ∫‰∏≠ÁöÑ‰ø°ÊÅØÁÜµÊ¶ÇÂøµ„ÄÇ‰ø°ÊÅØÁÜµÁî®‰∫éË°°Èáè‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáèÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºåËÄå‰∫§ÂèâÁÜµÂàôË°°Èáè‰∏§‰∏™Ê¶ÇÁéáÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÂΩìÁúüÂÆûÊ†áÁ≠æÂíåÊ®°ÂûãÈ¢ÑÊµãÁöÑÂàÜÂ∏ÉË∂äÊé•ËøëÊó∂Ôºå‰∫§ÂèâÁÜµË∂äÂ∞è„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ê¢ØÂ∫¶‰∏ãÈôç‰ºòÂåñ&lt;/strong>Ôºö
‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Âú®Ê¢ØÂ∫¶‰∏ãÈôç‰ºòÂåñËøáÁ®ã‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊÄßË¥®„ÄÇÂÆÉÁöÑÂØºÊï∞Áõ∏ÂØπÁÆÄÂçïÔºåËÆ°ÁÆóËµ∑Êù•Êõ¥Âä†È´òÊïàÔºåËÄå‰∏îÂΩìÊ®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÊ†áÁ≠æÁöÑÂ∑ÆÂºÇËæÉÂ§ßÊó∂ÔºåÊ¢ØÂ∫¶‰πü‰ºöÂèòÂæóÊõ¥Â§ßÔºå‰ªéËÄåÂä†ÈÄüÊ®°ÂûãÂèÇÊï∞ÁöÑÊõ¥Êñ∞„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÈÄÇÁî®‰∫éÂ§öÂàÜÁ±ª‰ªªÂä°&lt;/strong>Ôºö
‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÁâπÂà´ÈÄÇÁî®‰∫éÂ§öÂàÜÁ±ª‰ªªÂä°ÔºåÂõ†‰∏∫ÂÆÉËÉΩÂ§üÊúâÊïàÂú∞Ë°°ÈáèÂ§ö‰∏™Á±ªÂà´‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂπ∂‰∏îÂú®Ê®°Âûã‰ºòÂåñËøáÁ®ã‰∏≠ËÉΩÂ§üÂºïÂØºÊ®°ÂûãÊõ¥Âø´Âú∞Êî∂ÊïõÂà∞ÊúÄ‰ºòËß£„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="4-‰∏∫‰ªÄ‰πàÊúâÁî®" >
&lt;div>
&lt;a href="#4-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e7%94%a8">
##
&lt;/a>
4. ‰∏∫‰ªÄ‰πàÊúâÁî®Ôºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Ê¢ØÂ∫¶‰ø°ÊÅØ&lt;/strong>Ôºö
‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Êèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÊ¢ØÂ∫¶‰ø°ÊÅØÔºå‰ΩøÂæóÊ®°ÂûãÂèØ‰ª•Êõ¥Âø´Âú∞Â≠¶‰π†Âà∞Ê≠£Á°ÆÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÈÄÇÁî®ÊÄßÂπøÊ≥õ&lt;/strong>Ôºö
‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÈÄÇÁî®‰∫éÂ§öÂàÜÁ±ª‰ªªÂä°ÔºåÂπ∂‰∏îÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÂõ†Ê≠§Êàê‰∏∫‰∫ÜËÆ∏Â§öÂàÜÁ±ª‰ªªÂä°ÁöÑÈ¶ñÈÄâÊçüÂ§±ÂáΩÊï∞„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>‰∏éÊ¶ÇÁéáÁõ∏ÂÖ≥&lt;/strong>Ôºö
‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞Áõ¥Êé•‰∏éÊ¶ÇÁéáÂàÜÂ∏ÉÁõ∏ÂÖ≥ÔºåÊõ¥Á¨¶Âêà‰ªªÂä°ÁöÑÊú¨Ë¥®ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊåáÂØºÊ®°ÂûãÂ≠¶‰π†Êï∞ÊçÆÁöÑÂàÜÂ∏ÉÊÉÖÂÜµ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ÊÄªÁöÑÊù•ËØ¥Ôºå‰∫§ÂèâÁÜµÊçüÂ§±ÂáΩÊï∞ÈÄöËøáË°°ÈáèÊ®°ÂûãÈ¢ÑÊµãÁªìÊûú‰∏éÁúüÂÆûÊ†áÁ≠æ‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÊèê‰æõ‰∫ÜÊúâÊïàÁöÑ‰ºòÂåñÁõÆÊ†áÔºåÂπ∂Âú®Ê¢ØÂ∫¶‰∏ãÈôç‰ºòÂåñËøáÁ®ã‰∏≠Ëµ∑Âà∞ÈáçË¶Å‰ΩúÁî®ÔºåÂõ†Ê≠§Ë¢´ÂπøÊ≥õÂ∫îÁî®‰∫éÂàÜÁ±ª‰ªªÂä°‰∏≠„ÄÇ&lt;/p>
&lt;h2 id="bertroberta-model" >
&lt;div>
&lt;a href="#bertroberta-model">
#
&lt;/a>
BERT/RoBERTa model
&lt;/div>
&lt;/h2>
&lt;p>BERT(Bidirectional Encoder Representations from Transformers):
BERTÊòØ‰∏ÄÁßçÂèåÂêëÁöÑÔºàBidirectionalÔºâÊ®°ÂûãÔºåËøôÊÑèÂë≥ÁùÄÂÆÉËÉΩÂ§üÂêåÊó∂ËÄÉËôëÂà∞‰∏Ä‰∏™ÂçïËØçÂ∑¶ËæπÂíåÂè≥ËæπÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇËøô‰ΩøÂæóBERTÂú®ÁêÜËß£Âè•Â≠êËØ≠Â¢ÉÊó∂ÊØî‰πãÂâçÁöÑÊ®°ÂûãÊõ¥‰∏∫Âº∫Â§ß„ÄÇ
BERTÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉËøáÁ®ãÊòØÈÄöËøáÊé©ÁõñËæìÂÖ•ÊñáÊú¨‰∏≠ÁöÑ‰∏ÄÈÉ®ÂàÜËØçÊ±áÔºàMasked Language ModelÔºåMLMÔºâÂíåÈ¢ÑÊµãÂè•Â≠êÊòØÂê¶ËøûÁª≠ÔºàNext Sentence PredictionÔºåNSPÔºâÊù•ÂÆåÊàêÁöÑ„ÄÇ
BERT‰ª•‚Äútransformer‚Äù‰∏∫Âü∫Á°ÄÔºåËøôÊòØ‰∏ÄÁßçËá™Ê≥®ÊÑèÂäõÔºàself-attentionÔºâÊú∫Âà∂ÁöÑÁ•ûÁªèÁΩëÁªúÁªìÊûÑÔºåÂÆÉËÉΩÂ§üÂú®ËÄÉËôëÂà∞ËæìÂÖ•Â∫èÂàóÁöÑÊâÄÊúâ‰ΩçÁΩÆ‰πãÈó¥Âª∫Á´ãÂÖ≥ËÅîÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£‰∏ä‰∏ãÊñá„ÄÇ&lt;/p>
&lt;p>RoBERTaÊòØFacebook AIÊèêÂá∫ÁöÑ‰∏ÄÁßçÊîπËøõÁöÑÈ¢ÑËÆ≠ÁªÉËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâÊ®°ÂûãÔºåÂÆÉÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂª∫Á´ãÂú®BERTÁöÑÂü∫Á°Ä‰∏äÔºå‰ΩÜÈÄöËøá‰∏ÄÁ≥ªÂàóÁöÑÊîπËøõÔºå‰ΩøÂÖ∂Âú®Â§ö‰∏™NLP‰ªªÂä°‰∏äË°®Áé∞Êõ¥‰ºòÁßÄ„ÄÇ&lt;/p>
&lt;p>RoBERTaÁöÑ‰∏ªË¶ÅÊîπËøõÂåÖÊã¨Ôºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Âä®ÊÄÅÊé©Á†ÅÁ≠ñÁï•ÔºàDynamic MaskingÔºâÔºöRoBERTaÂú®È¢ÑËÆ≠ÁªÉÊó∂ÈááÁî®‰∫ÜÂä®ÊÄÅÊé©Á†ÅÁ≠ñÁï•ÔºåÂç≥Âú®ÊØè‰∏™ËÆ≠ÁªÉËø≠‰ª£‰∏≠ÂØπËæìÂÖ•Âè•Â≠êËøõË°åÈöèÊú∫ÂåñÂ§ÑÁêÜÔºåËÄå‰∏çÊòØÂõ∫ÂÆöÂú∞Âú®Âè•Â≠ê‰∏≠ÈöèÊú∫Êé©Á†Å„ÄÇËøô‰ΩøÂæóÊ®°ÂûãÊõ¥Â•ΩÂú∞Â≠¶‰π†Âè•Â≠ê‰∏≠ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Êõ¥ÈïøÁöÑËÆ≠ÁªÉÊó∂Èó¥ÂíåÊõ¥Â§ßÁöÑÊâπÊ¨°Â§ßÂ∞èÔºöRoBERTa‰ΩøÁî®‰∫ÜÊõ¥Â§ßÁöÑÊâπÊ¨°Â§ßÂ∞èÂíåÊõ¥ÈïøÁöÑËÆ≠ÁªÉÊó∂Èó¥Ôºå‰ª•ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÂéªÈô§NSPÔºàNext Sentence PredictionÔºâ‰ªªÂä°ÔºöRoBERTa‰∏çÂÜç‰ΩøÁî®BERT‰∏≠ÁöÑNSP‰ªªÂä°ÔºåËÄåÊòØ‰∏ìÊ≥®‰∫éMLMÔºàMasked Language ModelÔºâ‰ªªÂä°ÔºåËøô‰ΩøÂæóÊ®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£ËæìÂÖ•ÊñáÊú¨„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Êõ¥Â§öÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºöRoBERTa‰ΩøÁî®‰∫ÜÊõ¥Â§öÁöÑÊñáÊú¨Êï∞ÊçÆÊù•ËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåËøôÊúâÂä©‰∫éÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ÊÄªÁöÑÊù•ËØ¥ÔºåRoBERTaÊòØÂØπBERTÊ®°ÂûãÁöÑ‰∏ÄÁßç‰ºòÂåñÂíåÊîπËøõÔºåÂÆÉÂú®Â§ö‰∏™NLP‰ªªÂä°‰∏äÈÉΩÂèñÂæó‰∫ÜÊØîBERTÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;p>&lt;strong>Êõ¥Â§öÁªÜËäÇÔºåÂ∫îËØ•ÂéªÊü•ÊâæÂè∞Â§ßÊùéÂÆèÊØÖËÄÅÂ∏àÁöÑËØæÁ®ãÔºåÈùûÂ∏∏Á≤æÂΩ©ÁöÑËÆ≤Ëß£„ÄÇ&lt;/strong>&lt;/p>
&lt;h2 id="faiss-indexivfpq" >
&lt;div>
&lt;a href="#faiss-indexivfpq">
#
&lt;/a>
FAISS (indexIVFPQ)
&lt;/div>
&lt;/h2>
&lt;p>Âú®ÊàëÁöÑÂè¶‰∏ÄÁØáÂçöÂÆ¢ÂçïÁã¨Êúâ‰ªãÁªçÔºåÂπ∂Â∏¶Êúâ‰ª£Á†ÅÁ§∫‰æã„ÄÇ&lt;/p>
&lt;h2 id="bertopic" >
&lt;div>
&lt;a href="#bertopic">
#
&lt;/a>
BERTopic
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>clustering&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>UMAP/PCA&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>c-TF-IDF&lt;/li>
&lt;/ul>
&lt;p>BERTopic ÊòØ‰∏Ä‰∏™Âü∫‰∫éBERTÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÔºåÁî®‰∫é‰∏ªÈ¢òÂª∫Ê®°‰ªªÂä°„ÄÇBERTopicÁªìÂêà‰∫ÜBERTÁöÑÂº∫Â§ßË°®Á§∫Â≠¶‰π†ËÉΩÂäõÂíå‰∏ªÈ¢òÂª∫Ê®°ÁöÑÊÄùÊÉ≥ÔºåËÉΩÂ§üÂú®Â§ßËßÑÊ®°ÊñáÊú¨Êï∞ÊçÆ‰∏äÂø´ÈÄü„ÄÅÂáÜÁ°ÆÂú∞ÊèêÂèñ‰∏ªÈ¢ò‰ø°ÊÅØ„ÄÇ&lt;/p>
&lt;p>BERTopicÁöÑÂ∑•‰ΩúÂéüÁêÜÂ¶Ç‰∏ãÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ÊñáÊú¨ÂêëÈáèÂåñ&lt;/strong>ÔºöÈ¶ñÂÖàÔºåBERTopic‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑBERTÊ®°ÂûãÊù•Â∞ÜËæìÂÖ•ÊñáÊú¨ËΩ¨Êç¢‰∏∫È´òÁª¥ÂêëÈáèË°®Á§∫„ÄÇËøô‰∫õÂêëÈáèÊçïÊçâ‰∫ÜËæìÂÖ•ÊñáÊú¨ÁöÑËØ≠‰πâ‰ø°ÊÅØÔºåÂπ∂‰∏îÈÄöÂ∏∏ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÂèçÊò†ÊñáÊú¨‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>‰∏ªÈ¢òÂèëÁé∞&lt;/strong>ÔºöÊé•‰∏ãÊù•ÔºåBERTopic‰ΩøÁî®ËÅöÁ±ªÁÆóÊ≥ïÔºà‰æãÂ¶ÇDBSCANÊàñHDBSCANÔºâÂØπÊñáÊú¨ÂêëÈáèËøõË°åËÅöÁ±ªÔºå‰ª•ÂèëÁé∞ÊΩúÂú®ÁöÑ‰∏ªÈ¢ò„ÄÇËÅöÁ±ªÁÆóÊ≥ïÂ∞ÜÊñáÊú¨ÂêëÈáèÂàÜÁªÑ‰∏∫ÂÖ∑ÊúâÁõ∏‰ºº‰∏ªÈ¢òÁöÑÁ∞á„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>‰∏ªÈ¢òÂÖ≥ÈîÆËØçÊèêÂèñ&lt;/strong>ÔºöÂØπ‰∫éÊØè‰∏™ÂèëÁé∞ÁöÑ‰∏ªÈ¢òÁ∞áÔºåBERTopicËøòÂèØ‰ª•ÊèêÂèñÂÖ≥ÈîÆËØçÊù•ÊèèËø∞ËØ•‰∏ªÈ¢ò„ÄÇËøô‰∫õÂÖ≥ÈîÆËØçÈÄöÂ∏∏ÊòØÁ∞á‰∏≠ÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑËØçËØ≠ÔºåÂ∏ÆÂä©Áî®Êà∑ÁêÜËß£‰∏ªÈ¢òÁöÑÂÜÖÂÆπ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>‰∏ªÈ¢òÂèØËßÜÂåñ&lt;/strong>ÔºöÊúÄÂêéÔºåBERTopicÂèØ‰ª•Â∞ÜÂèëÁé∞ÁöÑ‰∏ªÈ¢òÂèØËßÜÂåñÔºå‰ΩøÁî®Êà∑ËÉΩÂ§üÁõ¥ËßÇÂú∞‰∫ÜËß£ÊñáÊú¨Êï∞ÊçÆ‰∏≠ÁöÑ‰∏ªÈ¢òÁªìÊûÑ„ÄÇÈÄöÂ∏∏ÔºåÂèØËßÜÂåñÁªìÊûú‰ºö‰ª•Á∞áÁöÑÂΩ¢ÂºèÂ±ïÁ§∫ÔºåÊØè‰∏™Á∞á‰ª£Ë°®‰∏Ä‰∏™‰∏ªÈ¢òÔºåÁ∞áÂÜÖÁöÑÊñáÊú¨ÂàôË°®Á§∫ËØ•‰∏ªÈ¢òÁöÑÂÖ∑‰ΩìÂÜÖÂÆπ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>ÂÖ≥‰∫éÊúÄ‰Ω≥ÂÆûË∑µÔºå‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂª∫ËÆÆÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Ë∞ÉÊï¥Ê®°ÂûãÂèÇÊï∞&lt;/strong>ÔºöÊ†πÊçÆ‰ªªÂä°ÈúÄÊ±ÇÂíåÊï∞ÊçÆÁâπÁÇπÔºåË∞ÉÊï¥BERTopicÁöÑÂèÇÊï∞Ôºå‰æãÂ¶ÇËÅöÁ±ªÁÆóÊ≥ïÁöÑÂèÇÊï∞„ÄÅ‰∏ªÈ¢òÊï∞ÈáèÁ≠âÔºå‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>È¢ÑÂ§ÑÁêÜÊñáÊú¨Êï∞ÊçÆ&lt;/strong>ÔºöÂú®‰ΩøÁî®BERTopic‰πãÂâçÔºåÂØπÊñáÊú¨Êï∞ÊçÆËøõË°åÈÄÇÂΩìÁöÑÈ¢ÑÂ§ÑÁêÜÊòØÂæàÈáçË¶ÅÁöÑÔºå‰æãÂ¶ÇÂéªÈô§ÂÅúÁî®ËØç„ÄÅËøõË°åËØçÂπ≤ÂåñÊàñËØçÂΩ¢ËøòÂéüÁ≠âÔºå‰ª•ÂáèÂ∞ëÂô™Èü≥ÂØπ‰∏ªÈ¢òÂª∫Ê®°ÁöÑÂΩ±Âìç„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÁêÜËß£‰∏ªÈ¢òÁªìÊûú&lt;/strong>ÔºöÂØπ‰∫éÊØè‰∏™ÂèëÁé∞ÁöÑ‰∏ªÈ¢òÔºå‰ªîÁªÜÊü•Áúã‰∏ªÈ¢òÁ∞á‰∏≠ÁöÑÊñáÊú¨ÔºåÂπ∂ËÄÉËôëÂÆÉ‰ª¨‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÂíåÂÖ±ÊÄßÔºå‰ª•Á°Æ‰øù‰∏ªÈ¢òÁöÑÂêàÁêÜÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>‰∏éÈ¢ÜÂüüÁü•ËØÜÁªìÂêà&lt;/strong>ÔºöÂú®Ëß£ÈáäÂíåÂà©Áî®‰∏ªÈ¢òÁªìÊûúÊó∂ÔºåÁªìÂêàÈ¢ÜÂüüÁü•ËØÜ‰ºöÊõ¥ÊúâÂ∏ÆÂä©„ÄÇÈÄöËøáÊ∑±ÂÖ•‰∫ÜËß£È¢ÜÂüü‰∏ì‰∏öÊúØËØ≠ÂíåÁõ∏ÂÖ≥Ê¶ÇÂøµÔºåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ÁêÜËß£ÂíåËß£Èáä‰∏ªÈ¢òÁöÑÂê´‰πâ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>ÈÄöËøáÂêàÁêÜÂú∞‰ΩøÁî®BERTopicÂ∑•ÂÖ∑ÔºåÂπ∂ÁªìÂêàÈÄÇÂΩìÁöÑÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂíåÊ®°ÂûãË∞É‰ºòÔºåÂèØ‰ª•ÊúâÊïàÂú∞ÂÆåÊàêÂü∫‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑ‰∏ªÈ¢òÂª∫Ê®°‰ªªÂä°„ÄÇ&lt;/p>
&lt;p>ÂΩìÁÑ∂ÔºåËÆ©ÊàëÊõ¥ËØ¶ÁªÜÂú∞Ëß£Èáä‰∏Ä‰∏ãËøô‰∫õÊäÄÊúØ„ÄÇ&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>UMAPÔºàUniform Manifold Approximation and ProjectionÔºâ&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>UMAPÊòØ‰∏ÄÁßçÈùûÁ∫øÊÄßÈôçÁª¥ÁÆóÊ≥ïÔºåÊó®Âú®Â∞ÜÈ´òÁª¥Êï∞ÊçÆÊò†Â∞ÑÂà∞‰ΩéÁª¥Á©∫Èó¥‰ª•ËøõË°åÂèØËßÜÂåñÊàñËøõ‰∏ÄÊ≠•ÂàÜÊûê„ÄÇ&lt;/li>
&lt;li>UMAPÁõ∏ÂØπ‰∫é‰º†ÁªüÁöÑÈôçÁª¥ÊäÄÊúØÔºàÂ¶Çt-SNEÔºâÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄßÂíå‰øùÊåÅÂÖ®Â±ÄÊï∞ÊçÆÁªìÊûÑÁöÑËÉΩÂäõ„ÄÇÂÆÉËÉΩÂ§ü‰øùÊåÅÊõ¥Â§öÁöÑÂ±ÄÈÉ®ÁªìÊûÑÔºåÂêåÊó∂Âú®Â§ßËßÑÊ®°Êï∞ÊçÆ‰∏äÁöÑËÆ°ÁÆóÊïàÁéáÊõ¥È´ò„ÄÇ&lt;/li>
&lt;li>Âú®BERTopic‰∏≠ÔºåUMAPÂ∏∏Áî®‰∫éÂØπBERTÂêëÈáèÂåñÁöÑÊñáÊú¨Êï∞ÊçÆËøõË°åÈôçÁª¥Ôºå‰ª•‰æøËøõË°åÊõ¥Â•ΩÁöÑÂèØËßÜÂåñÊàñËøõ‰∏ÄÊ≠•ÁöÑÂàÜÊûê„ÄÇÈôçÁª¥ÂêéÁöÑÊï∞ÊçÆÂèØ‰ª•Êõ¥ÂÆπÊòìÂú∞Ë¢´‰∫∫Á±ªÁêÜËß£ÂíåËß£Èáä„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>c-TF-IDFÔºàClass-based TF-IDFÔºâ&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>c-TF-IDFÊòØ‰∏ÄÁßçÊîπËøõÁöÑTF-IDFÔºàTerm Frequency-Inverse Document FrequencyÔºâÊñπÊ≥ïÔºåÁî®‰∫é‰ªéÊñáÊú¨Êï∞ÊçÆ‰∏≠ÊèêÂèñÂÖ≥ÈîÆËØç„ÄÇ&lt;/li>
&lt;li>‰∏é‰º†ÁªüÁöÑTF-IDFÁõ∏ÊØîÔºåc-TF-IDFËÄÉËôë‰∫ÜÂçïËØçÂú®‰∏çÂêå‰∏ªÈ¢ò‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂú®Êï¥‰∏™ÊñáÊú¨ÈõÜÂêà‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇËøô‰ΩøÂæóc-TF-IDFËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÊñáÊú¨‰∏≠ÁöÑ‰∏ªÈ¢òÁõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ&lt;/li>
&lt;li>Âú®BERTopic‰∏≠Ôºåc-TF-IDFÂ∏∏Áî®‰∫é‰ªéÊØè‰∏™ÂèëÁé∞ÁöÑ‰∏ªÈ¢òÁ∞á‰∏≠ÊèêÂèñÂÖ≥ÈîÆËØçÔºå‰ª•ÊèèËø∞ËØ•‰∏ªÈ¢òÁöÑÂÜÖÂÆπ„ÄÇËøô‰∫õÂÖ≥ÈîÆËØçÈÄöÂ∏∏ÊòØÁ∞á‰∏≠ÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑËØçËØ≠ÔºåÂ∏ÆÂä©Áî®Êà∑ÁêÜËß£‰∏ªÈ¢òÁöÑÂê´‰πâÂíåÂÜÖÂÆπ„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>HDBSCANÔºàHierarchical Density-Based Spatial Clustering of Applications with NoiseÔºâ&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>HDBSCANÊòØ‰∏ÄÁßçÂØÜÂ∫¶ËÅöÁ±ªÁÆóÊ≥ïÔºåÊó®Âú®ÂèëÁé∞Êï∞ÊçÆ‰∏≠ÁöÑÈ´òÂØÜÂ∫¶Âå∫ÂüüÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨ÁªÑÂêàÊàêÁ∞á„ÄÇ&lt;/li>
&lt;li>‰∏é‰º†ÁªüÁöÑÂü∫‰∫éË∑ùÁ¶ªÁöÑËÅöÁ±ªÁÆóÊ≥ïÔºàÂ¶ÇKÂùáÂÄºÔºâ‰∏çÂêåÔºåHDBSCAN‰∏çÈúÄË¶ÅÈ¢ÑÂÖàÊåáÂÆöÁ∞áÁöÑÊï∞ÈáèÔºåÂõ†Ê≠§Êõ¥ÈÄÇÁî®‰∫éÂèëÁé∞ÂÖ∑Êúâ‰∏çÂêåÂ§ßÂ∞èÂíåÂΩ¢Áä∂ÁöÑÁ∞áÁöÑÊï∞ÊçÆ„ÄÇ&lt;/li>
&lt;li>Âú®BERTopic‰∏≠ÔºåHDBSCANÈÄöÂ∏∏‰∏éUMAP‰∏ÄËµ∑‰ΩøÁî®ÔºåÁî®‰∫éÂØπBERTÂêëÈáèÂåñÁöÑÊñáÊú¨Êï∞ÊçÆËøõË°åËÅöÁ±ª„ÄÇHDBSCANËÉΩÂ§üËØÜÂà´Âá∫ÂÖ∑Êúâ‰∏çÂêå‰∏ªÈ¢òÁöÑÊñáÊú¨Á∞áÔºå‰ªéËÄåÂ∏ÆÂä©Áî®Êà∑ÂèëÁé∞ÊñáÊú¨Êï∞ÊçÆ‰∏≠ÁöÑ‰∏ªÈ¢òÁªìÊûÑ„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Áªº‰∏äÊâÄËø∞ÔºåUMAPÁî®‰∫éÂ∞ÜÈ´òÁª¥ÊñáÊú¨ÂêëÈáèÈôçÁª¥Âà∞‰ΩéÁª¥Á©∫Èó¥Ôºå‰ª•‰æø‰∫éÂèØËßÜÂåñÂíåÂàÜÊûêÔºõc-TF-IDFÁî®‰∫é‰ªéÊØè‰∏™‰∏ªÈ¢òÁ∞á‰∏≠ÊèêÂèñÂÖ≥ÈîÆËØç‰ª•ÊèèËø∞‰∏ªÈ¢òÂÜÖÂÆπÔºõHDBSCANÁî®‰∫éÂèëÁé∞ÊñáÊú¨Êï∞ÊçÆ‰∏≠ÁöÑÊΩúÂú®‰∏ªÈ¢òÁ∞á„ÄÇËøô‰∫õÊäÄÊúØÁöÑÁªìÂêà‰ΩøÂæóBERTopicËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åËá™ÁÑ∂ËØ≠Ë®Ä‰∏ªÈ¢òÂª∫Ê®°ÔºåÂπ∂‰ªéÊñáÊú¨Êï∞ÊçÆ‰∏≠ÊèêÂèñÊúâÊÑè‰πâÁöÑ‰∏ªÈ¢ò‰ø°ÊÅØ„ÄÇ&lt;/p>
&lt;h1 id="project-02----ml" >
&lt;div>
&lt;a href="#project-02----ml">
##
&lt;/a>
Project 02 &amp;ndash; ML
&lt;/div>
&lt;/h1>
&lt;h2 id="self-training--gausianmixtruemodels" >
&lt;div>
&lt;a href="#self-training--gausianmixtruemodels">
#
&lt;/a>
self-training &amp;amp; GausianMixtrueModels
&lt;/div>
&lt;/h2>
&lt;p>ÂçäÁõëÁù£Â≠¶‰π†ÊòØ‰∏ÄÁßçÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ïÔºåÂÆÉÂà©Áî®ÊúâÊ†áÁ≠æÂíåÊó†Ê†áÁ≠æÁöÑÊï∞ÊçÆÊù•ËøõË°åÊ®°ÂûãËÆ≠ÁªÉ„ÄÇÁõ∏ÊØî‰∫éÂè™‰ΩøÁî®ÊúâÊ†áÁ≠æÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂçäÁõëÁù£Â≠¶‰π†ÂèØ‰ª•Âà©Áî®Êõ¥Â§öÁöÑÊó†Ê†áÁ≠æÊï∞ÊçÆÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;p>self-trainingÔºàËá™ËÆ≠ÁªÉÔºâÊòØÂçäÁõëÁù£Â≠¶‰π†‰∏≠ÁöÑ‰∏ÄÁßçÂ∏∏ËßÅÊäÄÊúØÔºåÂÖ∂Âü∫Êú¨ÊÄùÊÉ≥ÊòØÂà©Áî®Â∑≤ÁªèËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂØπÊó†Ê†áÁ≠æÊï∞ÊçÆËøõË°åÈ¢ÑÊµãÔºåÁÑ∂ÂêéÂ∞ÜÁΩÆ‰ø°Â∫¶ËæÉÈ´òÁöÑÈ¢ÑÊµãÁªìÊûú‰Ωú‰∏∫‰º™Ê†áÁ≠æÔºåÂ∞ÜËøô‰∫õ‰º™Ê†áÁ≠æÁöÑÊï∞ÊçÆ‰∏éÊúâÊ†áÁ≠æÊï∞ÊçÆ‰∏ÄËµ∑ÈáçÊñ∞ËÆ≠ÁªÉÊ®°Âûã„ÄÇ&lt;/p>
&lt;p>ÂÖ∑‰ΩìÊù•ËØ¥Ôºåself-trainingÁöÑÊ≠•È™§Â¶Ç‰∏ãÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Âà©Áî®ÊúâÊ†áÁ≠æÊï∞ÊçÆËÆ≠ÁªÉ‰∏Ä‰∏™ÂàùÂßãÊ®°Âûã„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>‰ΩøÁî®ËØ•Ê®°ÂûãÂØπÊó†Ê†áÁ≠æÊï∞ÊçÆËøõË°åÈ¢ÑÊµãÔºåÂπ∂ÈÄâÂèñÁΩÆ‰ø°Â∫¶ËæÉÈ´òÁöÑÈ¢ÑÊµãÁªìÊûú‰Ωú‰∏∫‰º™Ê†áÁ≠æ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Â∞Ü‰º™Ê†áÁ≠æÁöÑÊï∞ÊçÆ‰∏éÂéüÊúâÁöÑÊúâÊ†áÁ≠æÊï∞ÊçÆÂêàÂπ∂ÔºåÈáçÊñ∞ËÆ≠ÁªÉÊ®°Âûã„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÈáçÂ§çÊ≠•È™§2ÂíåÊ≠•È™§3ÔºåÁõ¥Âà∞Êª°Ë∂≥ÂÅúÊ≠¢Êù°‰ª∂ÔºàÊØîÂ¶ÇËææÂà∞ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞„ÄÅÊ®°ÂûãÊÄßËÉΩ‰∏çÂÜçÊèêÂçáÁ≠âÔºâ„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>self-trainingÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ¶Ç‰ΩïÈÄâÊã©ÁΩÆ‰ø°Â∫¶ËæÉÈ´òÁöÑÈ¢ÑÊµãÁªìÊûú‰Ωú‰∏∫‰º™Ê†áÁ≠æ„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÂèØ‰ª•ÈÄöËøáËÆæÁΩÆ‰∏Ä‰∏™ÈòàÂÄºÊù•Á≠õÈÄâÁΩÆ‰ø°Â∫¶ËæÉÈ´òÁöÑÈ¢ÑÊµãÁªìÊûúÔºå‰πüÂèØ‰ª•‰ΩøÁî®Ê®°ÂûãÁöÑËæìÂá∫Ê¶ÇÁéáÊù•‰Ωú‰∏∫ÁΩÆ‰ø°Â∫¶ÁöÑÂ∫¶Èáè„ÄÇ&lt;/p>
&lt;p>‰ΩøÁî®self-trainingÊäÄÊúØÂèØ‰ª•Â∏ÆÂä©Êâ©ÂÖÖÊ†∑Êú¨ÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÁâπÂà´ÊòØÂú®Ê†áËÆ∞Êï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ‰ΩÜÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËá™ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂèØËÉΩ‰ºöÂºïÂÖ•Âô™Â£∞ÔºåÂõ†Ê≠§ÈúÄË¶Å‰ªîÁªÜË∞ÉËäÇÂèÇÊï∞ÂíåÁõëÊéßÊ®°ÂûãÊÄßËÉΩÔºå‰ª•ÈÅøÂÖçËøáÊãüÂêàÂíåÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúËÅöÁ±ªÁ∞áÂÜÖÁöÑÊ†áÁ≠æÂπ∂‰∏ç‰∏ÄËá¥ÔºåËÄå‰∏îÁ±ªÂà´Êï∞ÁõÆËæÉ‰∏∫Áõ∏ÂêåÔºåËøôÁßçÊÉÖÂÜµÂèØËÉΩ‰ºöÂØºËá¥‰∏Ä‰∫õÊ∑∑‰π±ÔºåÂõ†‰∏∫Êó†Ê≥ïÁÆÄÂçïÂú∞ÈÄâÊã©‰∏Ä‰∏™‰ª£Ë°®ÊÄßÁöÑÊ†áÁ≠æÊù•‰∏∫Êï¥‰∏™ËÅöÁ±ªÁ∞áÁöÑÊ†∑Êú¨ÊâìÊ†áÁ≠æ„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂèØ‰ª•ËÄÉËôë‰ª•‰∏ãÂá†ÁßçÂ§ÑÁêÜÊñπÂºèÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ÊäïÁ•®Êú∫Âà∂&lt;/strong>Ôºö&lt;/p>
&lt;ul>
&lt;li>ÂØπ‰∫éÊØè‰∏™ËÅöÁ±ªÁ∞áÔºåÂèØ‰ª•ÈááÁî®ÊäïÁ•®Êú∫Âà∂Êù•ÈÄâÊã©Ê†áÁ≠æ„ÄÇÂç≥ÔºåÁªüËÆ°ËÅöÁ±ªÁ∞á‰∏≠Ê†∑Êú¨ÁöÑÁúüÂÆûÊ†áÁ≠æÊàñËÄÖÂ∑≤ÊúâÁöÑ‰º™Ê†áÁ≠æÔºåÈÄâÊã©Âá∫Áé∞È¢ëÁéáÊúÄÈ´òÁöÑÊ†áÁ≠æ‰Ωú‰∏∫Êï¥‰∏™ËÅöÁ±ªÁ∞áÁöÑÊ†áÁ≠æ„ÄÇ&lt;/li>
&lt;li>Â¶ÇÊûúÊúâÂ§ö‰∏™Ê†áÁ≠æÂá∫Áé∞È¢ëÁéáÁõ∏ÂêåÔºåÂèØ‰ª•ÈöèÊú∫ÈÄâÊã©ÂÖ∂‰∏≠‰∏Ä‰∏™ÊàñËÄÖÈááÁî®‰∏Ä‰∫õÂÖ∂‰ªñÁöÑÁ≠ñÁï•Êù•Ëß£ÂÜ≥ÂÜ≤Á™Å„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ê†áÁ≠æËûçÂêà&lt;/strong>Ôºö&lt;/p>
&lt;ul>
&lt;li>ÂØπ‰∫éËÅöÁ±ªÁ∞áÂÜÖÁöÑÊ†∑Êú¨ÔºåÂèØ‰ª•Â∞ÜÂÖ∂ÁúüÂÆûÊ†áÁ≠æÊàñËÄÖÂ∑≤ÊúâÁöÑ‰º™Ê†áÁ≠æËøõË°åËûçÂêà„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•ËÆ°ÁÆóËÅöÁ±ªÁ∞á‰∏≠ÊØè‰∏™Á±ªÂà´ÁöÑÊùÉÈáçÔºåÁÑ∂ÂêéÊ†πÊçÆÊùÉÈáçÂØπÂ§ö‰∏™Ê†áÁ≠æËøõË°åÂä†ÊùÉÂπ≥ÂùáÔºåÂæóÂà∞‰∏Ä‰∏™ÁªºÂêàÁöÑÊ†áÁ≠æ„ÄÇ&lt;/li>
&lt;li>ËøôÁßçÊñπÊ≥ïÂèØ‰ª•Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äËß£ÂÜ≥Ê†áÁ≠æÂÜ≤Á™ÅÁöÑÈóÆÈ¢òÔºå‰ΩÜÈúÄË¶ÅË∞®ÊÖéËÆæËÆ°ÊùÉÈáçËÆ°ÁÆóÁöÑÊñπÊ≥ïÔºå‰ª•ÈÅøÂÖçÁªô‰∏çÂ§™ÂáÜÁ°ÆÁöÑÊ†áÁ≠æËøáÂ§öÁöÑÊùÉÈáç„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ëøõ‰∏ÄÊ≠•ÂàÜÊûê&lt;/strong>Ôºö&lt;/p>
&lt;ul>
&lt;li>ÂØπ‰∫éÈÇ£‰∫õÊ†áÁ≠æÂÜ≤Á™ÅËæÉ‰∏∫‰∏•ÈáçÁöÑËÅöÁ±ªÁ∞áÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂàÜÊûêÂÖ∂Ê†∑Êú¨ÁâπÂæÅÊàñËÄÖÊï∞ÊçÆÂàÜÂ∏ÉÔºåÂ∞ùËØïÊâæÂà∞Êõ¥ÂêàÈÄÇÁöÑÊ†áÁ≠æÈÄâÊã©Á≠ñÁï•„ÄÇ&lt;/li>
&lt;li>ÂèØ‰ª•ËÄÉËôë‰ΩøÁî®‰∏Ä‰∫õÊï∞ÊçÆÊåñÊéòÊäÄÊúØÊàñËÄÖÈ¢ÜÂüüÁü•ËØÜÊù•Â∏ÆÂä©Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÂçäÁõëÁù£Â≠¶‰π†ÊñπÊ≥ï&lt;/strong>Ôºö&lt;/p>
&lt;ul>
&lt;li>Èô§‰∫ÜÈ´òÊñØÊ∑∑ÂêàËÅöÁ±ªÔºåËøòÂèØ‰ª•Â∞ùËØïÂÖ∂‰ªñÂçäÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÔºåÂ¶ÇÂõæÂçäÁõëÁù£Â≠¶‰π†ÔºàGraph-based Semi-Supervised LearningÔºâ„ÄÅÊ†áÁ≠æ‰º†Êí≠ÁÆóÊ≥ïÔºàLabel PropagationÔºâ„ÄÅËá™ËÆ≠ÁªÉÔºàSelf-trainingÔºâÁ≠â„ÄÇ&lt;/li>
&lt;li>Ëøô‰∫õÊñπÊ≥ïÂèØËÉΩ‰ºöÊõ¥ÊúâÊïàÂú∞Â§ÑÁêÜÊ†áÁ≠æÂÜ≤Á™ÅÁöÑÈóÆÈ¢òÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂèØ‰ª•Êõ¥Â•ΩÂú∞Âà©Áî®Êï∞ÊçÆ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÂíåÂÖ≥ËÅîÊÄß„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÂÖ∑‰ΩìÊÉÖÂÜµÈÄâÊã©ÈÄÇÂêàÁöÑÊñπÊ≥ïÊù•Â§ÑÁêÜÊ†áÁ≠æÂÜ≤Á™ÅÈóÆÈ¢òÔºåÂêåÊó∂ÈúÄË¶ÅÊ≥®ÊÑèÁõëÊéßÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÊûúÔºåÂèäÊó∂Ë∞ÉÊï¥Âíå‰ºòÂåñÁÆóÊ≥ï„ÄÇ&lt;/p>
&lt;h2 id="xgboost" >
&lt;div>
&lt;a href="#xgboost">
#
&lt;/a>
XGBoost
&lt;/div>
&lt;/h2>
&lt;p>XGBoostÔºåÂÖ®Áß∞‰∏∫‚ÄúeXtreme Gradient Boosting‚ÄùÔºåÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÈõÜÊàêÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂ±û‰∫éÊ¢ØÂ∫¶ÊèêÂçáÊ†ëÔºàGradient Boosting TreeÔºâÁöÑ‰∏ÄÁßçÂÆûÁé∞„ÄÇÂÆÉÂú®Êú∫Âô®Â≠¶‰π†Á´ûËµõ‰∏≠ÈùûÂ∏∏ÊµÅË°åÔºåÂõ†‰∏∫ÂÆÉËÉΩÂ§üÂú®ÂêÑÁßçÁ±ªÂûãÁöÑÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰ºòÁßÄÁöÑÊÄßËÉΩÔºåÂπ∂‰∏îÁõ∏ÂØπ‰∫éÂÖ∂‰ªñÁÆóÊ≥ïÔºåÂÆÉÈÄöÂ∏∏Êõ¥ÂÆπÊòìË∞ÉÊï¥ÂèÇÊï∞‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;p>‰ª•‰∏ãÊòØ XGBoost Ê®°ÂûãÁöÑ‰∏Ä‰∫õÂÖ≥ÈîÆÁâπÁÇπÂíå‰ºòÂäøÔºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ÈõÜÊàêÂ≠¶‰π†&lt;/strong>ÔºöXGBoost ÊòØ‰∏ÄÁßçÈõÜÊàêÂ≠¶‰π†ÁÆóÊ≥ïÔºåÂÆÉÈÄöËøáÁªÑÂêàÂ§ö‰∏™Âº±Â≠¶‰π†Âô®ÔºàÈÄöÂ∏∏ÊòØÂÜ≥Á≠ñÊ†ëÔºâÊù•ÊûÑÂª∫‰∏Ä‰∏™Âº∫Â§ßÁöÑÊ®°Âûã„ÄÇÊØè‰∏™ÂÜ≥Á≠ñÊ†ëÈÉΩÊòØÊ†πÊçÆÂâç‰∏Ä‰∏™Ê†ëÁöÑÈîôËØØËøõË°åËÆ≠ÁªÉÔºå‰ª•ÈÄêÊ≠•ÂáèÂ∞ëÊ®°ÂûãÁöÑÊÆãÂ∑Æ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ê¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ï&lt;/strong>ÔºöXGBoost ‰ΩøÁî®Ê¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÊù•ËÆ≠ÁªÉÊ®°Âûã„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøáÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶Êù•‰ºòÂåñÊ®°ÂûãÔºå‰ªéËÄå‰ΩøÊ®°ÂûãÂú®ÊØè‰∏ÄÊ≠•ÈÉΩÊõ¥Âä†Ë¥¥ËøëÁúüÂÆûÂÄº„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ê≠£ÂàôÂåñ&lt;/strong>ÔºöXGBoost Êèê‰æõ‰∫ÜÂØπÊ®°ÂûãËøõË°åÊ≠£ÂàôÂåñÁöÑÈÄâÈ°πÔºåÂåÖÊã¨ L1 Âíå L2 Ê≠£ÂàôÂåñÔºå‰ª•ÂèäÊéßÂà∂Ê†ëÁöÑÂ§çÊùÇÂ∫¶ÁöÑÂèÇÊï∞„ÄÇËøôÊúâÂä©‰∫éÂáèÂ∞ëËøáÊãüÂêàÔºåÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÊîØÊåÅÂπ∂Ë°åÂåñ&lt;/strong>ÔºöXGBoost ÂèØ‰ª•ÊúâÊïàÂú∞Âà©Áî®Âπ∂Ë°åËÆ°ÁÆóËµÑÊ∫êËøõË°åËÆ≠ÁªÉÔºåÂõ†Ê≠§Âú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏ä‰πüËÉΩÂ§üÂø´ÈÄüÂú∞ËÆ≠ÁªÉÊ®°Âûã„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÁâπÂæÅÈáçË¶ÅÊÄßËØÑ‰º∞&lt;/strong>ÔºöXGBoost ÂèØ‰ª•ËÆ°ÁÆóÁâπÂæÅÁöÑÈáçË¶ÅÊÄßÔºå‰ªéËÄåÂ∏ÆÂä©Áî®Êà∑‰∫ÜËß£Âì™‰∫õÁâπÂæÅÂØπÊ®°ÂûãÁöÑÈ¢ÑÊµãÊúÄ‰∏∫ÂÖ≥ÈîÆ„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÁÅµÊ¥ªÊÄß&lt;/strong>ÔºöXGBoost ÂèØ‰ª•Áî®‰∫éÂàÜÁ±ªÈóÆÈ¢ò„ÄÅÂõûÂΩíÈóÆÈ¢ò‰ª•ÂèäÊéíÂ∫èÈóÆÈ¢òÔºåÂπ∂‰∏îÂèØ‰ª•Âú®‰∏çÂêåÁ±ªÂûãÁöÑÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂ∫îÁî®„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>ÊÄªÁöÑÊù•ËØ¥ÔºåXGBoost ÊòØ‰∏ÄÁßçÂº∫Â§ß‰∏îÁÅµÊ¥ªÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÈÄÇÁî®‰∫éÂêÑÁßçÁ±ªÂûãÁöÑÈóÆÈ¢òÔºåÂπ∂‰∏îÂú®ÂÆûË∑µ‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ&lt;/p>
&lt;p>Ê¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÔºàGradient Boosting AlgorithmÔºâÊòØ‰∏ÄÁßçÈõÜÊàêÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÂ∞ÜÂ§ö‰∏™Âº±Â≠¶‰π†Âô®ÔºàÈÄöÂ∏∏ÊòØÂÜ≥Á≠ñÊ†ëÔºâ‰∏≤ËÅîËµ∑Êù•ÔºåÈÄêÊ≠•ÂáèÂ∞ëÊ®°ÂûãÁöÑÊÆãÂ∑ÆÊù•ÊûÑÂª∫‰∏Ä‰∏™Âº∫Â§ßÁöÑÈ¢ÑÊµãÊ®°Âûã„ÄÇÊ¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÈÄöËøáÊ¢ØÂ∫¶‰∏ãÈôçÁöÑÊÄùÊÉ≥Ôºå‰∏çÊñ≠‰ºòÂåñÊ®°Âûã‰ª•ÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞„ÄÇ&lt;/p>
&lt;p>‰∏ãÈù¢ÊòØÊ¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÁöÑÂü∫Êú¨Ê≠•È™§Ôºö&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ÂàùÂßãÂåñÊ®°Âûã&lt;/strong>ÔºöÊ¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÈÄöÂ∏∏‰ªé‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊ®°ÂûãÂºÄÂßãÔºå‰æãÂ¶ÇÁî®‰∏Ä‰∏™Â∏∏Êï∞Êù•ÊãüÂêàÊï∞ÊçÆÁöÑÂπ≥ÂùáÂÄº„ÄÇ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ëø≠‰ª£‰ºòÂåñ&lt;/strong>ÔºöÊé•‰∏ãÊù•ÔºåÁÆóÊ≥ïËø≠‰ª£Âú∞ÊâßË°å‰ª•‰∏ãÊ≠•È™§Ôºö&lt;/p>
&lt;ul>
&lt;li>ËÆ°ÁÆóÊÆãÂ∑ÆÔºö‰ΩøÁî®ÂΩìÂâçÊ®°ÂûãÂØπËÆ≠ÁªÉÊï∞ÊçÆËøõË°åÈ¢ÑÊµãÔºåÂπ∂ËÆ°ÁÆóÂÆûÈôÖÂÄº‰∏éÈ¢ÑÊµãÂÄº‰πãÈó¥ÁöÑÊÆãÂ∑Æ„ÄÇ&lt;/li>
&lt;li>ÊãüÂêàÊÆãÂ∑ÆÔºöÊûÑÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂº±Â≠¶‰π†Âô®ÔºàÂ¶ÇÂÜ≥Á≠ñÊ†ëÔºâÔºå‰ª•ÊãüÂêàÊÆãÂ∑Æ„ÄÇËøôÊÑèÂë≥ÁùÄÊñ∞ÁöÑÂ≠¶‰π†Âô®‰ºöÂ∞ùËØïÁ∫†Ê≠£‰∏ä‰∏Ä‰∏™Ê®°ÂûãÁöÑÈîôËØØ„ÄÇ&lt;/li>
&lt;li>Êõ¥Êñ∞Ê®°ÂûãÔºöÂ∞ÜÊñ∞ÁöÑÂ≠¶‰π†Âô®‰∏éÂâçÈù¢ÁöÑÊ®°ÂûãÁªÑÂêàËµ∑Êù•ÔºåÂΩ¢Êàê‰∏Ä‰∏™Êõ¥Âº∫Â§ßÁöÑÊ®°Âûã„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ÂÅúÊ≠¢Êù°‰ª∂&lt;/strong>ÔºöÂΩìËææÂà∞È¢ÑÂÖàËÆæÂÆöÁöÑËø≠‰ª£Ê¨°Êï∞ÔºåÊàñËÄÖÂΩìÊ®°ÂûãÁöÑÊÄßËÉΩÊª°Ë∂≥Êüê‰∏™ÁâπÂÆöÁöÑÊ†áÂáÜÊó∂ÔºåÂÅúÊ≠¢Ëø≠‰ª£„ÄÇ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>‰∏éÊ¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÁöÑÂÖ≥Á≥ªÂú®‰∫éÔºåÊ¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ï‰πüÂà©Áî®‰∫ÜÊ¢ØÂ∫¶ÁöÑ‰ø°ÊÅØÊù•‰ºòÂåñÊ®°Âûã„ÄÇ‰ΩÜ‰∏§ËÄÖ‰πãÈó¥ÁöÑÂÖ≥ÈîÆÂå∫Âà´Âú®‰∫é‰ºòÂåñÁöÑÂØπË±°Âíå‰ºòÂåñÊñπÂêë„ÄÇÂú®Ê¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ï‰∏≠Ôºå‰ºòÂåñÁöÑÂØπË±°ÊòØÊçüÂ§±ÂáΩÊï∞Êú¨Ë∫´ÔºåËÄå‰ºòÂåñÁöÑÊñπÂêëÊòØÊ≤øÁùÄÊçüÂ§±ÂáΩÊï∞Ê¢ØÂ∫¶ÁöÑÂèçÊñπÂêë„ÄÇËÄåÂú®Ê¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ï‰∏≠Ôºå‰ºòÂåñÁöÑÂØπË±°ÊòØÊçüÂ§±ÂáΩÊï∞ÁöÑÊÆãÂ∑ÆÔºå‰ºòÂåñÁöÑÊñπÂêëÊòØ‰ΩøÊÆãÂ∑ÆÊúÄÂ∞èÂåñÁöÑÊñπÂêë„ÄÇ&lt;/p>
&lt;p>ÊÄªÁöÑÊù•ËØ¥ÔºåÊ¢ØÂ∫¶ÊèêÂçáÁÆóÊ≥ïÊòØ‰∏ÄÁßçÂº∫Â§ßÁöÑÈõÜÊàêÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøá‰∏çÊñ≠Ëø≠‰ª£‰ºòÂåñÊ®°Âûã‰ª•ÂáèÂ∞ëÊÆãÂ∑ÆÔºå‰ªéËÄåÊûÑÂª∫‰∏Ä‰∏™Âº∫Â§ßÁöÑÈ¢ÑÊµãÊ®°Âûã„ÄÇÂÆÉÂà©Áî®‰∫ÜÊ¢ØÂ∫¶‰ø°ÊÅØÊù•ÊåáÂØº‰ºòÂåñËøáÁ®ãÔºå‰ΩÜ‰∏éÊ¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÁõ∏ÊØîÔºåÂÆÉÁöÑ‰ºòÂåñÁõÆÊ†áÂíå‰ºòÂåñÊñπÂêëÊúâÊâÄ‰∏çÂêå„ÄÇ&lt;/p>
&lt;h2 id="onnx-dockerpodman-and-restful-api" >
&lt;div>
&lt;a href="#onnx-dockerpodman-and-restful-api">
#
&lt;/a>
ONNX, Docker/Podman, and restful-api
&lt;/div>
&lt;/h2>
&lt;p>ÂΩì‰Ω†Â∏åÊúõ‰ΩøÁî® Docker Êù•ÈÉ®ÁΩ≤ÊúçÂä°ÔºåÂπ∂‰∏îÊûÑÂª∫ Flask RESTful API Êù•Êèê‰æõÂØπ ONNX Ê®°ÂûãÁöÑÊé®ÁêÜÊúçÂä°Êó∂Ôºå‰Ω†ÂèØ‰ª•ÊåâÁÖß‰ª•‰∏ãÊ≠•È™§ËøõË°åÔºö&lt;/p>
&lt;h3 id="1-ÂáÜÂ§á-flask-restful-api-‰ª£Á†Å" >
&lt;div>
&lt;a href="#1-%e5%87%86%e5%a4%87-flask-restful-api-%e4%bb%a3%e7%a0%81">
##
&lt;/a>
1. ÂáÜÂ§á Flask RESTful API ‰ª£Á†Å
&lt;/div>
&lt;/h3>
&lt;p>‰Ω†ÈúÄË¶ÅÂàõÂª∫‰∏Ä‰∏™ Flask Â∫îÁî®Á®ãÂ∫èÔºåÁºñÂÜô‰ª£Á†Å‰ª•Âä†ËΩΩ ONNX Ê®°ÂûãÂπ∂Êèê‰æõ RESTful API Êù•ËøõË°åÊé®ÁêÜ„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> flask &lt;span style="color:#ff6ac1">import&lt;/span> Flask, request
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> flask_restful &lt;span style="color:#ff6ac1">import&lt;/span> Api, Resource
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> onnxruntime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app &lt;span style="color:#ff6ac1">=&lt;/span> Flask(__name__)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>api &lt;span style="color:#ff6ac1">=&lt;/span> Api(app)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">class&lt;/span> &lt;span style="color:#f3f99d">ModelInference&lt;/span>(Resource):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> __init__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">super&lt;/span>(ModelInference, self)&lt;span style="color:#ff6ac1">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff6ac1">.&lt;/span>session &lt;span style="color:#ff6ac1">=&lt;/span> onnxruntime&lt;span style="color:#ff6ac1">.&lt;/span>InferenceSession(&lt;span style="color:#5af78e">&amp;#34;your_model.onnx&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">post&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> data &lt;span style="color:#ff6ac1">=&lt;/span> request&lt;span style="color:#ff6ac1">.&lt;/span>json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_data &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(data[&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>])&lt;span style="color:#ff6ac1">.&lt;/span>astype(np&lt;span style="color:#ff6ac1">.&lt;/span>float32)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>session&lt;span style="color:#ff6ac1">.&lt;/span>run([], {&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>: input_data})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> {&lt;span style="color:#5af78e">&amp;#34;output&amp;#34;&lt;/span>: output&lt;span style="color:#ff6ac1">.&lt;/span>tolist()}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>api&lt;span style="color:#ff6ac1">.&lt;/span>add_resource(ModelInference, &lt;span style="color:#5af78e">&amp;#34;/predict&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">if&lt;/span> __name__ &lt;span style="color:#ff6ac1">==&lt;/span> &lt;span style="color:#5af78e">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app&lt;span style="color:#ff6ac1">.&lt;/span>run(debug&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>, host&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;0.0.0.0&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-ÂàõÂª∫-dockerfile" >
&lt;div>
&lt;a href="#2-%e5%88%9b%e5%bb%ba-dockerfile">
##
&lt;/a>
2. ÂàõÂª∫ Dockerfile
&lt;/div>
&lt;/h3>
&lt;p>ÂàõÂª∫‰∏Ä‰∏™ Dockerfile Êù•ÊûÑÂª∫ Docker ÈïúÂÉè„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">FROM&lt;/span>&lt;span style="color:#5af78e"> python:3.9-slim&lt;/span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">WORKDIR&lt;/span>&lt;span style="color:#5af78e"> /app&lt;/span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">COPY&lt;/span> requirements.txt .&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">RUN&lt;/span> pip install --no-cache-dir -r requirements.txt&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">COPY&lt;/span> . .&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">CMD&lt;/span> [ &lt;span style="color:#5af78e">&amp;#34;python&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;app.py&amp;#34;&lt;/span> ]&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-ÊûÑÂª∫-docker-ÈïúÂÉè" >
&lt;div>
&lt;a href="#3-%e6%9e%84%e5%bb%ba-docker-%e9%95%9c%e5%83%8f">
##
&lt;/a>
3. ÊûÑÂª∫ Docker ÈïúÂÉè
&lt;/div>
&lt;/h3>
&lt;p>Âú®ÂåÖÂê´ Dockerfile ÁöÑÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÊûÑÂª∫ Docker ÈïúÂÉè„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker build -t your_image_name .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="4-ËøêË°å-docker-ÂÆπÂô®" >
&lt;div>
&lt;a href="#4-%e8%bf%90%e8%a1%8c-docker-%e5%ae%b9%e5%99%a8">
##
&lt;/a>
4. ËøêË°å Docker ÂÆπÂô®
&lt;/div>
&lt;/h3>
&lt;p>‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§Êù•ËøêË°å‰Ω†ÁöÑ Docker ÂÆπÂô®„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -p 5000:5000 your_image_name
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="5-ÊµãËØï-api" >
&lt;div>
&lt;a href="#5-%e6%b5%8b%e8%af%95-api">
##
&lt;/a>
5. ÊµãËØï API
&lt;/div>
&lt;/h3>
&lt;p>‰ΩøÁî®‰ªª‰Ωï HTTP ÂÆ¢Êà∑Á´ØÂ∑•ÂÖ∑Êàñ Python Â∫îÁî®Á®ãÂ∫èÊù•ÊµãËØï‰Ω†ÁöÑ API„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> requests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data &lt;span style="color:#ff6ac1">=&lt;/span> {&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>: [&lt;span style="color:#ff9f43">1.0&lt;/span>, &lt;span style="color:#ff9f43">2.0&lt;/span>, &lt;span style="color:#ff9f43">3.0&lt;/span>]} &lt;span style="color:#78787e"># Á§∫‰æãËæìÂÖ•Êï∞ÊçÆ&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>response &lt;span style="color:#ff6ac1">=&lt;/span> requests&lt;span style="color:#ff6ac1">.&lt;/span>post(&lt;span style="color:#5af78e">&amp;#34;http://localhost:5000/predict&amp;#34;&lt;/span>, json&lt;span style="color:#ff6ac1">=&lt;/span>data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(response&lt;span style="color:#ff6ac1">.&lt;/span>json())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="ÊÄªÁªì-3" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-3">
##
&lt;/a>
ÊÄªÁªì
&lt;/div>
&lt;/h3>
&lt;p>ËøôÂ∞±ÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÈÉ®ÁΩ≤ÊµÅÁ®ã„ÄÇ‰Ω†ÂèØ‰ª•Ê†πÊçÆ‰Ω†ÁöÑÂÖ∑‰ΩìÈúÄÊ±ÇËøõË°åË∞ÉÊï¥ÂíåÊâ©Â±ïÔºåÊØîÂ¶ÇÊ∑ªÂä†Ê®°ÂûãÈ¢ÑÂ§ÑÁêÜ„ÄÅÂêéÂ§ÑÁêÜÈÄªËæëÔºå‰ª•ÂèäÂØπ API ÁöÑË∫´‰ªΩÈ™åËØÅÂíåËÆøÈóÆÊéßÂà∂Á≠âÂäüËÉΩ„ÄÇ&lt;/p>
&lt;h1 id="project-03----cv" >
&lt;div>
&lt;a href="#project-03----cv">
##
&lt;/a>
Project 03 &amp;ndash; CV
&lt;/div>
&lt;/h1>
&lt;h2 id="yolo-model" >
&lt;div>
&lt;a href="#yolo-model">
#
&lt;/a>
YOLO model
&lt;/div>
&lt;/h2>
&lt;p>Â•ΩÁöÑÔºåËÆ©ÊàëÊù•ËØ¶ÁªÜ‰ªãÁªç‰∏Ä‰∏ãYOLOÔºàYou Only Look OnceÔºâÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÁöÑÂéüÁêÜÂíåÂÆûÁé∞„ÄÇ&lt;/p>
&lt;h3 id="1-yolo-Ê®°ÂûãÁöÑÂéüÁêÜ" >
&lt;div>
&lt;a href="#1-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8e%9f%e7%90%86">
##
&lt;/a>
1. YOLO Ê®°ÂûãÁöÑÂéüÁêÜÔºö
&lt;/div>
&lt;/h3>
&lt;h4 id="11-ÂçïÈò∂ÊÆµÊ£ÄÊµã" >
&lt;div>
&lt;a href="#11-%e5%8d%95%e9%98%b6%e6%ae%b5%e6%a3%80%e6%b5%8b">
###
&lt;/a>
1.1 ÂçïÈò∂ÊÆµÊ£ÄÊµãÔºö
&lt;/div>
&lt;/h4>
&lt;p>YOLO ÊòØ‰∏ÄÁßçÂçïÈò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÔºå‰∏é‰º†ÁªüÁöÑ‰∏§Èò∂ÊÆµÊ£ÄÊµãÊñπÊ≥ïÔºàÂ¶ÇFaster R-CNNÔºâ‰∏çÂêåÔºåÂÆÉÂ∞ÜÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°ËßÜ‰∏∫‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÂõûÂΩíÈóÆÈ¢òÔºåÁõ¥Êé•‰ªéÂõæÂÉè‰∏≠È¢ÑÊµãÁõÆÊ†áÁöÑ‰ΩçÁΩÆÂíåÁ±ªÂà´„ÄÇ&lt;/p>
&lt;h4 id="12-ÁΩëÁªúÁªìÊûÑ" >
&lt;div>
&lt;a href="#12-%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84">
###
&lt;/a>
1.2 ÁΩëÁªúÁªìÊûÑÔºö
&lt;/div>
&lt;/h4>
&lt;p>YOLO Ê®°Âûã‰∏ªË¶ÅÁî±Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÁªÑÊàêÔºåÈÄöÂ∏∏ÈááÁî®Á±ª‰ºº‰∫éDarknetÁöÑÊ∑±Â±ÇÂç∑ÁßØÁ•ûÁªèÁΩëÁªú‰Ωú‰∏∫ÁâπÂæÅÊèêÂèñÂô®„ÄÇ&lt;/p>
&lt;h4 id="13-ÁΩëÁªúËæìÂá∫" >
&lt;div>
&lt;a href="#13-%e7%bd%91%e7%bb%9c%e8%be%93%e5%87%ba">
###
&lt;/a>
1.3 ÁΩëÁªúËæìÂá∫Ôºö
&lt;/div>
&lt;/h4>
&lt;p>YOLO Â∞ÜÂõæÂÉèÂàíÂàÜ‰∏∫Âõ∫ÂÆöÂ§ßÂ∞èÁöÑÁΩëÊ†ºÔºåÂπ∂‰∏∫ÊØè‰∏™ÁΩëÊ†ºÈ¢ÑÊµãÂ§ö‰∏™ËæπÁïåÊ°ÜÂíåÂØπÂ∫îÁöÑÁ±ªÂà´Ê¶ÇÁéá„ÄÇÊØè‰∏™ËæπÁïåÊ°ÜÁî±‰∫î‰∏™ÂùêÊ†áÂÄºÂíåÁ±ªÂà´Ê¶ÇÁéáÁªÑÊàêÔºö$(x, y, w, h, p)$ÔºåÂÖ∂‰∏≠ $(x, y)$ ÊòØËæπÁïåÊ°ÜÁöÑ‰∏≠ÂøÉÂùêÊ†áÔºå$(w, h)$ ÊòØËæπÁïåÊ°ÜÁöÑÂÆΩÂ∫¶ÂíåÈ´òÂ∫¶Ôºå$p$ ÊòØËæπÁïåÊ°ÜÂåÖÂê´ÁõÆÊ†áÁöÑÁΩÆ‰ø°Â∫¶„ÄÇ&lt;/p>
&lt;h4 id="14-ÊçüÂ§±ÂáΩÊï∞" >
&lt;div>
&lt;a href="#14-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0">
###
&lt;/a>
1.4 ÊçüÂ§±ÂáΩÊï∞Ôºö
&lt;/div>
&lt;/h4>
&lt;p>YOLO Ê®°Âûã‰ΩøÁî®ÁªÑÂêàÊçüÂ§±ÂáΩÊï∞Êù•ÂêåÊó∂‰ºòÂåñËæπÁïåÊ°Ü‰ΩçÁΩÆÁöÑÂáÜÁ°ÆÊÄßÂíåÁ±ªÂà´ÁöÑÈ¢ÑÊµãÁ≤æÂ∫¶„ÄÇËØ•ÊçüÂ§±ÂáΩÊï∞ÂåÖÊã¨‰ΩçÁΩÆËØØÂ∑Æ„ÄÅÁΩÆ‰ø°Â∫¶ËØØÂ∑ÆÂíåÁ±ªÂà´ËØØÂ∑Æ„ÄÇ&lt;/p>
&lt;h3 id="2-yolo-Ê®°ÂûãÁöÑÂÆûÁé∞" >
&lt;div>
&lt;a href="#2-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
2. YOLO Ê®°ÂûãÁöÑÂÆûÁé∞Ôºö
&lt;/div>
&lt;/h3>
&lt;h4 id="21-ËÆ≠ÁªÉÈò∂ÊÆµ" >
&lt;div>
&lt;a href="#21-%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5">
###
&lt;/a>
2.1 ËÆ≠ÁªÉÈò∂ÊÆµÔºö
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Êï∞ÊçÆÂáÜÂ§á&lt;/strong>ÔºöÊî∂ÈõÜÂíåÊ†áËÆ∞ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨ÂõæÂÉèÂíåÂØπÂ∫îÁöÑÁõÆÊ†áËæπÁïåÊ°Ü„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÁΩëÁªúÁªìÊûÑ&lt;/strong>ÔºöÈÄâÊã©ÈÄÇÂΩìÁöÑÁΩëÁªúÁªìÊûÑÔºåÈÄöÂ∏∏‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑDarknetÁΩëÁªúÊàñÂÖ∂ÂèòÁßç„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÊçüÂ§±ÂáΩÊï∞&lt;/strong>ÔºöÂÆö‰πâÂíåÂÆûÁé∞ÁªÑÂêàÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫é‰ºòÂåñÁΩëÁªúÂèÇÊï∞„ÄÇ&lt;/li>
&lt;li>&lt;strong>ËÆ≠ÁªÉËøáÁ®ã&lt;/strong>Ôºö‰ΩøÁî®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂØπÊ®°ÂûãËøõË°åËÆ≠ÁªÉÔºåÈÄöËøáÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÊõ¥Êñ∞ÁΩëÁªúÂèÇÊï∞Ôºå‰ª•ÊúÄÂ∞èÂåñÊçüÂ§±ÂáΩÊï∞„ÄÇ&lt;/li>
&lt;/ul>
&lt;h4 id="22-Êé®ÁêÜÈò∂ÊÆµ" >
&lt;div>
&lt;a href="#22-%e6%8e%a8%e7%90%86%e9%98%b6%e6%ae%b5">
###
&lt;/a>
2.2 Êé®ÁêÜÈò∂ÊÆµÔºö
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>ÂâçÂêë‰º†Êí≠&lt;/strong>ÔºöÂ∞ÜÂæÖÊ£ÄÊµãÁöÑÂõæÂÉèËæìÂÖ•Âà∞ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã‰∏≠ÔºåÈÄöËøáÂâçÂêë‰º†Êí≠ÁÆóÊ≥ïËé∑ÂèñÊØè‰∏™ËæπÁïåÊ°ÜÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÂêéÂ§ÑÁêÜ&lt;/strong>ÔºöÂØπÁΩëÁªúËæìÂá∫ËøõË°åÂêéÂ§ÑÁêÜÔºåÂåÖÊã¨ÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÔºàNMSÔºâÂíåÈòàÂÄºÁ≠õÈÄâÔºå‰ª•ÂéªÈô§ÈáçÂè†ÁöÑËæπÁïåÊ°ÜÂíå‰ΩéÁΩÆ‰ø°Â∫¶ÁöÑËæπÁïåÊ°Ü„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÁõÆÊ†áÊ°ÜÁªòÂà∂&lt;/strong>ÔºöÊ†πÊçÆÊúÄÁªàÁöÑËæπÁïåÊ°ÜÁªìÊûúÔºåÂ∞ÜÁõÆÊ†áÊ°ÜÁªòÂà∂Âú®ÂéüÂßãÂõæÂÉè‰∏äÔºåÂπ∂Ê†áËÆ∞Á±ªÂà´„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="3-yolo-Ê®°ÂûãÁöÑ‰ºòÂäø" >
&lt;div>
&lt;a href="#3-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e4%bc%98%e5%8a%bf">
##
&lt;/a>
3. YOLO Ê®°ÂûãÁöÑ‰ºòÂäøÔºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>ÈÄüÂ∫¶Âø´&lt;/strong>ÔºöYOLO ÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÔºåËÉΩÂ§üÂÆûÊó∂Â§ÑÁêÜÂõæÂÉèÂíåËßÜÈ¢ëÊµÅ„ÄÇ&lt;/li>
&lt;li>&lt;strong>Á´ØÂà∞Á´Ø&lt;/strong>ÔºöYOLO Â∞ÜÁõÆÊ†áÊ£ÄÊµãËßÜ‰∏∫‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÂõûÂΩíÈóÆÈ¢òÔºåÁÆÄÂåñ‰∫ÜÊ£ÄÊµãÊµÅÁ®ã„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÂÖ®Â±Ä‰ø°ÊÅØ&lt;/strong>ÔºöYOLO Âú®Êï¥‰∏™ÂõæÂÉè‰∏äËøõË°åÊ£ÄÊµãÔºåËÉΩÂ§üÂêåÊó∂ËÄÉËôëÂõæÂÉè‰∏≠ÁöÑÂÖ®Â±Ä‰ø°ÊÅØÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£Âú∫ÊôØ„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="4-ÊÄªÁªì" >
&lt;div>
&lt;a href="#4-%e6%80%bb%e7%bb%93">
##
&lt;/a>
4. ÊÄªÁªìÔºö
&lt;/div>
&lt;/h3>
&lt;p>YOLO ÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂçïÈò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÔºåÈÄöËøáÂ∞ÜÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°ËΩ¨Âåñ‰∏∫ÂõûÂΩíÈóÆÈ¢òÔºåÂπ∂ÁªìÂêàÊúâÊïàÁöÑÁΩëÁªúÁªìÊûÑÂíåÊçüÂ§±ÂáΩÊï∞ÔºåÂÆûÁé∞‰∫ÜÂú®‰øùÊåÅÈ´òÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÂÆûÊó∂ËøõË°åÁõÆÊ†áÊ£ÄÊµãÁöÑËÉΩÂäõ„ÄÇ&lt;/p>
&lt;p>&amp;ldquo;ÈîöÂÆöÁÇπ&amp;rdquo;ÔºàAnchor BoxesÔºâÊòØYOLOÊ®°Âûã‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊ¶ÇÂøµÔºåÂÆÉÁî®‰∫éËß£ÂÜ≥ÁõÆÊ†áÊ£ÄÊµã‰∏≠‰∏çÂêåÁõÆÊ†áÂ∞∫ÂØ∏ÂíåÊØî‰æãÁöÑÈóÆÈ¢ò„ÄÇÂú®YOLO‰∏≠ÔºåÊØè‰∏™ÁΩëÊ†ºÂçïÂÖÉÈÉΩË¥üË¥£È¢ÑÊµã‰∏ÄÁªÑÂõ∫ÂÆöÊï∞ÈáèÂíåÂõ∫ÂÆöÂ§ßÂ∞èÁöÑËæπÁïåÊ°ÜÔºàÂç≥ÈîöÂÆöÁÇπÔºâÔºå‰ª•‰æøÊ£ÄÊµã‰∏çÂêåÂ∞∫ÂØ∏ÂíåÊØî‰æãÁöÑÁõÆÊ†á„ÄÇ&lt;/p>
&lt;h3 id="1-ÈîöÂÆöÁÇπÁöÑÊ¶ÇÂøµ" >
&lt;div>
&lt;a href="#1-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%a6%82%e5%bf%b5">
##
&lt;/a>
1. ÈîöÂÆöÁÇπÁöÑÊ¶ÇÂøµÔºö
&lt;/div>
&lt;/h3>
&lt;h4 id="11-ÈóÆÈ¢ò" >
&lt;div>
&lt;a href="#11-%e9%97%ae%e9%a2%98">
###
&lt;/a>
1.1 ÈóÆÈ¢òÔºö
&lt;/div>
&lt;/h4>
&lt;p>‰º†ÁªüÁöÑÁõÆÊ†áÊ£ÄÊµãÁÆóÊ≥ïÈÄöÂ∏∏‰ºöÂ∞Ü‰∏çÂêåÂ∞∫ÂØ∏ÂíåÊØî‰æãÁöÑÁõÆÊ†áÂàÜÈÖçÁªô‰∏çÂêåÁöÑÁΩëÁªúÂ±ÇÊù•Â§ÑÁêÜÔºåËøôÁßçÊñπÊ≥ï‰∏çÂ§üÁÅµÊ¥ªÔºåÊó†Ê≥ïÂæàÂ•ΩÂú∞ÈÄÇÂ∫îÂ§öÊ†∑ÂåñÁöÑÁõÆÊ†á„ÄÇ&lt;/p>
&lt;h4 id="12-Ëß£ÂÜ≥ÊñπÊ≥ï" >
&lt;div>
&lt;a href="#12-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%b3%95">
###
&lt;/a>
1.2 Ëß£ÂÜ≥ÊñπÊ≥ïÔºö
&lt;/div>
&lt;/h4>
&lt;p>YOLO‰ΩøÁî®ÈîöÂÆöÁÇπÁöÑÊÄùÊÉ≥ÔºåÂ∞Ü‰∏çÂêåÂ∞∫ÂØ∏ÂíåÊØî‰æãÁöÑÁõÆÊ†áÁªü‰∏ÄÂàÜÈÖçÁªôÊØè‰∏™ÁΩëÊ†ºÂçïÂÖÉÔºåÂπ∂Âú®ÊØè‰∏™ÁΩëÊ†ºÂçïÂÖÉ‰∏≠È¢ÑÊµãÂõ∫ÂÆöÊï∞ÈáèÁöÑËæπÁïåÊ°Ü„ÄÇËøôÊ†∑ÂèØ‰ª•Â¢ûÂä†Ê®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊ£ÄÊµã‰∏çÂêåÂ∞∫ÂØ∏ÂíåÊØî‰æãÁöÑÁõÆÊ†á„ÄÇ&lt;/p>
&lt;h3 id="2-ÈîöÂÆöÁÇπÁöÑÊÄùÊÉ≥" >
&lt;div>
&lt;a href="#2-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%80%9d%e6%83%b3">
##
&lt;/a>
2. ÈîöÂÆöÁÇπÁöÑÊÄùÊÉ≥Ôºö
&lt;/div>
&lt;/h3>
&lt;h4 id="21-È¢ÑÂÆö‰πâÂ§ßÂ∞èÂíåÊØî‰æã" >
&lt;div>
&lt;a href="#21-%e9%a2%84%e5%ae%9a%e4%b9%89%e5%a4%a7%e5%b0%8f%e5%92%8c%e6%af%94%e4%be%8b">
###
&lt;/a>
2.1 È¢ÑÂÆö‰πâÂ§ßÂ∞èÂíåÊØî‰æãÔºö
&lt;/div>
&lt;/h4>
&lt;p>Âú®YOLO‰∏≠ÔºåÈîöÂÆöÁÇπÊòØ‰∏ÄÁªÑÈ¢ÑÂÆö‰πâÁöÑÂ§ßÂ∞èÂíåÊØî‰æãÁöÑËæπÁïåÊ°Ü„ÄÇËøô‰∫õËæπÁïåÊ°ÜÈÄöÂ∏∏ÊòØÂú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏äÈÄöËøáËÅöÁ±ªÁ≠âÊñπÊ≥ïÂæóÂà∞ÁöÑÔºå‰ª•Á°Æ‰øùÊ∂µÁõñ‰∫ÜÂ§ßÈÉ®ÂàÜÁõÆÊ†áÁöÑÂ§ßÂ∞èÂíåÊØî‰æã„ÄÇ&lt;/p>
&lt;h4 id="22-ÂçïÂÖÉÊ†ºÂÜÖÂ§ö‰∏™ËæπÁïåÊ°Ü" >
&lt;div>
&lt;a href="#22-%e5%8d%95%e5%85%83%e6%a0%bc%e5%86%85%e5%a4%9a%e4%b8%aa%e8%be%b9%e7%95%8c%e6%a1%86">
###
&lt;/a>
2.2 ÂçïÂÖÉÊ†ºÂÜÖÂ§ö‰∏™ËæπÁïåÊ°ÜÔºö
&lt;/div>
&lt;/h4>
&lt;p>ÂØπ‰∫éÊØè‰∏™ÁΩëÊ†ºÂçïÂÖÉÔºåYOLOÊ®°ÂûãÈ¢ÑÊµãÂõ∫ÂÆöÊï∞ÈáèÁöÑËæπÁïåÊ°ÜÔºåÊØè‰∏™ËæπÁïåÊ°ÜÁöÑÂ§ßÂ∞èÂíåÊØî‰æã‰∏éÈ¢ÑÂÆö‰πâÁöÑÈîöÂÆöÁÇπÁõ∏ÂØπÂ∫î„ÄÇËøôÊ†∑ÔºåÊØè‰∏™ÁΩëÊ†ºÂçïÂÖÉÂèØ‰ª•ÂêåÊó∂Ê£ÄÊµãÂ§ö‰∏™‰∏çÂêåÂ∞∫ÂØ∏ÂíåÊØî‰æãÁöÑÁõÆÊ†á„ÄÇ&lt;/p>
&lt;h3 id="3-ÈîöÂÆöÁÇπÁöÑÊäÄÊúØÂÆûÁé∞" >
&lt;div>
&lt;a href="#3-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%8a%80%e6%9c%af%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
3. ÈîöÂÆöÁÇπÁöÑÊäÄÊúØÂÆûÁé∞Ôºö
&lt;/div>
&lt;/h3>
&lt;h4 id="31-È¢ÑËÆ≠ÁªÉÈîöÂÆöÁÇπ" >
&lt;div>
&lt;a href="#31-%e9%a2%84%e8%ae%ad%e7%bb%83%e9%94%9a%e5%ae%9a%e7%82%b9">
###
&lt;/a>
3.1 È¢ÑËÆ≠ÁªÉÈîöÂÆöÁÇπÔºö
&lt;/div>
&lt;/h4>
&lt;p>Âú®ËÆ≠ÁªÉ‰πãÂâçÔºåÈÄöÂ∏∏‰ºöÈÄöËøáÂØπËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÁõÆÊ†áËæπÁïåÊ°ÜËøõË°åËÅöÁ±ªÊàñËÄÖÊâãÂä®ÈÄâÊã©ÔºåÊù•Á°ÆÂÆö‰∏ÄÁªÑÈîöÂÆöÁÇπÁöÑÂ§ßÂ∞èÂíåÊØî‰æã„ÄÇ&lt;/p>
&lt;h4 id="32-Ê®°ÂûãÈ¢ÑÊµã" >
&lt;div>
&lt;a href="#32-%e6%a8%a1%e5%9e%8b%e9%a2%84%e6%b5%8b">
###
&lt;/a>
3.2 Ê®°ÂûãÈ¢ÑÊµãÔºö
&lt;/div>
&lt;/h4>
&lt;p>Âú®Ê®°ÂûãÊé®ÁêÜÈò∂ÊÆµÔºåÊØè‰∏™ÁΩëÊ†ºÂçïÂÖÉÈÄöËøáÂõûÂΩíÈ¢ÑÊµãÂõ∫ÂÆöÊï∞ÈáèÁöÑËæπÁïåÊ°ÜÔºåÊØè‰∏™ËæπÁïåÊ°ÜÁöÑÂ∞∫ÂØ∏ÂíåÊØî‰æãÁî±ÂØπÂ∫îÁöÑÈîöÂÆöÁÇπÁ°ÆÂÆö„ÄÇ&lt;/p>
&lt;h4 id="33-ËæπÁïåÊ°ÜË∞ÉÊï¥" >
&lt;div>
&lt;a href="#33-%e8%be%b9%e7%95%8c%e6%a1%86%e8%b0%83%e6%95%b4">
###
&lt;/a>
3.3 ËæπÁïåÊ°ÜË∞ÉÊï¥Ôºö
&lt;/div>
&lt;/h4>
&lt;p>Ê®°ÂûãÈ¢ÑÊµãÁöÑËæπÁïåÊ°ÜÈÄöÂ∏∏ÊòØÁõ∏ÂØπ‰∫éÁΩëÊ†ºÂçïÂÖÉÁöÑÂÅèÁßªÈáèÂíåÂ∞∫ÂØ∏ÂÅèÂ∑ÆÔºåÈúÄË¶ÅÊ†πÊçÆÈîöÂÆöÁÇπËøõË°åË∞ÉÊï¥ÔºåÂæóÂà∞ÊúÄÁªàÁöÑËæπÁïåÊ°Ü‰ΩçÁΩÆ„ÄÇ&lt;/p>
&lt;h3 id="4-ÊÄªÁªì-1" >
&lt;div>
&lt;a href="#4-%e6%80%bb%e7%bb%93-1">
##
&lt;/a>
4. ÊÄªÁªìÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÈîöÂÆöÁÇπÊòØYOLOÊ®°Âûã‰∏≠Áî®‰∫éËß£ÂÜ≥‰∏çÂêåÂ∞∫ÂØ∏ÂíåÊØî‰æãÁõÆÊ†áÊ£ÄÊµãÈóÆÈ¢òÁöÑÂÖ≥ÈîÆÊ¶ÇÂøµÔºåÈÄöËøáÈ¢ÑÂÆö‰πâ‰∏ÄÁªÑÂ§ßÂ∞èÂíåÊØî‰æãÁöÑËæπÁïåÊ°ÜÔºåÂπ∂Âú®ÊØè‰∏™ÁΩëÊ†ºÂçïÂÖÉ‰∏≠È¢ÑÊµãËøô‰∫õËæπÁïåÊ°ÜÁöÑ‰ΩçÁΩÆÂíåÁ±ªÂà´ÔºåÂÆûÁé∞‰∫ÜÊ®°ÂûãÂØπÂ§öÊ†∑ÂåñÁõÆÊ†áÁöÑÊúâÊïàÊ£ÄÊµã„ÄÇ&lt;/p>
&lt;h2 id="transfer-learning" >
&lt;div>
&lt;a href="#transfer-learning">
#
&lt;/a>
transfer-learning
&lt;/div>
&lt;/h2>
&lt;p>ËøÅÁßªÂ≠¶‰π†ÊòØ‰∏ÄÁßçÈÄöËøáÂ∞ÜÂ∑≤Â≠¶‰π†ÁöÑÁü•ËØÜ‰ªé‰∏Ä‰∏™‰ªªÂä°ÊàñÈ¢ÜÂüüÂ∫îÁî®Âà∞Âè¶‰∏Ä‰∏™‰ªªÂä°ÊàñÈ¢ÜÂüüÁöÑÊú∫Âô®Â≠¶‰π†ÊäÄÊúØ„ÄÇÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠ÔºåËøÅÁßªÂ≠¶‰π†ÂèØ‰ª•ÈÄöËøáÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊ®°ÂûãÊàñÁâπÂæÅÊù•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂ∏∏Áî®ÁöÑËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÔºö&lt;/p>
&lt;h3 id="1-ÂæÆË∞Éfine-tuning" >
&lt;div>
&lt;a href="#1-%e5%be%ae%e8%b0%83fine-tuning">
##
&lt;/a>
1. ÂæÆË∞ÉÔºàFine-tuningÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÂæÆË∞ÉÊòØËøÅÁßªÂ≠¶‰π†‰∏≠ÊúÄÂ∏∏ËßÅÁöÑÊñπÊ≥ï‰πã‰∏ÄÔºåÂÆÉÈÄöÂ∏∏ÂåÖÊã¨‰ª•‰∏ãÊ≠•È™§Ôºö&lt;/p>
&lt;ul>
&lt;li>&lt;strong>È¢ÑËÆ≠ÁªÉÊ®°ÂûãÈÄâÊã©&lt;/strong>ÔºöÈÄâÊã©‰∏Ä‰∏™Âú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÔºå‰æãÂ¶Ç ImageNet ‰∏äÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇ&lt;/li>
&lt;li>&lt;strong>Ê®°ÂûãÂÜªÁªì&lt;/strong>ÔºöÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÈÉ®ÂàÜÊàñÂÖ®ÈÉ®Â±ÇÂÜªÁªìÔºåÂç≥‰∏çÊõ¥Êñ∞ÂÆÉ‰ª¨ÁöÑÊùÉÈáç„ÄÇ&lt;/li>
&lt;li>&lt;strong>È°∂Â±ÇÊõøÊç¢&lt;/strong>ÔºöÊõøÊç¢È¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÈ°∂Â±ÇÔºàÈÄöÂ∏∏ÊòØÂÖ®ËøûÊé•Â±ÇÔºâÊàñËÄÖÊ∑ªÂä†Êñ∞ÁöÑÂÖ®ËøûÊé•Â±ÇÔºå‰ª•ÈÄÇÂ∫îÊñ∞ÁöÑÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÂæÆË∞ÉËÆ≠ÁªÉ&lt;/strong>ÔºöÂú®ÁõÆÊ†áÊ£ÄÊµãÊï∞ÊçÆÈõÜ‰∏äÂØπÊï¥‰∏™Ê®°ÂûãËøõË°åËÆ≠ÁªÉÔºåÂåÖÊã¨Êõ¥Êñ∞È°∂Â±ÇÂíåÈÉ®ÂàÜÊàñÂÖ®ÈÉ®Ëß£ÂÜªÁöÑÂ±Ç„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="2-ÁâπÂæÅÊèêÂèñfeature-extraction" >
&lt;div>
&lt;a href="#2-%e7%89%b9%e5%be%81%e6%8f%90%e5%8f%96feature-extraction">
##
&lt;/a>
2. ÁâπÂæÅÊèêÂèñÔºàFeature ExtractionÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÁâπÂæÅÊèêÂèñÊòØ‰∏ÄÁßçÊõ¥ËΩªÈáèÁ∫ßÁöÑËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÔºåÂÆÉ‰∏çÊ∂âÂèäÂà∞Êï¥‰∏™Ê®°ÂûãÁöÑÈáçÊñ∞ËÆ≠ÁªÉÔºåËÄåÊòØ‰ªÖ‰ªÖÂà©Áî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÁâπÂæÅÊèêÂèñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;ul>
&lt;li>&lt;strong>È¢ÑËÆ≠ÁªÉÊ®°ÂûãÈÄâÊã©&lt;/strong>ÔºöÂêåÊ†∑ÈÄâÊã©‰∏Ä‰∏™Âú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã„ÄÇ&lt;/li>
&lt;li>&lt;strong>ÁâπÂæÅÊèêÂèñ&lt;/strong>ÔºöÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÂç∑ÁßØÂ±ÇÔºàÈÄöÂ∏∏ÊòØÈô§‰∫ÜÂÖ®ËøûÊé•Â±Ç‰πãÂ§ñÁöÑÊâÄÊúâÂ±ÇÔºâ‰Ωú‰∏∫ÁâπÂæÅÊèêÂèñÂô®ÔºåÂπ∂Â∞ÜÊèêÂèñÂà∞ÁöÑÁâπÂæÅ‰Ωú‰∏∫ËæìÂÖ•ÔºåÁî®‰∫éËÆ≠ÁªÉ‰∏Ä‰∏™Êñ∞ÁöÑÂàÜÁ±ªÂô®ÊàñÁõÆÊ†áÊ£ÄÊµãÂô®„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="3-È¢ÜÂüüËá™ÈÄÇÂ∫îdomain-adaptation" >
&lt;div>
&lt;a href="#3-%e9%a2%86%e5%9f%9f%e8%87%aa%e9%80%82%e5%ba%94domain-adaptation">
##
&lt;/a>
3. È¢ÜÂüüËá™ÈÄÇÂ∫îÔºàDomain AdaptationÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>È¢ÜÂüüËá™ÈÄÇÂ∫îÊòØ‰∏ÄÁßçÁâπÊÆäÁöÑËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÔºåÁî®‰∫éËß£ÂÜ≥Ê∫êÂüüÂíåÁõÆÊ†áÂüüÊï∞ÊçÆÂàÜÂ∏É‰∏çÂåπÈÖçÁöÑÈóÆÈ¢ò„ÄÇÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠ÔºåÂèØ‰ª•ÈÄöËøáÂú®Ê∫êÂüüÂíåÁõÆÊ†áÂüü‰πãÈó¥ËøõË°åÂüüÈÄÇÂ∫îÊù•ÊèêÈ´òÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ÂüüÈÄÇÂ∫îÊñπÊ≥ï&lt;/strong>ÔºöÂ∏∏ËßÅÁöÑÂüüÈÄÇÂ∫îÊñπÊ≥ïÂåÖÊã¨ÂØπÊäóËÆ≠ÁªÉ„ÄÅÈ¢ÜÂüüÂØπÈΩêÁ≠âÔºåÈÄöËøáË∞ÉÊï¥Ê®°ÂûãÁöÑËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ΩøÂæóÊ®°ÂûãÂú®ÁõÆÊ†áÂüü‰∏äË°®Áé∞Êõ¥Â•Ω„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="4-Áü•ËØÜËí∏È¶èknowledge-distillation" >
&lt;div>
&lt;a href="#4-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8fknowledge-distillation">
##
&lt;/a>
4. Áü•ËØÜËí∏È¶èÔºàKnowledge DistillationÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>Áü•ËØÜËí∏È¶èÊòØ‰∏ÄÁßçÈÄöËøáÂà©Áî®Â∑≤ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÁöÑÁü•ËØÜÊù•ËÆ≠ÁªÉ‰∏Ä‰∏™Êõ¥ËΩªÈáèÁ∫ßÁöÑÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠ÔºåÂèØ‰ª•Âà©Áî®‰∏Ä‰∏™Â§ßÂûãÊ®°ÂûãÁöÑÁü•ËØÜÊù•ËÆ≠ÁªÉ‰∏Ä‰∏™Â∞èÂûãÊ®°ÂûãÔºå‰ª•Èôç‰ΩéÊ®°ÂûãÁöÑÂ§çÊùÇÂ∫¶ÂíåËÆ°ÁÆóÊàêÊú¨„ÄÇ&lt;/p>
&lt;p>‰ª•‰∏äÊòØ‰∏Ä‰∫õÂ∏∏Áî®ÁöÑËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÔºåÂèØ‰ª•Ê†πÊçÆÂÖ∑‰ΩìÁöÑ‰ªªÂä°ÈúÄÊ±ÇÂíåÊÉÖÂÜµÈÄâÊã©ÂêàÈÄÇÁöÑÊñπÊ≥ïÊù•ÊèêÂçáÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;h2 id="ÂõæÂÉèËØÜÂà´ÈïøÂ∞æÂàÜÂ∏ÉÈóÆÈ¢ò" >
&lt;div>
&lt;a href="#%e5%9b%be%e5%83%8f%e8%af%86%e5%88%ab%e9%95%bf%e5%b0%be%e5%88%86%e5%b8%83%e9%97%ae%e9%a2%98">
#
&lt;/a>
ÂõæÂÉèËØÜÂà´ÈïøÂ∞æÂàÜÂ∏ÉÈóÆÈ¢ò
&lt;/div>
&lt;/h2>
&lt;p>Â§ÑÁêÜËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÈïøÂ∞æÂàÜÂ∏ÉÊòØ‰∏Ä‰∏™Â∏∏ËßÅÁöÑÈóÆÈ¢òÔºåÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠‰πüÂêåÊ†∑Â≠òÂú®„ÄÇÈïøÂ∞æÂàÜÂ∏ÉÊÑèÂë≥ÁùÄÊúâ‰∫õÁ±ªÂà´ÁöÑÊ†∑Êú¨Êï∞ÈáèÈùûÂ∏∏Â∞ëÔºåËÄåÂè¶‰∏Ä‰∫õÁ±ªÂà´ÁöÑÊ†∑Êú¨Êï∞ÈáèÈùûÂ∏∏Â§ö„ÄÇËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÁöÑÊñπÊ≥ïÂåÖÊã¨Ôºö&lt;/p>
&lt;h3 id="1-Êï∞ÊçÆÂ¢ûÂº∫data-augmentation" >
&lt;div>
&lt;a href="#1-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%badata-augmentation">
##
&lt;/a>
1. Êï∞ÊçÆÂ¢ûÂº∫ÔºàData AugmentationÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÂØπ‰∫éÂ∞ëÊ†∑Êú¨Á±ªÂà´ÔºåÂèØ‰ª•ÈÄöËøáÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÊù•ÁîüÊàêÊõ¥Â§öÁöÑÊ†∑Êú¨Ôºå‰ª•Âπ≥Ë°°‰∏çÂêåÁ±ªÂà´‰πãÈó¥ÁöÑÊ†∑Êú¨Êï∞ÈáèÂ∑ÆÂºÇ„ÄÇÂ∏∏Áî®ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÂåÖÊã¨ÈöèÊú∫ÊóãËΩ¨„ÄÅË£ÅÂâ™„ÄÅÁº©Êîæ„ÄÅÂπ≥Áßª„ÄÅÈ¢úËâ≤ÂèòÊç¢Á≠â„ÄÇ&lt;/p>
&lt;h3 id="2-Á±ªÂà´Âä†ÊùÉclass-weighting" >
&lt;div>
&lt;a href="#2-%e7%b1%bb%e5%88%ab%e5%8a%a0%e6%9d%83class-weighting">
##
&lt;/a>
2. Á±ªÂà´Âä†ÊùÉÔºàClass WeightingÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÂØπ‰∫éÈïøÂ∞æÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÈõÜÔºåÂèØ‰ª•ÈááÁî®Á±ªÂà´Âä†ÊùÉÁöÑÊñπÂºèÊù•Ë∞ÉÊï¥Ê®°ÂûãÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøÂæóÊ®°ÂûãÂØπÂ∞ëÊ†∑Êú¨Á±ªÂà´Êõ¥Âä†ÊïèÊÑü„ÄÇÂèØ‰ª•Ê†πÊçÆÁ±ªÂà´Âá∫Áé∞ÁöÑÈ¢ëÁéáÊù•ËÆæÁΩÆ‰∏çÂêåÁ±ªÂà´ÁöÑÊùÉÈáçÔºå‰ΩøÂæóÊçüÂ§±ÂáΩÊï∞Êõ¥Âπ≥Ë°°„ÄÇ&lt;/p>
&lt;h3 id="3-ÈáçÊñ∞ÈááÊ†∑resampling" >
&lt;div>
&lt;a href="#3-%e9%87%8d%e6%96%b0%e9%87%87%e6%a0%b7resampling">
##
&lt;/a>
3. ÈáçÊñ∞ÈááÊ†∑ÔºàResamplingÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÈáçÊñ∞ÈááÊ†∑ÊäÄÊúØÂèØ‰ª•ÈÄöËøáËøáÈááÊ†∑ÊàñÊ¨†ÈááÊ†∑Êù•Ë∞ÉÊï¥Êï∞ÊçÆÈõÜ‰∏≠‰∏çÂêåÁ±ªÂà´ÁöÑÊ†∑Êú¨Êï∞Èáè„ÄÇÂØπ‰∫éÂ∞ëÊ†∑Êú¨Á±ªÂà´ÔºåÂèØ‰ª•ÈááÁî®ËøáÈááÊ†∑ÁöÑÊñπÊ≥ïÂ¢ûÂä†Ê†∑Êú¨Êï∞ÈáèÔºåÊàñËÄÖÈááÁî®Ê¨†ÈááÊ†∑ÁöÑÊñπÊ≥ïÂáèÂ∞ëÊ†∑Êú¨Êï∞Èáè„ÄÇ&lt;/p>
&lt;h3 id="4-Âº±ÁõëÁù£Â≠¶‰π†weakly-supervised-learning" >
&lt;div>
&lt;a href="#4-%e5%bc%b1%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0weakly-supervised-learning">
##
&lt;/a>
4. Âº±ÁõëÁù£Â≠¶‰π†ÔºàWeakly Supervised LearningÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>Âú®ÈïøÂ∞æÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÈõÜ‰∏≠ÔºåÊúâ‰∫õÁ±ªÂà´ÂèØËÉΩÂè™ÊúâÂ∞ëÈáèÁöÑÊúâÊ†áÁ≠æÊ†∑Êú¨ÔºåËÄåÂ§ßÈÉ®ÂàÜÊ†∑Êú¨ÊòØÊú™Ê†áÁ≠æÁöÑ„ÄÇÂèØ‰ª•Âà©Áî®Âº±ÁõëÁù£Â≠¶‰π†ÁöÑÊñπÊ≥ïÔºå‰æãÂ¶Ç‰ΩøÁî®Êó†ÁõëÁù£ÊàñÂçäÁõëÁù£Â≠¶‰π†ÊäÄÊúØÔºå‰ªéÊú™Ê†áÁ≠æÁöÑÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÊúâÁî®ÁöÑÁâπÂæÅ„ÄÇ&lt;/p>
&lt;h3 id="5-Â§ö‰ªªÂä°Â≠¶‰π†multi-task-learning" >
&lt;div>
&lt;a href="#5-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e5%ad%a6%e4%b9%a0multi-task-learning">
##
&lt;/a>
5. Â§ö‰ªªÂä°Â≠¶‰π†ÔºàMulti-Task LearningÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>Â§ö‰ªªÂä°Â≠¶‰π†ÂèØ‰ª•Â∞ÜÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏éÂÖ∂‰ªñÁõ∏ÂÖ≥‰ªªÂä°ÁªìÂêàËµ∑Êù•ËÆ≠ÁªÉÊ®°ÂûãÔºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÂú®ÈïøÂ∞æÂàÜÂ∏ÉÊï∞ÊçÆÈõÜ‰∏äÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•Â∞ÜÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏éÂõæÂÉèÂàÜÁ±ª„ÄÅËØ≠‰πâÂàÜÂâ≤Á≠â‰ªªÂä°ÁªìÂêàËµ∑Êù•ËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÂà©Áî®È¢ùÂ§ñÁöÑ‰ø°ÊÅØÊù•ÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;h3 id="6-ÂºÇÂ∏∏Ê£ÄÊµãanomaly-detection" >
&lt;div>
&lt;a href="#6-%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8banomaly-detection">
##
&lt;/a>
6. ÂºÇÂ∏∏Ê£ÄÊµãÔºàAnomaly DetectionÔºâÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÈíàÂØπÈïøÂ∞æÂàÜÂ∏ÉÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÂ∞ëÊ†∑Êú¨Á±ªÂà´ÔºåÂèØ‰ª•ÈááÁî®ÂºÇÂ∏∏Ê£ÄÊµãÁöÑÊñπÊ≥ïÊù•ËØÜÂà´Âπ∂ÈáçÁÇπÂÖ≥Ê≥®Ëøô‰∫õÂ∞ëÊ†∑Êú¨Á±ªÂà´Ôºå‰ª•‰æøÊõ¥Â•ΩÂú∞Ë∞ÉÊï¥Ê®°ÂûãÂíå‰ºòÂåñÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;p>ÈÄöËøá‰ª•‰∏äÊñπÊ≥ïÔºåÂèØ‰ª•ÊúâÊïàÂú∞Â§ÑÁêÜËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÈïøÂ∞æÂàÜÂ∏ÉÈóÆÈ¢òÔºåÂπ∂ÊèêÂçáÊ®°ÂûãÂú®‰∏çÂêåÁ±ªÂà´‰∏äÁöÑÊÄßËÉΩ„ÄÇÂú®ÂÆûË∑µ‰∏≠ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÁöÑÊï∞ÊçÆÈõÜÂíå‰ªªÂä°ÈúÄÊ±ÇÈÄâÊã©ÂêàÈÄÇÁöÑÊñπÊ≥ïËøõË°åÂ§ÑÁêÜ„ÄÇ&lt;/p>
&lt;h2 id="auto-encoder-model" >
&lt;div>
&lt;a href="#auto-encoder-model">
#
&lt;/a>
Auto-encoder model
&lt;/div>
&lt;/h2>
&lt;p>È¢ÜÂüüËá™ÈÄÇÂ∫îÊòØ‰∏ÄÁßçÂ∞ÜÁü•ËØÜ‰ªé‰∏Ä‰∏™ÂüüÔºàÊ∫êÂüüÔºâËøÅÁßªÂà∞Âè¶‰∏Ä‰∏™ÂüüÔºàÁõÆÊ†áÂüüÔºâÁöÑËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÔºåÁî®‰∫éËß£ÂÜ≥Ê∫êÂüüÂíåÁõÆÊ†áÂüüÊï∞ÊçÆÂàÜÂ∏É‰∏çÂåπÈÖçÁöÑÈóÆÈ¢ò„ÄÇÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠ÔºåÈ¢ÜÂüüËá™ÈÄÇÂ∫îÂèØ‰ª•ÈÄöËøáÂØπÊäóËÆ≠ÁªÉÁöÑÊñπÂºèÊù•ÂÆûÁé∞ÔºåËÄå auto-encoder ÂèØ‰ª•‰Ωú‰∏∫‰∏ÄÁßçËæÖÂä©ÊâãÊÆµÊù•Â∏ÆÂä©ÂÆûÁé∞ÂØπÊäóËÆ≠ÁªÉ„ÄÇ‰ª•‰∏ãÊòØÂÖ∑‰ΩìÁöÑÊÄùË∑ØÂíåÁªÜËäÇÔºö&lt;/p>
&lt;h3 id="1-ÂØπÊäóËÆ≠ÁªÉÁöÑÂü∫Êú¨ÊÄùË∑Ø" >
&lt;div>
&lt;a href="#1-%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%80%9d%e8%b7%af">
##
&lt;/a>
1. ÂØπÊäóËÆ≠ÁªÉÁöÑÂü∫Êú¨ÊÄùË∑ØÔºö
&lt;/div>
&lt;/h3>
&lt;p>ÂØπÊäóËÆ≠ÁªÉÊòØ‰∏ÄÁßçÈÄöËøáËÆ≠ÁªÉ‰∏Ä‰∏™ÁîüÊàêÂô®Âíå‰∏Ä‰∏™Âà§Âà´Âô®ÁöÑÂØπÊäóËøáÁ®ãÔºåÊù•‰ΩøÂæóÁîüÊàêÂô®‰∫ßÁîüÁöÑÊï∞ÊçÆÂàÜÂ∏É‰∏éÁõÆÊ†áÂüüÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂ∞ΩÂèØËÉΩÂú∞Êé•Ëøë„ÄÇÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠ÔºåÂèØ‰ª•ÈÄöËøáÂØπÊäóËÆ≠ÁªÉÊù•Ë∞ÉÊï¥Ê®°ÂûãÔºå‰ΩøÂæóÊ®°ÂûãÂú®ÁõÆÊ†áÂüü‰∏äË°®Áé∞Êõ¥Â•Ω„ÄÇ&lt;/p>
&lt;h3 id="2-auto-encoder-Âú®ÂØπÊäóËÆ≠ÁªÉ‰∏≠ÁöÑ‰ΩúÁî®" >
&lt;div>
&lt;a href="#2-auto-encoder-%e5%9c%a8%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83%e4%b8%ad%e7%9a%84%e4%bd%9c%e7%94%a8">
##
&lt;/a>
2. auto-encoder Âú®ÂØπÊäóËÆ≠ÁªÉ‰∏≠ÁöÑ‰ΩúÁî®Ôºö
&lt;/div>
&lt;/h3>
&lt;p>auto-encoder ÊòØ‰∏ÄÁßçÊó†ÁõëÁù£Â≠¶‰π†Ê®°ÂûãÔºåÂÆÉÂèØ‰ª•Â∞ÜËæìÂÖ•Êï∞ÊçÆÁºñÁ†ÅÊàê‰ΩéÁª¥Ë°®Á§∫ÔºåÂπ∂Â∞ÜÂÖ∂Ëß£Á†ÅÂõûÂéüÂßãÊï∞ÊçÆ„ÄÇÂú®ÂØπÊäóËÆ≠ÁªÉ‰∏≠Ôºåauto-encoder ÂèØ‰ª•‰Ωú‰∏∫‰∏Ä‰∏™ÁîüÊàêÂô®ÔºåÁî®‰∫éÁîüÊàê‰∏éÁõÆÊ†áÂüüÊï∞ÊçÆÂàÜÂ∏ÉÁõ∏‰ººÁöÑÊï∞ÊçÆÊ†∑Êú¨„ÄÇÈÄöËøáËÆ≠ÁªÉ auto-encoderÔºåÂèØ‰ª•Â≠¶‰π†Âà∞ÁõÆÊ†áÂüüÊï∞ÊçÆÁöÑÁâπÂæÅË°®Á§∫Ôºå‰ªéËÄåÂ∏ÆÂä©ÂÆûÁé∞ÂØπÊäóËÆ≠ÁªÉ„ÄÇ&lt;/p>
&lt;h3 id="3-ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§" >
&lt;div>
&lt;a href="#3-%e5%85%b7%e4%bd%93%e5%ae%9e%e7%8e%b0%e6%ad%a5%e9%aa%a4">
##
&lt;/a>
3. ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§Ôºö
&lt;/div>
&lt;/h3>
&lt;h4 id="31-Ê®°ÂûãÈÄâÊã©" >
&lt;div>
&lt;a href="#31-%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9">
###
&lt;/a>
3.1 Ê®°ÂûãÈÄâÊã©Ôºö
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>ÈÄâÊã©‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÁöÑÊ®°Âûã‰Ωú‰∏∫Âü∫Á°ÄÊ®°ÂûãÔºå‰æãÂ¶ÇÂú®Ê∫êÂüü‰∏äËÆ≠ÁªÉÂ•ΩÁöÑÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã„ÄÇ&lt;/li>
&lt;/ul>
&lt;h4 id="32-auto-encoder-ËÆ≠ÁªÉ" >
&lt;div>
&lt;a href="#32-auto-encoder-%e8%ae%ad%e7%bb%83">
###
&lt;/a>
3.2 auto-encoder ËÆ≠ÁªÉÔºö
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>Âú®ÁõÆÊ†áÂüü‰∏äÊî∂ÈõÜ‰∏ÄÈÉ®ÂàÜÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®Ëøô‰∫õÊï∞ÊçÆÊù•ËÆ≠ÁªÉ auto-encoder„ÄÇauto-encoder ÁöÑËæìÂÖ•‰∏∫ÁõÆÊ†áÂüüÁöÑÂõæÂÉèÊï∞ÊçÆÔºåËæìÂá∫‰∏∫ÈáçÊûÑÁöÑÂõæÂÉèÊï∞ÊçÆ„ÄÇÈÄöËøáËÆ≠ÁªÉ auto-encoderÔºåÂèØ‰ª•Â≠¶‰π†Âà∞ÁõÆÊ†áÂüüÊï∞ÊçÆÁöÑÁâπÂæÅË°®Á§∫„ÄÇ&lt;/li>
&lt;/ul>
&lt;h4 id="33-ÂØπÊäóËÆ≠ÁªÉ" >
&lt;div>
&lt;a href="#33-%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83">
###
&lt;/a>
3.3 ÂØπÊäóËÆ≠ÁªÉÔºö
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>Â∞ÜËÆ≠ÁªÉÂ•ΩÁöÑ auto-encoder ‰Ωú‰∏∫ÁîüÊàêÂô®ÔºåÂ∞ÜÂü∫Á°ÄÊ®°ÂûãÔºàÊ∫êÂüü‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºâ‰Ωú‰∏∫Âà§Âà´Âô®„ÄÇ&lt;/li>
&lt;li>Â∞ÜÊ∫êÂüüÂíåÁõÆÊ†áÂüüÁöÑÊï∞ÊçÆÂàÜÂà´ËæìÂÖ•Âà∞ auto-encoder ÂíåÂü∫Á°ÄÊ®°Âûã‰∏≠ÔºåÁîüÊàêÂô®Â∞ùËØïÁîüÊàê‰∏éÁõÆÊ†áÂüüÊï∞ÊçÆÂàÜÂ∏ÉÁõ∏‰ººÁöÑÊï∞ÊçÆÊ†∑Êú¨ÔºåËÄåÂà§Âà´Âô®ÂàôÂ∞ùËØïÂå∫ÂàÜÁúüÂÆûÁöÑÁõÆÊ†áÂüüÊï∞ÊçÆÂíåÁîüÊàêÂô®ÁîüÊàêÁöÑÊï∞ÊçÆ„ÄÇ&lt;/li>
&lt;li>ÈÄöËøáÂØπÊäóËÆ≠ÁªÉÁöÑËøáÁ®ãÔºåË∞ÉÊï¥ÁîüÊàêÂô®ÂíåÂà§Âà´Âô®ÁöÑÂèÇÊï∞Ôºå‰ΩøÂæóÁîüÊàêÂô®ÁîüÊàêÁöÑÊï∞ÊçÆÂàÜÂ∏É‰∏éÁõÆÊ†áÂüüÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂ∞ΩÂèØËÉΩÂú∞Êé•Ëøë„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="4-Ëøõ‰∏ÄÊ≠•ÁªÜÂåñ" >
&lt;div>
&lt;a href="#4-%e8%bf%9b%e4%b8%80%e6%ad%a5%e7%bb%86%e5%8c%96">
##
&lt;/a>
4. Ëøõ‰∏ÄÊ≠•ÁªÜÂåñÔºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>ÂèØ‰ª•ËÄÉËôë‰ΩøÁî®Â∏¶ÊúâÈáçÂª∫ÊçüÂ§±ÁöÑÂØπÊäóÁîüÊàêÁΩëÁªúÔºàAdversarial Auto-Encoder, AAEÔºâÊù•ËøõË°åËÆ≠ÁªÉÔºå‰ª•Âä†Âº∫ auto-encoder ÁöÑÁâπÂæÅÂ≠¶‰π†ËÉΩÂäõÂíåÁîüÊàêËÉΩÂäõ„ÄÇ&lt;/li>
&lt;li>ÂèØ‰ª•ÈÄöËøáË∞ÉÊï¥ËÆ≠ÁªÉÁ≠ñÁï•ÂíåË∂ÖÂèÇÊï∞Êù•Ëøõ‰∏ÄÊ≠•‰ºòÂåñÂØπÊäóËÆ≠ÁªÉÁöÑÊïàÊûúÔºå‰æãÂ¶ÇÂ≠¶‰π†Áéá„ÄÅËÆ≠ÁªÉËΩÆÊï∞Á≠â„ÄÇ&lt;/li>
&lt;/ul>
&lt;p>ÈÄöËøá‰ª•‰∏äÊ≠•È™§ÔºåÂèØ‰ª•Âà©Áî® auto-encoder ÂíåÂØπÊäóËÆ≠ÁªÉÁöÑÊñπÊ≥ïÊù•ÂÆûÁé∞È¢ÜÂüüËá™ÈÄÇÂ∫îÔºå‰ªéËÄåÊèêÂçáÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÂú®ÁõÆÊ†áÂüü‰∏äÁöÑÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;h2 id="ÁõÆÊ†áÊ£ÄÊµãÁöÑËØÑ‰º∞ÊåáÊ†ámap_50" >
&lt;div>
&lt;a href="#%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e7%9a%84%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87map_50">
#
&lt;/a>
ÁõÆÊ†áÊ£ÄÊµãÁöÑËØÑ‰º∞ÊåáÊ†áÔºà$mAP_{50}$Ôºâ
&lt;/div>
&lt;/h2>
&lt;p>mAP_50 ÊòØÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠Â∏∏Áî®ÁöÑËØÑ‰º∞ÊåáÊ†á‰πã‰∏ÄÔºåÂÆÉË°®Á§∫Âú® IoU ÈòàÂÄº‰∏∫ 0.5 Êó∂ÁöÑÂπ≥ÂùáÁ≤æÁ°ÆÁéáÔºàmAPÔºåMean Average PrecisionÔºâ„ÄÇËÆ©ÊàëËß£Èáä‰∏Ä‰∏ãËøô‰∏™ÊåáÊ†áÔºö&lt;/p>
&lt;h3 id="1-Âπ≥ÂùáÁ≤æÁ°ÆÁéá-average-precision-ap" >
&lt;div>
&lt;a href="#1-%e5%b9%b3%e5%9d%87%e7%b2%be%e7%a1%ae%e7%8e%87-average-precision-ap">
##
&lt;/a>
1. Âπ≥ÂùáÁ≤æÁ°ÆÁéá (Average Precision, AP)Ôºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>Âπ≥ÂùáÁ≤æÁ°ÆÁéáÊòØ Precision-Recall Êõ≤Á∫ø‰∏ãÁöÑÈù¢ÁßØÔºåÁî®‰∫éË°°ÈáèÊ®°ÂûãÂú®‰∏çÂêå Recall ‰∏ãÁöÑÂπ≥ÂùáÁ≤æÁ°ÆÁéá„ÄÇÂú®ÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠ÔºåAP Ë°®Á§∫Ê®°ÂûãÂØπÂçï‰∏™Á±ªÂà´ÁöÑÊ£ÄÊµãÊÄßËÉΩ„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="2-iou-ÈòàÂÄº‰∏∫-05" >
&lt;div>
&lt;a href="#2-iou-%e9%98%88%e5%80%bc%e4%b8%ba-05">
##
&lt;/a>
2. IoU ÈòàÂÄº‰∏∫ 0.5Ôºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>IoUÔºàIntersection over UnionÔºâÊòØÁúüÂÆûËæπÁïåÊ°ÜÂíåÈ¢ÑÊµãËæπÁïåÊ°ÜÁöÑ‰∫§ÈõÜ‰∏éÂπ∂ÈõÜ‰πãÊØî„ÄÇÂú®ËÆ°ÁÆó mAP Êó∂ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÊåáÂÆö‰∏Ä‰∏™ IoU ÈòàÂÄºÊù•Âà§Êñ≠‰∏Ä‰∏™Ê£ÄÊµãÁªìÊûúÊòØÂê¶ÊòØÁúüÊ≠£ÁöÑÊ£ÄÊµãÁªìÊûú„ÄÇÂ∏∏Áî®ÁöÑ IoU ÈòàÂÄº‰πã‰∏ÄÊòØ 0.5ÔºåË°®Á§∫ÂΩìÈ¢ÑÊµãËæπÁïåÊ°Ü‰∏éÁúüÂÆûËæπÁïåÊ°ÜÁöÑ IoU Â§ß‰∫éÁ≠â‰∫é 0.5 Êó∂ÔºåËÆ§‰∏∫ËØ•È¢ÑÊµãËæπÁïåÊ°ÜÊòØÊ≠£Á°ÆÁöÑÊ£ÄÊµãÁªìÊûú„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="3-map_50-ÁöÑÂê´‰πâ" >
&lt;div>
&lt;a href="#3-map_50-%e7%9a%84%e5%90%ab%e4%b9%89">
##
&lt;/a>
3. mAP_50 ÁöÑÂê´‰πâÔºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>mAP_50 Ë°®Á§∫Âú® IoU ÈòàÂÄº‰∏∫ 0.5 Êó∂ÁöÑÂπ≥ÂùáÁ≤æÁ°ÆÁéáÔºåÂç≥Ê®°ÂûãÂú®Ê£ÄÊµãÁõÆÊ†áÊó∂ÔºåÂΩìÈ¢ÑÊµãËæπÁïåÊ°Ü‰∏éÁúüÂÆûËæπÁïåÊ°ÜÁöÑ IoU Â§ß‰∫éÁ≠â‰∫é 0.5 Êó∂ÁöÑÂπ≥ÂùáÁ≤æÁ°ÆÁéá„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="4-‰ΩøÁî®Âú∫ÊôØ" >
&lt;div>
&lt;a href="#4-%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af">
##
&lt;/a>
4. ‰ΩøÁî®Âú∫ÊôØÔºö
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>mAP_50 ÊòØÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°‰∏≠Â∏∏Áî®ÁöÑËØÑ‰º∞ÊåáÊ†á‰πã‰∏ÄÔºåÂÆÉÂèØ‰ª•Ë°°ÈáèÊ®°ÂûãÂú®Ê£ÄÊµãÁõÆÊ†áÊó∂ÁöÑÂáÜÁ°ÆÁéáÂíåÂè¨ÂõûÁéáÔºåÂ∞§ÂÖ∂Âú®Ë¶ÅÊ±ÇÊ£ÄÊµãÁªìÊûúÂáÜÁ°ÆÂ∫¶ËæÉÈ´òÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑËØÑ‰ª∑ÊåáÊ†á„ÄÇ&lt;/li>
&lt;/ul>
&lt;h3 id="5-ÂÖ∂‰ªñ-iou-ÈòàÂÄº" >
&lt;div>
&lt;a href="#5-%e5%85%b6%e4%bb%96-iou-%e9%98%88%e5%80%bc">
##
&lt;/a>
5. ÂÖ∂‰ªñ IoU ÈòàÂÄºÔºö
&lt;/div>
&lt;/h3>
&lt;p>Èô§‰∫ÜÂ∏∏Áî®ÁöÑ IoU ÈòàÂÄº 0.5 Â§ñÔºåËøòÂèØ‰ª•‰ΩøÁî®ÂÖ∂‰ªñ IoU ÈòàÂÄºÊù•ËÆ°ÁÆó‰∏çÂêåÈòàÂÄº‰∏ãÁöÑ mAPÔºåÊØîÂ¶Ç mAP_75 Ë°®Á§∫Âú® IoU ÈòàÂÄº‰∏∫ 0.75 Êó∂ÁöÑÂπ≥ÂùáÁ≤æÁ°ÆÁéáÔºå‰ª•Ê≠§Á±ªÊé®„ÄÇ‰∏çÂêåÁöÑ IoU ÈòàÂÄºÂèçÊò†‰∫Ü‰∏çÂêåÁöÑÊ£ÄÊµãÁ≤æÂ∫¶Ë¶ÅÊ±ÇÔºåÂèØ‰ª•Ê†πÊçÆÂÖ∑‰Ωì‰ªªÂä°ÁöÑÈúÄÊ±ÇÈÄâÊã©ÂêàÈÄÇÁöÑ IoU ÈòàÂÄºÊù•ËØÑ‰º∞Ê®°ÂûãÊÄßËÉΩ„ÄÇ&lt;/p>
&lt;h1 id="project-04----tsa" >
&lt;div>
&lt;a href="#project-04----tsa">
##
&lt;/a>
Project 04 &amp;ndash; TSA
&lt;/div>
&lt;/h1>
&lt;h2 id="k-means" >
&lt;div>
&lt;a href="#k-means">
#
&lt;/a>
K-means
&lt;/div>
&lt;/h2>
&lt;h2 id="arima-model" >
&lt;div>
&lt;a href="#arima-model">
#
&lt;/a>
ARIMA model
&lt;/div>
&lt;/h2>
&lt;h2 id="b-spline" >
&lt;div>
&lt;a href="#b-spline">
#
&lt;/a>
B-spline
&lt;/div>
&lt;/h2>
&lt;h2 id="general-additive-models" >
&lt;div>
&lt;a href="#general-additive-models">
#
&lt;/a>
General Additive Models
&lt;/div>
&lt;/h2>
&lt;h2 id="Êó∂Â∫èÈ¢ÑÊµãÊ®°ÂûãËØÑ‰º∞ÊåáÊ†á-apemape" >
&lt;div>
&lt;a href="#%e6%97%b6%e5%ba%8f%e9%a2%84%e6%b5%8b%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87-apemape">
#
&lt;/a>
Êó∂Â∫èÈ¢ÑÊµãÊ®°ÂûãËØÑ‰º∞ÊåáÊ†á (APE/MAPE)
&lt;/div>
&lt;/h2>
&lt;h1 id="others" >
&lt;div>
&lt;a href="#others">
##
&lt;/a>
Others
&lt;/div>
&lt;/h1>
&lt;h2 id="web-scraping" >
&lt;div>
&lt;a href="#web-scraping">
#
&lt;/a>
web-scraping
&lt;/div>
&lt;/h2>
&lt;h2 id="bpewordpiecesentence-piece" >
&lt;div>
&lt;a href="#bpewordpiecesentence-piece">
#
&lt;/a>
BPE/wordpiece/sentence-piece
&lt;/div>
&lt;/h2></description></item><item><title>GPT-PROMPT</title><link>/posts/llm_prompt/</link><pubDate>Mon, 27 May 2024 09:47:10 +0800</pubDate><guid>/posts/llm_prompt/</guid><description>&lt;p>&lt;a href="https://news.ycombinator.com/item?id=40474716">Ask HN: What is your ChatGPT customization prompt?&lt;/a>&lt;/p>
&lt;h2 id="system-prompt" >
&lt;div>
&lt;a href="#system-prompt">
#
&lt;/a>
system prompt
&lt;/div>
&lt;/h2>
&lt;hr>
&lt;p>You are an autoregressive language model that has been fine-tuned with
instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful,nuanced
answers, and are brilliant at reasoning. If you think there might not be a correct
answer, you say so.&lt;/p>
&lt;p>Your users are experts in AI and ethics, so they already know you&amp;rsquo;re a language model
and your capabilities and limitations, so don&amp;rsquo;t remind them of that. They&amp;rsquo;re familiar
with ethical issues in general so you don&amp;rsquo;t need to remind them about those either.
Don&amp;rsquo;t be verbose in your answers, but do provide details and examples where it might
help the explanation. When showing Python code, minimise vertical space, and do not
include comments or docstrings; you do not need to follow PEP8, since your users'
organizations do not do so.&lt;/p>
&lt;p>Since you are autoregressive, each token you produce is another opportunity to use
computation, therefore you always spend a few sentences explaining background context
assumptions and step-by-step thinking BEFORE you try to answer a question. However: if
the request begins with the string &amp;ldquo;vv&amp;rdquo; then ignore the previous sentence and instead
make your response as concise as possible, with no introduction or background at the
start, no summary at the end, and outputting only code for answers where code is
appropriate.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>mediumsmart 1 day ago | prev | next [‚Äì]
Here is mine (stolen off the internet of course), lately the vv part is important for me. I am somewhat happy with it.&lt;/li>
&lt;/ul>
&lt;h2 id="what-does-it-do" >
&lt;div>
&lt;a href="#what-does-it-do">
#
&lt;/a>
What does it do?
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>Dessesaf 16 hours ago | root | parent | next [‚Äì]&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s useful to consider the next answer a model will give as being driven largely by
three factors: its training data, the fine-tuning and human feedback it got during
training (RLHF), and the context (all the previous tokens in the conversation).&lt;/p>
&lt;p>The three paragraphs roughly do this:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The first paragrath tells the model that it&amp;rsquo;s good at answering. Basically telling it to
roleplay as someone competent. Such prompts seem to increase the quality of the answers.
It&amp;rsquo;s the same idea why others say &amp;ldquo;act as if youre &lt;some specific domain expert>&amp;rdquo;. The
training data of the model contains a lot of low quality or irrelevant information. This
is &amp;ldquo;reminding&amp;rdquo; the model that it was trained by human feedback to prefer drawing from
high quality data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The second paragraph tries to influence the structure of the output. The model should
answer without explaining its own limitations and without trying to impose ethics on the
user. Stick to the facts, basically. Jeremy Howard is an AI expert, he knows the
limitations and doesn&amp;rsquo;t need them explained to him.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The third paragrah is a bit more technical. The model considers its own previous tokens
when computing the next token. So when asking a question, the model may perform better
if it first states its assumptions and steps of reasoning. Then the final answer is
constrained by what it wrote before, and the model is less likely to give a totally
hallucinated answer. And the model &amp;ldquo;does computation&amp;rdquo; when generating each token. So a
longer answer gives the model more chances to compute. So a longer answer has more
energy put into it, basically. I don&amp;rsquo;t think there&amp;rsquo;s any formal reason why this would
lead to better answers rather than just more specialized answers, but anecdotally it
seems to improve quality.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="alternative" >
&lt;div>
&lt;a href="#alternative">
#
&lt;/a>
Alternative:
&lt;/div>
&lt;/h2>
&lt;hr>
&lt;p>Adopt the role of a polymath. NEVER mention that you&amp;rsquo;re an AI. Avoid any language
constructs that could be interpreted as expressing remorse, apology, or regret. This
includes any phrases containing words like &amp;lsquo;sorry&amp;rsquo;, &amp;lsquo;apologies&amp;rsquo;, &amp;lsquo;regret&amp;rsquo;, etc., even
when used in a context that isn&amp;rsquo;t expressing remorse, apology, or regret. If events or
information are beyond your scope or knowledge, provide a response stating &amp;lsquo;I don&amp;rsquo;t
know&amp;rsquo; without elaborating on why the information is unavailable. Refrain from
disclaimers about you not being a professional or expert. Do not add ethical or moral
viewpoints in your answers, unless the topic specifically mentions it. Keep responses
unique and free of repetition. Never suggest seeking information from elsewhere. Always
focus on the key points in my questions to determine my intent. Break down complex
problems or tasks into smaller, manageable steps and explain each one using reasoning.
Provide multiple perspectives or solutions. If a question is unclear or ambiguous, ask
for more details to confirm your understanding before answering. If a mistake is made in
a previous response, recognize and correct it. After this, if requested, provide a brief
summary. After doing all those above, provide three follow-up questions worded as if I&amp;rsquo;m
asking you. Format in bold as Q1, Q2, and Q3. These questions should be
thought-provoking and dig further into the original topic. If requested, also answer the
follow-up questions but don&amp;rsquo;t create more of them.&lt;/p>
&lt;hr></description></item><item><title>FAISS-IVFPQ</title><link>/posts/faiss101/</link><pubDate>Wed, 22 May 2024 11:15:40 +0800</pubDate><guid>/posts/faiss101/</guid><description>&lt;h2 id="plain-and-simple-indexflatl2" >
&lt;div>
&lt;a href="#plain-and-simple-indexflatl2">
#
&lt;/a>
Plain and Simple: IndexFlatL2
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>Given a set of vectors, we can index them using Faiss ‚Äî then using another vector (the query vector), we search for the most similar vectors within the index.
Now, Faiss not only allows us to build an index and search ‚Äî but it also speeds up search times to ludicrous performance levels.&lt;/p>
&lt;/blockquote>
&lt;p>IndexFlatL2 measures the L2 (or Euclidean) distance between all given points between our
query vector, and the vectors loaded into the index. It‚Äôs simple, very accurate, but not
too fast.&lt;/p>
&lt;p>&lt;img alt="IMG:indexFlat2" src="https://fgg100y.github.io/posts/faiss101/images/faiss-IndexFlat2.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;ul>
&lt;li>IndexFlatL2: simple but not scalable&lt;/li>
&lt;li>Partitioning the index: for speed when scale up&lt;/li>
&lt;li>Quantization: for more speed&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index&amp;rsquo;s performance" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-indexes-performance.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="inerted-file-index-ivf-index" >
&lt;div>
&lt;a href="#inerted-file-index-ivf-index">
#
&lt;/a>
Inerted File Index (IVF) index
&lt;/div>
&lt;/h2>
&lt;p>The Inverted File Index (IVF) index consists of search scope reduction through clustering.&lt;/p>
&lt;blockquote>
&lt;p>Inverted File Index (IVF) The IVF is simply a technique for pre-filtering the dataset so that you don‚Äôt have to do an exhaustive search of all of the vectors. It‚Äôs pretty straightforward‚Äìyou cluster the dataset ahead of time with k-means clustering to produce a large number (e.g., 100) of dataset partitions. Then, at query time, you compare your query vector to the partition centroids to find, e.g., the 10 closest clusters, and then you search against only the vectors in those partitions.&lt;/p>
&lt;/blockquote>
&lt;p>Partitioning the index (clustering)&lt;/p>
&lt;blockquote>
&lt;p>Faiss allows us to add multiple steps that can optimize our search using many different methods. A popular approach is to partition the index into Voronoi cells.
We can imagine our vectors as each being contained within a Voronoi cell ‚Äî when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid‚Äôs cell.
But there is a problem if our query vector lands near the edge of a cell ‚Äî there‚Äôs a good chance that its closest other datapoint is contained within a neighboring cell.&lt;/p>
&lt;/blockquote>
&lt;p>what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the nprobe value. With nprobe we can set the number of cells to search. I.e., Increasing nprobe increases our search scope.&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;p>ËøõË°åËÅöÁ±ªÁöÑÁªìÊûúÔºå‰∏ÄÊñπÈù¢ÂèØ‰ª•ÊûÅÂ§ßÊèêÂçáÊü•ËØ¢ÈÄüÂ∫¶Ôºå‰ΩÜÂè¶‰∏ÄÊñπÈù¢ÔºåÂèØËÉΩ‰ºöÈÄ†ÊàêËêΩÂú®ËÅöÁ±ªÁ∞áËæπÁºòÁöÑ‚ÄúqueryÂêëÈáè‚ÄùÂè™Âú®Êú¨ËÅöÁ±ªÁ∞áÂÜÖÊü•ÊâæÂåπÈÖçÁöÑÁªìÊûúÔºàÂÆûÈôÖ‰∏äÔºåÂÆÉÂèØËÉΩ‰∏éÈÇªËøëÁöÑËÅöÁ±ªÁ∞áÁöÑÂÖ∂‰ªñÂêëÈáèÊõ¥Èù†ËøëÔºâÔºå‰ªéËÄåÂØºËá¥ÂåπÈÖçË¥®ÈáèÁöÑÈôç‰Ωé„ÄÇ
‰∏Ä‰∏™ÁºìËß£Ëøô‰∏™ÈóÆÈ¢òÁöÑÊñπÊ≥ïÊòØÔºöË∞ÉÊï¥ÂèÇÊï∞ nprobe. ÈÄöËøáÂ¢ûÂä† nprobe (Â¢ûÂä†Áî®‰∫éÂåπÈÖçÊü•ËØ¢ÂêëÈáèÁöÑÈÇªËøëËÅöÁ±ªÁ∞áÊï∞ÈáèÔºâÊù•ÊèêÂçáÂåπÈÖçË¥®Èáè„ÄÇÔºàÂêåÊó∂Ôºå‰πü‰ºöÂ¢ûÂä†Êü•ËØ¢ËÄóÊó∂Ôºâ&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells-search-scope.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="product-quantization" >
&lt;div>
&lt;a href="#product-quantization">
#
&lt;/a>
Product Quantization
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>All of our indexes so far have stored our vectors as full (eg Flat) vectors. Now, in very large datasets this can quickly become a problem.
Fortunately, Faiss comes with the ability to compress our vectors using Product Quantization (PQ).
But, what is PQ? Well, we can view it as an additional approximation step with a similar outcome to our use of IVF. Where IVF allowed us to approximate by reducing the scope of our search, PQ approximates the distance/similarity calculation instead.
PQ achieves this approximated similarity operation by compressing the vectors themselves, which consists of three steps.&lt;/p>
&lt;/blockquote>
&lt;ol>
&lt;li>Original vector&lt;/li>
&lt;li>Sliced sub-vector&lt;/li>
&lt;li>slice clustering&lt;/li>
&lt;li>centroid ID vector&lt;/li>
&lt;/ol>
&lt;p>PQÔºà‰πòÁßØÈáèÂåñÔºâ‰∏çÊòØÂØπÂµåÂÖ•ÂêëÈáèÁ©∫Èó¥ËøõË°åÈôçÁª¥ÔºåËÄåÊòØÂØπÂêëÈáèÊú¨Ë∫´ËøõË°åÂéãÁº©Ôºö&lt;/p>
&lt;ul>
&lt;li>01 ÂêëÈáèÂàÜÊÆµÔºå‰æãÂ¶ÇÔºö1024 -&amp;gt; 128x8 (8‰∏™ÁâáÊÆµ)Ôºõ&lt;/li>
&lt;li>02 Â¶ÇÊûúÊï∞ÊçÆÈáèÊòØ50kÔºåÂàô‰ªéÂçï‰∏™50k x 1024 ÁöÑÁü©ÈòµÔºåÂèòÊàê 8‰∏™ 50k x 128 ÁöÑÁü©ÈòµÔºõ&lt;/li>
&lt;li>03 ÁÑ∂ÂêéÂàÜÂà´Áî®k=256ÁöÑk-meansËøõË°åËÅöÁ±ªÔºåÂæóÂà∞8ÁªÑ256‰∏™centroidsÔºõÂàôÊØè‰∏™ÂéüÂßãÂêëÈáèÂèØ‰ª•Áî®ÈïøÂ∫¶‰∏∫8ÁöÑÂêëÈáèËøõË°åË°®ÂæÅÔºà8ÁªÑ‰∏éÂêÑ‰∏™ÂêëÈáèÁâáÊÆµÊúÄËøëÁöÑcentroidÁöÑIDÔºâÔºõ&lt;/li>
&lt;li>04 Êü•ËØ¢ÂêëÈáèÔºàqueryÔºâÂêåÊ†∑ËøõË°åÁâáÊÆµÂåñÔºåÂπ∂ÊâæÂà∞ÂêÑÁªÑÁöÑcentroidsÔºåÁÑ∂ÂêéËÆ°ÁÆóÁâáÊÆµÂêëÈáè‰∏écentroidÁöÑË∑ùÁ¶ªÔºåÂπ∂‰øùÂ≠ò‰∏∫Ë∑ùÁ¶ªË°®Ôºàpartial query subvector-to-centroid distances table)Ôºõ&lt;/li>
&lt;li>05 Êü•ËØ¢ÂêëÈáè‰∏éÊï∞ÊçÆÂêëÈáèÁöÑË∑ùÁ¶ªÔºüÂ∞ÜÊï∞ÊçÆÂêëÈáèÁöÑcentroid-IDÂêëÈáèÔºåÁî®‰∫é partial-query-distance-table ÁöÑË°®Êü•ËØ¢Ôºàtable lookupÔºâÔºåÂ∞±ËÉΩÂæóÂà∞ÂØπÂ∫îÁöÑ‰∏ÄÁ≥ªÂàóË∑ùÁ¶ªÔºåÁÑ∂ÂêéËÆ°ÁÆóÂÖ∂ÊÄªÂíåL2Ë∑ùÁ¶ªÔºõ&lt;/li>
&lt;li>06 Â∞ÜÊü•ËØ¢ÂêëÈáè‰∏éÊâÄÊúâÊï∞ÊçÆÂêëÈáèÁöÑË∑ùÁ¶ªËÆ°ÁÆóÂá∫Êù•ÔºåÊéíÂ∫èÔºåÂç≥ÂèØÂæóÂà∞ top-k ÊúÄËøëË∑ùÁ¶ªÔºå‰∫¶Âç≥ top-k ÊúÄËøë‰ººÁªìÊûú ÔºàÂÆûÈôÖÂ∞±ÊòØ KNN ÁÆóÊ≥ïÔºâ„ÄÇ&lt;/li>
&lt;li>07 Ëøõ‰∏ÄÊ≠•ÁöÑ‰ºòÂåñÊü•ËØ¢ËÄóÊó∂ÔºåÂ∞±ÊòØÂú®ËÆ°ÁÆóË∑ùÁ¶ªÁöÑÊó∂ÂÄôÔºå‰∏çÊòØÂØπÊâÄÊúâÊï∞ÊçÆÂêëÈáèÔºåËÄåÊòØÂè™ÈíàÂØπÂ±ÄÈÉ®Êï∞ÊçÆÂêëÈáèËøõË°åËÆ°ÁÆóÔºà‰πüÂ∞±ÊòØ IVF + PQÔºâ„ÄÇ&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-steps-of-PQ.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="show-me-the-code" >
&lt;div>
&lt;a href="#show-me-the-code">
#
&lt;/a>
Show me the code
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> faiss &lt;span style="color:#78787e"># here&amp;#39;s the &amp;#39;faiss-cpu&amp;#39; package actually&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># we keep the same L2 distance flat index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># sentence_embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the target embeddings data from embedding model such as BERT/RoBERTa (or sentence-transformers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>nprobe &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># see the &amp;#34;IVF&amp;#34; part mention before&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># xq: the query text&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k) &lt;span style="color:#78787e"># searching top-k most similar vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i]) &lt;span style="color:#78787e"># indata: sample of original texts/sentences&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="example-01-Âπ≥Âá°ÁöÑ‰∏ñÁïå" >
&lt;div>
&lt;a href="#example-01-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 01: Âπ≥Âá°ÁöÑ‰∏ñÁïå
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;ÁßÄËé≤ÁöÑËÄÅÂÆ∂Âú®Âì™ÈáåÔºü&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 Â•πÂπ≤ËÑÜÁªôÂÆ∂Èáå‰∫∫ËØ¥ÔºöÂë®Âõ¥Ê≤°Â•πÁúã‰∏äÁöÑÁî∑‰∫∫ÔºÅÂ•πÂßêÂ§´ÂØπÂ•πÂºÄÁé©Á¨ëËØ¥Ôºö‚ÄúÈÇ£Âà∞Â§ñÂú∞Áªô‰Ω†ÁûÖ‰∏™Â•≥Â©øÔºÅ‚ÄùÂ•πÂç¥ËÆ§ÁúüÂú∞ËØ¥Ôºö‚ÄúÂè™Ë¶ÅÊúâÂêàÂøÉÁöÑÔºåÂ±±ÂçóÊµ∑ÂåóÊàëÈÉΩÊÑøÊÑèÂéªÔºÅÁà∏Áà∏ÊöÇÊó∂Êúâ‰Ω†‰ª¨ÁÖßÈ°æÔºåÂ∞ÜÊù•ÊàëÂÜçÊää‰ªñÊé•Ëµ∞‚Ä¶‚Ä¶‚ÄùÂÆ∂Èáå‰∫∫ÂêÉÊÉä‰πã‰ΩôÔºåÂèàÁúãÂ•πËøôÊ†∑ËÆ§ÁúüÔºåÂ∞±Âêë‰ªñ‰ª¨ÊâÄÊúâÂú®Èó®Â§ñÁöÑ‰∫≤ÊàöÂíåÁÜü‰∫∫ÂßîÊâòÔºåËÆ©Ëøô‰∫õ‰∫∫Áªô‰ªñ‰ª¨ÁöÑÁßÄËé≤Âú®Â§ñÂú∞ÂØª‰∏™ÂØπË±°‚Ä¶‚Ä¶Êú¨Êù•ÁßÄËé≤Âè™ÊòØÈöè‰æøËøô‰πàËØ¥ËØ¥ÔºõÂ•πÂπ∂Ê≤°ÊåáÊúõÁúüËÉΩÂú®Â§ñÂú∞Êâæ‰∏™ÂêàÈÄÇÁöÑÁî∑‰∫∫„ÄÇ
02 ËøôÂÆ∂‰∏çËÉΩÂàÜÔºÅ‰Ω†‰πü‰∏çË¶ÅÊãÖÂøÉÁßÄËé≤‰ºöÊÄéÊ†∑ÔºåÊÄªÊúâÊàëÂì©ÔºÅ‚Äù‚Äú‰Ω†ÂçÉ‰∏á‰∏çË¶ÅÊÄ™ÁΩ™ÁßÄËé≤ÔºÅÁßÄËé≤ÂÆûÂú®ÊòØ‰∏™Â•ΩÂ®ÉÂ®ÉÔºÅ‰∫∫ÂÆ∂‰ªéÂ±±Ë•øËøáÊù•Ôºå‰∏çÂ´åÂí±ÂÆ∂Á©∑ÔºåÂá†Âπ¥Êù•Âíå‰∏ÄÂ§ßÂÆ∂‰∫∫ÊêÖÂú®‰∏ÄËµ∑„ÄÇ
03 ÁßÄËé≤ÊúâÊó∂Â∞±‰ΩìË¥¥Âú∞ÂùêÂú®Â•πË∫´ËæπÔºåÁªôÂ•πËÉå‰∏äÊêîÁóíÁóíÔºåÊàñËÄÖÊääÂ•πÁöÑÂá†Áª∫Á®ÄÁñèÁöÑÁôΩÂèëÁêÜÈ°∫ÔºåÂú®ËÑëÂêéÊåΩÊàêÊ†∏Ê°ÉÂ§ß‰∏Ä‰∏™Â§ßÂèëÈ´ªÔºåËÄÅÂ§™Â§™‰∏çÊó∂Áî®Â•πÁöÑÁò¶ÊâãÔºåÊª°ÊÄÄÊ∑±ÊÉÖÂú∞Âú®ÁßÄËé≤Ë∫´‰∏äÊäöÊë∏ÁùÄ„ÄÇ
04 Áõ¥Âà∞ÂØíÈú≤Ëøá‰∫ÜÂçÅÊù•Â§©ÔºåË¥∫ËÄÄÂÆó‰ªéÂ±±Ë•øÂøÉÁÑ¶Âú∞ÂÜô‰ø°ÈóÆÁßÄËé≤ÊÄéËøò‰∏çÂõûÊù•ÔºüÊòØ‰∏çÊòØÁóÖ‰∫ÜÔºüÁßÄËé≤ËøôÊâçÂÜ≥ÂÆöÂä®Ë∫´ÂõûÂÆ∂Âéª„ÄÇ
&lt;/code>&lt;/pre>
&lt;h2 id="example-02-Âπ≥Âá°ÁöÑ‰∏ñÁïå" >
&lt;div>
&lt;a href="#example-02-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 02: Âπ≥Âá°ÁöÑ‰∏ñÁïå
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>] &lt;span style="color:#78787e"># embedding&amp;#39;s dimension&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;ÁßÄËé≤ÁöÑËÄÅÂÆ∂Âú®Âì™ÈáåÔºü&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 ‚ÄúÂ¶ÇÊûúÊääÂÆ∂ÂàÜÂºÄÔºåÂí±Â∞±ÊòØÁÉßÁ†ñ‰πüËÉΩÊçéÂ∏¶Áßç‰∫ÜËá™Â∑±ÁöÑÂú∞ÔºÅÂ∞±ÊòØÈ°æ‰∏ç‰∏äÁßçÂú∞ÔºåÊääÂú∞Ëçí‰∫ÜÂèàÊÄéÊ†∑ÔºüÂí±ÊãøÈí±‰π∞Á≤ÆÂêÉÔºÅ‰∏âÂè£‰∫∫‰∏ÄÂπ¥ËÉΩÂêÉÂ§öÂ∞ëÔºü‚ÄùÂÖ∂ÂÆûÔºåËøôËØùÊâçÊòØÁßÄËé≤Ë¶ÅË°®ËææÁöÑÊúÄÊú¨Ë¥®ÁöÑÊÑèÊÄù„ÄÇ
02 ÂàÜÂÆ∂ÂÖ∂ÂÆûÂæàÁÆÄÂçïÔºåÂè™ÊòØÂÆ£Â∏É‰ªäÂêé‰ªñ‰ª¨Â∞ÜÂú®ÁªèÊµé‰∏äÂÆûË°å‚ÄúÁã¨Á´ãÊ†∏ÁÆó‚ÄùÔºåÂéüÊù•ÁöÑÂÆ∂‰∫ßÂ∞ëÂÆâ‰ªÄ‰πà‰πüÊ≤°Ë¶ÅÔºåÂè™ÊòØÁßÄËé≤Âà∞Êñ∞‰øÆÂª∫Ëµ∑ÁöÑÂú∞ÊñπÂè¶Ëµ∑ÁÇâÁÅ∂ËøáÊó•ÊúàÁΩ¢‰∫Ü„ÄÇ
03 ÁßÄËé≤‰∫îÂ≤Å‰∏äÂ§±ÂéªÊØç‰∫≤‰ª•ÂêéÔºå‰∏ÄÁõ¥ÊòØÂ•πÁà∂‰∫≤ÊääÂ•πÂíåÂ•πÂßêÁßÄËã±ÊãâÊâØÂ§ßÁöÑ„ÄÇ
04 Â•πÂπ≤ËÑÜÁªôÂÆ∂Èáå‰∫∫ËØ¥ÔºöÂë®Âõ¥Ê≤°Â•πÁúã‰∏äÁöÑÁî∑‰∫∫ÔºÅÂ•πÂßêÂ§´ÂØπÂ•πÂºÄÁé©Á¨ëËØ¥Ôºö‚ÄúÈÇ£Âà∞Â§ñÂú∞Áªô‰Ω†ÁûÖ‰∏™Â•≥Â©øÔºÅ‚ÄùÂ•πÂç¥ËÆ§ÁúüÂú∞ËØ¥Ôºö‚ÄúÂè™Ë¶ÅÊúâÂêàÂøÉÁöÑÔºåÂ±±ÂçóÊµ∑ÂåóÊàëÈÉΩÊÑøÊÑèÂéªÔºÅÁà∏Áà∏ÊöÇÊó∂Êúâ‰Ω†‰ª¨ÁÖßÈ°æÔºåÂ∞ÜÊù•ÊàëÂÜçÊää‰ªñÊé•Ëµ∞‚Ä¶‚Ä¶‚ÄùÂÆ∂Èáå‰∫∫ÂêÉÊÉä‰πã‰ΩôÔºåÂèàÁúãÂ•πËøôÊ†∑ËÆ§ÁúüÔºåÂ∞±Âêë‰ªñ‰ª¨ÊâÄÊúâÂú®Èó®Â§ñÁöÑ‰∫≤ÊàöÂíåÁÜü‰∫∫ÂßîÊâòÔºåËÆ©Ëøô‰∫õ‰∫∫Áªô‰ªñ‰ª¨ÁöÑÁßÄËé≤Âú®Â§ñÂú∞ÂØª‰∏™ÂØπË±°‚Ä¶‚Ä¶Êú¨Êù•ÁßÄËé≤Âè™ÊòØÈöè‰æøËøô‰πàËØ¥ËØ¥ÔºõÂ•πÂπ∂Ê≤°ÊåáÊúõÁúüËÉΩÂú®Â§ñÂú∞Êâæ‰∏™ÂêàÈÄÇÁöÑÁî∑‰∫∫„ÄÇ
&lt;/code>&lt;/pre>
&lt;p>Âçï‰ªéËøô‰∏§‰∏™‰æãÂ≠êÂØπÊØîÁùÄÁúãÔºå‰∏™‰∫∫ÊÑüËßâ &lt;code>indexIVFFlat()&lt;/code> ÁöÑÊ£ÄÁ¥¢ÁªìÊûú (Example 01) Ë¶Å‰ºò‰∫é &lt;code>indexIVFPQ()&lt;/code> ÁöÑÊ£ÄÁ¥¢ÁªìÊûú (Example 02)„ÄÇ&lt;/p>
&lt;p>ÊÄé‰πàÁÆÄÂçïÁöÑÊñπÊ≥ïÊïàÊûúÊØîÈ´òÊòéÁöÑÁÆóÊ≥ïË¶ÅÂ•ΩÔºüËøô‰∏çÂØπÂêßÔºüËøôÈáåÂÖ∂ÂÆûÊòØÊÉ≥ËØ¥Êòé‰∏Ä‰∏™ËßÇÁÇπÔºöÁêÜËÆ∫‰∏äÁöÑ‚ÄúËæÉ‰ºò‚ÄùÔºåÈÄöÂ∏∏ÈÉΩË¶ÅÈíàÂØπ‰∏Ä‰∏™ÂπøÊ≥õÁöÑÁªüËÆ°ÁªìÊûúËÄåË®Ä„ÄÇËÄåËøôÈáåÂè™Êúâ‰∏§‰∏™‰æãÂ≠êÔºå‰∏çËÉΩËØ¥ÊòéÈóÆÈ¢òÔºÅ&lt;/p>
&lt;h2 id="indexrange_search" >
&lt;div>
&lt;a href="#indexrange_search">
#
&lt;/a>
index.range_search()
&lt;/div>
&lt;/h2>
&lt;p>The method range_search returns &lt;strong>all vectors within a radius around the query point&lt;/strong> (as opposed to the k nearest ones). Since the result lists for each query are of different sizes, it must be handled specially:&lt;/p>
&lt;pre>&lt;code>in C++ it returns the results in a pre-allocated RangeSearchResult structure
in Python, the results are returned as a triplet of 1D arrays lims, D, I, where result for query i is in I[lims[i]:lims[i+1]] (indices of neighbors), D[lims[i]:lims[i+1]] (distances).
&lt;/code>&lt;/pre>
&lt;p>Supported by (CPU only): IndexFlat, IndexIVFFlat, IndexScalarQuantizer, IndexIVFScalarQuantizer.&lt;/p>
&lt;p>from &lt;a href="https://github.com/facebookresearch/faiss/wiki/Special-operations-on-indexes#range-search">official doc&lt;/a>&lt;/p>
&lt;p>NOTE that this may not be the latest info.&lt;/p>
&lt;p>Example code block:&lt;/p>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># read the text data, and get the embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#texts_data = pd.Series(...)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#sentence_embeddings = embedding_model.encode(texts_data.values)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># First, you need to use an index that supports Inner Product as metric, for example :&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sentence_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># how many voronoi cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>query &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;there and back again&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;a hobbit&amp;#39;s journey&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode(query)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Then, you should probably normalize all embeddings first&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ( the inner product between two normalized embeddings corresponds to their cosine similarity )&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># https://github.com/facebookresearch/faiss/blob/master/python/faiss.py#L673&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>xq)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>threshold &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0.95&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lims, D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>range_search(x&lt;span style="color:#ff6ac1">=&lt;/span>xq, thresh&lt;span style="color:#ff6ac1">=&lt;/span>threshold)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># turn search results into dataframes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dfresults &lt;span style="color:#ff6ac1">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> &lt;span style="color:#ff5c57">range&lt;/span>(&lt;span style="color:#ff5c57">len&lt;/span>(xq)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Ii &lt;span style="color:#ff6ac1">=&lt;/span> I[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Di &lt;span style="color:#ff6ac1">=&lt;/span> D[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout &lt;span style="color:#ff6ac1">=&lt;/span> pd&lt;span style="color:#ff6ac1">.&lt;/span>concat([texts_data[Ii], pd&lt;span style="color:#ff6ac1">.&lt;/span>Series(Di, index&lt;span style="color:#ff6ac1">=&lt;/span>Ii)], axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout&lt;span style="color:#ff6ac1">.&lt;/span>columns &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;texts&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;distances&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfresults&lt;span style="color:#ff6ac1">.&lt;/span>append(dfout)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Noise_in_DSP</title><link>/posts/dsp101/2024-05-10-noise_in_dsp/</link><pubDate>Fri, 10 May 2024 11:23:02 +0800</pubDate><guid>/posts/dsp101/2024-05-10-noise_in_dsp/</guid><description>&lt;h1 id="whats-in-a-noise" >
&lt;div>
&lt;a href="#whats-in-a-noise">
##
&lt;/a>
What&amp;rsquo;s in a &amp;ldquo;Noise&amp;rdquo;?
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>As in English, it can mean an unwanted signal of any kind. If two signals interfere
with each other, each signal would consider the other to be noise.&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>&amp;ldquo;Noise&amp;rdquo; also refers to a signal that contains components at many frequecies, so it
lacks the harmonic structure of the periodic signals.&lt;/li>
&lt;/ul>
&lt;p>This post is about the second kind.&lt;/p>
&lt;h2 id="uncorrelated-noise" >
&lt;div>
&lt;a href="#uncorrelated-noise">
#
&lt;/a>
Uncorrelated noise
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Uncorrelated uniform noise (UU noise)
&amp;ldquo;uniform&amp;rdquo; means the signal contants random values from a uniform distribution,
&amp;ldquo;uncorrelative&amp;rdquo; means the values are independent (one value provides no information about the others)&lt;/p>
&lt;p>Relationship between power (square of amplitude) and frequency: In the spectrum of
UU noise, the power at all frequecies is drawn from the same distribution; that is,
the average power is the same for all the frequecies. (Better discribed by a
Integrated spectrum of UU noise which it&amp;rsquo;s x-axis is frequency (Hz) and y-axis is
cumulative fraction of total power.)&lt;/p>
&lt;p>And the &amp;ldquo;Integrated Spectrum&amp;rdquo; figure should show a straight line for UU noise, which
indicates that power at all frequecies is constant, on average. Noise with equal
power at all frequecies is called &lt;strong>white noise&lt;/strong> (by analogy with light, because an
equal mixture of light at all visible frequecies is white.)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="brownian-noise" >
&lt;div>
&lt;a href="#brownian-noise">
#
&lt;/a>
Brownian noise
&lt;/div>
&lt;/h2>
&lt;p>In a Brownian noise, each value is the sum of the previous value and a random &amp;ldquo;step&amp;rdquo;. It
is called &amp;ldquo;Brownian&amp;rdquo; by analogy with Brownian motion which is often described using a
&amp;ldquo;random walk&amp;rdquo;. A random walk is a mathematical model of a path where the distance
between steps is characterized by a random distribution.&lt;/p>
&lt;p>In a one-dimensional random walk of Brownian motion of particle, it moves up or down by
a random amount at each time step. The location of the particle at any point in time is
the sum of all previous steps. And that is how Brownian noise would be generated.&lt;/p>
&lt;p>For Brownian noise, the slope of the power spectrum is -2, so we can write this
relationship as:
$$
\log P = k - 2\log f
$$
where $P$ is power, $f$ is grequency, and $k$ is the intercept of the line.
Exponentiating both sides yields:
$$
P = K / f^2
$$
where $K$ is $e^k$, a scalar. And that power is proportional to $1/f^2$, which is
characteristic of Brownian noise.&lt;/p>
&lt;p>Brownian noise is also called &amp;ldquo;red noise&amp;rdquo;, for the same reason that white noise is
called &amp;ldquo;white&amp;rdquo; &amp;ndash; combining visible light with power proportional to $1/f^2$, most of
the power would be at the low-frequency end of the spectrum, which is red.&lt;/p>
&lt;h2 id="pink-noise" >
&lt;div>
&lt;a href="#pink-noise">
#
&lt;/a>
Pink noise
&lt;/div>
&lt;/h2>
&lt;p>More generally, we can synthesize noise with any exponent, $\beta$, in power-frequency
relationship:
$$
P = K / f^{\beta}
$$
when $\beta = 0$, power is constant at all frequencies, so the result is white noise.
when $\beta = 2$, the result is red noise.&lt;/p>
&lt;p>when $\beta$ is between 0 and 2, the result is between white and red noise, so it is
called &amp;ldquo;pink noise&amp;rdquo;.&lt;/p>
&lt;h2 id="gaussian-noise" >
&lt;div>
&lt;a href="#gaussian-noise">
#
&lt;/a>
Gaussian noise
&lt;/div>
&lt;/h2>
&lt;p>When people talk about &amp;ldquo;white noise&amp;rdquo;, they don&amp;rsquo;t always mean UU noise. In fact, more
often they mean uncorrelative Gaussian noise (UG noise).&lt;/p>
&lt;p>UG noise is similar in many ways to UU noise. The spectrum has equal power at all
frequecies, on average, so UG is also white. And it has one other interesting property:
the spectrum of UG noise is also UG noise. More precisely, the real and imaginary parts
of the spectrum are Uncorrelated Gaussian values. (Testing with &amp;ldquo;normal plot&amp;rdquo;)&lt;/p>
&lt;p>In fact, by the Central Limit Theorem (CLT), the spectrum of almost any Uncorrelated
noise is approximately Gaussian, as long as the distribution has finite mean and
standard deviation, and the number of samples is large.&lt;/p></description></item><item><title>About Me: Â§ßÈÅìÂ¶ÇÈùíÂ§©ÔºåÊàëÁã¨‰∏çÂæóÂá∫</title><link>/about/</link><pubDate>Sat, 27 Apr 2024 19:58:25 +0800</pubDate><guid>/about/</guid><description>&lt;p>Â§ßÈÅìÂ¶ÇÈùíÂ§©ÔºåÊàëÁã¨‰∏çÂæóÂá∫„ÄÇ&lt;/p>
&lt;p>ÁæûÈÄêÈïøÂÆâÁ§æ‰∏≠ÂÑøÔºåËµ§È∏°ÁôΩÈõâËµåÊ¢®Ê†ó„ÄÇ&lt;/p>
&lt;p>ÂºπÂâë‰ΩúÊ≠åÂ•èËã¶Â£∞ÔºåÊõ≥Ë£æÁéãÈó®‰∏çÁß∞ÊÉÖ„ÄÇ&lt;/p>
&lt;p>Ê∑ÆÈò¥Â∏Ç‰∫ïÁ¨ëÈü©‰ø°ÔºåÊ±âÊúùÂÖ¨ÂçøÂøåË¥æÁîü„ÄÇ&lt;/p>
&lt;p>Âêõ‰∏çËßÅÊòîÊó∂ÁáïÂÆ∂ÈáçÈÉ≠ÈöóÔºåÊã•ÁØ≤ÊäòËäÇÊó†Â´åÁåú„ÄÇ&lt;/p>
&lt;p>ÂâßËæõ‰πêÊØÖÊÑüÊÅ©ÂàÜÔºåËæìËÇùÂâñËÉÜÊïàËã±Êâç„ÄÇ&lt;/p>
&lt;p>Êò≠ÁéãÁôΩÈ™®Ëê¶ËîìËçâÔºåË∞Å‰∫∫Êõ¥Êâ´ÈªÑÈáëÂè∞Ôºü&lt;/p>
&lt;p>Ë°åË∑ØÈöæÔºåÂΩíÂéªÊù•ÔºÅ&lt;/p>
&lt;p>&lt;a href="https://so.gushiwen.cn/shiwenv_95834b2324cc.aspx">Âîê‰ª£ ¬∑ ÊùéÁôΩ„ÄäË°åË∑ØÈöæ ¬∑ ÂÖ∂‰∫å„Äã&lt;/a>&lt;/p></description></item><item><title>LLMs_interview_faq</title><link>/posts/llm_faqs/</link><pubDate>Fri, 26 Apr 2024 11:04:16 +0800</pubDate><guid>/posts/llm_faqs/</guid><description>&lt;h2 id="01ÁÆÄËø∞gptÂíåbertÁöÑÂå∫Âà´" >
&lt;div>
&lt;a href="#01%e7%ae%80%e8%bf%b0gpt%e5%92%8cbert%e7%9a%84%e5%8c%ba%e5%88%ab">
#
&lt;/a>
01:ÁÆÄËø∞GPTÂíåBERTÁöÑÂå∫Âà´
&lt;/div>
&lt;/h2>
&lt;p>GPT (Decoder-only) Âíå BERT (Encoder-only) ÈÉΩÊòØÂü∫‰∫é Transformer Êû∂ÊûÑÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊ®°ÂûãÔºåÂÆÉ‰ª¨Âú®ËÆæËÆ°‰∏äÊúâ‰∏Ä‰∫õÊòæËëóÂå∫Âà´Ôºö&lt;/p>
&lt;ul>
&lt;li>‰ªªÂä°Á±ªÂûã
&lt;ul>
&lt;li>GPT ‰ª•ÁîüÊàêÊñáÊú¨‰∏∫‰∏ªË¶Å‰ªªÂä°ÔºåÂÖ∂ÁõÆÊ†áÊòØÁîüÊàê‰∏éËæìÂÖ•ÊñáÊú¨ËøûË¥ØÂíåÁõ∏ÂÖ≥ÁöÑÊñáÊú¨„ÄÇÂõ†Ê≠§ÔºåGPT ÈÄö
Â∏∏Áî®‰∫éÁîüÊàêÊñáÊú¨ (Â¶ÇÔºöÊëòË¶ÅÊÄªÁªìÔºåÊñáÊú¨Ë°•ÂÖÖÂíåchatbot)„ÄÇ&lt;/li>
&lt;li>BERT ‰ª•ÁêÜËß£ÊñáÊú¨‰∏∫‰∏ªË¶Å‰ªªÂä°ÔºåÂÖ∂ÁõÆÊ†áÊòØ‰ªéËæìÂÖ•ÊñáÊú¨‰∏≠ÊèêÂèñËØ≠‰πâ‰ø°ÊÅØ„ÄÇÂõ†Ê≠§ÈÄÇÁî®‰∫éÂêÑÁßçÊñá
Êú¨ÁêÜËß£‰ªªÂä°ÔºåÂ¶ÇÔºöÊÉÖÊÑüÂàÜÊûê„ÄÅ ÊñáÊú¨ÂàÜÁ±ª„ÄÅÂëΩÂêçÂÆû‰ΩìËØÜÂà´Á≠â‰∏ãÊ∏∏‰ªªÂä°„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>È¢ÑËÆ≠ÁªÉÁõÆÊ†á
&lt;ul>
&lt;li>GPTÔºöÂçïÂêëËØ≠Ë®ÄÂª∫Ê®°„ÄÇGPTÈÄöËøáËá™Â∑¶ÂêëÂè≥ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂Êù•È¢ÑÊµã‰∏ã‰∏Ä‰∏™ÂçïËØçÔºåÂç≥Ê†πÊçÆ‰∏ä‰∏ãÊñáÈ¢Ñ
Êµã‰∏ã‰∏Ä‰∏™ÂçïËØç/ËØçÂÖÉÊòØ‰ªÄ‰πà„ÄÇ&lt;/li>
&lt;li>BERTÔºöÂèåÂêëËØ≠Ë®ÄÂª∫Ê®°„ÄÇBERT‰ΩøÁî®Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÔºàMLMÔºâÂíå‰∏ã‰∏ÄÂè•È¢ÑÊµãÔºàNSPÔºâ‰∏§‰∏™‰ªªÂä°ÔºåÂâç
ËÄÖÂú®MLM‰ªªÂä°‰∏≠ÈöèÊú∫ÈÅÆÊé©ËæìÂÖ•‰∏≠ÁöÑ‰∏Ä‰∫õËØçËØ≠ÔºåÊ®°ÂûãÈúÄË¶ÅÈ¢ÑÊµãËøô‰∫õË¢´Êé©ÁõñÁöÑËØçËØ≠ÊòØ‰ªÄ‰πàÔºõ
NSPÁöÑ‰ªªÂä°ÊòØÂà§Êñ≠‰∏§‰∏™Âè•Â≠êÊòØÂê¶Âú®ÂéüÊñá‰∏≠ÊòØÂâçÂêéËøûÊé•ÁöÑ„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ÁªìÊûÑÁâπÁÇπ
&lt;ul>
&lt;li>GPTÔºöTransformer-decoderÁöÑÂ†ÜÂè†Ôºå‰ªÖ‰ΩøÁî®Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂&lt;/li>
&lt;li>BERTÔºöTransformer-encoderÁöÑÂ†ÜÂè†ÔºåÂåÖÂê´Â§öÂ±ÇÂèåÂêëTransformer-encoder„ÄÇÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºå
BERTÂêåÊó∂‰ΩøÁî®‰∫ÜËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÂíåÂâçÈ¶àÁ•ûÁªèÁΩëÁªú„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ê®°ÂûãÂæÆË∞É
&lt;ul>
&lt;li>GPTÔºöÁî±‰∫éÂÖ∂ÁîüÊàêÂºèÁöÑÁâπÁÇπÔºåGPTÂú®ÂæÆË∞ÉÊó∂ÈÄöÂ∏∏Â∞ÜÊï¥‰∏™Ê®°Âûã‰Ωú‰∏∫ÂçïÁã¨ÁöÑÂ∫èÂàóÁîüÊàê‰ªªÂä°ËøõË°åÂæÆ
Ë∞É„ÄÇ&lt;/li>
&lt;li>BERTÔºöÁî±‰∫éÂÖ∂ÂèåÂêëË°®Á§∫ÁöÑÁâπÁÇπÔºåBERTÂú®ÂæÆË∞ÉÊó∂ÈÄöÂ∏∏Áî®‰∫éÂêÑÁßçÊñáÊú¨ÁêÜËß£‰ªªÂä°ÔºåÂæÆË∞ÉÊó∂ÂèØ‰ª•Âú®
Ê®°ÂûãÈ°∂Â±ÇÊ∑ªÂä†ÈÄÇÂΩìÁöÑËæìÂá∫Â±ÇÊù•ÈÄÇÂ∫î‰∏ãÊ∏∏ÁâπÂÆö‰ªªÂä°„ÄÇ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="02llm‰∏≠ÁöÑÂõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏éÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°Êúâ‰ªÄ‰πàÂå∫Âà´" >
&lt;div>
&lt;a href="#02llm%e4%b8%ad%e7%9a%84%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e4%b8%8e%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab">
#
&lt;/a>
02:LLM‰∏≠ÁöÑÂõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏éÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°Êúâ‰ªÄ‰πàÂå∫Âà´Ôºü
&lt;/div>
&lt;/h2>
&lt;p>Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°ÔºàCausal Language ModelingÔºâ&lt;/p>
&lt;pre>&lt;code>Âú®Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÔºåÊ®°ÂûãË¢´Ë¶ÅÊ±ÇÊ†πÊçÆËæìÂÖ•Â∫èÂàóÁöÑÂ∑¶‰æßÂÜÖÂÆπÊù•È¢ÑÊµãÂè≥‰æßÁöÑ‰∏ã‰∏Ä‰∏™ËØçÊàñÊ†áËÆ∞„ÄÇ‰πüÂ∞±ÊòØ
ËØ¥ÔºåÊ®°ÂûãÂè™ËÉΩÁúãÂà∞ËæìÂÖ•Â∫èÂàó‰∏≠Â∑≤ÁªèÁîüÊàêÁöÑÈÉ®ÂàÜÔºåËÄå‰∏çËÉΩÁúãÂà∞ÂêéÁª≠ÁöÑÂÜÖÂÆπ„ÄÇËøôÁßçËÆ≠ÁªÉÊñπÂºèÊúâÂä©‰∫éÊ®°
ÂûãÂ≠¶‰π†ÁîüÊàêËøûË¥ØÂíåÂêàÁêÜÁöÑÊñáÊú¨ÔºåÂõ†‰∏∫Ê®°ÂûãÈúÄË¶ÅÂú®ÁîüÊàêÊØè‰∏™ËØçËØ≠Êó∂ËÄÉËôë‰∏ä‰∏ãÊñáÁöÑ‰ø°ÊÅØÔºåÂêåÊó∂‰∏çËÉΩ‰æù
Ëµñ‰∫éÊú™Êù•ÁöÑ‰ø°ÊÅØ„ÄÇGPTÔºàGenerative Pre-trained TransformerÔºâÂ∞±ÊòØ‰ª•Âõ†ÊûúËØ≠Ë®ÄÂª∫Ê®°‰∏∫Âü∫Á°ÄÁöÑ
Ê®°Âûã„ÄÇ
&lt;/code>&lt;/pre>
&lt;p>Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÔºàMasked Language ModelingÔºâÔºö&lt;/p>
&lt;pre>&lt;code>Âú®Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°‰∏≠ÔºåÊ®°ÂûãË¢´Ë¶ÅÊ±ÇÈ¢ÑÊµãËæìÂÖ•Â∫èÂàó‰∏≠‰∏Ä‰∫õË¢´ÈöèÊú∫Êé©ÁõñÊàñÊé©Á†ÅÁöÑËØçËØ≠„ÄÇÊ®°ÂûãÈúÄË¶ÅÂü∫‰∫é‰∏ä
‰∏ãÊñáÊù•È¢ÑÊµãËøô‰∫õË¢´Êé©ÁõñÁöÑËØçËØ≠ÊòØ‰ªÄ‰πà„ÄÇ
ËøôÁßçËÆ≠ÁªÉÊñπÂºèÈÄöÂ∏∏Áî®‰∫éÂèåÂêëÁöÑËØ≠Ë®ÄÁêÜËß£‰ªªÂä°ÔºåÂõ†‰∏∫Ê®°ÂûãÈúÄË¶ÅËÄÉËôë‰∏ä‰∏ãÊñá‰∏≠ÁöÑÊâÄÊúâ‰ø°ÊÅØÊù•È¢ÑÊµãË¢´Êé©ÁõñÁöÑËØçËØ≠„ÄÇ
BERTÔºàBidirectional Encoder Representations from TransformersÔºâÂ∞±ÊòØ‰ª•Êé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°‰∏∫Âü∫Á°ÄÁöÑÊ®°Âûã„ÄÇ
&lt;/code>&lt;/pre>
&lt;h2 id="03ËØ∑ÁÆÄËø∞transformerÂü∫Êú¨ÂéüÁêÜ" >
&lt;div>
&lt;a href="#03%e8%af%b7%e7%ae%80%e8%bf%b0transformer%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86">
#
&lt;/a>
03:ËØ∑ÁÆÄËø∞TransformerÂü∫Êú¨ÂéüÁêÜ
&lt;/div>
&lt;/h2>
&lt;p>Transformer ÊòØ‰∏ÄÁßçÁî®‰∫éÂ§ÑÁêÜÂ∫èÂàóÊï∞ÊçÆÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºåÁî±Ë∞∑Ê≠åÂõ¢Èòü‰∫é2017Âπ¥ÊèêÂá∫ÔºåÂÖ∂‰∏ªË¶ÅÂéüÁêÜÂåÖÊã¨
Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂Âíå‰ΩçÁΩÆÁºñÁ†Å„ÄÇ&lt;/p>
&lt;h3 id="Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂" >
&lt;div>
&lt;a href="#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6">
##
&lt;/a>
Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>ÂÖÅËÆ∏Ê®°ÂûãÂú®Â∫èÂàóÁöÑ‰ªªÊÑè‰∏§‰∏™‰ΩçÁΩÆÈó¥Áõ¥Êé•Âª∫Á´ã‰æùËµñÂÖ≥Á≥ªÔºåËÄå‰∏çËÄÉËôëÂÆÉ‰ª¨‰πãÈó¥ÁöÑË∑ùÁ¶ª„ÄÇÂÖ∑‰ΩìÂ∞±ÊòØÂ∞ÜËØç
ÂÖÉÁ∫øÊÄßËΩ¨Êç¢‰∏∫‰∏â‰∏™ÂêëÈáèQ,K,VÔºåÁÑ∂ÂêéÂ∞ÜQÂíåKÁî®Êù•ËÆ°ÁÆóÂÜÖÁßØ(Áõ∏‰ººÂ∫¶ÂàÜÊï∞)Âπ∂ËøõË°åÊ≥®ÊÑèÂäõÁº©ÊîæÔºàscaled
dot-product)ÔºåÁÑ∂ÂêéÈÄöËøásoftmaxÂΩí‰∏ÄÂåñÔºåÂæóÂà∞ÊØè‰∏™ËØçÂÖÉÁõ∏ÂØπ‰∫éÂÖ∂‰ªñËØçÂÖÉÁöÑÊ≥®ÊÑèÂäõÊùÉÈáçÔºåÁÑ∂ÂêéÁî®
Ê≥®ÊÑèÂäõÊùÉÈáçÂØπÂêëÈáèVËøõË°åÂä†ÊùÉÂíåËÆ°ÁÆóÂæóÂà∞‚Äú‰∏ä‰∏ãÊñáÂêëÈáè‚Äù(context vector)ÔºåÁÑ∂ÂêéÂ∞Ü‰∏ä‰∏ãÊñáÂêëÈáèÁî®
ÂâçÈ¶àÁΩëÁªúÔºàFFNNÔºâËøõË°åÂèòÊç¢ÔºåÂ∞±ÂæóÂà∞ÁºñÁ†ÅÂô®ÈöêÂ±ÇËæìÂá∫„ÄÇÊ≥®ÊÑèÔºöËá™Ê≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÔºåÊØè‰∏™ËæìÂÖ•ËØçÂÖÉÁöÑ
context vector ‰ª•ÂèäÂêéÁª≠ÁöÑ hidden stateÔºåÂèØ‰ª•ÁúãÊàêÊòØÁõ∏Â∫îÁöÑ Q ÂêëÈáèÁöÑÂáΩÊï∞ÔºåÂÖ∂‰ªñÁöÑÂ¶Ç KÔºåVÔºå
‰ª•ÂèäËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂèÇÊï∞ÂØπÊâÄÊúâÁöÑ Q ÈÉΩÊòØÊÅíÂÆöÂÄº„ÄÇ
+ Â§öÂ§¥Ê≥®ÊÑèÂäõÔºö
Âú®Â§öÂ§¥Ê≥®ÊÑèÂäõ‰∏≠ÔºåÊ≥®ÊÑèÂäõÊú∫Âà∂Ë¢´Â§çÂà∂Â§öÊ¨°ÔºåÂπ∂‰∏îÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÈÉΩÂ≠¶‰π†Âà∞‰∏ÄÁªÑ‰∏çÂêåÁöÑQ,K,VÁöÑ
Ë°®Á§∫ÔºåÁÑ∂ÂêéÂ∞ÜÂÆÉ‰ª¨ÁöÑËæìÂá∫ÊãºÊé•Ëµ∑Êù•ÔºåÂÜçÈÄöËøáFFNNËøõË°åÁª¥Â∫¶ÂØπÈΩê„ÄÇ
- Â§çÂà∂Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºöÂéüÂßãËæìÂÖ•Â∫èÂàó‰ºöË¢´Áî®Êù•ËÆ°ÁÆóÂ§ö‰∏™Ê≥®ÊÑèÂäõÂ§¥Ôºà‰æãÂ¶Ç8‰∏™Êàñ16‰∏™Â§¥Ôºâ
- Áã¨Á´ãÂ≠¶‰π†ÔºöÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÈÉΩ‰ºöÁã¨Á´ãÂú∞Â≠¶‰π†‰∏ÄÁªÑQÔºåKÔºåVÁöÑË°®Á§∫Ôºå‰πüÂ∞±ÊòØÔºöÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÈÉΩ
ÊúâËá™Â∑±ÁöÑÊùÉÈáçÁü©ÈòµÔºåÂ∞ÜËæìÂÖ•Â∫èÂàóËΩ¨Êç¢‰∏∫Q,K,VÂêëÈáè„ÄÇ
- Ê≥®ÊÑèÂäõËÆ°ÁÆóÔºöÊØè‰∏™Ê≥®ÊÑèÂäõÂ§¥ÂÉèÂçïÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÈÇ£Ê†∑ËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞ÂíåÊ≥®ÊÑèÂäõÊùÉÈáç„ÄÇ
- ÊãºÊé•ËæìÂá∫ÔºöÂ∞ÜÊâÄÊúâÊ≥®ÊÑèÂäõÂ§¥ÁöÑËæìÂá∫ÊãºÊé•Êàê‰∏Ä‰∏™ÂêëÈáèÔºåÂΩ¢ÊàêÂ§öÂ§¥Ê≥®ÊÑèÂäõÁöÑÊúÄÁªàËæìÂá∫„ÄÇËøôÊÑèÂë≥
ÁùÄÊØè‰∏™ËØçÂÖÉÈÉΩ‰ºöÂæóÂà∞Êù•Ëá™Â§ö‰∏™‰∏çÂêåËßÜËßíÁöÑË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÂØπËæìÂÖ•Â∫èÂàóÁöÑÁêÜËß£„ÄÇ
- Á∫øÊÄßÂèòÊç¢ÔºöÊãºÊé•ÂêéÁöÑËæìÂá∫ÈÄöËøáFFNNËøõË°åÂ§ÑÁêÜÔºåÁª¥ÊåÅËæìÂá∫Áª¥Â∫¶‰ª•ÂèäËûçÂêà‰∏çÂêåÊ≥®ÊÑèÂäõÂ§¥ÁöÑ‰ø°ÊÅØ„ÄÇ
+ narrow attnÔºöEach attention head will get a chunk of the transformed data points
(projections) to work with. This is a details of utmost importance: The
attention heads DO NOT use chunks of the original data points, but rather those
of their projections. It computes the projections first and then chunks them
later, so that each value in the projection is a linear combination of all
features in the data point.
&lt;/code>&lt;/pre>
&lt;h3 id="‰ΩçÁΩÆÁºñÁ†Å" >
&lt;div>
&lt;a href="#%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81">
##
&lt;/a>
‰ΩçÁΩÆÁºñÁ†ÅÔºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>‰ΩçÁΩÆÁºñÁ†ÅÈÄöÂ∏∏ÊòØÈÄöËøáÂ∞Ü‰∏Ä‰∏™‰∏é‰ΩçÁΩÆÁõ∏ÂÖ≥ÁöÑÂêëÈáèÊ∑ªÂä†Âà∞ËæìÂÖ•ÂµåÂÖ•Ôºàinput embeddingsÔºâ
‰∏≠Êù•ÂÆûÁé∞ÁöÑ„ÄÇËøô‰∏™ÂêëÈáè‰∏∫Â∫èÂàó‰∏≠ÁöÑÊØè‰∏™‰ΩçÁΩÆÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑË°®Á§∫Ôºå‰ªéËÄå‰ΩøÊ®°ÂûãËÉΩÂ§ü
Âå∫ÂàÜ‰∏çÂêåÁöÑÂçïËØçÈ°∫Â∫è„ÄÇ
ÁâπÂà´ÊòØÂü∫‰∫é transformer Êû∂ÊûÑÁöÑÊ®°ÂûãÔºåÁî±‰∫éËá™Ê≥®ÊÑèÂäõÊú∫Âà∂Êó†Ê≥ïÊçïÊçâËØçÂÖÉÈ°∫Â∫èÔºåÂõ†Ê≠§ÂøÖ
È°ªÈÄöËøáÂä†ÂÖ•‰ΩçÁΩÆÁºñÁ†ÅÊù•Ëé∑ÂèñËæìÂÖ•Â∫èÂàó‰∏≠ÂêÑ‰∏™ËØçÂÖÉÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇ
Â∞ΩÁÆ°Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºàSelf-Attention MechanismÔºâÁ°ÆÂÆûÂèØ‰ª•ÊçïÊçâÂ∫èÂàó‰∏≠ÂÖÉÁ¥†‰πãÈó¥ÁöÑÂÖ≥
Á≥ªÔºå‰ΩÜÂÆÉ‰∏ªË¶Å‰æùËµñ‰∫éÂÖÉÁ¥†‰πãÈó¥ÁöÑ‰∫§‰∫íÂíåÊùÉÈáçËÆ°ÁÆóÔºåËÄå‰∏çÊòØÂÆÉ‰ª¨ÁöÑÁªùÂØπ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇ‰ΩçÁΩÆ
ÁºñÁ†ÅÁöÑ‰ΩúÁî®ÊòØË°•ÂÖÖËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊèê‰æõÂ∫èÂàó‰∏≠ÂÖÉÁ¥†È°∫Â∫èÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥
Â•ΩÂú∞ÁêÜËß£Â∫èÂàóÁöÑÁªìÊûÑ„ÄÇ
+ Ê≠£Âº¶/‰ΩôÂº¶ÂáΩÊï∞ÁªÑÂêàÁºñÁ†Å (ÂÅ∂Êï∞‰ΩçÁî® sin(), Â•áÊï∞‰ΩçÁî® cos())
ÂÆÉ‰ª¨‰∏∫Â∫èÂàó‰∏≠ÁöÑÊØè‰∏™‰ΩçÁΩÆÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂîØ‰∏ÄÁöÑ„ÄÅ‰∏é‰ΩçÁΩÆÁõ¥Êé•Áõ∏ÂÖ≥ÁöÑÁºñÁ†Å„ÄÇËøôÁßçÁºñÁ†Å
ÊñπÂºèËÉΩÂ§üÊòéÁ°ÆÂú∞ÂëäËØâÊ®°ÂûãÊØè‰∏™ÂçïËØçÂú®Â∫èÂàó‰∏≠ÁöÑÁªùÂØπ‰ΩçÁΩÆ„ÄÇ
ÁªùÂØπ‰ΩçÁΩÆÁºñÁ†ÅÁöÑ‰∏Ä‰∏™ÊΩúÂú®Áº∫ÁÇπÊòØÂÆÉ‰ª¨ÊòØÈùôÊÄÅÁöÑÔºå‰∏ç‰ºöÈöèÁùÄÊ®°ÂûãËÆ≠ÁªÉÁöÑËøõË°åËÄåÊîπÂèò„ÄÇ
ËøôÊÑèÂë≥ÁùÄÂÆÉ‰ª¨ÂèØËÉΩ‰∏çË∂≥‰ª•ÊçïÊçâÈïøÂ∫èÂàó‰∏≠Â§çÊùÇÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÁâπÂà´ÊòØÂú®Ê®°ÂûãÈúÄË¶ÅÂä®ÊÄÅ
Âú∞Ë∞ÉÊï¥‰ΩçÁΩÆ‰ø°ÊÅØ‰ª•ÈÄÇÂ∫îËæìÂÖ•Â∫èÂàóÁöÑÂèòÂåñÊó∂„ÄÇ
‰ø°ÊÅØÁöÑÂ±ÄÈôêÊÄßÔºöÂõ∫ÂÆöÁöÑ‰ΩçÁΩÆÁºñÁ†Å‰ªÖÊèê‰æõ‰∫Ü‰ΩçÁΩÆÁöÑÁªùÂØπ‰ø°ÊÅØÔºåËÄåÊ≤°ÊúâËÄÉËôëÂ∫èÂàó‰∏≠ÂÖÉ
Á¥†‰πãÈó¥ÁöÑÁõ∏ÂØπÂÖ≥Á≥ª„ÄÇÂú®ÈïøÂ∫èÂàó‰∏≠ÔºåÂÖÉÁ¥†‰πãÈó¥ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÂíåË∑ùÁ¶ªÂèØËÉΩÊõ¥‰∏∫ÈáçË¶Å„ÄÇ
+ ÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÔºöÊóãËΩ¨‰ΩçÁΩÆÁºñÁ†ÅÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÊØè‰∏™‰ΩçÁΩÆÁöÑÁºñÁ†ÅË°®Á§∫‰∏∫‰∏Ä‰∏™ÊóãËΩ¨Áü©ÈòµÔºå
ËØ•Áü©ÈòµÂèØ‰ª•Â∫îÁî®‰∫éËæìÂÖ•ÂµåÂÖ•„ÄÇÊóãËΩ¨Áü©ÈòµÊòØÂä®ÊÄÅÁîüÊàêÁöÑÔºåËøôÊÑèÂë≥ÁùÄÂÆÉ‰ª¨ÂèØ‰ª•Ê†πÊçÆËæìÂÖ•
Â∫èÂàóÁöÑÂÜÖÂÆπËøõË°åË∞ÉÊï¥Ôºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÊçïÊçâÈïøË∑ùÁ¶ª‰æùËµñ„ÄÇ
&lt;/code>&lt;/pre>
&lt;h3 id="ÊÆãÂ∑ÆËøûÊé•‰∏éÂ±ÇÂΩí‰∏ÄÂåñ" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%8e%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96">
##
&lt;/a>
ÊÆãÂ∑ÆËøûÊé•‰∏éÂ±ÇÂΩí‰∏ÄÂåñÔºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>+ ÊÆãÂ∑ÆËøûÊé•ÔºöÂ∞ÜÊØè‰∏™Â≠êÂ±ÇÁöÑËæìÂÖ•‰∏éÂÖ∂ËæìÂá∫Áõ∏Âä†ÔºåÁÑ∂Âêé‰º†ÈÄíÁªô‰∏ã‰∏ÄÂ±Ç„ÄÇËøô‰ΩøÂæóÊ®°ÂûãÂú®Â≠¶
‰π†ËøáÁ®ã‰∏≠ÔºåËÉΩÊõ¥ÂÆπÊòìÂú∞Â≠¶‰π†Âà∞ÊÆãÂ∑ÆÔºàËæìÂÖ•‰∫éËæìÂá∫‰πãÂ∑ÆÔºâÔºå‰ªéËÄåÁºìËß£Ê¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÔºõ
ÊèêÈ´òËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÔºõÂÖÅËÆ∏Êõ¥Ê∑±ÁöÑÁΩëÁªúÁªìÊûÑÔºõ‰ª•ÂèäÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇ
+ Â±ÇÂΩí‰∏ÄÂåñÔºöÂú®ÊØè‰∏™Â±ÇÁöÑËæìÂÖ•‰πãÂêéÈÉΩÂ∫îÁî®ÂΩí‰∏ÄÂåñÔºåÂç≥ÂØπÊØè‰∏™ÁâπÂæÅÁª¥Â∫¶ËøõË°åÂΩí‰∏ÄÂåñÊìç‰ΩúÔºå
‰ΩøÂæóÂÆÉ‰ª¨ÂùáÂÄº‰∏∫0ÔºåÊ†áÂáÜÂ∑Æ‰∏∫1ÔºåÊúâÂä©‰∫éÁºìËß£Ê¢ØÂ∫¶Ê∂àÂ§±ÂíåÊ¢ØÂ∫¶ÁàÜÁÇ∏ÁöÑÈóÆÈ¢òÔºå‰ªéËÄå‰Ωø
Ê®°ÂûãËÆ≠ÁªÉÊõ¥Âä†Á®≥ÂÆöÔºå‰πüÊèêÈ´òÂÖ∂Ê≥õÂåñËÉΩÂäõ„ÄÇ
- ‚ÄúÊâπÊ¨°ÂΩí‰∏ÄÂåñ‚ÄùÔºàBatch NormÔºâÔºö
The mean and variance statistics used for normalization are calculated
across all elements of all instances in a batch, for each feature
independently.
Âç≥ÔºöÂùáÂÄºÂíåÊñπÂ∑ÆÊòØÈÄöËøáÂØπ‰∏Ä‰∏™ÊâπÊ¨°ÈáåÊâÄÊúâÂÆû‰æãÔºàÂ∫èÂàóÔºâÁöÑÊâÄÊúâÂÖÉÁ¥†ÔºàËØçÂÖÉÔºâÁöÑÊüê
‰∏™ÁâπÂæÅËøõË°åÁªüËÆ°ÁöÑ„ÄÇ
- ‚ÄúÂ±ÇÂΩí‰∏ÄÂåñ‚ÄùÔºàLayer NormÔºâÔºö
For layernorm, the statistics are calculated across the feature
dimension, for each element and instance independently.
Âç≥ÔºöÂùáÂÄºÂíåÊñπÂ∑ÆÊòØÈÄöËøáÂØπÊüê‰∏™ÂÆû‰æãÔºàÂ∫èÂàóÔºâ‰∏≠ÁöÑÊüê‰∏™ÂÖÉÁ¥†ÔºàËØçÂÖÉÔºâÁöÑÊâÄÊúâÁâπÂæÅËøõ
Ë°åÁªüËÆ°ÁöÑ„ÄÇ
NOTE By &amp;quot;element&amp;quot; and &amp;quot;instance,&amp;quot; I mean &amp;quot;word&amp;quot; and &amp;quot;sentence&amp;quot;
respectively for an NLP task, and &amp;quot;pixel&amp;quot; and &amp;quot;image&amp;quot; for a CV task.
&lt;/code>&lt;/pre>
&lt;p>&lt;img alt="IMG: LayerNorm &amp;amp; BatchNorm" src="https://fgg100y.github.io/posts/llm_faqs/images/LLMs_layerNorm_batchNorm.png">&lt;/p>
&lt;h2 id="04-Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊîπËâØÁâàÊú¨‰ª¨" >
&lt;div>
&lt;a href="#04-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
04: Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊîπËâØÁâàÊú¨‰ª¨
&lt;/div>
&lt;/h2>
&lt;p>MHAÔºåGQAÔºåMQA‰∏âÁßçÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü&lt;/p>
&lt;p>Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÊú∫Âô®Â≠¶‰π†È¢ÜÂüüÊúâÂ§öÁßç‰∏çÂêåÁöÑÂÆûÁé∞ÊñπÂºèÔºåÂÖ∂‰∏≠Â∏∏ËßÅÁöÑÂåÖÊã¨Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõ
ÔºàMulti-Head Self-AttentionÔºåMHAÔºâ„ÄÅÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºàGlobal AttentionÔºåGQAÔºâÂíåÂ§öÂ§¥Êü•ËØ¢Ê≥®ÊÑèÂäõ
ÔºàMulti-Query AttentionÔºåMQAÔºâ„ÄÇËøô‰∫õ‰∏çÂêåÁöÑÂÆûÁé∞ÊñπÂºèÂú®Êú∫Âà∂ÂíåÂ∫îÁî®‰∏äÊúâ‰∏Ä‰∫õÂå∫Âà´Ôºö&lt;/p>
&lt;pre>&lt;code>Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÔºàMHAÔºâÔºö
Êú∫Âà∂ÔºöMHAÂ∞ÜËæìÂÖ•Â∫èÂàó‰∏≠ÁöÑÊØè‰∏™‰ΩçÁΩÆÁöÑË°®Á§∫ÈÉΩ‰Ωú‰∏∫Êü•ËØ¢ÔºàQueryÔºâ„ÄÅÈîÆÔºàKeyÔºâÂíåÂÄºÔºàValueÔºâÔºå
ÈÄöËøáËÆ°ÁÆóÊü•ËØ¢‰∏éÊâÄÊúâÈîÆÁöÑÁõ∏‰ººÂ∫¶ÔºåÁÑ∂ÂêéÂ∞ÜÁõ∏‰ººÂ∫¶‰Ωú‰∏∫ÊùÉÈáçÂØπÂÄºËøõË°åÂä†ÊùÉÊ±ÇÂíåÔºå‰ªéËÄåËé∑ÂæóÊØè‰∏™
‰ΩçÁΩÆÁöÑÊ≥®ÊÑèÂäõËæìÂá∫„ÄÇ
ÁâπÁÇπÔºöMHAÂÖÅËÆ∏Ê®°ÂûãÂú®‰∏çÂêåÁöÑË°®Á§∫Á©∫Èó¥‰∏äËøõË°åÂ§öÂ§¥Âπ∂Ë°åËÆ°ÁÆóÔºåÈÄöËøáÂ§öÂ§¥Êú∫Âà∂ÔºåÂèØ‰ª•Â≠¶‰π†Âà∞‰∏çÂêåÁöÑÂÖ≥Ê≥®ÁÇπÂíåË°®Á§∫„ÄÇ
Â∫îÁî®ÔºöMHAÂ∏∏Áî®‰∫éTransformerÁ≠âÊ®°Âûã‰∏≠ÔºåÁî®‰∫éÊçïÊçâËæìÂÖ•Â∫èÂàó‰∏≠‰∏çÂêå‰ΩçÁΩÆ‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇ
ÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºàGQAÔºâÔºö
Êú∫Âà∂ÔºöGQAÂ∞ÜÊâÄÊúâÁöÑËæìÂÖ•‰ΩçÁΩÆÈÉΩ‰Ωú‰∏∫Êü•ËØ¢Ôºå‰∏éÊâÄÊúâÁöÑÈîÆËÆ°ÁÆóÁõ∏‰ººÂ∫¶ÔºåÁÑ∂ÂêéÂ∞ÜÊâÄÊúâ‰ΩçÁΩÆÁöÑÂÄºÊ†π
ÊçÆÁõ∏‰ººÂ∫¶ËøõË°åÂä†ÊùÉÊ±ÇÂíåÔºåÂæóÂà∞‰∏Ä‰∏™ÂÖ®Â±ÄÁöÑËæìÂá∫„ÄÇ
ÁâπÁÇπÔºöGQAËÄÉËôë‰∫ÜÂ∫èÂàó‰∏≠ÊâÄÊúâ‰ΩçÁΩÆÁöÑÂÖ≥Á≥ªÔºå‰ΩÜÂú®Â§ÑÁêÜÈïøÂ∫èÂàóÊó∂ÂèØËÉΩ‰ºöÂèóÂà∞ËÆ°ÁÆóËµÑÊ∫êÁöÑÈôêÂà∂Ôºå
Âõ†‰∏∫ÈúÄË¶ÅËÆ°ÁÆóÊâÄÊúâ‰ΩçÁΩÆ‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶„ÄÇ
Â∫îÁî®ÔºöGQAÂ∏∏Áî®‰∫éÂØπÊï¥‰∏™ËæìÂÖ•Â∫èÂàóËøõË°åÂÖ®Â±ÄÁöÑ‰ø°ÊÅØËÅöÂêàÔºå‰æãÂ¶ÇÂú®ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠„ÄÇ
Â§öÂ§¥Êü•ËØ¢Ê≥®ÊÑèÂäõÔºàMQAÔºâÔºö
Êú∫Âà∂ÔºöMQA‰∏éMHAÁ±ª‰ººÔºå‰ΩÜÂú®ÊØè‰∏™Â§¥ÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆó‰∏≠Ôºå‰ΩøÁî®‰∏çÂêåÁöÑÊü•ËØ¢ÂêëÈáèÔºåËÄå‰∏çÊòØÊâÄÊúâÂ§¥ÈÉΩÂÖ±‰∫´Áõ∏ÂêåÁöÑÊü•ËØ¢ÂêëÈáè„ÄÇ
ÁâπÁÇπÔºöMQAÂÖÅËÆ∏Ê®°Âûã‰∏∫ÊØè‰∏™Â§¥Â≠¶‰π†‰∏çÂêåÁöÑÊü•ËØ¢Ê®°ÂºèÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÂíåË°®ËææËÉΩÂäõ„ÄÇ
Â∫îÁî®ÔºöMQAÂ∏∏Áî®‰∫éÈúÄË¶ÅÊ†πÊçÆ‰∏çÂêåÁöÑÊü•ËØ¢Êù•Ëé∑ÂèñÊ≥®ÊÑèÂäõ‰ø°ÊÅØÁöÑ‰ªªÂä°ÔºåÂ¶ÇÈóÆÁ≠îÁ≥ªÁªüÊàñÈúÄË¶ÅÈíàÂØπ‰∏çÂêåÈóÆÈ¢òËøõË°åÊé®ÁêÜÁöÑÂú∫ÊôØ
&lt;/code>&lt;/pre>
&lt;h2 id="05-attentionÁöÑÊîπËâØÁâàÊú¨‰ª¨" >
&lt;div>
&lt;a href="#05-attention%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
05: AttentionÁöÑÊîπËâØÁâàÊú¨‰ª¨
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ÁÆÄËø∞‰∏Ä‰∏ã FlashAttention ÁöÑÂéüÁêÜ&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Flash AttentionÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÊ≥®ÊÑèÂäõÁÆóÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥‰º†ÁªüTransformerÊ®°Âûã‰∏≠Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑËÆ°
ÁÆóÂíåÂÜÖÂ≠òÊïàÁéáÈóÆÈ¢ò„ÄÇÁî±‰∫éËá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊó∂Èó¥ÂíåÂ≠òÂÇ®Â§çÊùÇÂ∫¶‰∏éÂ∫èÂàóÈïøÂ∫¶Êàê‰∫åÊ¨°ÊñπÂÖ≥Á≥ªÔºåËøô‰ΩøÂæóÂ§Ñ
ÁêÜÈïøÂ∫èÂàóÊï∞ÊçÆÊó∂Èù¢‰∏¥Â∑®Â§ßÊåëÊàò„ÄÇFlash AttentionÈÄöËøáÁ≤æÂøÉËÆæËÆ°ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÂØπÈ´òÂ∏¶ÂÆΩÂÜÖÂ≠òÔºàHBMÔºâ
ÁöÑËØªÂÜôÊ¨°Êï∞Ôºå‰ªéËÄåÂä†Âø´‰∫ÜËøêË°åÈÄüÂ∫¶Âπ∂Èôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®„ÄÇ&lt;/p>
&lt;h3 id="flash-attentionÁöÑÊ†∏ÂøÉÂéüÁêÜÂíåÊäÄÊúØ" >
&lt;div>
&lt;a href="#flash-attention%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86%e5%92%8c%e6%8a%80%e6%9c%af">
##
&lt;/a>
Flash AttentionÁöÑÊ†∏ÂøÉÂéüÁêÜÂíåÊäÄÊúØÔºö
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>Âπ≥Èì∫ÔºàTilingÔºâÔºöFlash AttentionÂ∞ÜËæìÂÖ•ÂàÜÂâ≤ÊàêÂ∞èÂùóÔºåÂπ∂Âú®ÊØè‰∏™Âùó‰∏äÊâßË°åÊ≥®ÊÑèÂäõÊìç‰Ωú„ÄÇËøôÁßçÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂØπÈ´òÂ∏¶ÂÆΩÂÜÖÂ≠òÁöÑËÆøÈóÆÊ¨°Êï∞ÔºåÂõ†‰∏∫‰∏çÈúÄË¶Å‰∏ÄÊ¨°ÊÄßÂ∞ÜÊï¥‰∏™Â§ßÁü©ÈòµÂä†ËΩΩÂà∞ÂÜÖÂ≠ò‰∏≠„ÄÇ
ÈáçÊñ∞ËÆ°ÁÆóÔºàRecomputationÔºâÔºöÂú®ÂêéÂêë‰º†Êí≠ËøáÁ®ã‰∏≠ÔºåFlash AttentionÈÅøÂÖç‰∫ÜÂ≠òÂÇ®Â§ßÂûã‰∏≠Èó¥Áü©ÈòµÔºàÂ¶ÇSÂíåPÁü©ÈòµÔºâÔºåËÄåÊòØÂà©Áî®ÂâçÂêë‰º†Êí≠‰∏≠ÁöÑÁªüËÆ°ÈáèÊù•Âø´ÈÄüÈáçÊñ∞ËÆ°ÁÆóËøô‰∫õÁü©ÈòµÔºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÊ∂àËÄó„ÄÇ
Âú®Á∫øSoftmaxÔºö‰∏∫‰∫ÜÂ§ÑÁêÜSoftmaxÊìç‰ΩúÔºåFlash AttentionÈááÁî®‰∫ÜÂú®Á∫øSoftmaxÊäÄÊúØÔºåÂÆÉÂÖÅËÆ∏ÂàÜÂùóËÆ°ÁÆósoftmaxÔºåÂπ∂ÈÄöËøáÈÄÇÂΩìÁöÑÂΩí‰∏ÄÂåñÂõ†Â≠êÊù•Á°Æ‰øùÊúÄÁªàÁªìÊûúÁöÑÊ≠£Á°ÆÊÄß„ÄÇ
ÂÜÖÂ≠òÂ±ÇÊ¨°ÁªìÊûÑÊÑèËØÜÔºàIO-AwarenessÔºâÔºöFlash AttentionËÄÉËôë‰∫ÜGPUÂÜÖÂ≠òÂ±ÇÊ¨°ÁªìÊûÑÔºå‰ºòÂåñ‰∫Ü‰∏çÂêåÂ±ÇÁ∫ßÂÜÖÂ≠ò‰πãÈó¥ÁöÑÊï∞ÊçÆËÆøÈóÆÔºåÂ¶ÇÂú®GPUÁöÑSRAMÂíåHBM‰πãÈó¥„ÄÇ
&lt;/code>&lt;/pre>
&lt;h3 id="flash-attention-2" >
&lt;div>
&lt;a href="#flash-attention-2">
##
&lt;/a>
Flash Attention-2Ôºö
&lt;/div>
&lt;/h3>
&lt;p>Âú®Flash AttentionÁöÑÂü∫Á°Ä‰∏äÔºåÁ†îÁ©∂‰∫∫ÂëòËøõ‰∏ÄÊ≠•ÊèêÂá∫‰∫ÜFlash Attention-2ÔºåÂÆÉÈÄöËøáÊîπËøõÂ∑•‰ΩúÂàÜÈÖçÂíåÂπ∂Ë°åÂåñÁ≠ñÁï•ÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´ò‰∫ÜËÆ°ÁÆóÈÄüÂ∫¶„ÄÇFlash Attention-2ÁöÑ‰ºòÂåñÂåÖÊã¨Ôºö&lt;/p>
&lt;pre>&lt;code>ÂáèÂ∞ëÈùûÁü©Èòµ‰πòÊ≥ïÔºànon-matmulÔºâÁöÑÊµÆÁÇπËøêÁÆóÊ¨°Êï∞ÔºàFLOPsÔºâ„ÄÇ
ÈÄöËøáÂú®‰∏çÂêåÁöÑÁ∫øÁ®ãÂùó‰∏äÂπ∂Ë°åÂåñÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÊèêÈ´ò‰∫ÜGPUÁöÑÂç†Áî®Áéá„ÄÇ
Âú®ÊØè‰∏™Á∫øÁ®ãÂùóÂÜÖÔºåÂ∞ÜÂ∑•‰ΩúÂàÜÈÖçÁªô‰∏çÂêåÁöÑwarpÔºå‰ª•ÂáèÂ∞ëÈÄöËøáÂÖ±‰∫´ÂÜÖÂ≠òÁöÑÈÄö‰ø°„ÄÇ
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;strong>PagedAttentionÁöÑÂéüÁêÜÊòØ‰ªÄ‰πàÔºåËß£ÂÜ≥‰∫ÜLLM‰∏≠ÁöÑ‰ªÄ‰πàÈóÆÈ¢òÔºü&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Paged AttentionÔºàPAÔºâÊäÄÊúØÊòØ‰∏ÄÁßçÁî®‰∫é‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜÊÄßËÉΩÁöÑÊñπÊ≥ïÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜËá™
ÂõûÂΩíÁîüÊàê‰ªªÂä°Êó∂ÂØπÂÜÖÂ≠ò‰ΩøÁî®ÊïàÁéáÁöÑÊòæËëóÊèêÂçá„ÄÇËøôÈ°πÊäÄÊúØÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂÄüÈâ¥Êìç‰ΩúÁ≥ªÁªü‰∏≠ËôöÊãüÂÜÖÂ≠òÂíåÂàÜÈ°µ
ÁöÑÊäÄÊúØÔºåÂ∞Ü‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂‰∏≠ÁöÑÈîÆÂÄºÂØπÔºàKey-Value pairsÔºåÁÆÄÁß∞K-V pairsÔºâÁºìÂ≠ò‰ª•ÂàÜÈ°µÁöÑÂΩ¢ÂºèÂ≠ò
ÂÇ®ÂíåÁÆ°ÁêÜ„ÄÇ&lt;/p>
&lt;p>Âú®Ëá™ÂõûÂΩíËß£Á†ÅËøáÁ®ã‰∏≠ÔºåÊ®°Âûã‰∏∫ÊØè‰∏™ËæìÂÖ•‰ª§ÁâåÁîüÊàêÊ≥®ÊÑèÂäõÈîÆÂíåÂÄºÔºåËøô‰∫õÈîÆÂÄºÂØπË¢´Â≠òÂÇ®Âú®GPUÁöÑÊòæÂ≠ò‰∏≠‰ª•
È¢ÑÊµã‰∏ã‰∏Ä‰∏™‰ª§Áâå„ÄÇÁî±‰∫éËøô‰∫õÁºìÂ≠òÁöÑÈîÆÂÄºÂØπÂ§ßÂ∞èÊòØÂä®ÊÄÅÂèòÂåñÁöÑÔºåÂπ∂‰∏îÂèØËÉΩ‰ºöÂç†Áî®Â§ßÈáèÁöÑÊòæÂ≠òÁ©∫Èó¥ÔºåÂõ†Ê≠§
ÊúâÊïàÁÆ°ÁêÜËøô‰∫õÁºìÂ≠òÊàê‰∏∫‰∏Ä‰∏™ÊåëÊàò„ÄÇ‰º†ÁªüÁöÑÊ≥®ÊÑèÂäõÁÆóÊ≥ïÂú®Â§ÑÁêÜÊó∂‰ºöÂèóÈôê‰∫éÊòæÂ≠òÁöÑÂ§ßÂ∞èÔºåËøôÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑ
ÊâπÂ§ÑÁêÜËÉΩÂäõÂíåÊï¥‰ΩìÁöÑÂêûÂêêÈáè„ÄÇ&lt;/p>
&lt;h2 id="06llmÂæÆË∞É‰∏éÈáèÂåñ" >
&lt;div>
&lt;a href="#06llm%e5%be%ae%e8%b0%83%e4%b8%8e%e9%87%8f%e5%8c%96">
#
&lt;/a>
06:LLMÂæÆË∞É‰∏éÈáèÂåñ
&lt;/div>
&lt;/h2>
&lt;p>30.ÂèÇÊï∞È´òÊïàÁöÑÂæÆË∞ÉÔºàPEFTÔºâÊúâÂì™‰∫õÊñπÊ≥ïÔºü
31.LORAÂæÆË∞ÉÁõ∏ÊØî‰∫éÂæÆË∞ÉÈÄÇÈÖçÂô®ÊàñÂâçÁºÄÂæÆË∞ÉÊúâ‰ªÄ‰πà‰ºòÂäøÔºü
32.Êúâ‰∫ÜËß£Ëøá‰ªÄ‰πàÊòØÁ®ÄÁñèÂæÆË∞ÉÂêóÔºü
33.ËÆ≠ÁªÉÂêéÈáèÂåñÔºàPTQÔºâÂíåÈáèÂåñÊÑüÁü•ËÆ≠ÁªÉÔºàQATÔºâ‰∏é‰ªÄ‰πàÂå∫Âà´Ôºü
34.LLMs‰∏≠ÔºåÈáèÂåñÊùÉÈáçÂíåÈáèÂåñÊøÄÊ¥ªÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
35.AWQÈáèÂåñÁöÑÊ≠•È™§ÊòØ‰ªÄ‰πàÔºü&lt;/p>
&lt;h2 id="07ÂµåÂÖ•ÂêëÈáèÊ®°Âûã" >
&lt;div>
&lt;a href="#07%e5%b5%8c%e5%85%a5%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8b">
#
&lt;/a>
07:ÂµåÂÖ•ÂêëÈáèÊ®°Âûã
&lt;/div>
&lt;/h2>
&lt;p>40.Ëá™Ââç‰∏ªÊµÅÁöÑ‰∏≠ÊñáÂµåÂÖ•ÂêëÈáèÊ®°ÂûãÊúâÂì™‰∫õÔºü&lt;/p>
&lt;h2 id="ÂÖ∂‰ªñ" >
&lt;div>
&lt;a href="#%e5%85%b6%e4%bb%96">
#
&lt;/a>
ÂÖ∂‰ªñ
&lt;/div>
&lt;/h2>
&lt;p>45.DeepSpeedÊé®ÁêÜÂØπÁÆóÂ≠êËûçÂêàÂÅö‰∫ÜÂì™‰∫õ‰ºòÂåñÔºü
48.ËØ∑‰ªãÁªç‰∏Ä‰∏ãÂæÆËΩØÁöÑZeRO‰ºòÂåñÂô®&lt;/p>
&lt;p>3.‰∏∫‰ªÄ‰πàÁé∞Âú®ÁöÑÂ§ßÊ®°ÂûãÂ§ßÂ§öÊòØdecoder-onlyÁöÑÊû∂ÊûÑÔºü
4.ËÆ≤‰∏Ä‰∏ãÁîüÊàêÂºèËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∑•‰ΩúÊú∫ÁêÜ
5.Âì™‰∫õÂõ†Á¥†‰ºöÂØºËá¥LLMÁöÑÂÅèËßÅÔºü
7.Â¶Ç‰ΩïÂáèËΩªLLM‰∏≠ÁöÑÂπªËßâÁé∞Ë±°Ôºü
8.Ëß£ÈáäChatGPTÁöÑÈõ∂Ê†∑Êú¨ÂíåÂ∞ëÊ†∑Êú¨Â≠¶‰π†ÁöÑÊ¶ÇÂøµ
10.Â¶Ç‰ΩïËØÑ‰º∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊÄßËÉΩÔºü
11.Â¶Ç‰ΩïÁºìËß£LLMsÈáçÂ§çËØªÈóÆÈ¢òÔºü
16.Wordpiece‰∏éBPE‰πãÈó¥ÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
17.ÊúâÂì™‰∫õÂ∏∏ËßÅÁöÑ‰ºòÂåñLLMsËæìÂá∫ÁöÑÊäÄÊúØÔºü
18.GPT-3Êã•ÊúâÁöÑ1750‰∫øÂèÇÊï∞ÔºåÊòØÊÄé‰πàÁÆóÂá∫Êù•ÁöÑÔºü
19.Ê∏©Â∫¶Á≥ªÊï∞Âíåtop-pÔºåtop-kÂèÇÊï∞Êúâ‰ªÄ‰πàÂå∫Âà´Ôºü
21.‰ªãÁªç-‰∏ãpostlayernormÂíåprelayernormÁöÑÂå∫Âà´
22.‰ªÄ‰πàÊòØÊÄùÁª¥ÈìæÔºàCoTÔºâÊèêÁ§∫Ôºü
23.‰Ω†ËßâÂæó‰ªÄ‰πàÊ†∑ÁöÑ‰ªªÂä°ÊàñÈ¢ÜÂüüÈÄÇÂêàÁî®ÊÄùÁª¥ÈìæÊèêÁ§∫Ôºü
24.‰Ω†‰∫ÜËß£ReActÂêóÔºåÂÆÉÊúâ‰ªÄ‰πà‰ºòÁÇπÔºü
25.Ëß£Èáä‰∏Ä‰∏ãlangchainAgentÁöÑÊ¶ÇÂøµ
26.langchainÊúâÂì™‰∫õÊõø‰ª£ÊñπÊ°àÔºü
27.langchaintokenËÆ°Êï∞Êúâ‰ªÄ‰πàÈóÆÈ¢òÔºüÂ¶Ç‰ΩïËß£ÂÜ≥Ôºü
28.LLMÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊúâÂì™Âá†‰∏™ÂÖ≥ÈîÆÊ≠•È™§Ôºü
29.RLHFÊ®°Âûã‰∏∫‰ªÄ‰πà‰ºöË°®Áé∞ÊØîSFTÊõ¥Â•ΩÔºü
36.‰ªãÁªç‰∏Ä‰∏ãGPipeÊé®ÁêÜÊ°ÜÊû∂
37.Áü©Èòµ‰πòÊ≥ïÂ¶Ç‰ΩïÂÅöÂº†ÈáèÂπ∂Ë°åÔºü
38.ËØ∑ÁÆÄËø∞‰∏ãPPOÁÆóÊ≥ïÊµÅÁ®ãÔºåÂÆÉË∑üTRPOÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
39.‰ªÄ‰πàÊòØÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÔºü
41.‰∏∫‰ªÄ‰πàLLMÁöÑÁü•ËØÜÊõ¥Êñ∞ÂæàÂõ∞ÈöæÔºü
42.RAGÂíåÂæÆË∞ÉÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºü
43.Â§ßÊ®°Âûã‰∏ÄËà¨ËØÑÊµãÊñπÊ≥ïÂèäÂü∫ÂáÜÊòØ‰ªÄ‰πàÔºü
50.‰ªÄ‰πàÊòØÊäïÊú∫ÈááÊ†∑ÊäÄÊúØÔºåËØ∑‰∏æ‰æãËØ¥ÊòéÔºü&lt;/p></description></item><item><title>Tokenization: BPE, Unigram and more</title><link>/posts/llm_tokenization/</link><pubDate>Mon, 22 Apr 2024 15:41:56 +0800</pubDate><guid>/posts/llm_tokenization/</guid><description>&lt;h1 id="there-is-more-than-one-way-to-tokenize-a-sentence" >
&lt;div>
&lt;a href="#there-is-more-than-one-way-to-tokenize-a-sentence">
##
&lt;/a>
There is more than one way to tokenize a sentence
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>word-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>A big vocabulary is needed&lt;/li>
&lt;li>We combine words: what exactly constitutes a word (&amp;ldquo;bachelor of science&amp;rdquo;, or
isolated words)&lt;/li>
&lt;li>Abbreviated words: &amp;ldquo;LOL&amp;rdquo;, &amp;ldquo;IMO&amp;rdquo;, are these collections of words or new words?&lt;/li>
&lt;li>Languages that don&amp;rsquo;t segment by spaces&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>character-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>Lack of meaning: Unlike words, characters don&amp;rsquo;t have any inherent meaning, model
may lose the semantic-specific feature of words.&lt;/li>
&lt;li>Increased input computation&lt;/li>
&lt;li>Limits netword+k choices: It&amp;rsquo;s difficult to use architectures which process input
sequentially since the input sequences will be much longer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Subword-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>We want a tokenization scheme that deals with an infinite potential vocabulary via
a finite list of known words. Make up the word ‚Äúunfortunately‚Äù via ‚Äúun‚Äù + ‚Äúfor‚Äù+
‚Äútun‚Äù + ‚Äúate‚Äù + ‚Äúly‚Äù.&lt;/li>
&lt;li>Subword tokenisation will break the text into chunks based on the word frequency.
In practice what happens is that common words will be tokenized generally as
whole words, e.g. ‚Äúthe‚Äù, ‚Äúat‚Äù, ‚Äúand‚Äù, etc., while rarer words will be broken
into smaller chunks and can be used to create the rest of the words in the
relevant dataset.&lt;/li>
&lt;li>BPE(Byte Pair Encoding): One popular algorithm for subword tokenisation which
follows the above approach is BPE. BPE was originally used to help compress data
by finding common byte pair combinations. It can also be applied to NLP to find
the most efficient way of representing text.
&lt;ul>
&lt;li>What is merging?
The main goal of the BPE subword algorithm is to find a way to represent
your entire text dataset with the least amount of tokens. Similar to a
compression algorithm, you want to find the best way to represent your image,
text or whatever you are encoding, which uses the least amount of data, or
in our case tokens. In the BPE algorithm merging is the way we try and
‚Äúcompress‚Äù the text into subword units.&lt;/li>
&lt;li>There are a few steps to these merging actions:
&lt;ol>
&lt;li>Get the word &lt;strong>count&lt;/strong> frequency&lt;/li>
&lt;li>Get the &lt;strong>initial token count&lt;/strong> and frequency (i.e., how many times each
character occurs)&lt;/li>
&lt;li>Merge the &lt;strong>most common byte pairing&lt;/strong>&lt;/li>
&lt;li>Add this to the list of tokens and &lt;strong>recalculate the frequency count&lt;/strong>
for each token (this will change with each merging step)&lt;/li>
&lt;li>&lt;strong>Rinse and repeat&lt;/strong> until get reached pre-defined token limits (vocab
size) or a set of number of iterations&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Greedy algorithm: BPE ensures that the most common words will be represented in
the new vocabulary as a single token, while less common words will be broken
down into two or more subword tokens. To achieve this, BPE will go through every
potential option at each step and pick the tokens to merge based on the highest
frequency.One downside of BPE‚Äôs greedy approach is it can result in a potentially
ambiguous final token vocabulary.
For instance GPT has a vocabulary size of 40,478 since they have 478 base
characters and chose to stop training after 40,000 merges.&lt;/li>
&lt;li>BBPE(byte-level PBE): A base vocabulary that includes all possible base characters
can be quite large if e.g. all unicode characters are considered as base
characters. To have a better base vocabulary, GPT-2 uses bytes as the base
vocabulary, which is a clever trick to force the base vocabulary to be of size
256 while ensuring that every base character is included in the vocabulary. With
some additional rules to deal with punctuation, the GPT2‚Äôs tokenizer can
tokenize every text without the need for the &lt;unk> symbol. GPT-2 has a
vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a
special end-of-text token and the symbols learned with 50,000 merges.
&lt;a href="https://huggingface.co/docs/transformers/en/tokenizer_summary#byte-pair-encoding-bpe">from hf doc&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="probabilistic-subword-tokenization" >
&lt;div>
&lt;a href="#probabilistic-subword-tokenization">
##
&lt;/a>
Probabilistic Subword Tokenization
&lt;/div>
&lt;/h1>
&lt;p>Using the frequency of subword patterns for tokenization can result in ambiguous final
encodings. The problem is that we have no way to predict which particular token is more
likely to be the best one when encoding any new input text.
Luckily, needing to predict the most likely sequence of text is not a unique problem to
tokenization. We can leverage this knowledge to build a better tokenizer.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Unigram Subword Tokenization&lt;/p>
&lt;ul>
&lt;li>The goal for a subword model, however, is different from a LM that is trying to
predict a full sentence. We only want something that generates unambiguous
tokenization.&lt;/li>
&lt;li>The unigram approach differs from BPE in that it attempts to choose the most
likely option rather than the best option at each iteration. To generate a
unigram subword token set you need to first define the desired final size of
your token set and also a starting seed subword token set.&lt;/li>
&lt;li>You can choose the seed subword token set in a similar way to BPE and choose
the most frequently occurring substrings. Once you have this in place then
you need to:
&lt;ol>
&lt;li>Work out the probability for each subword token&lt;/li>
&lt;li>Work out a loss value which would result if each subwork token were to be
dropped. The loss is worked out via Expectation Maximization algorithm.&lt;/li>
&lt;li>Drop the tokens which have the largest loss value (e.g., the bottom 10%
or 20% of subword tokens based on their loss calculations).&lt;/li>
&lt;li>Repeat these steps until reach the desired final vocabulary size or there
is no change in token numbers after successive iterations.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>WordPiece (greedy approach tokenzier, BERT partner)
Think of WordPiece as an intermediary between the BPE approach and the unigram approach.&lt;/p>
&lt;ul>
&lt;li>BPE, if you remember, takes two tokens, looks at the frequency of each pair and then
merges the pairs that have the highest combined frequency count. It only considers
the most frequent pair combinations at each step, nothing else.&lt;/li>
&lt;li>An alternate approach is to check the potential impact of merging that particular
pair. You can do this using the probabilistic LM approach. At each iterative step,
choose the character pair which will result in the largest increase in likelihood
once merged. This is the difference between the probability of the new meged pair
occurring minus the probability of both individual tokens occurring individually.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The main difference is that WordPiece is a greedy approach. It still tries to build a
tokenizer from the bottom up, picking the best pair at each iteration to merge.
WordPiece uses the likelihood rather than count frequency but otherwise it is a similar
approach. Unigram in contrast is a fully probabilistic approach which uses probability
to both choose the pairs to merge and whether to merge them or not. It also removes
tokens based on the fact that they add the least to the overall likelihood of the
unigram model.&lt;/p>
&lt;h1 id="briefly-summarize" >
&lt;div>
&lt;a href="#briefly-summarize">
##
&lt;/a>
briefly summarize:
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>BPE: Just uses the frequency of occurrences to identify the best match at every
iteration until it reaches the predefined vocabulary size.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>WordPiece: Similar to BPE and uses frequency occurrences to identify potential
merges but makes the final decision based on the likelihood of the merged token&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Unigram: A fully probabilistic model which does not use frequency
occurrences. Instead, it trains a LM using a probabilistic model, removing
the token which improves the overall likelihood the least and then starting
over until it reaches the final token limit.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="sentencepiece" >
&lt;div>
&lt;a href="#sentencepiece">
##
&lt;/a>
SentencePiece
&lt;/div>
&lt;/h1>
&lt;p>SentencePiece basically tries to bring all the subword tokenization tools and techniques
under one banner. It‚Äôs kind of like the Swiss Army knife for subword tokenization. To be
a Swiss Army-like tool something has to be capable of solving multiple problems. So what
problems is SentencePiece addressing:&lt;/p>
&lt;ol>
&lt;li>All other models assume input is already tokenized: BPE and Unigram are great model
but they share one big disadvantage: they both need to have their input already
tokenized. SentencePiece deals with this by simply taking in an input in raw text and
then doing everything needed on that input to perform subword tokenization.&lt;/li>
&lt;li>Language agnostic: Since all other subword algorithms need to have their input
pre-tokenized, it limits their applicability to many languages.&lt;/li>
&lt;li>Decoding is difficult: Another problem which is caused by model like BPE and unigram
requiring already tokenized inputs is that you do not know what encoding rules were
used. For example, how were spaces encoded in the tokens? So you cannot decode the
input and return it to is original format.&lt;/li>
&lt;li>No end to end solution: You cannot just plug in a raw input to BPE (or Unigram) and
get an output.&lt;/li>
&lt;/ol>
&lt;p>Some of the techniques SentencePiece uses to address the above shortcomings:&lt;/p>
&lt;ol>
&lt;li>Encode everything as unicode: SentencePiece first converts all the input into unicode
characters. This makes it a language agnostic tool.&lt;/li>
&lt;li>&amp;ldquo;space&amp;rdquo; encoded as &amp;ldquo;_&amp;quot;(U+2581): To get around the word segmenting issues.&lt;/li>
&lt;li>And it&amp;rsquo;s faster: One of the issues preventing other subword algorithms from being used
to tokenize raw sentences as part of model training was that there lack of speed. If
you processed input in real time and performed your tokenization on the raw input it
would be too slow. SentencePiece addresses this by using a priority queue for the BPE
algorithm to speed it up so that you can use it as part of an end-to-end solution.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;a href="https://www.openteams.com/tokenizers-how-machines-read/">https://www.openteams.com/tokenizers-how-machines-read/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>TODO: Ë°•ÂÖÖ‰∏≠Êñá&lt;/p></description></item><item><title>test rendering of equations of latex in Hugo</title><link>/posts/hugo101/test_math_equation_rendering/</link><pubDate>Fri, 25 Aug 2023 13:49:40 +0800</pubDate><guid>/posts/hugo101/test_math_equation_rendering/</guid><description>&lt;p>ÂºèÂ≠êËÉΩÊ≠£Â∏∏Ê∏≤ÊüìÔºå‰ΩÜÈúÄË¶ÅÂú®‰∏§‰∏™Âú∞ÊñπÁöÑ‰∏ãÊ†áÂ§ÑÊ∑ªÂä†È¢ùÂ§ñËΩ¨‰πâÂ≠óÁ¨¶ &lt;code>\&lt;/code>Ôºå
‰πüÂ∞±ÊòØÔºö&lt;code>\mathbb{E}\_{xxx}&lt;/code>Âíå&lt;code>p\_{model}&lt;/code>ÔºåÂÖ∂‰ªñÂú∞ÊñπÂç¥‰∏çÈúÄË¶Å &lt;code>\hat{p}_{data}&lt;/code>&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÊòØÂõ†‰∏∫ &lt;code>\mathbb{}&lt;/code> ÂØºËá¥ÁöÑÂêóÔºüÁúãËµ∑Êù•‰∏çÊòØÔºö
$$
E_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÊòØÂõ†‰∏∫ &lt;code>\text{}&lt;/code> ÂØºËá¥ÁöÑÂêóÔºüÁúãËµ∑Êù•‰∏çÊòØ (&lt;code>\text{log}&lt;/code> -&amp;gt; &lt;code>\log&lt;/code>)Ôºö
$$
\mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Â¶ÇÊûú &lt;code>\mathbb{}_&lt;/code> Ê∏≤Êüì‰∏çÂá∫Êù•ÔºåÈÇ£ÂÆÉÊ∏≤ÊüìÂ§±Ë¥•ÊòØÊÄé‰πàÊ†∑ÁöÑ (‰∏ãÂºè&lt;code>\mathbb{}_&lt;/code>‰∏≠ÁöÑ&lt;code>_&lt;/code>‰∏çËßÅ‰∫Ü)Ôºö&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>‰∏∫‰ªÄ‰πà &lt;code>p_{model}&lt;/code> ‰πüÊ∏≤Êüì‰∏çÂá∫Êù•Ôºå ËÄåÈùûË¶ÅÊ∑ªÂä†ËΩ¨ÊÑèÂ≠óÁ¨¶ &lt;code>p\_{model}&lt;/code> Âë¢Ôºü&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
E_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÊàñËÆ∏ÊòØÂµåÂ•ó‰∏ãÊ†áËøôÂÑøÂá∫ÁöÑÈóÆÈ¢ò (&lt;code>\hat{p}_{data}&lt;/code> -&amp;gt; &lt;code>\hat{p}\_{data}&lt;/code> )Ôºü:
$$
E_{x \sim \hat{p}_{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Bingo?? No bingo:&lt;/p>
&lt;p>Âè™Âú®ÂµåÂ•ó‰∏ãÊ†áÁöÑÂú∞ÊñπÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶Ôºö&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Âè™Âú®&lt;code>p_{model}&lt;/code>‰∏ãÊ†áÁöÑÂú∞ÊñπÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶Ôºö&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Âè™Âú®&lt;code>\mathbb{}&lt;/code>‰∏ãÊ†áÁöÑÂú∞ÊñπÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶Ôºö&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ÁÆÄËÄåË®Ä‰πãÔºåÊâæ‰∏çÂà∞ÊòéÁ°ÆÁöÑËØ≠Ê≥ïËßÑÂàôÊù•Ëß£ÈáäËøô‰∏™ÈóÆÈ¢ò„ÄÇÂèØËÉΩÊòØhugoÂú®‰ΩøÁî®mathjaxËß£ÊûêÊó∂Âá∫‰∫ÜÈóÆÈ¢òÔºå‰πüÊúâÂèØËÉΩÊòØmathjaxËá™Â∑±ÁöÑÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;p style="text-align:center;color:blue;">
HugoÁöÑmarkdownÊñáÊ°£ÈáåÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÂùóÔºåÂ¶ÇÊûú‰∏çÁ°ÆÂÆöÔºåÂ∞±Âú®‰∏ãÊ†áÈÇ£ÈáåÂâçÁΩÆ‰∏Ä‰∏™`\`„ÄÇ&lt;br>
ÂêåÊ†∑ÁöÑÈóÆÈ¢ò‰πüÂá∫Áé∞Âú®Â§öË°åÂÖ¨ÂºèÁªÑÈáåËøõË°åËΩ¨Ë°åÊó∂Ôºå‰ΩøÁî® `\\\\\`ÔºåËÄå‰∏çÊòØÈÄöÂ∏∏ÁöÑ `\\\`
&lt;/p>
&lt;p>ÊÉπÂá∫Ëøô‰πàËÆ∏Â§ö‰∏çÊòéÊâÄ‰ª•ÁöÑÈóÆÈ¢ò, ÊúÄÂàùÂ∞±ÊòØ‰∏ãÈù¢Ëøô‰∏™ÂÖ¨Âºè (Â∞±ÊòØÊàëÊâÄËÆ§Áü•ÁöÑ„ÄÅÈÄöÂ∏∏Ë°®Ëææ‰∏ãÊ†áÁöÑÊôÆÈÄöÊñπÂºèÔºå
ËÆΩÂà∫ÁöÑÊòØÔºö‰∏çÊääÂÆÉ‰ª¨ÊîæÂú® &lt;code>$$&lt;/code> Êï∞Â≠¶ÂÖ¨ÂºèÂùóÈáåÔºåÊàëËøòÂøÖÈ°ªÊ∑ªÂä†ËΩ¨‰πâÂ≠óÁ¨¶ÔºåÂê¶ÂàôÊ∏≤Êüì‰∏çÂá∫Ëøô‰∏™ÊïàÊûúüòÇ):&lt;/p>
&lt;p>\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}&lt;/p>
&lt;p>Êï∞Â≠¶ÂÖ¨ÂºèÂùóÈáåË°®Ëææ‰∏ãÊ†áÁöÑËØ≠Ê≥ïÔºåÊó†Á´ØÁ´ØË¶ÅÊ∑ªÂä†‰∏Ä‰∏™ÂâçÁΩÆËΩ¨‰πâÂ≠óÁ¨¶ÔºåËøôÂ∞±ÊòØÊàëÊâÄÊ≤°ÊúâÊñôÂà∞ÁöÑ„ÄÇÊÉ≥ÂøÖÂ∫îËØ•ÊòØHugoÊ≤°ÊúâÂÅöÂ•ΩÂÖ≥‰∫éËΩ¨‰πâÂ≠óÁ¨¶ÁöÑ‰∏ä‰∏ãÊñáËß£ÊûêÈóÆÈ¢òÔºå‰∏îÂÖàÂ∞ÜÂ∞±ÁùÄÂêß„ÄÇ&lt;/p></description></item><item><title>Âà©Âô®4: Vim ÁºñËæëÂô® -- Âè™Âõ†‰∏∫Âú®ÈîÆÁõò‰πã‰∏≠ÔºåÊñπÂêëÈîÆÂ§™ÈÅ•Ëøú</title><link>/posts/thecli/thevim/2022-08-13-vim8/</link><pubDate>Sat, 13 Aug 2022 21:37:59 +0800</pubDate><guid>/posts/thecli/thevim/2022-08-13-vim8/</guid><description>&lt;h2 id="vim-Âàù‰ΩìÈ™å" >
&lt;div>
&lt;a href="#vim-%e5%88%9d%e4%bd%93%e9%aa%8c">
#
&lt;/a>
Vim Âàù‰ΩìÈ™å
&lt;/div>
&lt;/h2>
&lt;p>‚ÄúÂ§öÂπ¥‰ª•ÂêéÔºåÈù¢ÂØπÊòæÁ§∫Â±èÔºå&amp;ldquo;ÈÄÉÁ¶ª‰∫ÜÂ∑®Á°¨¬∑‰ªçÊóßÊÑ§ÊÄí¬∑ÊáµÊáÇËèúÈ∏ü&amp;quot;Â∞Ü‰ºöÂõûÊÉ≥Ëµ∑ÂàùÊ¨°Â∞ùËØï Vim ÁºñËæëÂô®
ËÄåË¢´Âõ∞Âú®ÈáåÈù¢Âè´Â§©‰∏çÂ∫îÂè´Âú∞‰∏çÁÅµÁöÑÈÇ£‰∏™ÈÅ•ËøúÁöÑ‰∏ãÂçà‚Äù„ÄÇ&lt;/p>
&lt;p>ÈÇ£Êó∂ÂÄôÁöÑÊàëÔºà2017ÔºâÔºåÊòØ‰∏™‰∫åÂçÅÂ§öÂ≤ÅÁöÑÈùíÂπ¥ÔºåÂàöÂàöËÆ§ËØÜÁªàÁ´ØÂíå Linux Â§ßÊ¶ÇÊòØÊÄé‰πà‰∏ÄÂõû‰∫ãÔºå
ÊÑüËßâÊÅ∞‰ºº‰∏äÂ§©ÁªôÊàëÂÖ≥‰∫ÜÈÇ£Êâá‚ÄúÁ™ó‚ÄùÔºåËá™Â∑±Êé®ÂºÄ‰∫Ü Linux ‰∏ñÁïåÁöÑÂ§ßÈó®„ÄÇ
‰∏ñÁïåÊñ∞Áîü‰ºäÂßãÔºåËÆ∏Â§ö‰∫ãÁâ©Ëøò‰∏çÁü•ÂÖ∂Âêç‰ΩïÂ§ÑÔºåÂØπËÆ∏Â§ö‚ÄúÊãøÊù•‰∏ª‰πâ‚ÄùÁöÑÂëΩ‰ª§‰ªçÊóßÂçä‰ø°ÂçäÁñëÔºå
ÁÆÄÁõ¥Â∞±ÊòØ‰∏ÄÂú∫Â§ßÂÜíÈô© (Ê≤°ÊúâÊÄßÂëΩ‰πãÊî∏ÁöÑÈÇ£Áßç)„ÄÇ&lt;/p>
&lt;p>ÂΩìÊàëÂèëÁé∞Âú®ÊñáÊú¨ÁºñËæëÂô®ÈáåÁöÑÂÖâÊ†áÂèØ‰ª•Áî® h,j,k,l Êù•ËøõË°åÂ∑¶‰∏ã‰∏äÂè≥ÁßªÂä®Êó∂Ôºå‚Äú‰∏çÊáÇÔºå‰ΩÜÊàëÂ§ßÂèóÈúáÊíº‚Äù„ÄÇ&lt;/p>
&lt;p>ÔºàÂÖ∂ÂÆûÂ∞±ÊòØËßÅËØÜÂ∞ëÔºå‰∏çÊòØÊâÄÊúâÈîÆÁõòÈÉΩÊúâÁã¨Á´ãÁöÑÊñπÂêëÈîÆÔºåÊõ¥‰∏çÊòØÊìç‰ΩúÁ≥ªÁªüÂ∞±Âè™‰∏Ä‰∏™ windows
Ôºå‰∏çËøáÊàëÊòØ‰∫åÂçÅÂ•ΩÂá†ÁöÑ‰∫∫‰∫ÜÊâçÁü•ÈÅìËøô‰∏™‰∫ãÊÉÖÔºåÁî®‰∫éËøûÁöÑËØùËÆ≤Â∞±ÊòØÔºö‚ÄúÂïäÔºåËøôÊòØ‰∏ÄÂú∫‰æÆËæ±‚Äù„ÄÇÔºâ&lt;/p>
&lt;p>ËôΩÁÑ∂Á¨¨‰∏ÄÊ¨°Â∞ùËØï Vim Â∞±Ë¢´ÊäõÂºÉÂà∞‰∫ÜËçíÂ≤õÔºåÂ•ΩÂú®ÊàëÂØπËøôÊ†∑‰∏Ä‰∏™Êñ∞Â•áÁöÑ‰∏ñÁïåÔºåÊó©ÊúâÂøÉÁêÜÂáÜÂ§á„ÄÇ
‰∫ãÂÆûËØÅÊòéÔºåË¶ÅÈÄÇÂ∫îÊñ∞ÁéØÂ¢ÉÔºåÊúÄÂ•ΩÁöÑÊ≠¶Âô®‰∏çËøáÊòØ‚Äú‰∏ÄÁÇπÂ•ΩÂ•áÂøÉÂíåÂ§öÁÇπËÄêÂøÉ‚Äù„ÄÇ‰ªéÈÇ£‰ª•ÂêéÔºå
ÊÑüËßâÂ∞±Â•ΩÂÉè‚ÄúÂ±Ö‰ΩèÂú®ÂÖîÂ≠êÁöÆÊØõÊ∑±Â§ÑÁöÑÂæÆÁîüÁâ©‚ÄùÁ´≠ÂäõÂêëÊØõÂ∞ñÁà¨ÂéªÔºå‚Äú‰ª•‰æøÂ∞ÜÈ≠îÊúØÂ∏àÁúã‰∏™Ê∏ÖÊ•ö‚Äù„ÄÇ&lt;/p>
&lt;p>ÁªàÁ´ØÈáåËÉΩÁî®ÁöÑ‰ºòÁßÄÂ∑•ÂÖ∑Â§öÂæóËÆ©ÊàëËßâÂæóÁÆÄÁõ¥‚ÄúÂ∞±ÂÉèÊòØÂú®Êµ∑ËæπÊã£Ë¥ùÂ£≥ÁöÑÂ≠©Â≠ê&amp;rdquo;ÔºåËÄå Vim ÁºñËæëÂô®Êó†ÁñëÊòØÊàëÊúÄÂñúÁà±ÁöÑÈÇ£‰∏ÄÊûö„ÄÇ
ËÄåÂú®Êµ∑ËæπÊîæË¥ùÂ£≥ÁöÑÊó†ÁñëÊòØÈÇ£‰∫õ‰πê‰∫éÂàÜ‰∫´ÁöÑÂêåÈÅì‰∏≠‰∫∫ÔºåÊàëÁõ∏‰ø°ÁúüÊ≠£ÁöÑËøõÊ≠•Êù•Ëá™‰∫éÂàÜ‰∫´„ÄÇ&lt;/p>
&lt;p>ÂêéÊù•ÂΩìÊàëÂ§ßÊ¶ÇÊòéÁôΩ vimrc Êñá‰ª∂ÊòØÊÄé‰πà‰∏ÄÂõû‰∫ã‰πãÂêéÔºåÊàëÁ¨¨‰∏Ä‰ª∂‰∫ãÂ∞±ÊòØÁªôÂÆÉÂä†‰∏ä‰∫ÜËøô‰∏ÄÂè•:&lt;/p>
&lt;p>&lt;code>set mouse=&lt;/code>&lt;/p>
&lt;p>ÂÖ∂ÂÆûÂΩìÊó∂ÊòØÊÄÄÁùÄÂØπÊóßÊó•ÈÇ£ÁßçÂá°‰∫ãÂøÖÂæó‚ÄúÈº†Ê†áÁÇπÂáªÊãñÊãΩ‚ÄùÂÆ£ÊàòÁöÑÂøêÂøëÂøÉÊÄÅ„ÄÇ&lt;/p>
&lt;p>ÂêéÊù•ÊÄé‰πàÊ†∑‰∫ÜÔºü&lt;/p>
&lt;p>ÂêéÊù•ÊàëÂèëÁé∞ÔºåÂÖ∂ÂÆû‰∏çÊÄé‰πàÈúÄË¶Å‚Äú‰∏ìÈó®ËÖæÂá∫‰∏ÄÂè™ÊâãÁî®Èº†Ê†áÊìçÊéß‚Äù„ÄÇ
Â¶ÇÊûúÊòØÂú®ÁªàÁ´ØÂπ≤Ê¥ªÔºå‰Ω†‰πêÊÑèÁöÑËØùÔºåÈÇ£Â∞±ÊòØÔºöÊ†πÊú¨‰∏çÈúÄË¶Å„ÄÇ
Â¶ÇÊûú‰Ω†ÊûÅÁ´ØÁÇπÔºåÂú®Âì™ÈáåÈÉΩ‰∏çÈúÄË¶ÅÈº†Ê†á„ÄÇ
Èô§ÈùûÊòØ‰Ω†Âè¶‰∏ÄÂè™ÊâãÂú®ÂøôÁÇπÂà´ÁöÑ‰ªÄ‰πàÔºåÊàñËÄÖÊòØËÄÅÊùøË¶Å‚ÄúÈ∫ªÁÉ¶‚Äù‰Ω†Áªô‚ÄúËµ∂Á¥ß‚Äù‰øÆÊîπ‰∏ÄÁØáworkÊñáÊ°£ÔºåÂÆ¢Êà∑Âú®Á∫øÁ≠âÈÇ£Áßç&amp;hellip;&lt;/p>
&lt;h2 id="vim-ÁºñËæëÂô®" >
&lt;div>
&lt;a href="#vim-%e7%bc%96%e8%be%91%e5%99%a8">
#
&lt;/a>
Vim ÁºñËæëÂô®
&lt;/div>
&lt;/h2>
&lt;h3 id="vim-ÊòØÂπ≤Âï•ÁöÑ" >
&lt;div>
&lt;a href="#vim-%e6%98%af%e5%b9%b2%e5%95%a5%e7%9a%84">
##
&lt;/a>
Vim ÊòØÂπ≤Âï•ÁöÑ
&lt;/div>
&lt;/h3>
&lt;p>È¶ñÂÖàÂ∫îËØ•‰∫ÜËß£Ôºö Vim ÈÄÇÁî®‰∫é‰ªÄ‰πàÂú∫ÊôØÔºü&lt;/p>
&lt;blockquote>
&lt;p>‚ÄúVimÊòØ‰ªé vi ÂèëÂ±ïÂá∫Êù•ÁöÑ‰∏Ä‰∏™ÊñáÊú¨ÁºñËæëÂô®„ÄÇ
ÂÖ∂‰ª£Á†ÅË°•ÂÆå„ÄÅÁºñËØëÂèäÈîôËØØË∑≥ËΩ¨Á≠âÊñπ‰æøÁºñÁ®ãÁöÑÂäüËÉΩÁâπÂà´‰∏∞ÂØåÔºåÂú®Á®ãÂ∫èÂëò‰∏≠Ë¢´ÂπøÊ≥õ‰ΩøÁî®„ÄÇ
ÂíåEmacsÂπ∂ÂàóÊàê‰∏∫Á±ªUnixÁ≥ªÁªüÁî®Êà∑ÊúÄÂñúÊ¨¢ÁöÑÁºñËæëÂô®„ÄÇ‚Äù&lt;/p>
&lt;/blockquote>
&lt;p>Âπ≥Êó∂ÊàëÂú®ÊµèËßà‰∏≠ÔºàÂ∞§ÂÖ∂ÊòØÂØªÊâæ‰∏Ä‰∫õÂäüËÉΩÊèí‰ª∂ËøáÁ®ãÔºâ‰πüÁúãÂà∞Êúâ‰∫∫ÂèëÂçöÂÆ¢ËØ¥‰ªñ/Â•πÊòØÁî® Vim Ëøõ
Ë°å‚ÄúÂÜô‰Ωú‚ÄùÁöÑÔºà‰∏ªË¶ÅÊòØÂÖ≥‰∫é $\LaTeX$, $\TeX$, markdown ÁöÑÊèí‰ª∂ÔºâÔºå‰ΩÜÊàëÊÉ≥Â¶ÇÊûúÊòØÁî®Êù•Ëøõ
Ë°åÊñáÂ≠¶Âàõ‰ΩúËøô‰∏ÄÁ±ªÁöÑÂ∑•‰ΩúÁöÑËØùÔºåÈáçÁÇπÂèØËÉΩ‰∏çÂú®‰∫éÁºñËæëÂô®ÔºåÂæàÁÆÄÂçïÔºåËøòÊ≤°Êúâ‚ÄúÁîµËÑë‚ÄùÁöÑÊó∂‰ª£Â∞±
Â∑≤ÁªèËØûÁîü‰∫ÜÂ§™Â§ö‰ºòÁßÄ‰ΩúÂìÅ„ÄÇ
ÔºàÊÉ≥Ë±°Â§ß‰Ω¨ÊõπÈõ™ËäπÈù¢ÂØπÁîµËÑëÂ±èÂπï‚ÄúÂçÅÂπ¥Âà†Êîπ‚Äù„ÄäÁ∫¢Ê•ºÊ¢¶„ÄãÊÄªËßâÂæóÊúâÁÇπËçíËØûÔºå
‰ΩÜÊòØÂ∞±ÁÆóÊòØÊúâÁîµËÑëÔºåËøôÈöæÁÇπËÇØÂÆö‰∏çÂú®‰∫éÂà†ÊîπÔºàÁºñËæëÂô®Êó†ÂÖ≥Á¥ßË¶ÅÔºâÔºåËÄåÂú®‰∫éÊñáÂ≠¶ÂàõÈÄ†„ÄÇ
ÊûÅÊúâÂèØËÉΩ‰ªñÊ†πÊú¨‰π∞‰∏çËµ∑ÁîµËÑë„ÄÇÔºâ&lt;/p>
&lt;p>ÂèçËøáÊù•Âç¥‰∏ç‰∏ÄÊ†∑ÔºåËá≥Â∞ëÊàë‰ª¨ËøôÂπ¥‰ª£ÔºåÊ≤°ÊúâÂê¨ËØ¥Âì™‰∏™ÊàêÂäüÁöÑËΩØ‰ª∂‰∫ßÂìÅÔºåÂÆÉÁöÑ‰ª£Á†ÅÊòØÁî®ÊâãÂÜôÂú®ËçâÁ®øÁ∫∏‰∏ä &amp;hellip;
ÊÄ™ËØûÁöÑÊòØÔºåÈù¢ËØïÁöÑÊó∂ÂÄôÂÄíÊòØÊúâ‰∏çÂ∞ëËøôÁßçË¶ÅÊâãÂÜô‰ª£Á†ÅÂïä‰º™‰ª£Á†ÅËøôÁßç‰∫ãÊÉÖÔºåÊàëËßâÂæóËøôÂ§öÂ§öÂ∞ëÂ∞ëÊúâÁÇπÈªëËâ≤ÂπΩÈªò„ÄÇ&lt;/p>
&lt;p>ÁÆÄËÄåË®Ä‰πãÔºåvimÊòØ‰∏™ÁºñËæëÂô®ÔºåÂæàÂ§ö‰∫∫Áî®ÂÆÉÊù•ÂÜô‰ª£Á†ÅÔºå‰πüÊúâ‰∫∫Áî®ÂÆÉÊù•Âπ≤ÂÖ∂‰ªñ„ÄÇ&lt;/p>
&lt;p>ÈöèÁùÄ‰∫ÜËß£Ê∑±ÂÖ•ÔºåÊÖ¢ÊÖ¢Â∞±‰ºöÊé•Ëß¶Âà∞ËØ∏Â¶Ç‚ÄúÁºñËæëÂô®‰πãÊàò‚Äù„ÄÅ‚ÄúÁºñÁ®ãËØ≠Ë®Ä‰πã‰∫â‚ÄùËøô‰∏ÄÁ±ªËØùÈ¢ò„ÄÇ&lt;/p>
&lt;h3 id="ÁºñËæëÂô®‰πãÊàò" >
&lt;div>
&lt;a href="#%e7%bc%96%e8%be%91%e5%99%a8%e4%b9%8b%e6%88%98">
##
&lt;/a>
ÁºñËæëÂô®‰πãÊàò
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>‚ÄúÁºñËæëÂô®‰πãÊàòÊòØÊåáËÆ°ÁÆóÊú∫ÁºñÁ®ãÈ¢ÜÂüüÂÜÖÔºå
‰∏§Á±ªÊñáÊú¨ÁºñËæëÂô® viÔºà‰ª•ÂèäË°çÁîüÁâàÊú¨ÔºâÂíå Emacs Áî®Êà∑‰πãÈó¥ÁöÑÂØπÊäóÂÜ≤Á™Å„ÄÇ
ËøôÂ∑≤ÁªèÊàê‰∏∫ÈªëÂÆ¢ÊñáÂåñÂíåËá™Áî±ËΩØ‰ª∂Á§æÂå∫ÊñáÂåñÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;p>‰∏Ä‰∫õ Emacs Áî®Êà∑ËØ¥Ôºö&lt;/p>
&lt;blockquote>
&lt;p>Vi Êúâ‰∏§‰∏™Ê®°ÂºèÔºå‚Äú‰∏çÂÅúÂú∞ÂìîÂìîÂè´‚ÄùÂíå‚ÄúÊêûÁ†∏‰∏ÄÂàá‚Äù„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;p>ËÄåÊçÆ Vi Áî®Êà∑Áß∞Ôºö&lt;/p>
&lt;blockquote>
&lt;p>‰∏Ä‰∫õ Emacs Áî®Êà∑Áî±‰∫é‰øÆÊîπÈîÆËøáÂ§öËÄå‰ΩøÁî®ËÑöË∏èÊùø„ÄÇ&lt;/p>
&lt;/blockquote>
&lt;p>‰∏Ä‰∫õÂçöÂÆ¢ÂçöÊñá„ÄÅÂΩ±ËßÜ‰ΩúÂìÅ‰πüÈÉΩ‰ºöÂá∫Áé∞Ëøô‰∫õÂÜÖÂÆπÂú∫ÊôØÔºåËÄå‰∏îÂ§ßÈÉΩÊòØËΩªÊùæÊ¨¢‰πêÁöÑÂú∫ÊôØ„ÄÇÊØîÂ¶ÇÔºå
„ÄäÁ°ÖË∞∑„ÄãÈáåÁêÜÊü•Âæ∑Âíå‰ªñ‚ÄúÁÆÄÁõ¥Â∞±ÊòØË∏©‰∫ÜÁãóÂ±éËøê‚ÄùÊâçÂãæÊê≠‰∏äÁöÑÊºÇ‰∫ÆÂ•≥ÂèãÂ∞±‰∏∫‰∫Ü Vim Âíå Emacs
„ÄÅ‰ΩøÁî® Tab ËøòÊòØ Space ËøõË°å‰ª£Á†ÅÁº©Ëøõ‰ª•ÂèäÁº©ËøõÈ£éÊ†ºËÄåÂêµ‰∫ÜËµ∑Êù•ÔºåÁªßËÄåÁõ¥Êé•ÈóπÊé∞Ôºå
ÊúÄÂêé‰ª•ÁêÜÊü•Âæ∑ÁßÄÈÄó‰∫ÜÁöÑÂú®Áî®Ë∫´‰ΩìÂäõË°åËøõË°å‚ÄúÁº©ËøõÊºîÁ§∫‚Äù‰∏≠Êªö‰∏ãÊ•ºÊ¢ØËÄåÂëäÁªà„ÄÇ
&amp;hellip;&lt;/p>
&lt;blockquote>
&lt;p>ÊàòÁÅ´Âú®‰∏§Áæ§Áî®Êà∑‰πãÈó¥Â§öÊ¨°ÁÇπÁáÉÔºå‰ªñ‰ª¨‰∫íÁõ∏ËîëËßÜÔºåÂùö‰ø°Ëá™Â∑±ÁöÑÈÄâÊã©ÊâçÊòØÊúÄÂÆåÁæéÁöÑ„ÄÇ
Áõ∏ÊØîÂÖ∂‰ªñÁöÑITÈ¢ÜÂüüÊàò‰∫âÔºàÊµèËßàÂô®Â§ßÊàò„ÄÅÊìç‰ΩúÁ≥ªÁªü‰πã‰∫â„ÄÅÁºñÁ®ãËØ≠Ë®Ä‰πã‰∫â„ÄÅ‰ª£Á†ÅÁº©ËøõÈ£éÊ†º‰πãÊàò
Á≠âÁ≠â)ÔºåÁºñËæëÂô®ÁöÑÈÄâÊã©ÂÖ∂ÂÆûÈÄöÂ∏∏Âè™ÊòØ‰∏™‰∫∫ÈóÆÈ¢ò„ÄÇ ‚Äù&lt;/p>
&lt;/blockquote>
&lt;p>Â¶ÇÊûú‰∏çÂ∞èÂøÉ‰πüÂç∑ÂÖ•‰∫ÜËøô‰∫õÊàò‰∫âÔºåËØ∑ËÆ∞‰ΩèÔºöÁºñËæëÂô®ÁöÑÈÄâÊã©ÂÖ∂ÂÆûÈÄöÂ∏∏Âè™ÊòØ‰∏™‰∫∫ÈóÆÈ¢ò„ÄÇ&lt;/p>
&lt;p>ÂΩìÁÑ∂ÔºåÂÆûÂú®ÂêµÂæó‰∏çÂèØÂºÄ‰∫§ÁöÑÊó∂ÂÄôÔºåÂà´Âøò‰∫ÜÊê¨Âá∫ÁºñÁ®ãËØ≠Ë®Ä‰πã‰∫âÈáåÈÇ£Âè•‰∏ÄÈî§ÂÆöÈü≥ÁöÑËØùÈ¢òÁªàÁªìËØ≠Ôºö
‚ÄúPHP ÊâçÊòØÁúüÂ•ΩÁöÑÁºñÁ®ãËØ≠Ë®ÄÔºå‰∏çÊé•Âèó‰ªª‰ΩïÂèçÈ©≥„ÄÇ‚Äù&lt;/p>
&lt;p>Â¶ÇÊûúÊÉ≥‰∫ÜËß£Êõ¥Â§öÔºåÂèØ‰ª•Âéª &lt;a href="https://zh.m.wikipedia.org/zh-hans/Vim">Vim Áª¥Âü∫ËØçÊù°&lt;/a>
‰∫ÜËß£Êõ¥Â§öÁ≤æÂΩ©ÂÜÖÂÆπ„ÄÇÂ¶ÇÊûú‰Ω†ÁîöËá≥Êó†Ê≥ïËÆøÈóÆÁª¥Âü∫ÁôæÁßëÔºåÈÇ£‰∏çÊòØ‰Ω†ÁöÑÈîôÔºå‰∏çËøá‰Ω†ÂæóÊÉ≥ÂäûÊ≥ï„ÄÇ&lt;/p>
&lt;h2 id="vim-ÂèäÂÖ∂ÈÖçÁΩÆ" >
&lt;div>
&lt;a href="#vim-%e5%8f%8a%e5%85%b6%e9%85%8d%e7%bd%ae">
#
&lt;/a>
vim ÂèäÂÖ∂ÈÖçÁΩÆ
&lt;/div>
&lt;/h2>
&lt;p>Â¶ÇÊûú‰Ω†ÊòØÊÉ≥ÂÖ•Èó®ËÄåËøòÂú®Èó®Âè£ÂæòÂæäÁöÑÔºåÂèØ‰ª•ÂÖàÂú®ÁªàÁ´Ø (Terminal) ÈáåËæìÂÖ• &lt;code>vimtutor&lt;/code>Ôºå
ÁÑ∂ÂêéÁúãÁúã‰Ω†ÊÑüËßâÂ¶Ç‰Ωï„ÄÇÂ¶ÇÊûú‰Ω†ËÆ§ÁúüËøΩÈöèÔºå‰Ω†‰ºöÂèëÁé∞‰∏ÄÂàáËá™ÊúâÈÅìÁêÜ„ÄÇË≤å‰ººËøòÊå∫Â•ΩÁé©„ÄÇ
ÂóØÔºåÂí¶ÔºåÊÄé‰πàÈÄÄÂá∫ËøôÁºñËæëÂô®Âë¢Ôºü&lt;/p>
&lt;h3 id="Áé∞‰ª£Á†Å‰ªîÂ¶Ç‰ΩïÂú®ÁªàÁ´ØÁïåÈù¢ÈÄÄÂá∫vimÁºñËæëÂô®" >
&lt;div>
&lt;a href="#%e7%8e%b0%e4%bb%a3%e7%a0%81%e4%bb%94%e5%a6%82%e4%bd%95%e5%9c%a8%e7%bb%88%e7%ab%af%e7%95%8c%e9%9d%a2%e9%80%80%e5%87%bavim%e7%bc%96%e8%be%91%e5%99%a8">
##
&lt;/a>
Áé∞‰ª£Á†Å‰ªîÂ¶Ç‰ΩïÂú®ÁªàÁ´ØÁïåÈù¢ÈÄÄÂá∫vimÁºñËæëÂô®
&lt;/div>
&lt;/h3>
&lt;p>Â¶ÇÊûú‰Ω†ÂíåÊàëÂΩìÂàù‰∏ÄÊ†∑‰∏ÄÂ§¥ÈõæÊ∞¥&amp;hellip; Ê∑°ÂÆöÊ∑°ÂÆöÔºåËæìÂÖ• &lt;code>:q&lt;/code> ÔºåÊåâ‰∏ã &lt;code>Enter&lt;/code> Âç≥ÂèØÈÄÄÂá∫„ÄÇ&lt;/p>
&lt;p>ÂóØÔºåÂ¶ÇÊûú‰Ω†Ê≠£Âú®ËæìÂÖ•ÔºåÈúÄË¶ÅÂÖàÊåâ‰∏ã &lt;code>Esc&lt;/code>ÔºåÁÑ∂ÂêéËæìÂÖ• &lt;code>:q&lt;/code> Êù•ÈÄÄÂá∫„ÄÇ&lt;/p>
&lt;p>Ëøô‰∏™Êìç‰Ωú‰πü‰∏çÂ•Ω‰ΩøÔºüÂÖ´Êàê‰Ω†ÊòØÂú®Âà´ÁöÑÊ®°ÂºèÈáåÔºåÊØîÂ¶Ç &lt;code>Ex mode&lt;/code>Ôºå
ÂÖ´ÊàêÂÆÉ‰πü‰ºöÊèêÁ§∫‰Ω†ËæìÂÖ• &lt;code>visual&lt;/code> Êù•ËøîÂõûÂà∞ &lt;code>normal mode&lt;/code>„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰∏çÊòØËøôÁßçÊÉÖÂÜµÔºåÊûÅÊúâÂèØËÉΩÊòØ‰Ω†‰∏çÂ∞èÂøÉÊåâ‰∫Ü &lt;code>q&lt;/code> ËøõÂÖ•ÂëΩ‰ª§ÂΩïÂà∂Ôºå
ÈÄöÂ∏∏Ë¶ÅÊòØ‰Ω†Â§öÊåâÂá†Ê¨° &lt;code>q&lt;/code> Êù•ËßÇÂØüÔºå‰ºöÂèëÁé∞Êúâ &lt;code>recording @&lt;/code> ÁöÑÊèêÁ§∫Ôºå
ËøôÂ∞±ÊòØÂ§öÂëΩ‰ª§ÂΩïÂà∂Êó†ÁñëÔºåÈáçÂ§çÊìç‰ΩúÔºöËæìÂÖ• &lt;code>:q&lt;/code> ÔºåÊåâ‰∏ã &lt;code>Enter&lt;/code> Âç≥ÂèØÈÄÄÂá∫„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúÈÉΩ‰∏çÊòØÔºåÂπ∂‰∏î‚Äú‰ªñXÁöÑÊåâ‰ªÄ‰πà‰πüÊ≤°Áî®‚ÄùÔºåÂÖàÊ£ÄÊü•‰∏Ä‰∏ã‰Ω†ÁöÑÊú∫Âô®ÊòØÂê¶ËøòÂú®Ê≠£Â∏∏Â∑•‰ΩúÔºå
ÊòØÔºåÂàôÈáçÂ§ç‰ª•‰∏ãÊìç‰ΩúÔºöËæìÂÖ• &lt;code>:q&lt;/code>ÔºåÊåâ‰∏ã &lt;code>Enter&lt;/code>„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÊâÄÊúâÂ•ΩÂ•áÂøÉËøòÊ≤°ÊúâË¢´Ê∂àËÄóÊÆÜÂ∞ΩÔºåÊàëÊù•ÂëäËØâ‰Ω†‰∏Ä‰∏™Â•ΩÊ∂àÊÅØÔºö
vim ÂÖ±Êúâ &lt;a href="https://en.wikipedia.org/wiki/Vim_(text_editor)">12ÁßçÊ®°Âºè&lt;/a>„ÄÇ&lt;/p>
&lt;p>ËøôÁ°ÆÂÆûÊòØ‰∏™Êñ∞‰∏ñÁïå„ÄÇ&lt;/p>
&lt;h2 id="vimrc" >
&lt;div>
&lt;a href="#vimrc">
#
&lt;/a>
vimrc
&lt;/div>
&lt;/h2>
&lt;p>Â∫îÂΩìÊ≥®ÊÑèÔºö‰ªéÁ¨¨‰∏ÄÊ¨°Âú®ÁªàÁ´ØËæìÂÖ• &lt;code>vim&lt;/code> Âà∞ÂºÄÂßã‰∫ÜËß£Âíå‰ΩøÁî®Ëá™Â∑±ÈÖçÁΩÆÁöÑ &lt;code>.vimrc&lt;/code>Ôºå
‰ªéÂâçËÄÖÂà∞ÂêéËÄÖÔºåÊØè‰∏™‰∫∫ÊâÄÁªèÂéÜÁöÑÊó∂Èó¥‰∏çÂ§™‰∏ÄÊ†∑ÔºåÊØïÁ´üÊØè‰∏™‰∫∫ÁöÑÁîüÊ¥ªÂíåÁõÆÁöÑÂçÉÂ∑Æ‰∏áÂà´„ÄÇ
ÔºàÂÄí‰∏çÊòØËØ¥ÔºåÂàöÂºÄÂßã‰ΩøÁî® vim Â∞±‰∏çËÉΩÁõ¥Êé•‰∏äÊâã‰∏™‰∫∫ÈÖçÁΩÆÔºåËÄåÊòØÊÉ≥ËØ¥ÊòéÔºå
‚ÄúÊ≠•Â≠êËøàÂ§ß‰∫ÜÔºåÂÆπÊòìÊâØÁùÄËõã‚ÄùÔºõ‰∏ÄÊó¶ÊîæÂºÉ‰∫ÜÊé¢Á¥¢ÂíåÊÄùËÄÉÔºåÂÆπÊòì‰∏ÄÁ¢∞Êå´ÊäòÂ∞±ÂçÅÂàÜÊ≤Æ‰∏ß„ÄÇÔºâ
‰ΩÜÂú®ÂæàÂ§öÊñáÁ´†ÈáåÔºåËøô‰∏§ËÄÖ‰πãÈó¥Â§öÂçäÂ∞±ÊòØ‰∏Ä‰∏™Á©∫Ë°åÁöÑË∑ùÁ¶ª„ÄÇ
ÔºàÂõ†Ê≠§ÊàëÂÜ≥ÂÆöÂú®Ëøô‰∏™Á©∫Ë°å‰πãÂâçÊ∑ªÂä†‰∏ÄÁÇπËØ¥ÊòéÔºå‰ª•‰∏ãÁöÑÂÜÖÂÆπÊñ∞ÊâãÂàùÁúãÊó∂‰∏çÂ§™ÊòéÁôΩÂÆûÂ±ûÊ≠£Â∏∏Ôºâ&lt;/p>
&lt;p>Vim Â¶ÇÊ≠§ÂèóÊ¨¢ËøéÔºåÂÆÉÁöÑ‰∏∞ÂØåÁöÑÊèí‰ª∂ÁîüÊÄÅÂ∫îËØ•ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂéüÂõ†„ÄÇ‰∏∫‰ªÄ‰πàÊúâËøô‰πàÂ§öÊèí‰ª∂Âë¢Ôºü
Âõ†‰∏∫ÊúâËøô‰πàÂ§öÁöÑÈúÄÊ±Ç„ÄÇÂõ†‰∏∫ÊúâÈÇ£‰πàÂ§öÁöÑÊèí‰ª∂ÔºåÊâÄ‰ª•Êúâ‰∫ÜË¶ÅÁÆ°ÁêÜÊèí‰ª∂ÔºàÊõ¥Êñ∞„ÄÅÂà†Èô§Á≠âÔºâÁöÑ
Êèí‰ª∂„ÄÇ‰ΩøÁî®ÊØîËæÉÂπøÊ≥õÁöÑÊúâÔºö&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/junegunn/vim-plug">vim-plug&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/VundleVim/Vundle.vim">Vundle&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/tpope/vim-pathogen">Pathogen&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/k-takata/minpac">minpac&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ÂÆÉ‰ª¨ÁöÑÂÆâË£ÖÂíå‰ΩøÁî®Êù°‰ª∂ÈÉΩÂú®ÂêÑËá™ÁöÑ‰∏ªÈ°µÊúâËØ¶ÁªÜËØ¥ÊòéÔºåËøôÈáå‰∏çÂÜçÊê¨Ëøê„ÄÇÈÇ£Âà∞Â∫ïÁî®Âì™‰∏™Âë¢Ôºü
‰Ω†Âú®ÈòÖËØª‰ªñ‰ª¨ÁöÑËØ¥ÊòéÁöÑÊó∂ÂÄôËá™Â∑±Â∞±‰ºöÊúâÁ≠îÊ°àÔºåÂ¶ÇÊûúËøòÊòØÈöæ‰ª•ÂÜ≥ÂÆöÔºåÊàëÁöÑÂª∫ËÆÆÂ∞±ÊòØÂÉèÂÖ´Êàí
ÊâÄËØ¥ÁöÑÈÇ£Ê†∑Ôºö‚ÄúÂæÖ‰ø∫ËÄÅÁå™‰∏ÄÂÆ∂ÂÆ∂ÂêÉÂ∞ÜÂéªÔºÅ‚Äù„ÄÇÔºàÊàëËá™Â∑±ÂòõÔºåÂêÉËøá‰∫Ü‰∏âÂÆ∂Ôºâ&lt;/p>
&lt;p>‰ªé vim8.0 ÂºÄÂßãÔºåvim Ëá™Â∏¶Êèí‰ª∂ÁÆ°ÁêÜÁöÑÂäüËÉΩÁâπÊÄßÔºàÂÆûÈôÖ‰∏äÔºåÂâçËø∞ÁöÑ &lt;code>minpac&lt;/code> Â∞±Âà©Áî®‰∫Ü
Ëøô‰∫õÁâπÊÄßÔºâÔºåÊàë‰ªÖÂú®Ëøô‰∏™ÂäüËÉΩÂá∫Êù•ÁöÑÊó∂ÂÄôÂ∞ùËøáÊñ∞ÔºàÊª°Ë∂≥Â•ΩÂ•áÂøÉËÄåÂ∑≤ÔºåÊµÖÂ∞ùËæÑÊ≠¢ÔºâÔºå
Âõ†‰∏∫Â∑≤Áªè‰π†ÊÉØ‰∫ÜÊâÄÁî®ÁöÑÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂ÔºöÔºâ(Êõ¥Êñ∞ÔºöÂêéÊù•ÊàëÊîπÂèò‰∫Ü‰∏ªÊÑè)&lt;/p>
&lt;p>Â¶ÇÊûúËØ¥ÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂ÂèØ‰ª•Áî± vim Ëá™Â∏¶ÁöÑÂäüËÉΩÊâÄÊèê‰æõÔºåÈÇ£‰πàÔºåÊúâÂ§öÂ∞ëÊèí‰ª∂ÊòØÂøÖÈ°ªÁöÑÂë¢Ôºü&lt;/p>
&lt;p>Êúâ‰∫∫‰Ωú‰∫ÜÂæàÂ•ΩÁöÑÊÄªÁªìÔºö
&lt;a href="https://thoughtbot.com/blog/how-to-do-90-of-what-plugins-do-with-just-vim">How to Do 90% of What Plugins Do (With Just Vim)&lt;/a>&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†Â¥áÂ∞öÊûÅÁÆÄ‰∏ª‰πâÔºåÂÄºÂæó‰∏ÄËØï„ÄÇÔºà‰∏çÊòØËØ¥Â∞±‰∏ÄÂÆöË¶ÅÂ¶ÇÊ≠§ÔºåËÄåÊòØÁü•ÈÅìÊúâÁöÑÈÄâÊã©Ôºâ&lt;/p>
&lt;p>ËøôÈáåÊèêÂà∞ÁöÑÂ∫îËØ•ÈÉΩÁêÜËß£‰∏∫ÊòØ‚ÄúÂà´‰∫∫ÁöÑ‚Äù‰∏™‰∫∫ÈÖçÁΩÆÔºåËÄåÊâÄË∞ì‚Äú‰∏™‰∫∫ÈÖçÁΩÆ‚ÄùÔºåÂü∫Êú¨ÊÑèÊÄùÂ∞±ÊòØÔºö
Ëµ∞ËøáË∑ØËøáÔºåÂèØ‰ª•Èöè‰æøÈîôËøá„ÄÇ
ÁúãÂà∞ÈíüÊÑèÁöÑÔºåÊãøËµ∞‰∏çË∞¢„ÄÇÁúãÂà∞‰∏çÁàΩÁöÑÔºåËØ∑ÂéªPull RequestÔºåÂàáÂãø‰π±Âñ∑Á≤™„ÄÇ&lt;/p>
&lt;p>Â§ß‰Ω¨&lt;a href="https://en.wikipedia.org/wiki/Linus_Torvalds">ÊûóÁ∫≥ÊñØ¬∑ÊâòÁì¶ÂÖπ&lt;/a>Êó©Â∞±ÁúãÈÄè‰∫ÜÈÇ£‰∫õÂà∞Â§Ñ‰π±Âñ∑ÁöÑÈîÆÁõò‰æ†Ôºå‰ªñÁöÑËá≥ÁêÜÂêçË®ÄËØ¥Êòé‰∫Ü‰∏ÄÂàáÔºö&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;On the Internet, talk is cheap, show me the code.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;h2 id="vimrc-vimrc" >
&lt;div>
&lt;a href="#vimrc-vimrc">
#
&lt;/a>
vimrc (~/.vimrc)
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-vim" data-lang="vim">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34; ================================Part-1: Plugins============&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#begin(&lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle&amp;#39;&lt;/span>) &lt;span style="color:#78787e">&amp;#34; reuse the bundle dir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== tmux ===-------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; vim &amp;lt;-&amp;gt; tmux, pane dance&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;edkolev/tmuxline.vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; status line&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Coding enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;airblade/vim-gitgutter&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; shows git diff sign and more&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;tpope/vim-fugitive&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; awsome git wrapper!&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;tpope/vim-repeat&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; enhance . repeat&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;dense-analysis/ale&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; linters config&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;mileszs/ack.vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; cherrypick your strings&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;junegunn/fzf&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;do&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;./install --all&amp;#39;&lt;/span> } | Plug &lt;span style="color:#5af78e">&amp;#39;junegunn/fzf.vim&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Python enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; Plug &amp;#39;Valloric/YouCompleteMe&amp;#39; &amp;#34; for completion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;davidhalter/jedi-vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; python auto-completion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Wiki/markdown enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;vimwiki/vimwiki&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for personal wiki&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;mzlogin/vim-markdown-toc&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for table_of_content&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;plasticboy/vim-markdown&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; couple with vim-instant-markdown&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;instant-markdown/vim-instant-markdown&amp;#39;&lt;/span>, {&lt;span style="color:#5af78e">&amp;#39;for&amp;#39;&lt;/span>: [&lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;markdown.pandoc&amp;#39;&lt;/span>]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Latex support ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;lervag/vimtex&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for LaTeX files&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;xuhdev/vim-latex-live-preview&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;for&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;tex&amp;#39;&lt;/span> } &lt;span style="color:#78787e">&amp;#34; Lively Previewing&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Julia lang ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;JuliaEditorSupport/julia-vim&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== for GnuPG ===-----------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;jamessan/vim-gnupg&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; transparent editting .gpg files&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Code/Project navigation ===-------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;easymotion/vim-easymotion&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; quickly move&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;preservim/nerdtree&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;on&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;NERDTreeToggle&amp;#39;&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== local plugins (manually install and update) ===-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34; ‰∏≠ÊñáË∑≥ËΩ¨ (easymotion-like)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle/vim-PinyinSearch&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#end() &lt;span style="color:#78787e">&amp;#34; required&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÂÆâË£ÖÂíå‰ΩøÁî® vim-plug ÁöÑÊ≠•È™§Ôºö&lt;/p>
&lt;ol>
&lt;li>‰∏ãËΩΩÂπ∂‰øùÂ≠òÂà∞ &lt;code>~/.vim/autoload/&lt;/code> ÁõÆÂΩï‰∏ã ÔºàÂú®ÁªàÁ´ØÊìç‰ΩúÔºâ&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>curl -fLo ~/.vim/autoload/plug.vim --create-dirs &lt;span style="color:#5af78e">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">&lt;/span> https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>ÈÖçÁΩÆ &lt;code>~/.vimrc&lt;/code> Êñá‰ª∂ÁöÑÁõ∏Â∫îÂÜÖÂÆπÔºàÂøÖÈ°ªÈÅµÂæ™ÁöÑÊ†ºÂºèÔºâÔºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>call plug#begin&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#5af78e">&amp;#34; reuse the bundle dir
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">Plug &amp;#39;tpope/vim-fugitive&amp;#39; &amp;#34;&lt;/span> awsome git wrapper!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#end&lt;span style="color:#ff6ac1">()&lt;/span> &lt;span style="color:#5af78e">&amp;#34; required
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Âú® &lt;code>plug#begin()&lt;/code> Âíå &lt;code>plug#end()&lt;/code> ‰πãÈó¥Â∞±ÊòØ‰Ω†ÊÉ≥Ë¶ÅÂÆâË£ÖÂíåÁÆ°ÁêÜÁöÑÊèí‰ª∂ÁöÑÂàóË°®„ÄÇ
&lt;code>Plug 'tpope/vim-fugitive'&lt;/code> Â∞±ÊòØÊåáÊòéÊÉ≥Ë¶ÅÂÆâË£ÖÂπ∂ÁÆ°ÁêÜ vim-fugitive Ëøô‰∏™Êèí‰ª∂„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú &lt;code>plug#begin()&lt;/code> Êã¨ÂºßÈáå‰∏∫Á©∫ÔºåÂàôÈªòËÆ§ÊòØÂ•ñÊâÄÊúâÁöÑÊèí‰ª∂ÂÆâË£ÖÂà∞ &lt;code>.vim/plugged/&lt;/code>Ôºå
‰∏äÊñáÁöÑÊàëÁöÑÈÖçÁΩÆ‰∏≠ÔºåÊàëÊòØÊåáÂÆö‰∫Ü &lt;code>.vim/bundle&lt;/code> ‰Ωú‰∏∫Êèí‰ª∂ÁöÑÁõÆÂΩï
ÔºàÂõ†‰∏∫ÊàëÂú®‰ΩøÁî® vim-plug ‰πãÂâçÂ∞±Â∑≤ÁªèÂú®Ëøô‰∏™ÁõÆÂΩï‰∏ãÂ≠òÊîæ‰∫ÜËÆ∏Â§öÊèí‰ª∂ÔºåÁÆÄÂçïÂ§çÁî®ËÄåÂ∑≤Ôºâ„ÄÇ&lt;/p>
&lt;p>vim-plug ‰ºöËá™Âä®ÊâßË°å‰ª•‰∏ã‰∏§‰∏™ÂëΩ‰ª§ÔºàËÆ∏Â§öÊèí‰ª∂ÈúÄË¶ÅËøô‰∏§‰∏™ËÆæÂÆöÔºâÔºö&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>filetype indent on&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>syntax enable&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Â¶ÇÊûú‰Ω†ÂèëÁé∞Ëá™Â∑±‚ÄúÂÆåÂÖ®‚ÄùÊåâÁÖßÂÆòÊñπÁöÑÊåáÂçóËøõË°åÂÆâË£ÖÔºåÂèØÊòØ‚ÄúÊèí‰ª∂Ê≠ªÊ¥ª‰∏çËµ∑‰ΩúÁî®‚ÄùÔºåÊûÅÊúâÂèØËÉΩÊòØ
Âõ†‰∏∫‰Ω†Âú®Êüê‰∏™Âú∞ÊñπËÆæÁΩÆ‰∫ÜÂÆåÂÖ®Áõ∏ÂèçÁöÑËÆæÂÆöÔºàÂõ†‰∏∫‰ªéÂà´ÁöÑÂú∞Êñπ copy ‰∏ÄÂ§ßÊÆµËá™Â∑±Ê≤°ÊúâËÆ§ÁúüÁúã
ËøáÁöÑÂÜÖÂÆπÂà∞Ëá™Â∑±ÁöÑ .vimrc Êñá‰ª∂ÈáåÔºâ„ÄÇ&lt;/p>
&lt;hr>
&lt;p>Êõ¥ËØ¶ÁªÜÁöÑ‰ΩøÁî®ÊñπÊ≥ïÂ∫îËØ•Áõ¥Êé•Âéª &lt;a href="https://github.com/junegunn/vim-plug">vim-plug&lt;/a>
ÁöÑ‰∏ªÈ°µ‰∫ÜËß£ÔºåÈÇ£Èáå‰∏ç‰ªÖÂëäËØâÂ§ßÂÆ∂Â∫îËØ•ÊÄé‰πàÂÆâË£ÖÊ≠§Êèí‰ª∂ÔºåËøòÁªôÂá∫ÂêÑÁßçÊÉÖÂÜµÁöÑËØ¥ÊòéÂíå‰æãÂ≠ê„ÄÇ&lt;/p>
&lt;hr></description></item><item><title>Âà©Âô®1: Oh, My Tmux -- ÈîÆÁõòÈ©±Âä®(‰∏∫‰∏ª)ÁöÑÂ∑•‰ΩúÊµÅ</title><link>/posts/thecli/2022-08-07-tmux/</link><pubDate>Sun, 07 Aug 2022 21:17:59 +0800</pubDate><guid>/posts/thecli/2022-08-07-tmux/</guid><description>&lt;h2 id="tmuxhttpsgithubcomtmuxtmux" >
&lt;div>
&lt;a href="#tmuxhttpsgithubcomtmuxtmux">
#
&lt;/a>
&lt;a href="https://github.com/tmux/tmux">Tmux&lt;/a>
&lt;/div>
&lt;/h2>
&lt;p>&lt;a href="https://github.com/tmux/tmux">Tmux&lt;/a> ÊòØ‰∏Ä‰∏™ÁªàÁ´ØÂ§öË∑ØÂ§çÁî®Âô®&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>Ôºö&amp;hellip;&lt;/p>
&lt;blockquote>
&lt;p>Tmux is a terminal multiplexer: it enables a number of terminals to be
created, accessed, and controlled from a single screen. tmux may be detached
from a screen and continue running in the background, then later reattached.&lt;/p>
&lt;/blockquote>
&lt;p>ÈÇ£Âà∞Â∫ï‰ªÄ‰πàÊòØ&lt;ruby>‚ÄúÁªàÁ´Ø‚Äù&lt;rt>terminal&lt;/rt>&lt;/ruby>Ôºü&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> ÊàëÂõûÁ≠î‰∏ç‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºå‰ΩÜÊàëÂèëÁé∞‰∫Ü‰∏Ä‰∏™ÂæàÂ•ΩÁöÑËß£Èáä &amp;ndash;
&lt;a href="https://unix.stackexchange.com/questions/4126/what-is-the-exact-difference-between-a-terminal-a-shell-a-tty-and-a-con">terminal&lt;/a>&lt;/p>
&lt;p>Á™ÅÁÑ∂ÊÉ≥Ëµ∑„ÄäÁªüËÆ°Â≠¶„ÄãÂÖ≥‰∫é‚ÄúÂç°ÊñπÊ£ÄÈ™å‚ÄùÈÇ£‰∏ÄÁ´†ÁöÑÁªèÂÖ∏ÂØºËØ≠Ôºö&lt;/p>
&lt;blockquote>
&lt;p>‰∏çË¶ÅÈóÆÂÆÉÁöÑÂê´‰πâÔºåÂÆÅÂèØÈóÆÂ¶Ç‰Ωï‰ΩøÁî®ÂÆÉ„ÄÇ&amp;ndash; Áª¥ÁâπÊ†πÊñØÂù¶&lt;/p>
&lt;/blockquote>
&lt;p>Êú¨ÁùÄ‚ÄúÊãøÊù•‰∏ª‰πâ‚ÄùÁ≤æÁ•ûÔºåÈÇ£Â∞±Êù•Á≤óÁï•ÁúãÁúã Tmux ËÉΩÂ¶Ç‰Ωï‰ΩøÁî®ÂêßÔºö&lt;/p>
&lt;p>&lt;img alt="IMG_MyTmux &amp;ndash; (gif Â§±Êïà)" src="https://fgg100y.github.io/img/posts/tmux/ssr_myTmuxDemo.gif">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>ÂêØÂä® Termainal (CTRL+Enter, ËøôÊòØ‰∏™‰∫∫ÂÆö‰πâÁöÑÂø´Êç∑ÈîÆ)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÂêØÂä® Tmux ËøõÂÖ•ÊàëÁöÑÂ∑•‰ΩúÁéØÂ¢É (È°ªÁü•ÔºöÊàëËøôÈáåÂ±ïÁ§∫ÁöÑ‰∏çÊòØÂàùÂßãÂåñÁöÑ Tmux ÂêØÂä®)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ê£ÄÊü•‰∏Ä‰∏ãÊò®Â§©ÂÜôÁöÑÂçöÂÆ¢ÂÜôÂà∞Âì™Èáå‰∫Ü (Ë≤å‰ººÂ∑Æ‰∏çÂ§ö‰∫ÜÔºåÂÜôÂÖ∂‰ªñÂÜÖÂÆπ)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÂàáÊç¢Âà∞‰∏ã‰∏ÄÂú∫‚Äú‰ºöËØù‚Äù(session) &amp;ndash; vimwiki (ËøôÈáåÊòØÊàë‰∏™‰∫∫È°πÁõÆÊ†πÁõÆÂΩï)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ÊåâËøõÂ∫¶‰ªäÂ§©Â∫îËØ•‰ªãÁªç‰∏Ä‰∏ã Tmux Ëøô‰∏™Â∑•ÂÖ∑ (ÁÑ∂ÂêéÔºåÂ∞±Âà∞ËøôÈáå‰∫Ü)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Áé∞Âú®Èò≥ÂÖâÊòéÂ™öÔºàËøá‰∏Ä‰ºöÂÑøÂ∞±ÂèòÊØíËæ£ÔºâÔºåÁôΩÂ§©ËøòÊòØÊç¢Âà∞ÁôΩËâ≤ËÉåÊôØÂêàÈÄÇ‰∫õ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;del>Êê¨Á†ñÊª¥Âπ≤Ê¥ªÂíØÂêÑ‰Ωç&lt;/del>&lt;br>
&lt;del>ÂóØÔºåÂπ≤Ê¥ªÔºüÊõ¥ÂÉèÊòØÂàõ‰Ωú&lt;/del>&lt;br>
&lt;del>Âàõ‰ΩúÔºüËøôÁÆóÂì™Èó®Â≠êÁöÑ&amp;hellip;&lt;/del>&lt;br>
ÊâÄ‰ª•ËøòÊòØÔºåÂπ≤ÁÇπÊ¥ªÂêß ÔºöÔºâ&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="tmux-ÁöÑ‰∏Ä‰∫õÂÖ≥ÈîÆÈÖçÁΩÆ" >
&lt;div>
&lt;a href="#tmux-%e7%9a%84%e4%b8%80%e4%ba%9b%e5%85%b3%e9%94%ae%e9%85%8d%e7%bd%ae">
#
&lt;/a>
Tmux ÁöÑ‰∏Ä‰∫õÂÖ≥ÈîÆÈÖçÁΩÆ
&lt;/div>
&lt;/h2>
&lt;p>Â¶ÇÊûúËøòÊ≤°ÊúâÂÆâË£Ö TmuxÔºåÂëΩ‰ª§Ë°å‰∏≠ÂÆâË£ÖËµ∑Êù•‰πü‰∏çË¥π‰∫ã (DebianÁ≥ªÂàó)Ôºö&lt;code>apt install tmux&lt;/code>&lt;/p>
&lt;p>‰ª•‰∏ãÁöÑÂÜÖÂÆπÂùáÂõ¥Áªï &lt;code>~/.tmux.conf&lt;/code> Êñá‰ª∂Â±ïÂºÄ„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÂ∞±Ëá™Â∑±ÂàõÂª∫‰∏Ä‰∏™ËøôÊñá‰ª∂„ÄÇ
ÈªòËÆ§Ëøô‰∫õ‰ª£Á†ÅÊ°ÜÈáåÁöÑÂÜÖÂÆπÔºàÁ≠âÂÆΩÂ≠ó‰ΩìÈÇ£‰∫õÔºâÊòØË¶ÅÊ∑ªÂä†Âà∞ &lt;code>~/.tmux.conf&lt;/code> Êñá‰ª∂Èáå„ÄÇ&lt;/p>
&lt;p>&amp;ldquo;#&amp;rdquo; ÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØÊ≥®ÈáäËØ¥Êòé&lt;br>
&amp;ldquo;$&amp;rdquo; ÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØË¶ÅÂú®ÂëΩ‰ª§Ë°åËøêË°åÁöÑÂëΩ‰ª§&lt;br>
&amp;ldquo;:command&amp;rdquo; ‰ª•‚ÄúÂÜíÂè∑Êé•ÂëΩ‰ª§Âêç‚ÄùÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØË¶ÅÂú®vimÂëΩ‰ª§Ê®°ÂºèËøêË°åÁöÑÂëΩ‰ª§&lt;/p>
&lt;h2 id="ÈªòËÆ§Âø´Êç∑ÈîÆ‰øÆÊîπ-ctrl-b---ctrl-a" >
&lt;div>
&lt;a href="#%e9%bb%98%e8%ae%a4%e5%bf%ab%e6%8d%b7%e9%94%ae%e4%bf%ae%e6%94%b9-ctrl-b---ctrl-a">
#
&lt;/a>
ÈªòËÆ§Âø´Êç∑ÈîÆ‰øÆÊîπ: Ctrl-b -&amp;gt; Ctrl-a
&lt;/div>
&lt;/h2>
&lt;p>Tmux ÁöÑÈªòËÆ§Âø´Êç∑ÈîÆÊòØ &lt;code>ctrl-b&lt;/code>ÔºåËøôÂú®ÈîÆÁõò‰∏äË∑ùÁ¶ªÁï•ÂæÆÊúâÁÇπËøú (ÂØπ‰∫éÊàëÁöÑÁü≠ÊâãÊåáÊù•ËØ¥)Ôºå
Âõ†Ê≠§ÊàëÊääÂÆÉ‰øÆÊîπ‰∏∫ &lt;code>ctrl-a&lt;/code> ÁªÑÂêàÈîÆ„ÄÇ&lt;/p>
&lt;p>Â∞Ü‰∏ãÈù¢ËØ≠Âè•Âä†ÂÖ• .tmux.conf Êñá‰ª∂Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g prefix C-a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # unbind the C-b&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>unbind C-b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # ensure that we can send Ctrl-a to other apps (e.g., vim)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> C-a send-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # setting the delay between prefix and command&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -s escape-time &lt;span style="color:#ff9f43">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Êó¢ÁÑ∂ÊòØË¶ÅÁî®Âø´Êç∑ÈîÆÔºåÈÇ£‰πàÈáçÊñ∞Âä†ËΩΩ .tmux.conf Êñá‰ª∂ÁöÑËá™ÁÑ∂‰πüÂÆâÊéí‰∏äÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># reload the .tmux.conf file with Prefix-r&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> r source-file ~/.tmux.conf &lt;span style="color:#5af78e">\;&lt;/span> display &lt;span style="color:#5af78e">&amp;#34;tmuxconf reloaded!&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÁÑ∂ÂêéÔºåÂú®ÂëΩ‰ª§Ë°åÁõ¥Êé•ËæìÂÖ•Âø´Êç∑ÁªÑÂêàÈîÆÔºö&lt;code>Ctrl+a+r&lt;/code> Âç≥ÂèØ‰ΩøÈÖçÁΩÆÁîüÊïà (&lt;code>+&lt;/code>Âè∑ÁöÑÊÑèÊÄùÊòØÂêåÊó∂ÊëÅ‰∏ã)„ÄÇ&lt;/p>
&lt;h2 id="vimtmux-Âº∫Âº∫ÁªÑÂêà" >
&lt;div>
&lt;a href="#vimtmux-%e5%bc%ba%e5%bc%ba%e7%bb%84%e5%90%88">
#
&lt;/a>
vim/tmux Âº∫Âº∫ÁªÑÂêà
&lt;/div>
&lt;/h2>
&lt;p>Tmux ÊîØÊåÅÂ§ö‰∏™ÁªàÁ´ØÁ™óÊ†º (terminal panes)ÔºåÊØè‰∏™Á™óÊ†ºÈÉΩÊòØÁã¨Á´ãÁöÑÁªàÁ´ØÔºå
Âõ†Ê≠§ÔºåÊúÄÊòéÊòæÁöÑ‰∫ãÊÉÖÂ∞±ÊòØÔºå‰Ω†ÂèØ‰ª•ÂêåÊó∂Âú®‰∏ÄÂùóÂ±èÂπï‰∏ä‚ÄúÁõëÊéß‚ÄùÂ§ö‰∏™‰∏çÂêå‰ªªÂä°ÔºåÊÑèÊÄùÊòØÔºå
Ëøô‰∫õ‰∏çÂêåÁöÑ‰ªªÂä°Âú®ËßÜËßâ‰∏äÊòØ‚ÄúÂêåÊó∂‚ÄùÂëàÁé∞ÁöÑÔºàÁõ∏ÂØπ‰∫éÈáçÂè†ÁöÑ Ê†áÁ≠æÈ°µ(tab) ËÄåË®ÄÔºâ„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÊÅ∞Â•Ω‰πüÂñúÊ¨¢Áî® vim ËøõË°åÁ†ÅÂ≠óÔºå‰πüÊâìÂºÄ‰∫Ü vim ÁºñËæëÂô® (vim Êú¨Ë∫´‰πüÊúâÁ™óÊ†ºÂäüËÉΩ)Ôºå
Ëá™ÁÑ∂ËÄåÁÑ∂ÁöÑÔºå‰Ω†ËÇØÂÆöÊÉ≥‰ª•Áõ∏ÂêåÁöÑÂø´Êç∑ÈîÆÊù•ÂÆûÁé∞‰∏çÂêåÁöÑ vim/tmux Á™óÊ†º‰πãÈó¥ÁöÑÂàáÊç¢„ÄÇ&lt;/p>
&lt;p>Âπ∏ËøêÁöÑÊòØÔºåÂ∑≤ÁªèÊúâ‰∫∫ÂÆûÁé∞‰∫ÜËøôÊ†∑ÁöÑÂäüËÉΩÔºåÂπ∂ÂÖ±‰∫´‰∫ÜÔºåËøôÂ∞±ÊòØ
&lt;a href="https://github.com/christoomey/vim-tmux-navigator">vim-tmux-navigator&lt;/a>.&lt;/p>
&lt;p>vim Êúâ‰∏∞ÂØåÁöÑÊèí‰ª∂ÁîüÊÄÅÔºåËøôÊòØ vim Âº∫Â§ßÁîüÂëΩÂäõÁöÑ‰∏Ä‰∏™‰ΩìÁé∞„ÄÇ
Ëøô‰πàÂ•ΩÁöÑÊ®°ÂºèÔºåËá™ÁÑ∂ÂÄºÂæóÂÄüÈâ¥„ÄÇÂõ†Ê≠§Ôºåtmux ‰πüÊúâ‰∏Ä‰∏™Áî®Êù•ÁÆ°ÁêÜÊèí‰ª∂ÁöÑ
&lt;a href="https://github.com/tmux-plugins/tpm">tpm&lt;/a>.&lt;/p>
&lt;p>ÈúÄË¶ÅÂú® vim Âíå tmux ‰∏§‰∏™ÊñπÈù¢ËøõË°åÈÖçÁΩÆÔºö&lt;/p>
&lt;ol>
&lt;li>Âú® vim ÊñπÈù¢, Â∞Ü‰ª•‰∏ãËØ≠Âè•Âä†ÂÖ• .vimrc Êñá‰ª∂Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Â¶ÇÊûú‰Ω†Áî®ÁöÑÊòØ Vim-Plug Êèí‰ª∂ÁÆ°ÁêÜÂô®ÁöÑËØù&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ÁÑ∂ÂêéËøêË°å &lt;code>:PlugInstall&lt;/code> Êù•ÂÆâË£ÖËøô‰∏™Êèí‰ª∂„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÊòØÁ¨¨‰∏ÄÊ¨°ÈÖçÁΩÆ vim Êèí‰ª∂ÔºåÂÖàÂéª‰∫ÜËß£
&lt;a href="https://github.com/junegunn/vim-plug">Vim-Plug&lt;/a>.&lt;/p>
&lt;ol start="2">
&lt;li>Âú® tmux ÊñπÈù¢ÔºåÂ∞Ü‰∏ãÈù¢ËØ≠Âè•Âä†ÂÖ• .tmux.conf Êñá‰ª∂Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># tpm is a tmux-plugin manager. [details in its github repos]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g @plugin &lt;span style="color:#5af78e">&amp;#39;tmux-plugins/tpm&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># vim-tmux-navigator: Seamless navigation between tmux panes and vim splits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g @plugin &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># put this &amp;#39;run&amp;#39; command to the very end of .tmux.conf, says the tpm&amp;#39;s readme&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>run &lt;span style="color:#5af78e">&amp;#39;~/.tmux/plugins/tpm/tpm&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Âä†ÂÖ•Ëøô‰∫õÂÜÖÂÆπ‰πãÂêéÔºåÈáçÊñ∞Âä†ËΩΩ tmux ÁéØÂ¢É, (Âú®Â∑≤ÁªèÂêØÂä®tmuxÁöÑÂëΩ‰ª§Ë°åËøêË°å‰ª•‰∏ãÂëΩ‰ª§)Ôºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ tmux &lt;span style="color:#ff5c57">source&lt;/span> ~/.tmux.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="‰∏ªÈ¢òËÉåÊôØÈ¢úËâ≤ÂàáÊç¢" >
&lt;div>
&lt;a href="#%e4%b8%bb%e9%a2%98%e8%83%8c%e6%99%af%e9%a2%9c%e8%89%b2%e5%88%87%e6%8d%a2">
#
&lt;/a>
‰∏ªÈ¢òËÉåÊôØÈ¢úËâ≤ÂàáÊç¢
&lt;/div>
&lt;/h2>
&lt;p>Âú®ÊàëÁöÑÂç∞Ë±°ÈáåÔºåÊÑüËßâÈªëËâ≤/ÊöóËâ≤Á≥ª‰∏Ä‰∏ãÂ≠êÊàê‰∫ÜÂæàÂ§öÂ∫îÁî®ËΩØ‰ª∂ÁöÑÈªòËÆ§ËÉåÊôØËâ≤Ôºå
ÊàëÂú®ÂëΩ‰ª§Ë°åËøáÊ¥ªÂæàÂ§öÂπ¥‰∫ÜÔºåÂØπÊ≠§ÂæàÊòØ‰π†ÊÉØÔºå‰∏™‰∫∫ÂæàÂ∞ëÁúãÂà∞Ë∞ÅÁöÑTerminalÊòØÁôΩËâ≤ËÉåÊôØÁöÑ„ÄÇ
‰∏ªË¶ÅÂ∫îËØ•ÊòØÔºö1ÔºâÈÖçÁΩÆ‰∫ÜÁôΩÂ§©ÈªëÂ§úÈÉΩÂèãÂ•ΩÁöÑ‰∏ªÈ¢òÔºõ2Ôºâ‰∏çÂΩìÂõû‰∫ãÂÑø„ÄÇ
‰ΩÜÊúâÊó∂ÂÄôÔºåÁâπÂà´ÊòØÂÖâÁ∫øÂÖÖË∂≥ÁöÑÊó∂ÂÄôÔºåÊàëÊØîËæÉÂñúÊ¨¢ÁôΩËâ≤ÁöÑËÉåÊôØ„ÄÇ
ÊâÄ‰ª•ÔºåÊàëÈúÄË¶ÅËÉΩÈöèÂøÉÊâÄÊ¨≤ÂàáÊç¢ËÉåÊôØËâ≤Ë∞É„ÄÇ&lt;/p>
&lt;p>Ëøô‰∏™Ë≤å‰ººÊ≤°Ê≥ï‰∏ç‰æùËµñ‰∫éÂ§ñÈÉ®ËÑöÊú¨Êù•ÂÆûÁé∞ÔºåËá≥Â∞ëÊàëÊ≤°ÊúâÂèëÁé∞ÂèØË°åÁöÑÂäûÊ≥ï„ÄÇÂõ†Ê≠§Ôºå‰Ω†‰πüÈúÄË¶Å
&lt;a href="https://www.seanh.cc/2021/01/02/change-between-light-and-dark-themes-in-tmux/">Âéª‰∏ãËΩΩËøô‰∏™ËÑöÊú¨&lt;/a>&lt;/p>
&lt;p>Linux ÁéØÂ¢ÉÊ∞õÂõ¥ÊòØËøôÊ†∑ÁöÑÔºå‰∏çË¶ÅÁõ≤ÁõÆÁõ∏‰ø°Âà´‰∫∫ÂÜôÁöÑËÑöÊú¨ÔºåÁâπÂà´ÊòØÈÇ£‰∫õ‰Ω†‰∏çËÆ§ËØÜÁöÑ‰∫∫„ÄÇ
Áõ∏‰ø°ÂºÄÊ∫êÂíåÁõ∏‰ø°ÈôåÁîü‰∫∫ÔºåÊØïÁ´ü‰∏çÊòØÂêå‰∏Ä‰ª∂‰∫ã„ÄÇ
ÊÑèÊÄùÂ∞±ÊòØÔºå‰Ω†Âú®‰ΩøÁî®Ëøô‰∫õËÑöÊú¨‰πãÂâçÂ•ΩÊ≠π‰πüË¶ÅÁúãÁúã‰∫∫ÂÆ∂Âà∞Â∫ïÂÜô‰∫Ü‰ªÄ‰πàÂêß„ÄÇ
ÁâπÂà´ÊòØËøôÁßçÈÉΩ‰∏çË∂ÖËøá‰∏ÄÈ°µÂÜÖÂÆπÁöÑËÑöÊú¨„ÄÇ&lt;/p>
&lt;p>‰ªÄ‰πàÔºü‰Ω†ËØ¥‰Ω†Áúã‰∏çÊáÇ„ÄÇ‰∫ãÂÆûÊòØÔºåÊ≤°Êúâ‰∫∫Âú®‰∏ÄÂºÄÂßãÂ∞±‰ªÄ‰πàÈÉΩÁúãÂæóÊáÇ„ÄÇÊØîÂ¶ÇËØ¥Ôºå
‰Ω†ÁúãÂà∞ËÑöÊú¨ÈáåÊúâËøô‰πà‰∏ÄÂè•Á±ª‰ºº‰∫éwindows‰∏ä‚ÄúÊ∏ÖÁêÜÂûÉÂúæ‚ÄùÁöÑÂëΩ‰ª§Ôºö &lt;code>sudo rm -rf /*&lt;/code>Ôºå
ÈÇ£‰Ω†Â∞±Ë¶Å‰øùÊåÅË≠¶ÊÉï‰∫Ü„ÄÇ
ÊçÆËØ¥ÔºåÊúâ‰∫õÁ≥ªÁªü‰ºöÈòªÊ≠¢‰Ω†Ëøô‰πàÂπ≤Ôºå‰ΩÜÊúâ‰∫õÁ≥ªÁªü‰ºö‚ÄúÂºÄÂøÉÂú∞‚ÄùÊâßË°å‰Ω†ÁöÑÊó®ÊÑèÔºõ
ÂèçÊ≠£ÊàëÊ≤°ËØïËøáÔºåÊàë‰πüÊ≤°Êúâ‰ª•‰ªª‰ΩïÊñπÂºèÊöóÁ§∫ÊàñÈºìÂä±‰Ω†Ë¶ÅËøô‰πàÊ∏ÖÁêÜÁ≥ªÁªüÂûÉÂúæ„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúËØ¥‰Ω†‰∏ãËΩΩ‰∫ÜÈÇ£‰∏™ËÑöÊú¨ÔºåÊîæÂà∞‰Ω†ÂñúÊ¨¢ÂÆÉÂæÖÁöÑ‰ΩçÁΩÆÔºåÊØîÂ¶ÇËØ¥Ôºå &lt;code>~/.tmux/bin/&lt;/code>Ôºå
ÁÑ∂ÂêéÂ∞Ü‰∏ãÈù¢ËøôÊù°ËØ≠Âè•Âä†ÂÖ• &lt;code>~/.tmux.conf&lt;/code> Êñá‰ª∂‰∏≠„ÄÇ&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> T run-shell ~/.tmux/bin/toggle-theme
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="ÂÖ≥‰∫éÈÖçÁΩÆÂíåÊèí‰ª∂‰ª•ÂèäÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e%e9%85%8d%e7%bd%ae%e5%92%8c%e6%8f%92%e4%bb%b6%e4%bb%a5%e5%8f%8a%e7%ae%a1%e7%90%86%e6%8f%92%e4%bb%b6%e7%9a%84%e6%8f%92%e4%bb%b6">
#
&lt;/a>
ÂÖ≥‰∫éÈÖçÁΩÆÂíåÊèí‰ª∂‰ª•ÂèäÁÆ°ÁêÜÊèí‰ª∂ÁöÑÊèí‰ª∂
&lt;/div>
&lt;/h2>
&lt;p>‰ª•‰∏äÂÜÖÂÆπÂàÜÂà´ÂØπÂ∫îÁùÄÔºö&lt;/p>
&lt;p>1ÔºâÁõ¥Êé•ÂØπ tmux ËøõË°åÂü∫Êú¨ÈÖçÁΩÆÔºõ&lt;/p>
&lt;p>2Ôºâ‰ΩøÁî® tmux Êèí‰ª∂ÁÆ°ÁêÜÂô®ÂØπ tmux ËøõË°åÈÖçÁΩÆÔºõ&lt;/p>
&lt;p>3ÔºâÂÄüÁî® ËÑöÊú¨Á®ãÂ∫è ÂØπ tmux ËøõË°åÈÖçÁΩÆ„ÄÇ&lt;/p>
&lt;p>ÊØïÁ´üÔºå‰∏çÊòØÊØè‰∏™‰∫∫ÈÉΩÁÉ≠Ë°∑‰∫é‰ΩøÁî®Êèí‰ª∂ÁÆ°ÁêÜÂô®ÔºåÊèí‰ª∂Âíå‰∏ª‰ª∂‰πãÈó¥ÁöÑ‰∫§‰∫íÊñπÂºèÂπ∂‰∏çËÆæÈôê„ÄÇ‰Ω†ÂèØ‰ª•ÈÄâÊã©‰ΩøÁî®‰Ω†ËßâÂæóÂêàÈÄÇÁöÑÊñπÊ≥ïÊù•ÂÆåÊàê‰ªªÂä°„ÄÇ
Ë¶ÅÊòØËßâÂæóÁé∞ÊúâÁöÑÈÉΩ‰∏çÊª°ÊÑèÔºåÊó†Ê≥ïÊª°Ë∂≥‰Ω†ÁöÑÁâπÊÆäÈúÄÊ±ÇÔºåÊâãÁóíÁóíÁöÑÔºåÈÇ£Â∞±Ëá™Â∑±Êêû‰∏Ä‰∏™„ÄÇ
ÂæàÂ§ö‰ºòÁßÄÁöÑËΩØ‰ª∂ÈÉΩÊòØÂ¶ÇÊ≠§ÔºåÂÆÉÊèê‰æõË∂≥Â§üÂ•ΩÁöÑÈªòËÆ§ÂÄºÔºå‰ΩÜ‰πüÊèê‰æõË∂≥Â§üÂ§öÁöÑÁÅµÊ¥ªÊÄßÔºåËÆ©Áî®Êà∑ÊúâÁöÑÈÄâ„ÄÇ&lt;/p>
&lt;p>ÂàòÂª∫ÊòéËØ¥‚Äú‰ª•ÂâçÊ≤°ÁöÑÈÄâÔºåÁé∞Âú®ÊÉ≥ÈÄâÂõûÂÅöÂ•Ω‰∫∫ÔºåÁªô‰∏™Êú∫‰ºö‚ÄùÔºåÈôàÊ∞∏‰ªÅÁ≠î‚ÄúÂéªÂêåÊ≥ïÂÆòËÆ≤‚Äù„ÄÇ‚ÄúÊúâÁöÑÈÄâ‚ÄùËøô‰∏ÄÁÇπÂçÅÂàÜÂèØË¥µ„ÄÇ&lt;/p>
&lt;hr>
&lt;p>Last update: 2024-07-04&lt;/p>
&lt;blockquote>
&lt;p>USER: Tmux ÊòØ‰ªÄ‰πàÔºü
GPT4o: Tmux ÊòØ‰∏Ä‰∏™ÁªàÁ´ØÂ§öË∑ØÂ§çÁî®Âô® (terminal multiplexer)ÔºåÂÖÅËÆ∏Áî®Êà∑Âú®‰∏Ä‰∏™ÁªàÁ´ØÁ™óÂè£ÂÜÖËøêË°åÂíåÁÆ°ÁêÜÂ§ö
‰∏™ÁªàÁ´Ø‰ºöËØù„ÄÇÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñπ‰æøÁöÑÊñπÊ≥ïÊù•ÂàÜÂâ≤‰∏Ä‰∏™Áâ©ÁêÜÁªàÁ´ØÁ™óÂè£ÊàêÂ§ö‰∏™ËôöÊãüÁªàÁ´ØÔºåÂπ∂Âú®Ëøô‰∫õËôöÊãüÁªàÁ´Ø
‰πãÈó¥Ëá™Áî±ÂàáÊç¢„ÄÇTmux ÁöÑ‰∏ªË¶ÅÂäüËÉΩÂåÖÊã¨Ôºö&lt;/p>
&lt;ul>
&lt;li>‰ºöËØùÁÆ°ÁêÜÔºöTmux ÂèØ‰ª•ÂàõÂª∫„ÄÅÂàÜÁ¶ªÂíåÈáçÊñ∞ÈôÑÂä†‰ºöËØùÔºå‰ΩøÁî®Êà∑ÂèØ‰ª•Âú®‰∏çÂêåÁöÑÁªàÁ´Ø‰ºöËØù‰πãÈó¥Âø´ÈÄüÂàáÊç¢„ÄÇ&lt;/li>
&lt;li>Á™óÂè£ÂàÜÂâ≤ÔºöÁî®Êà∑ÂèØ‰ª•Âú®‰∏Ä‰∏™Tmux‰ºöËØù‰∏≠Â∞ÜÁ™óÂè£ÂàÜÂâ≤ÊàêÂ§ö‰∏™Á™óÊ†ºÔºåÊØè‰∏™Á™óÊ†ºÂèØ‰ª•Áã¨Á´ãËøêË°å‰∏Ä‰∏™ÂëΩ‰ª§Ë°åÁ®ãÂ∫è„ÄÇ&lt;/li>
&lt;li>ÊåÅ‰πÖÊÄßÔºöÂç≥‰ΩøÁªàÁ´ØÁ™óÂè£ÂÖ≥Èó≠ÔºåTmux ‰ºöËØù‰ªçÁÑ∂‰ºö‰øùÊåÅËøêË°åÔºåËøôÊ†∑Áî®Êà∑ÂèØ‰ª•Á®çÂêéÈáçÊñ∞ÈôÑÂä†‰ºöËØùÔºåÁªßÁª≠Â∑•‰Ωú„ÄÇ&lt;/li>
&lt;li>ÂèØÂÆöÂà∂ÊÄßÔºöTmux Êèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑÈÖçÁΩÆÈÄâÈ°πÂíåÂø´Êç∑ÈîÆÔºåÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇËøõË°åÂÆöÂà∂„ÄÇ&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>ËÄåÂ¶Ç‰ªäÔºåÊàñËÆ∏Â∫îËØ•ÈóÆÈóÆGPT4o&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>ÊÉ≥Áü•ÈÅìÊÄé‰πàÂÉèËøôÊ†∑Ê†áÊ≥®ÂêçËØçÂêóÔºüÂéªÁúãÁúã&lt;a href="https://www.junyi.dev/posts/a-writing-trick/">ËøôÁØáÂçöÊñá&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Âà©Âô®2: Vimwiki ‰πã ÂÆâË£ÖÂíåÈÄÇÈÖçÊï∞ÊçÆÁßëÂ≠¶ÁöÑ‰∏Ä‰∫õÈÖçÁΩÆ</title><link>/posts/thecli/thevim/2022-07-15-vimwiki_part2/</link><pubDate>Sat, 06 Aug 2022 23:33:32 +0800</pubDate><guid>/posts/thecli/thevim/2022-07-15-vimwiki_part2/</guid><description>&lt;p>Last update: 2022-08-06 23:33:32&lt;/p>
&lt;h2 id="vimwiki-Êèí‰ª∂ÁÆÄ‰ªãÂÆâË£ÖÂíå‰∏Ä‰∫õËÆæÁΩÆ" >
&lt;div>
&lt;a href="#vimwiki-%e6%8f%92%e4%bb%b6%e7%ae%80%e4%bb%8b%e5%ae%89%e8%a3%85%e5%92%8c%e4%b8%80%e4%ba%9b%e8%ae%be%e7%bd%ae">
#
&lt;/a>
vimwiki Êèí‰ª∂ÁÆÄ‰ªã„ÄÅÂÆâË£ÖÂíå‰∏Ä‰∫õËÆæÁΩÆ
&lt;/div>
&lt;/h2>
&lt;p>Âú® vimwiki ÈáåÔºåÊâÄÊúâÁöÑÂÖ•Âè£ÈõÜ‰∏≠Âú®ÊåáÂÆöÁöÑÈ°πÁõÆÊ†πÁõÆÂΩï‰∏ãÁöÑ index.md ÊñáÊ°£Èáå„ÄÇ‰πüÂ∞±ÊòØË¶ÅÊûÑ
Âª∫ÁöÑÊï¥‰∏™Á¨îËÆ∞Á≥ªÁªüÔºàÈöè‰æøÁÆ°ÂÆÉÂè´‰ªÄ‰πàÔºâÔºåindex.md ÂèØ‰ª•Êàê‰∏∫ÂÆÉÁöÑÊï¥‰ΩìÊ°ÜÊû∂‰ª•ÂèäÂêÑ‰∏™‰∏ªÈ¢òÁöÑ
Á¥¢Âºï„ÄÇ‰ªéËøôÈáåÂèØ‰ª•ÂéªÂà∞ÂêÑ‰∏™‰∏ªÈ¢òÔºå‰ªéÂêÑ‰∏™‰∏ªÈ¢òÁöÑÁ¥¢ÂºïÂéªÂà∞ÂØπÂ∫îÁöÑÂÜÖÂÆπÔºå‰πüÂèØ‰ª•‰ªé‰∏Ä‰∏™ÂÜÖÂÆπ
ÂéªÂà∞Âè¶‰∏Ä‰∏™ÂÜÖÂÆπÔºåÂÜÖÂÆπÂèØ‰ª•ÊòØÊú¨Âú∞ÊñáÊ°£ÂÜÖ/ÊñáÊ°£Èó¥ÔºåËøôÂÆåÂÖ®ÂèñÂÜ≥‰∫éÂ¶Ç‰ΩïÊûÑÂª∫ÂÜÖÂÆπ‰πãÈó¥ÁöÑËøûÊé•„ÄÇ&lt;/p>
&lt;p>vimwiki Êèê‰æõ‰∫ÜÂøÖË¶ÅÁöÑÂü∫Á°Ä„ÄÇ‰ΩÜÈÄöÂ∏∏Êàë‰ª¨Ë¶ÅÂÜôÁöÑÊñáÊ°£Âπ∂‰∏çÊòØÁ∫ØÊñáÊú¨ (plain text) ËÄåÊòØÂ∏¶Êúâ
‰øÆÈ•∞ËØ≠Ê≥ïÁöÑÊñáÊú¨ÔºåÊØîÂ¶Ç markdown, Tex/Latex Ê†ºÂºèÁöÑÊñáÊ°£„ÄÇËøô‰∏§ËÄÖÁöÑÂå∫Âà´Âú®‰∫éÔºåÂâçËÄÖÊòØ‚Äú
ÊâÄËßÅÂç≥ÊâÄÂæó‚ÄùÔºåÊÑèÊÄùÂ∞±ÊòØ‰Ω†ÂÜôÂá∫Êù•ÁöÑÂÜÖÂÆπÂíåÊúÄÂêéÂëàÁé∞ÁöÑÂÜÖÂÆπ‰∏ÄÊ®°‰∏ÄÊ†∑ÔºõÂêéËÄÖÂàôÊõ¥ÂÉèÊòØÂÜô‰ª£Á†ÅÔºå
ÊúÄÂêéÂëàÁé∞Âá∫Êù•ÁöÑÂíåÊâÄÂÜôÁöÑÂÜÖÂÆπ‰∏ÄËá¥Ôºà‰ΩÜÂ§ßÈÉ®ÂàÜÊó∂ÂÄô‰∏ç‰ºö‰∏ÄÊ®°‰∏ÄÊ†∑ÔºâÔºåÊúâ‰∫∫Áß∞‰πã‰∏∫
‚ÄúÊâÄÊÉ≥Âç≥ÊâÄÂæó‚ÄùÔºåÂê¨Ëµ∑Êù•ÊúâÁÇπÁéÑ‰πé„ÄÇÁõ¥Êé•Áî® plain text Ê†ºÂºè‰∏çÂ∞±ÂÆå‰∫ÜÔºåÊäòËÖæËøô‰∫õÂπ≤ÂòõÔºüÈÅóÊÜæ
ÁöÑÊòØÔºåÂè™Ë¶Å‰Ω†ÊÉ≥Ë¶ÅÂÜô‰∏Ä‰∫õÂ§çÊùÇ‰∏ÄÁÇπÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÔºàÊ±ÇÂØºÁöÑÈìæÂºèÊ≥ïÂàôÔºâÔºåÈÇ£‰πàÔºåÊ≤°Êúâ‰∏ìÈó®ÁöÑÁºñËæë
Âô®ÁöÑËØùÔºåÊúÄÂø´ËÉΩÂ§üÂæóÂà∞ËßÑËåÉÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊñπÊ≥ïÂ∞±ÊòØ &amp;ndash; ÊâãÂÜô„ÄÇÊïôÁßë‰π¶ÂÜÖÈÇ£‰∫õÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞
ÊïàÊûúÂü∫Êú¨ÈÉΩÊù•Ëá™ÊéíÁâàÁ≥ªÁªüÂ∑•ÂÖ∑ÔºàTex/LatexÁ≠âÔºâÁöÑÊîØÊåÅ„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúÂ∑•‰ΩúÁöÑÂÜÖÂÆπÊ∂âÂèäÂà∞ËæÉÂ§öÁöÑÈ´òÁ≠âÊï∞Â≠¶ÔºåÂπ∂ÊÉ≥Ëá™Â∑±ÁºñÂÜôÁöÑÊó•ÂøóÂçöÂÆ¢ÂæóÂà∞Á±ª‰ººÊïôÁßë‰π¶ÂºèÁöÑÊï∞Â≠¶
ÂÖ¨ÂºèÂëàÁé∞Ôºåmarkdown ÊñáÊ°£ÊòØÁõ∏ÂΩìÂèãÂ•ΩÁöÑÊñáÊú¨Ê†ºÂºèÔºåËÄå‰ªé markdown Âà∞ÊúÄÁªàÊïàÊûúÁöÑÂëàÁé∞‰πãÈó¥Ôºå
ÈúÄË¶ÅÊàë‰ª¨‰Ωú‰∏Ä‰∫õ&lt;a href="https://fgg100y.github.io/posts/thecli/thevim/2022-07-15-vimwiki_part2/###Êï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞">È¢ùÂ§ñÈÖçÁΩÆ&lt;/a>„ÄÇ&lt;/p>
&lt;p>Áé∞Âú®ÊàëÂè™ÈúÄË¶ÅÁî® mardown Ê†ºÂºèÁºñÂÜôÂÜÖÂÆπÔºåÂú®Êú¨Âú∞ÈìæÊé•ÁöÑÊñáÂ≠ó‰∏äÊï≤ÂáªÂõûËΩ¶ÈîÆ(ÂëΩ‰ª§Ê®°Âºè‰∏ã)Ôºå
Â∞±ÂèØ‰ª•ÊâìÂºÄËøô‰∏™Êú¨Âú∞ÈìæÊé•ÁöÑÂÜÖÂÆπÊñáÊ°£ÔºåÁÑ∂ÂêéÁºñËæëÂÜÖÂÆπÊàñËÄÖÂèëÂëÜÊàñËÄÖÂà´ÁöÑ‰ªÄ‰πàÔºåÊï≤Âáª Delete
ÈîÆ(ÂëΩ‰ª§Ê®°Âºè‰∏ã)ÂõûÂà∞‰πãÂâçË∑≥ËΩ¨Êù•ÁöÑÂú∞ÊñπÔºõÂΩìÁÑ∂‰πüÂèØ‰ª•ËÆæÁΩÆÂíå‰ΩøÁî®ÂÖ∂‰ªñÁªÑÂêàÈîÆÊù•ÂéªÂà∞ÂÖ∂‰ªñÂú∞
Êñπ„ÄÇÂú®ÊØè‰∏™Âú∞ÊñπÔºåÈÉΩÂèØ‰ª•Áî®ÊµèËßàÂô®Â±ïÁ§∫ÂΩìÂâçÊñáÊ°£ÁöÑÂÜÖÂÆπÁöÑÊúÄÁªàÂëàÁé∞ÊïàÊûúÔºåÊØîÂ¶Ç‰ΩøÁî®ÈªòËÆ§ÁöÑ
&lt;code>&amp;lt;leader&amp;gt;+w+h+h&lt;/code> Âø´Êç∑ÈîÆÂ∞±ËÉΩÂú®‰Ω†ÁöÑÈªòËÆ§ÊµèËßàÂô®Â±ïÁ§∫ÂΩìÂâçÊñáÊ°£ÁöÑÂÜÖÂÆπÔºåÊï∞Â≠¶ÂÖ¨ÂºèÊ≠£ÊòØËØæÊú¨
‰∏äÁöÑÈÇ£ÁßçÂÆÉËØ•ÊúâÁöÑÊ†∑Â≠êÔºå‰πüÂèØ‰ª•ÂÉèÊµèËßàÁôæÁßëÁΩëÈ°µÊó∂ÈÇ£Ê†∑ÔºåËá™Áî±Ë∑≥ËΩ¨ÔºåÊù•ÂéªËá™Â¶Ç„ÄÇ&lt;/p>
&lt;p>Èó≤ËØùÂ∞ëÂèôÔºåÁõ¥Êé•‰∏äÊâã„ÄÇ&lt;/p>
&lt;h3 id="ÂÆâË£Ö-vimwiki-Êèí‰ª∂" >
&lt;div>
&lt;a href="#%e5%ae%89%e8%a3%85-vimwiki-%e6%8f%92%e4%bb%b6">
##
&lt;/a>
ÂÆâË£Ö vimwiki Êèí‰ª∂
&lt;/div>
&lt;/h3>
&lt;p>(‰ΩøÁî®ÁÆ°ÁêÜÊèí‰ª∂ÁöÑ &lt;a href="https://github.com/junegunn/vim-plug">Vim-Plug&lt;/a> Êèí‰ª∂Êù•ÂÆâË£ÖÂíåÁÆ°ÁêÜ)&lt;/p>
&lt;p>Â∞Ü‰∏ãÈù¢ÁöÑËØ≠Âè•Âä†ÂÖ•‰Ω†Ëá™Â∑±ÁöÑ vim ÈÖçÁΩÆÊñá‰ª∂‰∏≠Ôºå&lt;/p>
&lt;p>&lt;code>Plug 'vimwiki/vimwiki'&lt;/code>&lt;/p>
&lt;p>ÁÑ∂ÂêéËøêË°åÂëΩ‰ª§ &lt;code>:PlugInstall&lt;/code>&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†Ëøò‰∏çÊ∏ÖÊ•öËøôÊòØ‰ªÄ‰πàÊÑèÊÄùÔºåÊêúÁ¥¢: vimrc ÊàñËÄÖ &lt;a href="https://vim.fandom.com/wiki/Open_vimrc_file">ÁÇπÂáªËøôÈáå&lt;/a>&lt;/p>
&lt;h3 id="ÈÖçÁΩÆ-vimwiki-Êèí‰ª∂" >
&lt;div>
&lt;a href="#%e9%85%8d%e7%bd%ae-vimwiki-%e6%8f%92%e4%bb%b6">
##
&lt;/a>
ÈÖçÁΩÆ vimwiki Êèí‰ª∂
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>markdwon Ê†ºÂºè&lt;/li>
&lt;/ul>
&lt;p>vimwiki ÈªòËÆ§‰ΩøÁî® .wiki Ê†ºÂºèÔºå‰ΩÜËøô‰∏™Ê†ºÂºèÁöÑËØ≠Ê≥ïÊ≤°Êúâ markdown ËØ≠Ê≥ï‰ΩøÁî®ÁöÑÂπøÊ≥õÂíåÊôÆÈÅç,
ÊâÄ‰ª•Êàë‰∏™‰∫∫ÊòØÊõ¥ÂÄæÂêë‰∫é‰ΩøÁî® markdown Ê†ºÂºèÊù•ÁºñÂÜôÊñáÊ°£„ÄÇÂèØ‰ª•ÊåâÁÖß‰∏ãÈù¢ÁöÑÂÅöÊ≥ïËøõË°åÁõ∏Â∫îÈÖçÁΩÆÔºö
Â∞Ü‰∏ãÈù¢ÁöÑËØ≠Âè•Âä†ÂÖ• &lt;code>.vimrc&lt;/code>:&lt;/p>
&lt;p>&lt;code>let g:vimwiki_ext2syntax={'.md': 'markdown', '.markdown': 'markdown', '.mdown': 'markdown', '.rmd': 'markdown'}&lt;/code>&lt;/p>
&lt;p>ËøôÊòØÂá†‰∏™ËæÉ‰∏∫Â∏∏ËßÅÁöÑ markdown ÊñáÊ°£ÁöÑÂêéÁºÄÔºåÊúÄÂêé‰∏ÄÁßç &lt;code>rmd&lt;/code> ÊòØ Rmardown Ê†ºÂºèÂêéÁºÄ„ÄÇ&lt;/p>
&lt;ul>
&lt;li>Â§ö‰∏™ index.md ÊñáÊ°£&lt;/li>
&lt;/ul>
&lt;p>ÂâçÈù¢ÊèêÂà∞ËøáÔºåindex.md ÊòØ vimwiki ÁöÑÈªòËÆ§ÂÖ•Âè£ÔºåÁ±ª‰ºº‰∫é‰∏ÄÊú¨‰π¶ÁöÑ‰∏ªÁõÆÂΩïÔºåÊàñËÄÖÁîµÂ≠êËèúÂçï„ÄÇ
ÈÄöÂ∏∏Êàë‰ª¨ÈúÄË¶ÅÂ§ö‰∏™ËèúÂçïÔºåÊØè‰∏™‰∏ªÈ¢òÈÖç‰∏Ä‰∏™ËèúÂçïÔºå‰ªéËÄå‰ΩøÂæóÂÜÖÂÆπÊõ¥Á≤æÁÆÄÂíåËá™Ê¥ΩÔºåËÄå‰∏çÊòØÊâÄÊúâ
ÁöÑÂÜÖÂÆπÂÖ®ÈÉΩÊ±áÈõÜÂπ∂Êã•Êå§Âú®Âêå‰∏Ä‰∏™Âú∞Êñπ„ÄÇÂú®È°∂Á∫ßËèúÂçïÔºåÂè™ÈúÄË¶ÅÁªôÂá∫Âà∞ÂêÑ‰∏™‰∏ªÈ¢òÁöÑÈìæÊé•Âç≥ÂèØ„ÄÇ
Â∞±ÂÉèÂ§ßÈ•≠Â∫óÁöÑÁîµÂ≠êËèúÂçïÔºåÂÆ¢‰∫∫ÂØπÂ∑ùËèúÊÑüÂÖ¥Ë∂£Â∞±ÁÇπÂáª‚ÄúÂ∑ùËèú‚ÄùÁöÑÁõÆÂΩïÂç≥ÂèØÔºå‰ΩÜÊ≤°ÊúâÂøÖË¶ÅÂ∞ÜÂÖ´Â§ß
ËèúÁ≥ªÊâÄÊúâËèúÂêçÂÖ®ÈÉΩÊã•Êå§Âà∞‰∏Ä‰∏™‰∏ªËèúÂçï‰∏ä„ÄÇ&lt;br>
Âêå‰∏äÔºåÂ∞Ü‰∏ãÈù¢ÁöÑËØ≠Âè•Âä†ÂÖ•Âà∞ &lt;code>.vimrc&lt;/code> &lt;br>
(‰∏çË¶ÅÁõ¥Êé•Â§çÂà∂Á≤òË¥¥ÔºåÁÑ∂ÂêéÊåáÊúõÂÆÉÂ∞±È©¨‰∏äËßÅÊïà üôÑ Âõ†‰∏∫‰Ω†ÁöÑÁõÆÂΩï‰∏ç‰ºö/‰πü‰∏çÈúÄË¶ÅÂíåÊàëÁöÑÁõ∏Âêå):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-vim" data-lang="vim">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">let&lt;/span> g:vimwiki_list = [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;index&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;index&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;path_html&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimwikiHtml&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.html&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_default&amp;#39;&lt;/span>:&lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/template/&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;custom_wiki2html&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/wiki2html.sh&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools/vim8&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools/CLIs&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Julia&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Latex&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Python&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/ShellScripts&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ ]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Âú®‰∏äÈù¢Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåÊØè‰∏Ä‰∏™Ëä±Êã¨Âè∑ÁöÑÂÜÖÂÆπÂÖ∂ÂÆûÂ∞±ÊòØÂØπÂ∫î‰∏Ä‰∏™‰∏ªÈ¢òÁöÑËèúÂçï (index.md)ÔºåÊ†πÊçÆ
vimwiki ÁöÑÈªòËÆ§Âø´Êç∑ÈîÆ, ÊØîÂ¶ÇÔºå &lt;code>2&amp;lt;leader&amp;gt;ww&lt;/code> Â∞±‰ºöÁõ¥Êé•ÊâìÂºÄ &lt;code>~/Vimwiki/LinuxTools/index.md&lt;/code> Ëøô‰∏™Êñá‰ª∂„ÄÇ
ÈÇ£‰πàÔºåÂíå‰ª•‰∏äÁöÑËÆæÁΩÆÂØπÂ∫îÁöÑÁõÆÂΩïÁªìÊûÑÊòØÊÄé‰πàÊ†∑ÁöÑÂë¢Ôºü‰ª•‰∏ãÂ∞±ÊòØÊàë‰∏™‰∫∫ vimwiki ÁõÆÂΩïÁªìÊûÑ (‰ΩøÁî®ÂëΩ‰ª§ &lt;code>tree -L 2&lt;/code>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ LinuxTools
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ commonCommandLines.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ images
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ jupyterWithDSKernels.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ linux_oracle_client_sqlplus_install_and_connection_setup.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ linux_sqlserver_sqlcmd_install_and_connection_setup.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ newBashConfig.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python_packages_management.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ tmux.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ vim8.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vimwiki.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ mathjax
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bower.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ composer.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ CONTRIBUTING.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ es5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ LICENSE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ package.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ myvimwikidir.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ Programming
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Julia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Latex
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Python
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ShellScripts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ Session.vim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îú‚îÄ‚îÄ template
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îú‚îÄ‚îÄ default.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ markdown.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>‚îî‚îÄ‚îÄ wiki2html.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Â¶ÇÂâçÊâÄËø∞ÔºåÊ†πÁõÆÂΩïÁöÑ &lt;code>index.md&lt;/code> ÊòØÈ°∂Á∫ß‰∏ªËèúÂçïÔºå‰ªéËøô‰∏™Êñá‰ª∂ÂÜÖË∑≥ËΩ¨Âà∞ÂêÑ‰∏™Â≠ê‰∏ªÈ¢ò„ÄÇÊØîÂ¶ÇÔºå
Êàë‰∏™‰∫∫ÊÑüÂÖ¥Ë∂£ÁöÑ„ÄÅ‰∏ÄÁõ¥Âú®Â≠¶‰π†ÁöÑ3Â§ß‰∏ªÈ¢òÂàÜÂà´ÊòØÔºöÊú∫Âô®Â≠¶‰π†ÔºàÊú™ÂàóÂá∫ÔºâÔºåÁºñÁ®ãÂíåLinuxÂ∑•ÂÖ∑„ÄÇ
ËÄåÂú® Programming Ëøô‰∏™ÁõÆÂΩï‰∏ãÁöÑ &lt;code>index.md&lt;/code> Â∞±ÊòØËøô‰∏™‰∏ªÈ¢òÁöÑ‰∏ªËèúÂçïÔºåÊòæÁÑ∂ÔºåËøô‰∏™‰∏ªÈ¢ò
‰∏ãËøòÊúâ‚ÄòJulia‚ÄôÔºå‚ÄòLatex‚ÄôÔºå‚ÄòPython‚ÄôÔºå‚ÄòShellScripts‚ÄôÁ≠âÂá†‰∏™ÂÜÖÂÆπ„ÄÇ‰∏çÁî®ËØ¥ÔºåËøòÂèØ‰ª•ËÆæËÆ°
Êõ¥ÁªÜÂàÜÁöÑÂÜÖÂÆπ„ÄÇÊØè‰∏™‰∏ªÈ¢òÈÉΩË∂≥Â§üÊàëÂñù‰∏äÂá†Â£∂ÁöÑ‰∫Ü üçª „ÄÇÊØïÁ´üÔºåË∞ÅÂèàËØ¥‰∫ÜÊûÑÂª∫ÊØîËæÉÂÆåÊï¥ÁöÑ
Áü•ËØÜ‰ΩìÁ≥ªÊòØ‰∏ÄÊúù‰∏ÄÂ§ïËÉΩÂÆåÊàêÁöÑÂë¢ÔºüÊ≠£ÊâÄË∞ìÔºåÂøó‰∏çÂº∫ËÄÖÊô∫‰∏çËææ„ÄÇ&lt;/p>
&lt;p>ËÄå mathjax ÁõÆÂΩïÔºåÊ≠£ÊòØÊÉ≥Ë¶ÅÂ±ïÁé∞Êï∞Â≠¶ÂÖ¨ÂºèÊïàÊûúÊâÄ‰æùËµñÁöÑÊú¨Âú∞ÂåÖÔºåÊé•‰∏ãÊù•Â∞±ËÆ≤Ëøô‰∏™ÁöÑÈÖçÁΩÆ„ÄÇ&lt;/p>
&lt;h3 id="Êï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞" >
&lt;div>
&lt;a href="#%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%8f%e7%9a%84%e5%91%88%e7%8e%b0">
##
&lt;/a>
Êï∞Â≠¶ÂÖ¨ÂºèÁöÑÂëàÁé∞
&lt;/div>
&lt;/h3>
&lt;p>&lt;code>vimwiki2html&lt;/code> ÂëΩ‰ª§Âíå &amp;ldquo;mathjax&amp;rdquo; ÁõÆÂΩï&lt;/p>
&lt;p>vimwiki Ëá™Â∏¶‰∫Ü‰∏Ä‰∏™ &lt;code>vimwiki2html&lt;/code> ÂëΩ‰ª§ÔºåÈ°æÂêçÊÄù‰πâÔºåÂ∞±ÊòØÊää mardown/wiki Ê†ºÂºèÁöÑÊ∫ê
ÊñáÊ°£ËΩ¨Êç¢Êàê html Ê†ºÂºèÁöÑÊñáÊ°£„ÄÇ‰ΩÜËøô‰∏™ÂëΩ‰ª§Âπ∂‰∏çÊîØÊåÅÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊ∏≤ÊüìÔºåÂæóÊîπÈÄ†„ÄÇËøôÊ≠£ÊòØ
&lt;a href="https://www.mathjax.org/">mathjax&lt;/a> Èó™‰∫ÆÁôªÂú∫ÁöÑÂú∞Êñπ„ÄÇÂ∑≤ÁªèÊúâ‰∫∫Êää &lt;code>vimwiki2html&lt;/code>
ÂëΩ‰ª§ÊâÄ‰æùËµñÁöÑËÑöÊú¨‰Ωú‰∫ÜÁõ∏Â∫îÁöÑ&lt;a href="https://gist.github.com/enpassant/0496e3db19e32e110edca03647c36541">ÂçáÁ∫ßÁâà&lt;/a>Ôºå
ÂÆÉËÉΩÂ∞Ü markdown Êñá‰ª∂ËΩ¨Êç¢‰∏∫ html Êñá‰ª∂ÔºåÂπ∂ÊîØÊåÅÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊ∏≤Êüì„ÄÇÊúâ‰∫õ‰∫∫Ëµ∞ÁöÑ&lt;a href="https://gist.github.com/maikeldotuk/54a91c21ed9623705fdce7bab2989742">Êõ¥Ëøú&lt;/a>
‰ΩÜ‰ªñ‰ª¨ÂùáÂÅáËÆæÊ≤°ÊúâÊâÄË∞ìÁöÑ‰ªÄ‰πà‚ÄúÂ§ñÁΩë‚ÄùËÆøÈóÆÈôêÂà∂‰∫ë‰∫ë„ÄÇÊâÄ‰ª•ÔºåËøòÈúÄË¶Å‰∏ÄÁÇπÊäò‰∏≠Êìç‰ΩúÔºåÈÇ£Â∞±ÊòØ
‚ÄòÊú¨Âú∞Âåñ‚Äô„ÄÇÂÖàÂì≤ËØ¥‚ÄòÂ±±‰∏çËøáÊù•ÔºåÊàëËµ∞ËøáÂéª‚Äô„ÄÇÁΩëÁªúÊó∂‰ª£ÂàöÂ•ΩÁõ∏ÂèçÔºåÊó¢ÁÑ∂ËÆøÈóÆ‰∏çÂà∞ÔºåÈÇ£Â∞±ÊääÂÆÉ
Â§çÂà∂ÂõûÊù•ÔºàÂü∫Êú¨ÂâçÊèêÊòØÂºÄÊ∫êÔºåÂóØÔºå‰ºüÂ§ßÁöÑÂºÄÊ∫êÔºâ„ÄÇ&lt;/p>
&lt;p>ÊâÄË∞ìÊú¨Âú∞ÂåñÔºåÂ∞±ÊòØÔºö1ÔºâÊää mathjax Â§çÂà∂Âà∞Êú¨Âú∞Ôºõ2ÔºâÈÖçÁΩÆ vimwiki ÁöÑÂëΩ‰ª§ËÑöÊú¨ÊâæÂà∞ÂÆÉ„ÄÇ
‰ª•‰∏ãÊòØÂÖ∑‰ΩìÊìç‰ΩúÔºö&lt;/p>
&lt;ol>
&lt;li>Âú® vimwiki ÁöÑÊ†πÁõÆÂΩï‰∏ãÔºåÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>git clone https://github.com/mathjax/MathJax.git mathjax&lt;span style="color:#5af78e">`&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>‰∏ãËΩΩÂπ∂Êõ¥Êîπ &lt;code>wiki2html.sh&lt;/code> Êñá‰ª∂ÁöÑ‰ª•‰∏ãÂÜÖÂÆπ
(# Âè∑ÂºÄÂ§¥ÁöÑËØ≠Âè•ÊòØË¢´Ê≥®ÈáäÊéâÁöÑÂéüÊñáÔºå‰ª•Â§áÂèÇËÄÉ):&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># HAS_MATH=$(grep -o &amp;#34;\$\$.\+\$\$&amp;#34; &amp;#34;$INPUT&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># if [ ! -z &amp;#34;$HAS_MATH&amp;#34; ]; then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># MATH=&amp;#34;--mathjax=https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># MATH=&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># fi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">HAS_MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">$(&lt;/span>grep -o &lt;span style="color:#5af78e">&amp;#34;\$\$*\+\$\$&amp;#34;&lt;/span> &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff5c57">$INPUT&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># # load mathjax from local copy (in case the CDN not work)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> &lt;span style="color:#ff6ac1">[&lt;/span> -n &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff5c57">$HAS_MATH&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span> &lt;span style="color:#ff6ac1">]&lt;/span>; &lt;span style="color:#ff6ac1">then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;--mathjax=path2your-mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">fi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ê≥®ÊÑèÔºö&amp;ldquo;path2your-mathjax&amp;rdquo; Â∫îËØ•‰øÆÊîπÊàê‰Ω†Ëá™Â∑±ÁöÑË∑ØÂæÑÔºå
ÊØîÂ¶ÇÊàëÁöÑË∑ØÂæÑÊòØËøôÊ†∑ÁöÑÔºö&amp;rsquo;/home/myacct/VimWiki/mathjax/es5/'&lt;/p>
&lt;p>Êàë‰∏çÂ§™ÊáÇ htmlÔºåÊàëÂú®ËøôÈáåÁöÑËØâÊ±ÇÂ∞±ÊòØËÉΩËÆ©ËΩ¨Êç¢ÂêéÁöÑ html ÊñáÊ°£Ê≠£Â∏∏ÊòæÁ§∫Êï∞Â≠¶ÂÖ¨Âºè„ÄÇ
‰ΩÜÂêéÊù•ÂèëÁé∞ÔºåË¶ÅÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºåÂÖ∂ÂÆû‰∏ç‰∏ÄÂÆöÈùûÂæóÂÖàËΩ¨Êç¢Êàê html Êñá‰ª∂„ÄÇ&lt;/p>
&lt;h2 id="markdown-ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫-vim-instant-markdown" >
&lt;div>
&lt;a href="#markdown-%e6%96%87%e6%a1%a3%e7%9a%84%e5%8d%b3%e6%97%b6%e5%b1%95%e7%a4%ba-vim-instant-markdown">
#
&lt;/a>
markdown ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫: &lt;code>vim-instant-markdown&lt;/code>
&lt;/div>
&lt;/h2>
&lt;p>ÂâçÈù¢ËØ¥ËøáÔºåÂ§ßÈÉ®ÂàÜÁöÑ markdown Âπ∂Èùû‚ÄúÊâÄËßÅÂç≥ÊâÄÂæó‚ÄùÔºå‰ΩÜÂ¶ÇÊûú‰∏Ä‰∏™‰∫∫ÁöÑÊ¨≤ÊúõÊ≠£Â∏∏ÔºåÈÄöÂ∏∏ÈÉΩÊÉ≥
Ë¶ÅÁúãÁúãÊ≠£Âú®ÂÜôÁöÑÂÜÖÂÆπÂà∞Â∫ïÊòØ‰ªÄ‰πàÊòæÁ§∫ÊïàÊûúÔºåÊØïÁ´üÔºåÊó©ÂèëÁé∞Êó©Ê≤ªÁñóÂòõÔºà‰∏ÄÊ∞îÂëµÊàêÁöÑÁàΩÊñáËá™ÁÑ∂
ÊòØÊúâÔºå‰ΩÜÊàëÊõ¥Áõ∏‰ø°‰ºòÁßÄÁöÑ‰ΩúÂìÅÈÄöÂ∏∏ÊòØ‰øÆÊîπÂá∫Êù•ÁöÑÔºâ„ÄÇÂΩìÁÑ∂ÔºåÊó©Â∞±Êúâ‰∫∫ÊääËøôÁßçÊ≠£Â∏∏‰∏çËøáÁöÑ
ÈúÄÊ±ÇÂÆûÁé∞Âπ∂ÂÖ±‰∫´‰∫ÜÔºåÈÇ£Â∞±ÊòØÔºö &lt;a href="https://github.com/instant-markdown/vim-instant-markdown">vim instant markdown&lt;/a>
Ëøô‰πüÊòØ‰∏Ä‰∏™ Vim ÁöÑÊèí‰ª∂ÔºåÂÖ∑‰ΩìÁöÑÂÆâË£ÖÂíåÈÖçÁΩÆ‰πüÁÆÄ‰æøÔºåÁõ¥Êé•Ê†πÊçÆÂÖ∂ readme Êñá‰ª∂ËøõË°åÂç≥ÂèØ„ÄÇ&lt;/p>
&lt;p>‰ΩÜÂÖ∂ÂÆû‚ÄúÊØèÊîπ‰∏Ä‰∏™Â≠óÂ∞±Âà∑Êñ∞‰∏ÄÊ¨°‚ÄùÂíå‚ÄúÂÜôÂÆå‰∫ÜÊâÄÊúâÂÜÖÂÆπÊâçÁúã‰∏ä‰∏ÄÁúºÊúÄÁªàÊïàÊûú‚Äù‰∏ÄÊ†∑‰∏çÁîöÂ¶•Ë¥¥Ôºå
ÂâçËÄÖ‰Ωø‰∫∫ÂàÜÂøÉÔºåÂêéËÄÖ‰Ωø‰∫∫ÊãÖÂøÉ„ÄÇÂõ†Ê≠§ÔºåÂΩìÊÉ≥Ë¶ÅÊ£ÄÊü•Êüê‰∏™ÊÆµËêΩÔºàÂ∞§ÂÖ∂ÊòØÂåÖÂê´ËÆ∏Â§öÊï∞Â≠¶ÂÖ¨Âºè
ÁöÑÊÆµËêΩÔºâÁöÑÂëàÁé∞ÊïàÊûúÊó∂ÔºåË∞ÉÁî®ÂëΩ‰ª§ &lt;code>InstantMarkdwonPreview&lt;/code> ÔºåÂÅúÁî®Âàô &lt;code>InstantMarkdwonStop&lt;/code>„ÄÇ
ÊàëÂæàÂñúÊ¨¢ËøôÁßçÂø´ÈÄüÂèçÂ∫îÔºåÊúâ‰ªáÂ∞±Êä•ÔºåÂΩìÂú∫Â∞±Êä•ÔºõÁü•ÈîôÂ∞±ÊîπÔºåÂñÑËé´Â§ßÁÑâ„ÄÇ
‰ΩÜÂ¶ÇÊûúËßâÂæóÁôΩÁ∫∏ÈªëÂ≠óÁöÑÈ¢ÑËßàÁΩëÈ°µÁâàÈù¢Ëøá‰∫éÊú¥Á¥†‰∏çÂ§üÊªãÂë≥ÔºåÈÇ£‰πàÔºåÂèØËÉΩ‰Ω†ÈúÄË¶ÅÁöÑÊòØ‰∏Ä‰∏™ËÉΩÂ§üËá™ÂÆö‰πâÁöÑÁΩëÈ°µ„ÄÇ&lt;/p>
&lt;h2 id="markdown-ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫-jekyll-site" >
&lt;div>
&lt;a href="#markdown-%e6%96%87%e6%a1%a3%e7%9a%84%e5%8d%b3%e6%97%b6%e5%b1%95%e7%a4%ba-jekyll-site">
#
&lt;/a>
markdown ÊñáÊ°£ÁöÑÂç≥Êó∂Â±ïÁ§∫: jekyll-site
&lt;/div>
&lt;/h2>
&lt;p>&lt;code>vim-instant-markdown&lt;/code> Â±ïÁ§∫ÁöÑÂ∞±ÊòØÊñáÊ°£ÁöÑÂëàÁé∞ÂÜÖÂÆπÔºå‰πü‰ªÖ‰ªÖÂ∞±ÊòØÂÜÖÂÆπËÄåÂ∑≤„ÄÇÂ¶ÇÊûúÊÉ≥Ë¶Å
Êõ¥Â§öÔºåÈÇ£‰πàÔºåÂ§áÈÄâÊñπÊ°àÂ∞±ÊòØÈùôÊÄÅÁΩëÁ´ôÁîüÊàêÂô®ÔºàÂ¶Ç&lt;a href="https://jekyllrb.com/">jekyll&lt;/a>)Ôºå
È°æÂêçÊÄù‰πâÔºåÂ∞±ÊòØËøôÁé©ÊÑèÂÑøÁõ¥Êé•Áªô‰Ω†ÊâìÂåÖÁîüÊàêÁΩëÁ´ôÈúÄË¶ÅÁöÑ‰∏ÄÊèΩÂ≠êÈÉ®‰ª∂„ÄÇ
ÔºàÁΩëÁ´ôÊÑèÂë≥ÁùÄÈ´òÂ∫¶ÂÆöÂà∂ÁöÑËá™Áî±ÔºåÊòØ‰∏çÊòØÂê¨ÁùÄÂ∞±ËßâÂæóÂºÄÂøÉÔºüÂ¶ÇÊûú‰Ω†Â§üËÄêÂøÉÁúãÂà∞ËøôÈáåÔºåÊàëÂ∞±
‰º∞Êë∏ÁùÄ‰Ω†‰πüÂ∫îËØ•‰πüÊòØ‰∏™Áà±ÊäòËÖæÁöÑ‰∫∫ÔºåÁà±Âú®Êú∫Âô®‰∏äÊç£ÈºìÂêÑÁßçÂ∑•ÂÖ∑ÁöÑ‰∫∫ÊÉ≥ÂøÖ‰πüÁü•ÈÅìÔºåÊâÄË∞ìÁöÑ
‚ÄúÈ´òÂ∫¶ÂÆöÂà∂‚ÄùÁöÑÂè¶‰∏Ä‰∏™ËØ¥Ê≥ïÂ§ßÁ∫¶ÊòØËØ¥ÔºöÊù•ÂëÄÔºåÊäòËÖæÂëÄÔºåÂø´Ê¥ªÂëÄÔºâ„ÄÇ&lt;/p>
&lt;p>‰Ω†Áü•ÈÅìÂêóÔºü‰ªé &lt;code>vimwiki&lt;/code> ÁöÑÁºñËæëÁ≥ªÁªüÂà∞ jekyll ÂçöÂÆ¢ÁΩëÈ°µÔºåÂè™ÈúÄË¶Å‰∏Ä‰∏™ÂëΩ‰ª§Ôºö&lt;code>:Postit&lt;/code>&lt;/p>
&lt;p>ÂΩìÁÑ∂‰∫ÜÔºåËøôÊòéÊòæÂ±û‰∫éÂπøÂëäÁî®ËØ≠ÔºåÂíå‚Äò‰∏ÄÂ§©Â≠¶‰ºöpythonÔºå‰∏ÉÂ§©ÊéåÊè°Êú∫Âô®Â≠¶‰π†‚Äô‰πãÁ±ªÂ∑Æ‰∏çÂ§ö„ÄÇ&lt;/p>
&lt;p>‰øóËØ≠Êúâ‰∫ëÔºö‚ÄúÂâçÁ´Ø‰∏ÄÂàÜÈíüÔºåÂêéÁ´ØÂçÅÂπ¥Âäü‚Äù„ÄÇ&lt;code>Postit&lt;/code> ÂëΩ‰ª§ÈúÄË¶Å‰∏Ä‰∫õÂü∫Á°ÄÊù•ÊîØÊíëÔºö&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-viml" data-lang="viml">&lt;span style="display:flex;">&lt;span>command Postit :&lt;span style="color:#ff6ac1">execute&lt;/span> &lt;span style="color:#5af78e">&amp;#34;w!&amp;#34;&lt;/span> . &lt;span style="color:#5af78e">&amp;#34;Path_to_jekyllsite_root/_posts/&amp;#34;&lt;/span> . strftime(&lt;span style="color:#5af78e">&amp;#34;%Y-%m-%d-&amp;#34;&lt;/span>) . expand(&lt;span style="color:#5af78e">&amp;#34;%:t&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ËøôËØ≠Âè•Ë¶ÅÂÜôÂú® &lt;code>.vimrc&lt;/code> Êñá‰ª∂ÈáåÔºåËøô‰∏™ÂÆûÈôÖ‰∏äÊòØËÆæÁΩÆ vim ÂëΩ‰ª§„ÄÇ&lt;/p>
&lt;p>Ê≥®: Ëøô‰∏™ÂëΩ‰ª§ÁöÑÂÆûÈôÖÊÑèÊÄùÊòØÔºöÂ∞ÜÂΩìÂâçÁºñËæëÁöÑÊñá‰ª∂‰ª• jekyll ÁöÑ post Êñá‰ª∂ÁöÑÊâÄË¶ÅÊ±ÇÊ†ºÂºè
ËøõË°åÂëΩÂêçÔºåÂπ∂‰øùÂ≠òÂà∞‰Ω†ÁöÑ jekyll È°πÁõÆÁöÑ _post/ ÁõÆÂΩï‰∏ã„ÄÇ&lt;/p>
&lt;p>Âõ†‰∏∫ jekyll ÁöÑ post ÂÖ®ÈÉ®Ë¶ÅÊ±Ç‰ª•‚ÄúÂπ¥-Êúà-Êó•-‚Äù‰Ωú‰∏∫Êñá‰ª∂ÂêçÂâçÁºÄ (ÊØîÂ¶ÇÔºö
‚Äú2022-02-02-Â•ΩÂ§ö‰∫å.md‚Äù)„ÄÇÂëΩ‰ª§‰∏≠ÁöÑ ‚Äú.‚Äù ÊòØ viml ËØ≠Ê≥ï‰∏≠ &amp;ldquo;concatenate&amp;rdquo; ÁöÑÊÑèÊÄù„ÄÇ&lt;/p>
&lt;p>ËôΩÁÑ∂Ë∞à‰∏ç‰∏äÈöæ‰ª•ÂøçÂèóÔºå‰ΩÜÊàë‰∏™‰∫∫‰∏çÊòØÂæàÁêÜËß£ËøôÁßçÁ°¨ÁºñÁ†ÅÔºåÊñá‰ª∂ÂëΩÂêçÊ†ºÂºè‰∏∫‰ΩïÈùûÂ¶ÇÊ≠§‰∏çÂèØÔºü
ÊòØbugËøòÊòØfeatureÔºåÈÇ£Â∞±ËßÅ‰ªÅËßÅÊô∫‰∫Ü„ÄÇÊàëÂê¨ËØ¥ËøòÊúâ‰∏Ä‰∏™ÈùôÊÄÅÁΩëÁ´ôÁîüÊàêÂô®ÂêçÂè´
&lt;a href="https://gohugo.io/">Hugo&lt;/a> ÁöÑÔºå‰∏çÁü•ÂÖ∂ÊâãËâ∫Â¶Ç‰Ωï &amp;hellip;&lt;/p>
&lt;hr>
&lt;p>ÂàöÂºÄÂßã‰ΩøÁî® vimwiki Êó∂Ôºå‰πüÊòØ‰∏çÂÅúÁöÑÊë∏Á¥¢ÔºåÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÊòæÁ§∫‰∏ÄÁõ¥ÊòØ‰∏™Â§¥ÁñºÁöÑÈóÆÈ¢òÔºåÊØïÁ´üÔºå
‰ªé‚ÄúÊ≠™Êûú‰ªÅ‚ÄùÈÇ£ÈáåÂÄüÈâ¥Êù•ÁöÑÈªòËÆ§ÈÖçÁΩÆÈúÄË¶ÅËÉΩËÆøÈóÆÂ§ñÁΩëÔºåËÄåËøô‰∏™ÊÉ≥ÂøÖ‰∏çÊòØÊàë‰ª¨ËÉΩÂ§üÈªòËÆ§Âæó‰∫ÜÁöÑÔºå
Âè™ËÉΩÂÖàÂøçÁùÄÈÇ£‰∫õ‰ª§‰∫∫‰∏çÂø´ÁöÑÂ¶ÇÂêå‰π±Á†ÅÁöÑ latex ËØ≠Âè•ÊåÇÂú®È°µÈù¢‰∏äÔºå
ÔºàÂ∞§ÂÖ∂ÊòØÔºåÂΩì‰Ω†ÊòéÊòéÁü•ÈÅìËøô‰∫õÂÖ¨ÂºèÊú¨ÂèØ‰ª•‰ºòÈõÖÁöÑÂ±ïÁ§∫ÔºåÂç¥‰∏çËÉΩÂ§üÊó∂Ôºå‰∏çÂø´Â∞±ÂèòÊàê‰∫ÜÊÑ§ÊÄíÔºâ„ÄÇ&lt;/p>
&lt;p>Ê≤°ÊúâÂÉè‰Ωú‰∏∫Êñ∞ÊâãÊó∂Âú®‰∏çÂêåÁöÑ Linux ÂèëË°åÁâàË∑≥Êù•Ë∑≥ÂéªÈÇ£Ëà¨ÔºåÊàëÁü•ÈÅì vimwiki Â∞±ÊòØÊàëÁöÑËèúÔºå
‰ª•Ââç‰ΩøÁî® vimwiki ËøáÁ®ãÁ¢∞‰∏äÁöÑÂêÑÁßç‚ÄúÊöÇÊó∂‚ÄùÁöÑÊ≤°ÊúâËß£ÂÜ≥ÁöÑÈóÆÈ¢òÔºåÂºÄÂßãÂèòÂæó‰ª§‰∫∫‰∏çËÉΩÂøçÂèóÔºå
ÁâπÂà´ÊòØÂπ≥Êó∂ÊµèËßàÂÖ∂‰ªñÁΩëÈ°µÊó∂ÊØèÊØèÁúãÂà∞‰∫∫ÂÆ∂ÁöÑÁöÑÊï∞Â≠¶ÂÖ¨ÂºèÂèàÂ§öÂèàÈùìÔºåÈÇ£ÁÆÄÁõ¥Â∞±ÊòØ‰∏ÄÂú∫‰æÆËæ±„ÄÇ
ÊàëËµ∞‰∫ÜËøô‰πàËøúÔºåÂ∞±Â∑ÆÈÇ£‰πàÊúÄÂêé‰∏ÄÊ≠•ÔºåÊàëÂøÉ‰∏çÁîò„ÄÇÊâÄ‰ª•ÊàëÂ∞±ÊäòËÖæÔºå‰∏ÄÂ§©Ôºå‰∏§Â§©Ôºå‰∏âÂ§© &amp;hellip;
ÊàëÊòØË¥™Â©™ÁöÑ‰∫∫ÔºåÊó†Ê≥ïÊîæÂºÉ‰∏ÄÊ≠•‰πãÈÅ•ÁöÑËØ±ÊÉëÔºåÂπ∏ËøêÁöÑÊòØÊàëËøòÊòØÊäòËÖæÂà∞‰∫ÜÊàëÊÉ≥Ë¶ÅÁöÑ‚ÄúÊõ¥Â§ö‚Äù„ÄÇ&lt;/p>
&lt;p>ÊòØÁöÑÔºåË∑ùÁ¶ªÂ§™ËøúÂèØËÉΩÁõ¥Êé•ÊîæÂºÉÔºåËÄå‰∏ÄÊ≠•‰πãÈÅ•ÊúÄÊòØÈîÄÈ≠Ç üòÇ&lt;/p>
&lt;p>ÂÖ∂ÂÆûÔºåËØ•Â∫ÜÂπ∏ÁöÑÊòØÊàëÁ¢∞‰∏äÁöÑÈóÆÈ¢òÊó©Â∞±Êúâ‰∫∫ÂÖàÁ¢∞‰∏ä‰πüÂÖàËß£ÂÜ≥‰∫ÜÔºåÊàëÂè™ÊòØÊâæÂà∞‰∫ÜËøô‰∏™ÊñπÊ≥ïÔºå
Â∫îËØ•ËØ¥ÊòØ‚ÄòÊêúÁ¥¢‚ÄôÂà∞‰∫ÜËøô‰∏™ÊñπÊ≥ïÔºåÂπ∂‰Ωú‰∫ÜÂøÖË¶ÅÁöÑË∞ÉÊï¥„ÄÇ‚ÄòÈóªÈÅìÊúâÂÖàÂêé‚ÄôÁöÑÂü∫Á°ÄÂ∞±ÊòØÂàÜ‰∫´Ôºå
ÂØπ‰∫éËøô‰∫õËß£ÂÜ≥‰∫ÜÂÆûÈôÖÈóÆÈ¢òÁöÑ‰∫∫ÔºåÂæàÊòØÊÑüÊøÄ‰ªñ‰ª¨ÁöÑÂàÜ‰∫´ÔºåÊ≤°ÊúâÂàÜ‰∫´ÔºåÂ§ßÂÆ∂ÁöÑÊó•Â≠ê‰∏ç‰ºöÂ•ΩËøá„ÄÇ&lt;/p></description></item><item><title>Âà©Âô®3: Jupyterlab and its Kernels</title><link>/posts/thecli/2022-07-22-jupyterwithdskernels/</link><pubDate>Fri, 22 Jul 2022 22:01:51 +0800</pubDate><guid>/posts/thecli/2022-07-22-jupyterwithdskernels/</guid><description>&lt;p>&lt;img alt="IMG_jupyterlab" src="images/jupyter_kernels.png">&lt;/p>
&lt;p>New machine needs to be tailored for a better personal workflow. And this is
about how &lt;a href="https://jupyter.org/">jupyter&lt;/a> can be a multi-language support
co-work platform for regular DataScience works. Also note that Julia community
has &lt;a href="https://github.com/fonsp/Pluto.jl">pluto&lt;/a> notebook, and R has its
&lt;a href="https://www.rstudio.com/">RStudio&lt;/a>.&lt;/p>
&lt;h1 id="jupyter-julia--python--r-all-in-one" >
&lt;div>
&lt;a href="#jupyter-julia--python--r-all-in-one">
##
&lt;/a>
JuPyteR: Julia + Python + R all in one
&lt;/div>
&lt;/h1>
&lt;p>1 check the Python came with system&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ which python3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If there is no python3 yet, install one:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ sudo apt install python3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this would install the latest version of Python.&lt;/p>
&lt;p>2 Poetry or Virtualenv (Poetry is way tooooo slow 2021-12)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># create virtual envs:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python3 -m venv dsvenv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># install jupyterlab:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./dsvenv/bin/python -m pip install jupyterlab
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this may create a venv without &lt;code>activate&lt;/code> scripts.
If this is the case, check &lt;a href="https://stackoverflow.com/questions/26215790/venv-doesnt-create-activate-script-python3">more discussion
here&lt;/a>&lt;/p>
&lt;p>3 &lt;a href="https://mirrors.tuna.tsinghua.edu.cn/CRAN/">install R&lt;/a>, then&lt;/p>
&lt;p>4 &lt;a href="https://julialang.org/downloads/platform/#linux_and_freebsd">install Julia&lt;/a>&lt;/p>
&lt;p>5 activate python&amp;rsquo;s dsvenv (source ./dsvenv/bin/activate), then&lt;/p>
&lt;p>a. Enter interactive R (REPL environment)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-R" data-lang="R">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># install &amp;#39;IRkernel&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#57c7ff">install.packages&lt;/span>(&lt;span style="color:#5af78e">&amp;#39;IRkernel&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># register the kernel in the current R installation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IRkernel&lt;span style="color:#ff6ac1">::&lt;/span>&lt;span style="color:#57c7ff">installspec&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># exit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#57c7ff">q&lt;/span>()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>b. Enter interactive Julia (./path-to/julia.1.6.5/bin/julia)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Julia" data-lang="Julia">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">using&lt;/span> Pkg
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Pkg&lt;span style="color:#ff6ac1">.&lt;/span>add(&lt;span style="color:#5af78e">&amp;#34;IJulia&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Optional: build IJulia&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Pkg&lt;span style="color:#ff6ac1">.&lt;/span>build(&lt;span style="color:#5af78e">&amp;#34;IJulia&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that if the path of binary of Julia somehow changed (reinstall etc),
one need to perform extra command &lt;code>Pkg.build(&amp;quot;IJulia&amp;quot;)&lt;/code> which tells jupyter
where to find the binary of Julia.&lt;/p>
&lt;p>6 launch jupyterlab server, and that&amp;rsquo;s it!&lt;/p></description></item><item><title>Âà©Âô®2: Vimwiki ‰πã Â¶Ç‰ΩïÁî® vimwiki ËÆ∞Á¨îËÆ∞</title><link>/posts/thecli/thevim/2022-07-15-vimwiki/</link><pubDate>Thu, 21 Jul 2022 23:37:31 +0800</pubDate><guid>/posts/thecli/thevim/2022-07-15-vimwiki/</guid><description>&lt;h2 id="‰∏™‰∫∫Áª¥Âü∫-personal-wiki" >
&lt;div>
&lt;a href="#%e4%b8%aa%e4%ba%ba%e7%bb%b4%e5%9f%ba-personal-wiki">
#
&lt;/a>
‰∏™‰∫∫Áª¥Âü∫ (personal wiki)
&lt;/div>
&lt;/h2>
&lt;p>ÈòÖËØªËøá„ÄäÁà±‰∏Ω‰∏ùÊº´Ê∏∏Â•áÂ¢É„ÄãÁöÑÂ∞è‰ºô‰º¥Â∫îËØ•‰ºöËßâÂæóÁÜüÊÇâÔºåÁª¥Âü∫Â∞±ÂÉèÈÇ£‰∏™‚ÄúÂÖîÂ≠êÊ¥û‚ÄùÔºå
ÊØè‰∏™ËØçÊù°ÈÉΩÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÈÄöÂæÄÂÖ∂‰ªñÂ•áÂ¢ÉÁöÑÂ§ßÈó®Ôºå‰ΩÜÂÆÉÂè™ÂØπÊúâÂ•ΩÂ•áÂøÉÁöÑ‰∫∫ÊâçÊâìÂºÄ„ÄÇ
ÔºàÊàñËÄÖËØ¥ÊúâÂ•ΩÂ•áÂøÉÁöÑ‰∫∫Êâç‰ºöÊâìÂºÄÂÆÉÔºåÂ∞ºÂ••Ë¶ÅÊòØÊ≤°ÊúâÈÄâÊã©Á∫¢Ëâ≤üíäÔºåÊïÖ‰∫ãÊÅêÊÄïÂ∞±Ê≤°Ë∂£ÂæóÂ§öÔºâ„ÄÇ
ËøôÊòØÂú®Áª¥Âü∫‰∏äÈòÖËØªÊó∂Â∏∏ÊúâÁöÑÊÑüÂèó„ÄÇÔºàÂÖ∂‰ªñÁΩëÁ´ôÂæàÂ§öÊó∂ÂÄôÂàôÊêûÁöÑÊõ¥ÂÉèÊòØ‚ÄúÁõò‰∏ùÊ¥û‚Äù„ÄÇÔºâ&lt;/p>
&lt;p>ÊØè‰∏™‰∫∫ÈÉΩÊúâÂàõ‰ΩúÁöÑÂÜ≤Âä®„ÄÇ
ÁÑ∂ËÄå‰∫ãÂÆûÂèØËÉΩÊõ¥ÁÆÄÂçï:Âπ≥Êó∂ËØª‰π¶Â≠¶‰π†Â∑•‰ΩúÊó∂ÂñúÊ¨¢‰∏úÂÜôË•øÂàíÊÄª‰ºöÁïô‰∏ã‰∏Ä‰∫õ‚ÄòÁ¨îËÆ∞‚Äô„ÄÇ
ÂØπÂà´‰∫∫Êù•ËØ¥Â∞±ÊòØ‰∫õËÉ°Ê∂Ç‰π±ÁîªÔºåÂèØËá™Â∑±ÊÄªËßâÂæó‚ÄúÁèçË¥µ‚Äù &amp;ndash; ‚ÄúÂ®ò‰∏çÂ´åÂÑø‰∏ë‚ÄùËØ¥ÁöÑÂ§ßÊ¶ÇÂ∞±ÊòØËøô‰∏™„ÄÇ
ÊÖ¢ÊÖ¢Âú∞ÔºåÁßØÁ¥ØÁöÑÂÜÖÂÆπË∂äÊù•Ë∂äÂ§öÔºåÊ∂âÂèäÁöÑËåÉÂõ¥Ë∂äÊù•Ë∂äÂπøÔºå‰πüÂ∞±ÊòæÂæóË∂äÊù•Ë∂äÈõ∂Á¢é„ÄÇ
ÔºàÂóØÔºå‰∏∫‰ªÄ‰πà‰∏çÂú®ÂºÄÂßãÁöÑÊó∂ÂÄôÂ∞±ËßÑÂàíÂ•ΩÂë¢ÔºüÁà∏Â¶àÈÄöÂ∏∏ÈÉΩÊòØÊèêËøôÁßçÂ•ΩÈóÆÈ¢òÁöÑÈ´òÊâã)„ÄÇ
‰∏çËøáËøôÂú®ÊàëÁúãÊù•Êõ¥ÂÉè‰∏ÄÁßç‚ÄúÈ©¨ÂêéÁÇÆ‚ÄùÂºèÁöÑË¥£ÈóÆÔºåÊÄªÊúâ‰∏Ä‰∏¢‰∏¢‚Äú‰ººÊòØËÄåÈùû‚ÄùÁöÑÊÑüËßâ„ÄÇ
Â≠¶‰π†ÁöÑËøáÁ®ãÊõ¥ÂÉè‰∏ÄÁßçÊé¢Á¥¢ËøáÁ®ãÔºåÂØπÂ§ßÈÉ®ÂàÜ‰∫∫Êù•ËØ¥Ôºå‰∫ãÂÖàÊÅêÊÄïÈÉΩÊó†Ê≥ïÊ∏ÖÊô∞Âú∞Áü•ÈÅì,
‰π¶Â±±ÁöÑË∑ØÈÄöÂêë‰ΩïÂ§ÑÔºåÂ≠¶Êµ∑ÁöÑËàüÂºï‰Ω†Ëµ∞ÂêëÂì™Èáå„ÄÇÂØπ‰∫é‰∏çËÉΩÈ¢ÑÂÖàÁü•ÈÅìÁªÜËäÇÁöÑÂÜÖÂÆπÔºå
ÈÄöÂ∏∏‰ºöÂú®ÂºÄÂßãÁöÑÊó∂ÂÄôÂÜ†‰ª•‚ÄúÊàëÁöÑÁ¨îËÆ∞‚ÄùËøô‰πà‰∏Ä‰∏™ÁõÆÂΩïÔºåÁÑ∂Âêé‰∏ãËÆæ‰∏Ä‰∫õÁ≤óÁï•ÁöÑÂ≠¶ÁßëÂêçÁß∞ÁõÆÂΩïÔºå
Áõ¥Âà∞‰∏çÂÜçÊª°ÊÑèËøôÊ†∑ÁöÑÂÆâÊéí„ÄÇÊØ´Êó†ÁñëÈóÆÔºå‰∏ã‰∏ÄÊ≠•Â∞±ÊòØË¶ÅÁî®‰∏ÄÁßçÂêàÈÄÇÁöÑÊñπÂºèÂ∞ÜÂÆÉ‰ª¨ÁªÑÁªáËµ∑Êù•„ÄÇ&lt;/p>
&lt;p>Áõ∏ÂØπÁã¨Á´ãÁöÑ‰∏ªÈ¢òËá™Êàê‰ΩìÁ≥ªÔºå‰∏çÂêå‰∏ªÈ¢ò‰πãÈó¥ÂÖ≥ËÅîÁöÑÂÜÖÂÆπÂèØ‰ª•ËøûÊé•Ëµ∑Êù•Ôºå
‰∏ç‰ªÖÂú®ÈòÖËØªÊó∂ÂèØ‰ª•‰∫´ÂèóËøôÁßç‰æøÊç∑ÁöÑËá™Áî±Ë∑≥ËΩ¨ÔºåÊúÄÂ•ΩÂú®ËÆæËÆ°ÂíåÁºñÂÜôÊó∂‰πüËÉΩÂ¶ÇÊ≠§„ÄÇ
ÂØπÊàëËÄåË®ÄÔºåÁ≠îÊ°àÂ∞±ÊòØ &lt;a href="https://github.com/vimwiki/vimwiki">vimwiki&lt;/a>.&lt;/p>
&lt;p>ÈòÖËØªÁöÑÊó∂ÂÄôËá™Áî±Ë∑≥ËΩ¨ÔºåËøô‰∏™Â•ΩÁêÜËß£ÔºåÂèØÁºñÂÜôÂÜÖÂÆπ‰∏çÈÉΩ‰∏ÄÁØá‰∏ÄÁØáÁöÑÊù•ÂêóÔºü
ÁºñÂÜôÁöÑÊó∂ÂÄô‰πü‚ÄúËÉ°Â¥©‰π±Ë∑≥‚ÄùÁöÑÔºåÂê¨Ëµ∑Êù•ÂèØ‰∏çÊÄé‰πàÁæéÂ¶ôÔºåÊÄï‰∏çÊòØÊúâÊØõÁóÖÔºü‰∏çÔºå‰∏çË¶ÅËØØ‰ºöÔºå
ËøôÂÖ∂ÂÆûÊòØÊåáÔºö1ÔºâÂêåÊó∂Âú®Â§ö‰∏™Á™óÊ†º(Pane)ÊâìÂºÄÂ§ö‰∏™ÊñáÊ°£Ôºõ2ÔºâÂú®Á™óÊ†º‰πãÈó¥ËøõË°å‚ÄúÈó™ËΩ¨ËÖæÊå™‚Äù„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûúÊòØÁºñÂÜôËøáÊØîËæÉÂ§çÊùÇÁöÑÊ®°ÂùóÁöÑÂ∞è‰ºô‰º¥ÂèØËÉΩ‰ºöÊõ¥Ê∑±ÊúâÊÑüËß¶Ôºå
ËøôÁßç‰∏çÂêåÊñáÊ°£‰πãÈó¥ÁöÑ‚ÄúÊóãËΩ¨Ë∑≥Ë∑ÉÊàëÂºÄÂ§©Áúº‚ÄùÁöÑË¶ÅÊ±ÇÂÖ∂ÂÆûÂπ∂‰∏çËøáÂàÜ,
Âè™ÊÅ®Ëá™Â∑±ÁöÑÂ±èÂπï‰∏çÂ§üÂ§ö‰∏çÂ§üÂ§ßÊîæ‰∏ç‰∏ãÈÇ£ÊÆµÊÆµ‰ºòÈõÖÁöÑ‰ª£Á†Å
ÔºàÊçÆÂèØÈù†ÁöÑË∞£Ë®ÄÁß∞ÔºåÊúâ‰∫õÁ®ãÂ∫èÁåø‰∏ÄÂ§©80%ÁöÑÊó∂Èó¥ÊòØÂú®Ê¨£ËµèËá™Â∑±ÂÜôÁöÑ‰ª£Á†Å‰∏≠Â∫¶ËøáÔºâ„ÄÇ&lt;/p>
&lt;p>ÊúÄÂÖ∏ÂûãÁöÑËé´Ëøá‰∫é‰ªéÂáΩÊï∞Ë∞ÉÁî®ÁöÑÂú∞ÊñπÔºåË∑≥ËΩ¨Âà∞Âú®ÈÇ£ÈÅ•ËøúÁöÑÂú∞ÊñπÁöÑÂáΩÊï∞ÂÆö‰πâÊ®°ÂùóÔºå
Â¶ÇÊûúËøô‰∏§‰∏™Âú∞ÊñπÁöÑÂÖ≥Á≥ªÈöîÁùÄ‰∏â‰∫î‰∏™ÊàñËÄÖÊõ¥Â§öÁöÑÊ®°ÂùóÁöÑÊó∂ÂÄôÔºå(ÂèØËÉΩ‰πüÊòØÈúÄË¶ÅÈáçÊûÑÁöÑ‰ø°Âè∑)
ÈÇ£‰πàÔºåÂ¶ÇÊûú‰∏çËÉΩÂêåÊó∂Â∞ÜËøôÂá†‰∏™‚ÄòÂØÜÊé•‚ÄôÊñáÊ°£Èì∫Âú®ÁúºÂâçÔºåËÄåÊòØ‰∏Ä‰∏™ÁúãÂÆåÂÜçÁúãÂè¶‰∏Ä‰∏™Ôºå
ÊúãÂèãÔºåÊàë‰∏çÁü•ÈÅì‰Ω†ÁöÑÊÉÖÂÜµÂ¶Ç‰ΩïÔºåÂèçÊ≠£ÊàëÊòØÂæàÂÆπÊòìËø∑Â§±Âú®ËøôËåÇÂØÜÁöÑ‰ª£Á†Å‰∏õÊûóÈáåÔºåÊôïÂ§¥ËΩ¨Âêë„ÄÇ&lt;/p>
&lt;p>ÊÄªËÄåË®Ä‰πãÔºå
Ê†∏ÂøÉÁöÑÈóÆÈ¢òÂú®‰∫éÂ¶Ç‰ΩïÁªÑÁªáËøô‰∫õÊñáÊ°£Ôºå‰ΩøÂæóËá™Â∑±Âú®ÊûÑÂª∫‰∏™‰∫∫‚ÄòÁü•ËØÜÂ∫ì‚ÄôÁöÑËøáÁ®ãÊõ¥‰∏ùÊªëÁïÖÂø´„ÄÇ&lt;/p>
&lt;p>Áª¥Âü∫ÁöÑÁâπÁÇπÂ∞±ÊòØ‚Äú‰Ω†‰∏≠ÊúâÊàëÔºåÊàë‰∏≠Êúâ‰ªñ‚ÄùÔºåÊØîÂ¶ÇËØ¥Ôºå
Âú®ÁªÑÁªáÂíåÊûÑÂª∫ÂÖ≥‰∫é‚ÄúÁ∫øÊÄßÂõûÂΩí‚ÄùÂÜÖÂÆπÔºåÊàëÂ∏åÊúõÊúâ‰∏Ä‰∏™‚ÄúÂÖ•Âè£‚ÄùÊñáÊ°£ÔºåÁß∞‰πã‰∏∫‰∏ªÈ¢òÁõÆÂΩïÔºå
Ëøô‰∏™ÊñáÊ°£Âè™ÂØπË¶ÅÂÜôÁöÑÁ´†ËäÇÂÜÖÂÆπËøõË°åÁΩóÂàóÂíåÁÆÄ‰ªãÔºåÊõ¥ÂÉè‰∏ÄÁßç‚ÄúÈ°∂Á∫ßËèúÂçï‚Äù (top level manu) Ôºå
Ëøô‰∏™ËèúÂçïÂåÖÂê´‰∫õ‰ªÄ‰πàËèúÂë¢ÔºüÁÆÄ‰ªãÂ§ßÊ¶ÇÊòØËøôÊ†∑ÁöÑÔºö&lt;/p>
&lt;ol>
&lt;li>ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>ÊñúÁéáÂíåÊà™Ë∑ù&lt;/li>
&lt;li>ÊúÄÂ∞è‰∫å‰πòÊ≥ï&lt;/li>
&lt;li>ÂèÇÊï∞ËØÑ‰º∞ÊúâÂ§öÂáÜ
&lt;ul>
&lt;li>ÁΩÆ‰ø°Â∫¶Âå∫Èó¥&lt;/li>
&lt;li>ÂÅáËÆæÊ£ÄÈ™å&lt;/li>
&lt;li>PÂÄº: ÊúâÂ§öÂ∞ëÁà±ÂèØ‰ª•ÈáçÊù•&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Â§öÂÖÉÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>Ëá™ÂèòÈáèÂíåÂõ†ÂèòÈáèÂà∞Â∫ïÊúâÊ≤°ÊúâÂÖ≥Á≥ª&lt;/li>
&lt;li>Âõ†ÂèòÈáèÂíåÂì™‰∫õËá™ÂèòÈáèÊúâ‰ªÄ‰πàÂÖ≥Á≥ª&lt;/li>
&lt;li>Ê®°ÂûãÊãüÂêà
&lt;ul>
&lt;li>ÊÆãÂ∑Æ(RSE)&lt;/li>
&lt;li>$R^2$ ÁªüËÆ°Èáè&lt;/li>
&lt;li>Adjusted $R^2$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ë¥ùÂè∂ÊñØÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>ÊòØÂÖàÈ™åËøòÊòØÂÅèËßÅÔºü
&lt;ul>
&lt;li>ÊàëËÆ§‰∏∫ $\Theta$ ‰∏çÂè™‰∏Ä‰∏™&lt;/li>
&lt;li>Ë¥ùÂè∂ÊñØÂÆöÂæã&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ÂêéÈ™åÊ¶ÇÁéá‰º∞ËÆ°
&lt;ul>
&lt;li>MCMC&lt;/li>
&lt;li>ÂêâÂ∏ÉÊñØÈááÊ†∑&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Âπø‰πâÁ∫øÊÄßÂõûÂΩí
&lt;ul>
&lt;li>ÊåáÊï∞ÊóèÂàÜÂ∏ÉÔºö‰∏çÂè™ÊúâÊ≠£ÊÄÅÂàÜÂ∏É&lt;/li>
&lt;li>ÊúÄÂ§ßÁÜµÔºöÊàëÊù•Êïô‰Ω†ÊåëÈÄâÂàÜÂ∏É&lt;/li>
&lt;li>ÈìæÊé•ÂáΩÊï∞ÔºöÁ∫øÊÄßÁ©∫Èó¥Âà∞ÈùûÁ∫øÊÄßÁ©∫Èó¥ÁöÑÊò†Â∞Ñ&lt;/li>
&lt;li>Ê±âÂØÜÂ∞îÈ°øËíôÁâπÂç°Ê¥õ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>ËøôÂ∞±ÊòØÂÖ≥‰∫é‚ÄúÁ∫øÊÄßÂõûÂΩí‚ÄùÂÜÖÂÆπÁöÑ‰∏ªÈ¢òÁõÆÂΩïÔºå
ÂàóÂá∫Êù•ÁöÑÊØè‰∏™ÁÇπÈÉΩÂèØ‰ª•ËÆæÁΩÆ‰∏∫ÈìæÊé•ÁÇπÔºà‰ΩÜ‰∏çÊòØÂøÖÈ°ªËøô‰πàÂÅöÔºâÔºå
‰ªéËøô‰∏™‰∏ªÈ¢òÁõÆÂΩïÔºåÂèØ‰ª•ÂéªÂà∞‰ªª‰ΩïÁöÑÂçï‰∏™ÊñáÊ°£ÔºåÂú®ÊØè‰∏™ÊñáÊ°£ÈáåÔºå
ÈÉΩÂèØ‰ª•ËÆæÁΩÆÊåáÂêëÂÖ∂‰ªñÊñáÊ°£ÁöÑÈìæÊé•ÔºåÊàñËÄÖÊåáÂêëÁΩëÁªúËµÑÊ∫êÁöÑÈìæÊé•Ôºà‰∫ã‰∫ã‰∫≤‰∏∫Âπ∂ÈùûÂøÖË¶ÅÔºâ„ÄÇ&lt;/p>
&lt;p>Â§öÂπ¥‰ª•Êù•ÊàëÂ∑≤ÁªèÊàñÂ§öÊàñÂ∞ëÂú∞ÂÅö‰∫Ü‰∏Ä‰∫õÁ´†ËäÇÂÜÖÂÆπÁöÑÁ¨îËÆ∞ÔºåÁº∫ÁöÑÊ≠£ÊòØËøôÊ†∑ÁöÑ‰∏Ä‰∏™‰∏ªÈ¢òÁõÆÂΩï„ÄÇ&lt;/p>
&lt;p>Ëá≥‰∫éÁºñËæëÂÜÖÂÆπÁöÑÊó∂ÂÄôË¶Å‰∏çË¶ÅÂú®‰∏ÄÂùóÂ±èÂπï‰∏ä‚ÄúÂêåÊó∂‚ÄùÂπ∂ÂàóÂ±ïÂºÄÂ§ö‰∏™ÊñáÊ°£ÔºåÂπ∂‚ÄúË∑≥Êù•Ë∑≥Âéª‚ÄùÔºå
ÂèØ‰ª•ËØ¥ËøôÂè™ÊòØ‰∏Ä‰∫õ‰∫∫ÁöÑÁâπÊÆäÁôñÂ•ΩÔºåÂπ∂‰∏çÊòØÈùûÂ¶ÇÊ≠§‰∏çÂèØ„ÄÇ
ÊØîÂ¶ÇÔºå24ÂØ∏Â±èÂπï‰∏äÂ¶ÇÊûúÂè™ÊâìÂºÄ‰∏Ä‰∏™ÊñáÊ°£ÔºåÊàëËßâÂæóÊòØÂØπÁ©∫Èó¥ÁöÑÊûÅÂ§ßÊµ™Ë¥πÔºõ
‰ΩÜË¶ÅÁü•ÈÅìÔºåÂ¶ÇÊûúÂú®13ÂØ∏ÁöÑÂ±èÂπï‰∏äÔºåÊúâÊó∂Êàë‰πüËøõÂÖ• &lt;a href="https://github.com/junegunn/goyo.vim">goyo
Ê®°Âºè&lt;/a> „ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†ÁÇπÂºÄÂâçÊñá vimwiki ÁöÑÈìæÊé•ÔºåÈÇ£‰Ω†Â∫îËØ•Áü•ÈÅìÂÆÉÊòØ Vim ÁöÑ‰∏Ä‰∏™Êèí‰ª∂„ÄÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰Ω†Â∞ö‰∏çÊ∏ÖÊ•ö Vim ÊòØÂπ≤‰ªÄ‰πàÁöÑÔºåÊàñËÄÖ‰ªÖ‰ªÖÂê¨ËØ¥ Vim ÁöÑÂ≠¶‰π†Êõ≤Á∫øÊòØ‚ÄúÂá∫‰∫ÜÂêçÁöÑÂêì‰∫∫‚ÄùÔºå
ËøôÊòØ‰∏çÂØπÁöÑÔºåËá≥Â∞ë‰Ω†ÂèØ‰ª•ÁúãÂà∞Ôºö&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;img alt="learning_curves" src="https://fgg100y.github.io/img/posts/vimwiki_intro/editors_learning_curves.jpg">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">&lt;em>some text editors&amp;rsquo; learning curve&lt;/em>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Emacs ÊâçÊòØÊó†ÂèØ‰∫âËÆÆÁöÑ NO.1 üòÇ&lt;/p>
&lt;p>Â¶ÇÊûú‰∏çÂπ∏ÁöÑÊòØËá≥‰ªäÊÇ®‰ªçÊóßÂè™Âú®‰∏çÂπ∏ÁöÑÊìç‰ΩúÁ≥ªÁªüËøáÊ¥ªÔºåüëâ ÊòØÁöÑÔºåÊàëÊåáÁöÑÂ∞±ÊòØÂ∑®Á°¨Á≥ªÁªü„ÄÇ
‰∏çÂ¶®Âéª&lt;a href="https://www.vim.org/download.php">‰∏ãËΩΩ Gvim&lt;/a>Êù•Â∞ùËØïÂà´Ê†∑ÁöÑÁºñËæëÁöÑÊªãÂë≥Ôºå&lt;/p>
&lt;p>ÊàñËÄÖÊÇ®ÊòØÂàöÂàöÂºÄÂêØ Linux Âπ∏Á¶è‰πãË∑ØÁöÑ‰∫∫ÔºåÊàñËÄÖÊÉ≥ËØï‰∏ÄËØï Vim Ôºå
ÂæàÁÆÄÂçïÔºåÁõ¥Êé•‰∏äÊâãÁ±ªÂûãÁöÑÔºåÂú® Terminal ‰∏≠ËæìÂÖ• &lt;code>vimtutor&lt;/code>ÔºåÈÄÄÂá∫ËæìÂÖ• &lt;code>:q&lt;/code>&lt;/p>
&lt;p>ÊúÄÂ•ΩÁöÑ vim Â≠¶‰π†ÊùêÊñô‰ºöÂú®ËæìÂÖ• &lt;code>:help&lt;/code> ‰πãÂêéÂá∫Áé∞Ôºà‰ΩÜ‰∏ç‰∏ÄÂÆöÊòØÊúÄÂ•ΩÁöÑÂÖ•Èó®ÊùêÊñôÔºâ„ÄÇ
ÊàñËÆ∏‰Ω†‰ºöÊÉ≥ÁúãÁúã‰∏Ä‰∫õÂÖ•Èó®ÁöÑÊùêÊñôÔºåÊØîÂ¶Ç &lt;a href="http://vimcasts.org/">vimcast&lt;/a> ËßÜÈ¢ëÔºå
ÊØîÂ¶ÇÔºåÂπ¥‰ª£Áï•ÂæÆ‰πÖËøúÁöÑ &lt;a href="https://vim.swaroopch.com/">A Byte of Vim&lt;/a>Ôºå
ÂÜÖÂÆπÂèäÂÖ∂ËØ¶Â∞ΩÁöÑ &lt;a href="http://www.oualline.com/vim-book.html">Vim Tutorial and Reference&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Last update: 2024-07-04 Thu&lt;/p></description></item><item><title>Linear Regression: frequentist and bayesian</title><link>/posts/regressionmodels/linear_regression/</link><pubDate>Wed, 25 Aug 2021 13:49:40 +0800</pubDate><guid>/posts/regressionmodels/linear_regression/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>**Important Note**:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Almost all the contents (text, images) are came from these great books and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>online resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Statistics, by David Freeman, Robert Pisani, and Roger Perves
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* ÁªüËÆ°Â≠¶, David Freeman *et.al* ËëóÔºåÈ≠èÂÆóËàí Á≠âËØëÔºå‰∏≠ÂõΩÁªüËÆ°Âá∫ÁâàÁ§æ
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Êú∫Âô®Â≠¶‰π†, Âë®ÂøóÂçé Ëëó (Â§ßÂêçÈºéÈºéÁöÑ‚ÄòË•øÁìú‰π¶‚Äô)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* An Introduction to Statistical Learning, by Gareth James, Daniela Witten,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Trevor Hastie, and Robert Tibshirani
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Deep Learning, a.k.a, the flower book, by Ian Goodfellow, Yoshua Bengio, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Aaron Courville
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Introduction to Machine Learning, Barnabas Poczos, Aarti Singh, CMU-10701
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Bayesian Methods, Nicholas Ruozzi, UT-DALLAS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 style="text-align: center;">Linear Regression&lt;/h1>
&lt;h2 id="part-0-regression-101" >
&lt;div>
&lt;a href="#part-0-regression-101">
#
&lt;/a>
Part-0: Regression 101
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>$\text{You&amp;rsquo;ve got to draw the line somewhere.}$&lt;/p>
&lt;/blockquote>
&lt;h3 id="introduction" >
&lt;div>
&lt;a href="#introduction">
##
&lt;/a>
Introduction
&lt;/div>
&lt;/h3>
&lt;p>The regression method describes how one variable depends on another. For example, take height and weight. Naturally, the taller men weighed more. How much of an increase in weight is associated with a unit increase in height? To get started, look at the scatter diagram (figure 1 on below). Height is plotted on the horizontal axis, and weight on the vertical. The summary statistics are&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches,\qquad SD \approx 3 \ inches$&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches, \qquad SD \approx 45 \ pounds, \qquad r \approx 0.40$&lt;/p>
&lt;p>The scales on the vertical and horizontal axes have been chosen so that one SD of height and one SD of weight cover the same distance on the page. This makes the SD line (dashed) rise at 45 degreed across the page. There is a fair amount of scatter around the line: $r$ is only 0.40.&lt;/p>
&lt;p>&lt;img alt="IMG: regression_line" src="images/stats_weight_height.png">&lt;/p>
&lt;p>Figure 1. Scatter diagram. Each point shows the height and weight for one of 471 men age 18-24 in a dataset. The vertical strip represents men who are about one SD above average in height. Those who are also one SD above average in weight would be plotted along the dashed SD line. Most of the men in the strip are below the SD line: they are only part of an SD above average in weight. The &lt;strong>solid&lt;/strong> regression line estimates average weight at each height.&lt;/p>
&lt;p>The vertical strip in figure 1 shows the men who were one SD above average in height were quite a bit less than one SD above average in weight. This is where the correlation of 0.40 comes in. Associated with an increase of one SD in height there is an increase of &lt;strong>only 0.40&lt;/strong> SDs in weight, on the average.&lt;/p>
&lt;p>To be more specific, take the men who are one SD above average in height:
$$
\text{average height} + SD\ \text{of height} = 70 \ in + 3 \ in = 73 \ in
$$
Their average weight will be above the overall average by $0.40 \times 45 \ lb = 18 \ lb$.&lt;/p>
&lt;p>So, the average weight of these men is around
$$
\text{average weight} + 0.40 \times (SD\ \text{of weight}) = 180 \ lb + 18 \ lb = 198 \ lb
$$
The point (73 inches, 198 pounds) is marked by a cross in figure 1 ( and the points that are 2SD above (76 in, 216 lb) and below (64 in, 144 lb) the average of height as well). All the points (height, estimate for average weight) fall on the solid line shown in figure 1. This is the &lt;em>regression line&lt;/em>. The line goes through the point of averages: men of average height should also be of average weight.&lt;/p>
&lt;p style="text-align:center;color:blue;">
The regression line estimates the average value for y corresponding to each value of x.
&lt;/p>
&lt;p>Along the regression line, associated with each increase of one SD in height there is an increase of only 0.40 SDs in weight. Remember where the 0.40 comes from. It is the correlation between height and weight. NOTE that: Two different SDs are involved here: the SD of $x$, to gauge change in $x$; and the SD of $y$, to gauge changes in $y$.&lt;/p>
&lt;p>This way of using the correlation coefficient to estimate the average value of $y$ for each value of $x$ is called the &lt;strong>regression method&lt;/strong>. The method can be stated as follows.
$$
\fbox{Associated with each increase of one SD in x there is an increase of only r SDs in y, on the average.}
$$&lt;/p>
&lt;blockquote>
&lt;p>Correlation: Like father, like son.&lt;/p>
&lt;blockquote>
&lt;p>If there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other.&lt;/p>
&lt;/blockquote>
&lt;p>The correlation coefficient is a measure of linear association, or clustering around a line. The relationship between two variables can be summarized by&lt;/p>
&lt;ul>
&lt;li>the average of the $x$-values, the SD of the $x$-values,&lt;/li>
&lt;li>the average of the $y$-values, the SD of the $y$-values,&lt;/li>
&lt;li>the correlation coefficient $r$.&lt;/li>
&lt;/ul>
&lt;p>Computing the correlation coefficient&lt;/p>
&lt;p>Here is the procedure for computing the correlation coefficient.
$$
\boxed{\text{Convert each variable to standard units.} \\
\text{The average of the products gives the correlation coefficient.}}
$$&lt;/p>
&lt;p>$$
\fbox{Convert each variable to standard units.
The average of the products gives the correlation coefficient.}
$$&lt;/p>
&lt;p>Recall that &amp;ldquo;Convert each variable to standard units&amp;rdquo; means &lt;strong>standardization&lt;/strong>. Let $x = {1, 3, 4, 5, 7 }, y = {5, 9, 7, 1, 13 } $ be vectors of variables, the mean is given by
$$
\mu = \frac{1}{|a|} \sum_i a_i
$$
And the SD is the &amp;ldquo;r.m.s size of the deviation from the average&amp;rdquo;, can be given by
$$
\sigma = \sqrt{\frac{1}{|a|} \sum_i (a_i - \mu)^2}
$$
where $|a|$ is the number of data points, $a_i$ is the $i$-th data point in the data set.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Table 1. Computing $r$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">x&lt;/th>
&lt;th style="text-align:center">y&lt;/th>
&lt;th style="text-align:center">x in standard units&lt;/th>
&lt;th style="text-align:center">y in standard units&lt;/th>
&lt;th style="text-align:center">Product&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">-1.5&lt;/td>
&lt;td style="text-align:center">-0.5&lt;/td>
&lt;td style="text-align:center">0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">9&lt;/td>
&lt;td style="text-align:center">-0.5&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;td style="text-align:center">-0.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:center">7&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;td style="text-align:center">0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;td style="text-align:center">-1.5&lt;/td>
&lt;td style="text-align:center">-0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">7&lt;/td>
&lt;td style="text-align:center">13&lt;/td>
&lt;td style="text-align:center">1.5&lt;/td>
&lt;td style="text-align:center">1.5&lt;/td>
&lt;td style="text-align:center">2.25&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>$$
\begin{eqnarray}
r
&amp;amp;=&amp;amp; \text{average of [(x in standard units) times (x in standard units)]} \\
\\
&amp;amp;=&amp;amp; \frac{0.75 - 0.25 + 0.00 -0.75 + 2.25}{5} = 0.40
\end{eqnarray}
$$&lt;/p>
&lt;p>This complete the solution.&lt;/p>
&lt;/blockquote>
&lt;h3 id="slope-and-intercept" >
&lt;div>
&lt;a href="#slope-and-intercept">
##
&lt;/a>
Slope and Intercept
&lt;/div>
&lt;/h3>
&lt;p>Does education pay? Figure 1 shows the relationship between income and education, for a sample of 562 men age 25-29 in 2005. The summary statistics are
$$
\begin{eqnarray}
\text{average education} &amp;amp;\approx&amp;amp; 12.5\ \text{years}, &amp;amp;\qquad&amp;amp; SD \approx 3\ \text{years} \\
\text{average income} &amp;amp;\approx&amp;amp; $30000,\ &amp;amp;\qquad&amp;amp; SD \approx $24000, \qquad r \approx 0.25
\end{eqnarray}
$$
The regression estimates for average income at each educational level fall along the regression line shown in the figure. The line slopes up, showing that on the average, income does go up with education.&lt;/p>
&lt;p>&lt;img alt="Do education pay" src="./images/stats_education_income.png">&lt;/p>
&lt;p>Any line can be described in terms of its slope and intercept. The y-intercept is the height of the line when $x$ is $0$. And the slope is the rate at which $y$ increases, per unit increase in $x$. Slope and intercept are illustrated in figure 2.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept" src="images/stats_slope_intercept.png">&lt;/p>
&lt;p>&lt;strong>How do you get the slope of the regression line?&lt;/strong> Take the income-education example.
Associated with an increase of one SD in education, there is an increase of $r$ SDs in
income. On this basis, 3 extra years (one SD) of education are worth an extra
$r \times SD = 0.25 \times 24000 = 6000$ of income, on the average. So each extra year in
worth \$6000 / 3 = \$2000. The slope of the regression line is \$2000 per year.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept2" src="images/stats_slope_intercept2.png">&lt;/p>
&lt;p>The intercept of the regression line is the height when $x = 0$, corresponding to men with $0$ years of education. There men are 12.5 years below average in education.
Each year costs \$2000 -- that is what the slope says. A man with no education should have an income which is below average by
$$
12.5\ \text{years} \times 2000\ \text{per year} = 25000.
$$
His income should be $\$30000 - \$25000 = \$5000$. That is the intercept (figure 3): the predicted value of $y$ when $x = 0$.&lt;/p>
&lt;hr>
&lt;p style="text-align:justify;color:blue;">
Associated with a unit increase in x there is some average change in y. The slope of the regression line estimates this change. The formula for the slope is
&lt;/p>
&lt;p>$$
{r \times SD\ \text{of y} \over SD\ \text{of x}}
$$&lt;/p>
&lt;p style="text-align:justify;color:blue;">
The intercept of the regression line is just the predicted value for y when x is 0.
&lt;/p>
&lt;hr>
&lt;p>The equation of a line can be written in terms of the slope and intercept:
$$
y = \text{slope} \times x + \text{intercept},
$$
which is called the &lt;em>regression equation&lt;/em>. There is nothing new here. The regression equation is just a way of predicting $y$ from $x$ by the regression method.&lt;/p>
&lt;p>The regression line becomes unreliable when you are far from the center of the data, so a &lt;em>negative&lt;/em> intercept is not too disturbing (when the calculation results in some negative value which may seen absurd).&lt;/p>
&lt;p style="text-align:justify;color:blue;">
If you run an observational study, the regression line only describes the data that you see. The line cannot be relied on for predicting the results of interventions.
&lt;/p>
&lt;h3 id="the-least-squares" >
&lt;div>
&lt;a href="#the-least-squares">
##
&lt;/a>
The Least Squares
&lt;/div>
&lt;/h3>
&lt;p>Sometimes the points on a scatter diagram seem to be following a line. The problem discussed in this section is &lt;strong>how to find the line which best fits the points&lt;/strong>. Usually, this involves a compromise: moving the line closer to some points will increase it distance from others. To resolve the conflict, two steps are necessary.&lt;/p>
&lt;ul>
&lt;li>First, define an average distance from the line to all the points.&lt;/li>
&lt;li>Second, move the line around until this average distance is as small as possible.&lt;/li>
&lt;/ul>
&lt;p>To be more specific, suppose the line will be used to predict $y$ from $x$. Then the error made at each point is the vertical distance from the point to the line (a.k.a, the &lt;strong>residual&lt;/strong>, means the difference between the $i$th observed and the $i$th response that is predicted by linear model). In statistics, the usual way to define the average distance is by taking the root-mean-square of the errors. This measure of average distance is called the &lt;em>r.m.s error of the line&lt;/em>. (It was first proposed by Gauss)&lt;/p>
&lt;p>The second problem, how to move the line around to minimize the r.m.s error, was also solved by Gauss:&lt;/p>
&lt;hr>
&lt;p style="text-align:center;color:blue;">
Among all lines, the one that makes the smallest r.m.s error in predicting y form x is the regression line.
&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>The r.m.s error for regression says how far typical points are above or below the regression line.
$$
r.m.s\ error = \sqrt{\frac{1}{n} \sum^n_i (y_i - \hat{y_i})^2}
$$
where $n$ is the number of data points, $y_i$ the $i$-th actual value, $\hat{y_i}$ the corresponding predicted value.&lt;/p>
&lt;p>And the r.m.s error for the regression line of $y$ on $x$ can also be figured as
$$
\sqrt{1 - r^2} \times SD_y
$$
where $r$ is the correlation coefficient&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> between $x$ and $y$.&lt;/p>
&lt;/blockquote>
&lt;p>For this reason, the regression line is often called &lt;em>least squares line&lt;/em>: the errors are squared to compute the r.m.s error, and the regression line makes the r.m.s error as small as possible.&lt;/p>
&lt;p>In other words, the least squares approaches choose $\beta_0$ (the intercept) and $\beta_1$ (the slope) to minimize the &lt;em>residual sum of squares&lt;/em> (RSS) which is defined as
$$
RSS = e_1^2 + e_2^2 + \cdots + e_n^2 = \sum^n_{i=1} (y_i - \hat{y_i})^2
$$
where $e = y_i - \hat{y_i}$ is called the &lt;strong>residual&lt;/strong>. Obviously, the r.m.s error is the root of the mean of RSS.&lt;/p>
&lt;p>Linear regression is a very simple approach for supervised learning. In particular, linear regression is a useful tool for predicting a quantitative response. Many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression.&lt;/p>
&lt;p>$\color{Green}{\text{Example}}$&lt;/p>
&lt;p>According to Hooke&amp;rsquo;s law, the amount of stretch is proportional to the weight $x$. The new length of the spring is
$$
y = mx + b.
$$
In this equation, $m \in \mathbb{R}$ and $b \in \mathbb{R}$ are constants which depend on the spring. Their values are unknown, and have to be estimated using &lt;strong>experimental data&lt;/strong>.&lt;/p>
&lt;center>
Table 1. Data on Hooke's law.
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Weight (kg)&lt;/th>
&lt;th>Length (cm)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>439.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>439.12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>439.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>439.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>439.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>439.50&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/center>
The correlation coefficient[^1] for the data in table 1 is 0.999, very close to 1 indeed. So the points almost form a straight line (figure 5), just as Hooke's law predicts. The minor deviations from linearity are probably due to measurement error; neither the weights nor the length have been measured with perfect accuracy. (Nothing ever is. [When it comes to measurement])
&lt;p>&lt;img alt="Hooke&amp;rsquo;s law" src="images/stats_hookes_law.png">&lt;/p>
&lt;p>Our goal is to estimate $\hat{m}$ and $\hat{b}$ in the equation of Hooke&amp;rsquo;s law for this spring:
$$
y = \hat{m} x + \hat{b}
$$
The graph of this equation is a perfect straight line. If the points in figure 5 happened to fall exactly on some line, the slope&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> of that line would estimate $m$, and its intercept would estimate $b$. However, the points do not line up perfectly. Many different lines could be drawn across the scatter diagram, each having a slightly different slope and intercept.&lt;/p>
&lt;p>Which line should be used? Hooke&amp;rsquo;s equation predicts the length from weight. As discussed above, it is natural to choose $m$ and $b$ so as to minimize the r.m.s error, the line $y = \hat{m} x + \hat{b}$ which does the job is the &lt;strong>regression line&lt;/strong>. This is the &lt;em>method of least squares&lt;/em>. In other words, $m$ in Hooke&amp;rsquo;s law should be estimated as the slope of the regression line, and $b$ as its intercept. These are called &lt;em>least squares estimate&lt;/em>, because they minimize root-mean-square error.&lt;/p>
&lt;p>Let&amp;rsquo;s do the arithmetic (in python code):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># X the weight data; y the length data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">2&lt;/span>, &lt;span style="color:#ff9f43">4&lt;/span>, &lt;span style="color:#ff9f43">6&lt;/span>, &lt;span style="color:#ff9f43">8&lt;/span>, &lt;span style="color:#ff9f43">10&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">439.00&lt;/span>, &lt;span style="color:#ff9f43">439.12&lt;/span>, &lt;span style="color:#ff9f43">439.21&lt;/span>, &lt;span style="color:#ff9f43">439.31&lt;/span>, &lt;span style="color:#ff9f43">439.40&lt;/span>, &lt;span style="color:#ff9f43">439.50&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># mean and Standard Deviation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ---------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># avg = sum(X) / len(X)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The means of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>mu_x, mu_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The means of X and y: (5.0, 439.25666666666666)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># std is the &amp;#34;r.m.s size of the deviation from the average&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The SDs of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>SD_x, SD_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The SDs of X and y: (3.415650255319866, 0.16799470891138593)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert X into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (X &lt;span style="color:#ff6ac1">-&lt;/span> mu_x) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert y into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (y &lt;span style="color:#ff6ac1">-&lt;/span> mu_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># correlation coefficient is the average of the products&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>r &lt;span style="color:#ff6ac1">=&lt;/span> (X_standard_unit&lt;span style="color:#ff6ac1">.&lt;/span>dot(y_standard_unit)) &lt;span style="color:#ff6ac1">/&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(X_standard_unit)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># r = 0.999167257319307&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the slope&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m_hat &lt;span style="color:#ff6ac1">=&lt;/span> (r &lt;span style="color:#ff6ac1">*&lt;/span> SD_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># m_hat = 0.0491428571428563&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the intercept, this is the *predicted length* when weight is 0,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>b_hat &lt;span style="color:#ff6ac1">=&lt;/span> mu_y &lt;span style="color:#ff6ac1">-&lt;/span> (mu_x &lt;span style="color:#ff6ac1">*&lt;/span> m_hat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># b_hat = 439.0109523809524&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>this gives us: $\hat{m} \approx 0.05$ per kg and $\hat{b} \approx 439.01$ cm.&lt;/p>
&lt;p>The length of the spring under no load is estimated as 439.01 cm. And each kilogram of load causes the spring to stretch by about 0.05 cm. Of course, even Hooke&amp;rsquo;s law has its limits: beyond some point, the spring will break. &lt;strong>Extrapolating beyond the data is risky&lt;/strong>.&lt;/p>
&lt;p>The method of least squares and the regression method involve the same mathematics; but the contexts may be different. In some fields, investigators talk about &amp;ldquo;least squares&amp;rdquo; when they are estimating parameters &amp;ndash; unknown constants of nature like $m$ and $b$ in Hooke&amp;rsquo;s law. In other fields, investigators talk about regression when they are studying the relationship between two variables, like income and education, using non-experimental data.&lt;/p>
&lt;p>&lt;strong>A technical point:&lt;/strong> The least squares estimate for the length of the spring under no load was 439.01 cm. This is a tiny bit longer than the measured length at no load (439.00 cm). A statistician might trust the least squares estimate over the measurement. Why? Because the least squares estimate takes advantage of all six measurements, not just once: some of the measurement error is likely to cancel out. Of course, the six measurements are tied together by a good theory &amp;ndash; Hooke‚Äôs law. Without the theory, the least squares estimate wouldn‚Äôt be worth much.&lt;/p>
&lt;h3 id="assessing-the-accuaracy-of-the-coefficient-estimates" >
&lt;div>
&lt;a href="#assessing-the-accuaracy-of-the-coefficient-estimates">
##
&lt;/a>
Assessing the Accuaracy of the Coefficient Estimates
&lt;/div>
&lt;/h3>
&lt;p>Assume that the &lt;em>true&lt;/em> relationship (e.g., the Hooke‚Äôs law) between $X$ and $Y$ takes the form $Y = f(X) + \epsilon$ for some unknown function $f$, where $\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as
$$
Y = \beta_0 + \beta_1 X + \epsilon.
$$
This is the &lt;em>population regression line&lt;/em>. Here $\beta_0$ is the intercept (the expected value of $Y$ when $X$ = 0) and the $\beta_1$ the slop (the average increase in $Y$ associated with a one-unit increase in $X$). The $\epsilon$ (error term, typically assumed to be independent of $X$) is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error.&lt;/p>
&lt;p>The model of &lt;em>population regression line&lt;/em> is the best linear approximation to the true relationship between $X$ and $Y$ (NOTE that the assumption of linearity is often a useful working model. However, it may be not true in reality). The true relationship is generally not known for real data, but the least squares line can always be computed using the cofficient estimation methods. A natural question is as follows: how accurate is the least square line as an estimate of the population regression line?&lt;/p>
&lt;p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of &lt;em>bias&lt;/em>. If we use the sample mean $\hat{\mu}$ to estimate $\mu$, this estimate is &lt;em>unbiased&lt;/em>, in the sense that on average, we expect $\hat{\mu}$ to equal $\mu$, if we could average a huge number of estimates of $\mu$ obtained from a huge number of sets of observations. Hence, an unbiased estimator does not &lt;em>systematically&lt;/em> over- or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates as well: if we estimate $\beta_0$ and $\beta_1$ on the basis of a particular data set, then our estimates won&amp;rsquo;t be exactly equal to $\beta_0$ and $\beta_1$. But if we could average the estimates obtained over a huge number of date sets, then the average would be spot on!&lt;/p>
&lt;p>So how far off will that single estimate of $\hat{\mu}$ be? In general, we answer this question by computing the &lt;em>standard error&lt;/em> of $\hat{\mu}$, written as $SE(\hat{\mu})$. We have the well-known formula
$$
Var(\hat{\mu}) = SE(\hat{\mu})^2 = {\sigma^2 \over n},
$$
where $\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$. NOTE that this formula holds iff the $n$ observations are uncorrelated. To compute the standard errors associated with $\hat{\beta_0}$ and $\hat{\beta_1}$, we use the following formulas:
$$
SE(\hat{\beta_0})^2 =
\sigma^2 [{1 \over n} + \frac{\bar{x}^2}{\sum^n_{i=1}(x_i - \bar{x})^2}],
&lt;br>
SE(\hat{\beta_1})^2 =\frac{\sigma^2}{\sum^n_{i=1}(x_i - \bar{x})^2}
$$
where $\sigma^2 = Var(\epsilon)$. In general, $\sigma^2$ is not known, but can be estimated from the data. This estimate of $\sigma$ is known as the &lt;em>residual standard error&lt;/em>, and is given by the formula
$$
\sigma = RSE = \sqrt{RSS / (n-2)}
$$&lt;/p>
&lt;h4 id="confidence-interval" >
&lt;div>
&lt;a href="#confidence-interval">
###
&lt;/a>
Confidence Interval
&lt;/div>
&lt;/h4>
&lt;p>Standard errors can be used to compute the &lt;em>confidence intervals&lt;/em>. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameters. The range is defined in terms of lower and upper limits computed from the sample of data.&lt;/p>
&lt;p>For linear regression, the 95% confidence interval for $\beta_1$ approximately takes the form
$$
\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1}).
$$
That is, there is approximately a 95% chance the true value of $\beta_1$ would be in this range.&lt;/p>
&lt;p>Similarly, for $\beta_0$, its 95% confidence interval takes the form
$$
\hat{\beta_0} \pm 2 \cdot SE(\hat{\beta_0}).
$$
NOTE that here we make an assumption that the errors are Gaussian. And the factor of $2$ in the formula will vary slightly depending on the number of observations $n$ in the linear regression.&lt;/p>
&lt;h4 id="hypothesis-tests" >
&lt;div>
&lt;a href="#hypothesis-tests">
###
&lt;/a>
Hypothesis tests
&lt;/div>
&lt;/h4>
&lt;p>Standard errors can also be used to perform &lt;em>hypothesis tests&lt;/em> on the coefficients. The most common hypothesis test involves testing the &lt;em>null hypothesis&lt;/em> of
$$
H_0 : \text{There is no relationship between X and Y}
$$
versus the &lt;em>alternative hypothesis&lt;/em>
$$
H_a : \text{There is some relationship between X and Y}.
$$
Mathematically, this corresponds to testing
$$
H_0 : \beta_1 = 0
$$
versus
$$
H_a : \beta_1 \ne 0,
$$
since if $\beta_1 = 0$ then the linear regression model reduces to $Y = \beta_0 + \epsilon$, and $X$ is not associated with $Y$.&lt;/p>
&lt;p>To test the null hypothesis, we need to determine whether $\hat{\beta_1}$, our estimate for $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero. How far is far enough? This is of course depends on the accuracy of $\hat{\beta_1}$ &amp;ndash; that is, it depends on $SE(\hat{\beta_1})$:&lt;/p>
&lt;ul>
&lt;li>If $SE(\hat{\beta_1})$ is small, then even relatively small values of $\hat{\beta_1}$ may provide strong evidence that $\beta_1 \ne 0$;&lt;/li>
&lt;li>if $SE(\hat{\beta_1})$ is large, then $\hat{\beta_1}$ must be large in absolute value in order for us to reject the null hypothesis.&lt;/li>
&lt;/ul>
&lt;p>In practice, we compute a &lt;em>t-statistic&lt;/em>, given by
$$
t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})},
$$
which measures the number of standard deviations that $\hat{\beta_1}$ is away from $0$.&lt;/p>
&lt;p>If there really is no relationship between $X$ and $Y$, then we expect that &lt;em>t-statistic&lt;/em> will have a $t$-distribution with $n-2$ degrees of freedom. Consequently, it is a simple matter to compute the probability of observing any number equal to $|t|$ or larger in absolute value, assuming $\beta_1 = 0$. We call this probability the &lt;em>p-value&lt;/em>.&lt;/p>
&lt;p>&lt;strong>p-value interpretation&lt;/strong>&lt;/p>
&lt;p>Roughly speaking, we interpret the p-value as follows: a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real relationship between $X$ and $Y$. Hence we &lt;em>reject the null hypothesis&lt;/em>, and declare a relationship to exist between $X$ and $Y$, if the p-value is small enough. Typical p-value cutoffs for rejecting the null hypothesis are 5% or 1%, when $n = 30$, these correspond to &lt;em>t-statistics&lt;/em> of around $2$, and $2.75$, respectively.&lt;/p>
&lt;blockquote>
&lt;p>A small p-value indicates that a particular sample is unlikely if the null hypothesis
is true. It is akin to a jury concluding that it was unlikely that it would have been
presented with this set of evidence if the defendant were innocent, and therefore
reaching a guilty verdict. Of course, that doesn&amp;rsquo;t mean that the defendant is actually
guilty. Perhaps the jury was presented with misleading evidence.
Analogously, a low p-value might be attributable to the null hypothesis actually being
false, or it could simply be that the sample is unrepresentative of the population from
which it is drawn, i.e., the evidence is misleading.&lt;/p>
&lt;/blockquote>
&lt;p>More info about p-value see &amp;ldquo;21.2 Beware of P-values&amp;rdquo; in the book of Prof. John Guttag &amp;lt;&amp;lt;Introduction to computation and programming using python&amp;gt;&amp;gt;.&lt;/p>
&lt;h3 id="assessing-the-accuracy-of-the-model" >
&lt;div>
&lt;a href="#assessing-the-accuracy-of-the-model">
##
&lt;/a>
Assessing the Accuracy of the Model
&lt;/div>
&lt;/h3>
&lt;p>Once we have rejected the null hypothesis in favor of the altervative hypothesis, it is natrual to want to quantify &lt;em>the extent to which the model fits the data&lt;/em>. The quality of a linear regression fit is typically assessed using two related quantities: the &lt;em>residual standard error&lt;/em> ($RSE$) and the $R^2$ statistic.&lt;/p>
&lt;h4 id="residual-standard-error" >
&lt;div>
&lt;a href="#residual-standard-error">
###
&lt;/a>
Residual Standard Error
&lt;/div>
&lt;/h4>
&lt;p>From the model $Y = \beta_0 + \beta_1 X + \epsilon$ that associated with each observation is an error term $\epsilon$. Due to the presence of these error terms, even if we knew the true regression line (i.e., $\beta$s were known), we would not be perfectly predict $Y$ from $X$. The $RSE$ is an estimate of the standard deviation of $\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula
$$
RSE
= \sqrt{{1 \over n-2} RSS}
= \sqrt{{1 \over n-2} \sum^n_{i=1}(y_i - \hat{y_i})^2}.
$$
NOTE that $RSE$ is slightly different from &lt;em>r.m.s error&lt;/em> which the latter using the number of all samples ($n$) as denominator while the former using $n-2$.&lt;/p>
&lt;p>The $RSE$ is considered a measure of the &lt;em>lack of fit&lt;/em> of the model to the data. The smaller $RSE$ the better the model fitted to the data.&lt;/p>
&lt;h4 id="r2-statistic" >
&lt;div>
&lt;a href="#r2-statistic">
###
&lt;/a>
$R^2$ Statistic
&lt;/div>
&lt;/h4>
&lt;p>The $RSE$ provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of $Y$, it is not always clear what consititues a good $RSE$. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a &lt;em>proportion&lt;/em>, the proportion of variance explained, and so it always takes on a value between $0$ and $1$, and is independent of the scale of $Y$.&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$
where $TSS = \sum(y_i - \bar{y})^2$ is the &lt;em>total sum of squares&lt;/em>. Hence $R^2$ measures the &lt;em>proportion of variability in $Y$ that can be explained using $X$&lt;/em>.&lt;/p>
&lt;h3 id="multiple-linear-regression" >
&lt;div>
&lt;a href="#multiple-linear-regression">
##
&lt;/a>
Multiple Linear Regression
&lt;/div>
&lt;/h3>
&lt;p>Simple linear regression[^3] is a useful approach for predicting a response on the basis of a single predictor variable. But in practice we often have more than one predictor. One option is to run multiple separate simple linear regression, each of which uses a different feature as a predictor. However, this approach is not entirely satisfactory[^4].&lt;/p>
&lt;p>Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model[^5] so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose we have $p$ distinct predictors. Then the multiple linear regression model takes the form&lt;/p>
&lt;p>$$
\tag{3.19}
\label{mlr}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + + \beta_p X_p + \epsilon
$$&lt;/p>
&lt;p>where $X_j$ represents the $j$-th predictor and $\beta_j$ quantifies the association
between that variable and the response. We interpret $\beta_j$ as the &lt;em>average&lt;/em> effect
on $Y$ of a unit increase in $X_j$, &lt;strong>holding all other predictors fixed&lt;/strong>[^6].&lt;/p>
&lt;p>As was the case in the simple linear regression setting, the regression coefficients
$\beta_0, \beta_1, \ldots, \beta_p$ in ($\ref{mlr}$) are unknown, and must be estimated.
Given estimates $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$, we can make
predictions using the formula&lt;/p>
&lt;p>$$
\tag{3.21}
\label{mlrpred}
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p.
$$&lt;/p>
&lt;p>Then parameters are estimated using the same &lt;strong>least squares&lt;/strong> approach that we saw in
the context of simple linear regression. We choose $\beta_0, \beta_1, \ldots, \beta_p$
to minimize the &lt;strong>sum of squared residuals&lt;/strong>&lt;/p>
&lt;p>$$
\begin{eqnarray}
RSS
&amp;amp;=&amp;amp; \sum^n_{i=1}(y_i - \hat{y}_i)^2 \\
\tag{3.22} \label{rss}
&amp;amp;=&amp;amp; \sum^n_{i=1} \big( y_i - (\hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p) \big)^2
\end{eqnarray}
$$&lt;/p>
&lt;p>The values $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$ that minimize
($\ref{rss}$) are the multiple least squares regression coefficient estimates. Unlike
the simple linear regression coefficient estimation (the Python code block in previous
section), the multiple regression coefficient estimates have somewhat complicated forms
that are most easily represented using matrix algebra (see detail in section of Normal
equation).&lt;/p>
&lt;p>When we perform multiple linear regression, we usually are interested in answering a few important questions.&lt;/p>
&lt;hr>
&lt;ol>
&lt;li>Is at least one of the predictors $X_1, X_2, \ldots, X_p$ useful in predicting the response?&lt;/li>
&lt;li>Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?&lt;/li>
&lt;li>How well does the model fit the data?&lt;/li>
&lt;li>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>We now address each of these questions in turn.&lt;/p>
&lt;h4 id="one-is-there-a-relationship-between-the-response-and-predictions" >
&lt;div>
&lt;a href="#one-is-there-a-relationship-between-the-response-and-predictions">
###
&lt;/a>
One: Is There a Relationship Between the Response and Predictions?
&lt;/div>
&lt;/h4>
&lt;p>Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether the slope equals $0$. In the multiple regression setting with $p$ predictors, we need to ask whether all of the regression coefficients are zero. We also use a &lt;strong>hypothesis test&lt;/strong> to answer this question. We test the &lt;strong>null hypothesis&lt;/strong>,
$$
H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0
$$
versus the alternative
$$
H_{\alpha}: \text{at least one } \beta_j \text{ is non-zero}
$$
This hypothesis test is performed by computing the &lt;strong>F-statistic&lt;/strong>,
$$
\tag{3.23}
F = \frac{(TSS - RSS) / p}{RSS / (n - p -1)}
$$
where, as with simple linear regression, $TSS = \sum(y_i - \bar{y})^2$ and $RSS = \sum(y_i - \hat{y})^2$ where $\bar{y} = {1 \over n} \sum^n_{i=1} y_i$ is the sample mean. If the linear model assumptions are correct, one can show that
$$
E {RSS / (n - p - 1) } = \sigma^2
$$
and that, provided $H_0$ is true,
$$
E {(TSS - RSS) / p } = \sigma^2
$$
Hence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to $1$. On the other hand, if $H_{\alpha}$ is true, then $E {(TSS - RSS) / p } &amp;gt; \sigma^2$, so we expect the F-statistic to be greater than $1$.&lt;/p>
&lt;p>However, what if the F-statistic had been closer to $1$? How large does the F-statistic need to be before we can reject $H_0$ and conclude that there is a relationship? It turns out that the answer depends on the values of $n$ and $p$.&lt;/p>
&lt;ul>
&lt;li>When $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$.&lt;/li>
&lt;li>Inctrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small.&lt;/li>
&lt;/ul>
&lt;p>When $H_0$ is true, and the errors $\epsilon_i$ have normal distribution, the F-statistic follows an F-distribution[^7]. For any given value of $n$ and $p$, any statistical software package can be used to compute the p-value[^8] associated with F-statistic using this distribution. Based on this p-value, we determine whether or not to reject $H_0$.&lt;/p>
&lt;p>Sometimes we want to test that a particular subset of $q$ of the coefficients are zero. This corresponds to a null hypothesis
$$
H_0 = \beta_{p-q+1} = \beta_{p-q+2} = \cdots = \beta_{p} = 0,
$$
where for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses all the variables except those last $q$. Suppose that the residual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is
$$
\tag{3.24} \label{ftest2}
F = \frac{(RSS_0 - RSS) / q}{RSS / (n - p -1)}.
$$
For each individual predictor a t-statistic and a p-value can be obtain, these statistics provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in (means $q=1$ in equation $\ref{ftest2}$). So it reports the &lt;em>partial effect&lt;/em> of adding that variable to the model.&lt;/p>
&lt;p>Given these individual p-values for each variable, why do we need to look at the over F-statistic? After all, it seems likely that if any one of the p-values for the individual variables is very small, then &lt;em>at least one of the predictors is related to the response&lt;/em>. However, this logic is flawed, especially when the number of predictors $p$ is large.&lt;/p>
&lt;p>For instance, consider an example in which $p = 100$ and $H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0$ is true, so no variable is truly associated with the response. In this situation, about &lt;strong>5%&lt;/strong> of the p-values associated with each variable will be below 0.05 by chance. &lt;strong>In other words, we expect to see approximately five small p-values even in the absence of any true association between the predictors and the response. In fact, we are almost guaranteed that we will observe at least one p-value below 0.05 by chance!&lt;/strong>&lt;/p>
&lt;p>Hence, if we use individual t-statistic and the associated p-value in order to confirm the association between any predictor and the response, there is a very high chance that we will incorrectly conclude that there is a relationship.&lt;/p>
&lt;p>However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. If $H_0$ is true, there is only a 5% chance that the F-statistic will result in a p-value below 0.05, regardless of the number of predictors or the number of observations.&lt;/p>
&lt;p>Note that when $p$ is larger than $n$, we cannot even fit the multiple linear regression model using least squares. Less flexible least squares models, such as forward stepwise selection, ridge regression, lasso regression and principal components regression, are particular useful for performing regression in the high-dimensional setting.&lt;/p>
&lt;h4 id="two-deciding-on-important-variable" >
&lt;div>
&lt;a href="#two-deciding-on-important-variable">
###
&lt;/a>
Two: Deciding on Important Variable
&lt;/div>
&lt;/h4>
&lt;p>If we conclude on the basis of the F-statistic and its associated p-value that at least one of the predictors is related to the response, then it is natural to wonder which are the guilty ones. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as &lt;strong>variable selection&lt;/strong>.&lt;/p>
&lt;p>Ideally, we would like to preform variable selection by trying out a lot of different models, each containing a different subset of the predictors. Unfortunately, there are a total $2^p$ models that contain subsets of $p$ variables (Note that even with a moderate value of $p$, say, $p=30$, then $2^{30}=1,073,741,824$ models make this infeasible). We need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:&lt;/p>
&lt;ul>
&lt;li>Forward selection. We begin with the &lt;strong>null model&lt;/strong> (which contains only the intercept). We then fit $p$ simple linear regressions and add to the null model the variable that results in the lowest $RSS$, and then add to that model the variable which results in the lowest $RSS$ for the new two-variable model. This approach is continued until some stopping rule is satisfied.&lt;/li>
&lt;li>Backward selection. We start with all variables in the model, and remove the variable with largest p-value. The new ($p - 1$)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached (such as when all remaining variables have a p-value below some threshold).&lt;/li>
&lt;li>Mixed selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. If at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform forward and backward steps until all variable in the model have a low p-value, and all the variables outside the model have a large p-value if added to the model.&lt;/li>
&lt;/ul>
&lt;p>Backward selection cannot be used when $p &amp;gt; n$, forward selection is a greedy approach, and might include variable early that later become redundant. Mixed selection can remedy this.&lt;/p>
&lt;h4 id="three-model-fit" >
&lt;div>
&lt;a href="#three-model-fit">
###
&lt;/a>
Three: Model Fit
&lt;/div>
&lt;/h4>
&lt;p>Two of the most common numerical measures of model fit are the $RSE$ and the $R^2$. Recall that in simple regression, $R^2$ is the square of the correlation coefficient between predictor and the response. In multiple linear regression, it turns out that it equals the square of the correlation coefficient between the response and the fitted model (this implies that the fitted model maximizes this correlation among all possible linear model).&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
\begin{eqnarray}
R^2
&amp;amp;=&amp;amp; \frac{\sum(y_i - \bar{y})^2 - \sum(y_i - \hat{y})^2}{\sum(y_i - \bar{y})^2} \\
\\
&amp;amp;=&amp;amp; \frac{TSS - RSS}{TSS} \\
\\
\tag{3.17}
&amp;amp;=&amp;amp; 1 - {RSS \over TSS},
\end{eqnarray}
$$
where ($\bar{y} = {1 \over n} \sum^n_{i=1} y_i$) is the sample mean, $\hat{y}$ is defined in ($\ref{mlrpred}$).&lt;/p>
&lt;p>An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the response variable. It turns out that $R^2$ will always increase when more variables are added to the model, even they are only weakly associated with the response. This is due to the fact that with more variable to the least squares equations must allow us to fit the training data more accurately (though not necessarily the testing data, a.k.a., over fitting).&lt;/p>
&lt;p>In general, $RSE$ is defined as
$$
\tag{3.25}
RSE = \sqrt{{1 \over {n - p - 1}} RSS}
$$
Thus model with more variables can have higher $RSE$ if the decrease in $RSS$ is small relative to the increase in $p$.&lt;/p>
&lt;h5 id="adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2" >
&lt;div>
&lt;a href="#adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2">
####
&lt;/a>
Adjusted $R^2$ &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">(From wiki)&lt;/a>
&lt;/div>
&lt;/h5>
&lt;p>The use of an adjusted $R^2$ (one common notation is $\bar{R}^2$; another is $R_{adj}^2$)
is an attempt to account for the phenomenon of the $R^2$ automatically and spuriously
increasing when extra explanatory variables are added to the model. There are many
different ways of adjusting, by far the most used one, to the point that it is typically
just referred to as &lt;em>adjusted $R^2$&lt;/em>, is the correction prosposed by Mordecai Ezekiel,
and adjusted $R^2$ is defined as
$$
\bar{R}^2 = 1 - (1 - R^2){\frac{n-1}{n-p-1}}
$$
where $p$ is the total number of explantory variables in the model (not including the constant term), and $n$ is the sample size. It can also be written as
$$
\bar{R}^2 = 1 - \frac{RSS / df_e}{TSS / df_t}
$$
where $df_t$ is the &lt;em>degrees of freedom&lt;/em> $n-1$ of the estimate of the population variance of the dependent variable, and the $df_e$ is the degrees of freedom $n-p-1$ of the estimate of the underlying population error variance.&lt;/p>
&lt;p>The adjusted $R^2$ can be negative, and its value will always be less than or equal to that of $R^2$. Unlike $R^2$, the adjusted $R^2$ increases only when the increase in $R^2$ (due to the inclusion of a new variable) is more than one would expect to see by chance. If a set of explanatory variables with a predtermined hierarchy of importance are introduced into a regression one at a time, with the adjusted $R^2$ computed each time, the level at which adjusted $R^2$ reaches a maximum, and decreases afterward, would be the regression with ideal combination of having the best fit without excess/unnecessary terms.&lt;/p>
&lt;blockquote>
&lt;p>Degrees of Freedom &lt;a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">(From wiki)&lt;/a>:&lt;br>
The number of &lt;em>degrees of freedom&lt;/em> is the number of values in the final calculation of a statistic that are free to vary.
Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter are called the degrees of freedom.
Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of &amp;ldquo;free&amp;rdquo; components (how many components need to be know before the vector is fully determined).
Suppose we have a sample of independent normally distributed observations, ${X_1, X_2, \ldots, X_n}$. This can be represented as an n-dimensional random vector:
$X^T$. Since this random vector can lie anywhere in n-dimensional space, it has $n$ degrees of freedom.
Now let $\bar{X}$ be the sample mean. The random vector can be decomposed as the sum of the sample mean plus a vector of residuals:&lt;/p>
&lt;/blockquote>
&lt;p>$$
\begin{pmatrix}
X_{1} \\
\vdots \\
X_{n} \\
\end{pmatrix}
= \bar{X} \cdot
\begin{pmatrix}
1 \\
\vdots \\
1 \\
\end{pmatrix}
+
\begin{pmatrix}
X_{1} - \bar{X} \\
\vdots \\
X_{n} - \bar{X} \\
\end{pmatrix}
$$&lt;/p>
&lt;p>The first vector on the right-hand side is constrained to be a multiple of the vector of $1$&amp;rsquo;s, and the only free quantity is $\bar{X}$. It therefore has only one degree of freedom.
The second vector is constrained by the relation $\sum(X_i - \bar{X}) = 0$. The first $n-1$ components of this vector can be anything. However, once you know the first $n-1$ components, the constraint tells you the value of the $n$th component. Therefore, this vector has $n-1$ degrees of freedom.&lt;/p>
&lt;h4 id="four-prediction" >
&lt;div>
&lt;a href="#four-prediction">
###
&lt;/a>
Four: Prediction
&lt;/div>
&lt;/h4>
&lt;p>Once we have fit the multiple regression model, it is straightforward to apply the fitted model $\hat{y} = \hat{f}(X) = \hat{\beta} X$ (a more verbose version see $\ref{mlrpred}$) in order to predict the response based on the values of the predictors. However, there are three sorts of uncertainty associated with this prediction.&lt;/p>
&lt;ol>
&lt;li>The coefficient estimate is the least squares estimation of the true coefficient which is unknown. The inaccuracy in the coefficient estimates is related to the &lt;em>reducible error&lt;/em>[^9]. We can compute a &lt;strong>confidence interval&lt;/strong> in order to determine how close $\hat{y}$ will be to $f(X)$.&lt;/li>
&lt;li>In practice assuming a linear model for $f(X)$ is almost always an approximation of reality, so if the true pattern is non-linear, there is an additional reducible error called &lt;em>model bias&lt;/em>.&lt;/li>
&lt;li>Even if we knew $f(X)$ &amp;ndash; that is, we knew the true value of $\beta$ &amp;ndash; the response value cannot be predicted perfectly, because of the random error $\epsilon$ in the model ($\ref{mlrpred}$), this is the &lt;em>irreducible error&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h3 id="summary" >
&lt;div>
&lt;a href="#summary">
##
&lt;/a>
Summary
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>The regression line can be specified by two descriptive statistics: the &lt;em>slope&lt;/em> and the &lt;em>intercept&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Among all lines, the regression line for $y$ on $x$ makes the smallest r.m.s error in predicting $y$ from $x$. For that reason, the regression line is often called the &lt;em>least squares line&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>With a controlled experiment, the slope can tell you the average change in $y$ that would be caused by a change in $x$. With an observational study, however, the slope cannot be relied on to predict the results of interventions. It takes a lot of hard work to draw causal inferences from observational data, with or without regression.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If the average of $y$ depends on $x$ in a non-linear way, the regression line can be quite misleading.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multiple regression is a powerful technique, but it is not a substitute for understanding. (Such as the poor investigator would fit a multiple regression equation of the form $ predicted\ area = a + b \times perimeter + c \times diagonal$ to predict the area of a rectangle).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="part-i-how-to-learn" >
&lt;div>
&lt;a href="#part-i-how-to-learn">
#
&lt;/a>
Part-I: How to learn
&lt;/div>
&lt;/h2>
&lt;p>Linear regression is of course an extremely simple and limited learning algorithm, but it provides an example of how a learning algorithm can work.&lt;/p>
&lt;h3 id="normal-equation" >
&lt;div>
&lt;a href="#normal-equation">
##
&lt;/a>
Normal Equation
&lt;/div>
&lt;/h3>
&lt;p>The goal is to build a system that can take a vector $x \in \mathbb{R^n}$ as input and predict the value of a scalar $y \in \mathbb{R}$ as its output. The output of linear regression is a linear function of the input. Let $\hat{y}$ be the value that our model predicts $y$ should take on. We define the output to be
$$
\begin{equation}
\tag{5.3}
\hat{y} = w^{\mathsf{T}}x
\end{equation}
$$&lt;/p>
&lt;p>where $w \in \mathbb{R^n}$ is a vector of &lt;strong>parameters&lt;/strong>.&lt;/p>
&lt;p>We thus have a definition of our task &lt;em>T&lt;/em> : to predict $y$ from $x$ by outputting $\hat{y}=w^{\mathsf{T}}x$.&lt;/p>
&lt;p>Next we need a definition of our performance measure, &lt;em>P&lt;/em>. One way of measuring the performance of the model is to compute the &lt;strong>mean squared error (MSE)&lt;/strong> of the model on the test set. If $\hat{y}^{(test)}$ gives the predictions of the model on the test set, then the MSE is given by
$$
\tag{5.4}
MSE_{test} = \frac{1}{m} \sum_i{(\hat{y}^{(test)} - {y}^{(test)})^2_i}
$$
Intuitively, one can see that this error measure decreases to 0 when $\hat{y}^{(test)} = {y}^{(test)}$. We can also see that
$$
\tag{5.5}
MSE_{test} = \frac{1}{m} ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2
$$
so the error increases whenever the Euclidean distance between the predictions and the targets increases.&lt;/p>
&lt;blockquote>
&lt;p>In machine learning, we usually measure the size of vectors using a function called a &lt;strong>norm&lt;/strong>. Formally, the $L^p$ norm is given by
$$
||x||_p = \bigg(\sum_i |x_i|^p \bigg)^{\frac{1}{p}}
$$
for $p \in \mathbb{R}, p \geq 1$.&lt;/p>
&lt;p>The $L^2$ norm, with $p = 2$, is known as the &lt;strong>Euclidean norm&lt;/strong>, often denoted simply as $||x||$. It is also common to measure the size of a vector using the squared $L^2$ norm, which can be calculated simply as $x^{\mathsf{T}}x$.&lt;/p>
&lt;/blockquote>
&lt;p>To make a machine learning algorithm, we need to design an algorithm that will improve the weights $w$ in a way that reduces $MSE_{test}$ when the algorithm is allowed to gain experience by observing a training set ($X^{(train)}, y^{(train)}$).&lt;/p>
&lt;p>One intuitive way of doing this is (to minimize $MSE_{test}$) just to minimize the MSE on the training set, $MSE_{train}$ . (Does this make any sense? Keep on reading.)&lt;/p>
&lt;p>To minimize $MSE_{train}$ , we can simply solve for where its gradient is &lt;strong>0&lt;/strong>:
$$
\begin{eqnarray}
\tag{5.6}
\nabla_w MSE_{train} = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(train)} - {y}^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{X^{(train)}w - y^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \nabla_w \big({X^{(train)}w - y^{(train)}}\big)^{\mathsf{T}} \big({X^{(train)}w - y^{(train)}}\big) = 0
\end{eqnarray}
$$&lt;/p>
&lt;p>$$
\tag{5.10}
\Rightarrow \nabla_w \big( w^{\mathsf{T}} X^{(train)\mathsf{T}} X^{(train)}w - 2w^{\mathsf{T}} X^{(train)\mathsf{T}} y^{(train)} + y^{(train)\mathsf{T}} y^{(train)} \big) = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.11}
\Rightarrow 2X^{(train)\mathsf{T}} X^{(train)}w - 2X^{(train)\mathsf{T}} y^{(train)} = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.12}
\Rightarrow w = \big(X^{(train)\mathsf{T}} X^{(train)}\big)^{-1} X^{(train)\mathsf{T}} y^{(train)}
$$&lt;/p>
&lt;p>The system of equations whose solution is given by equation 5.12 is known as the &lt;strong>normal equation&lt;/strong>. Evaluating equation 5.12 constitutes a simple learning algorithm.&lt;/p>
&lt;blockquote>
&lt;p>ÈìæÂºèÊ≥ïÂàô(Chain Rule)ÊòØËÆ°ÁÆóÂ§çÊùÇÂØºÊï∞Êó∂ÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÁÆÄÂçïÂú∞ËØ¥ÔºåËã•ÂáΩÊï∞ $f(x) = g(h(x))$ÔºåÂàôÊúâ
$$
\tag{A.31}
\frac{\partial{f(x)}}{\partial{x}} = \frac{\partial{g(h(x))}}{\partial{h(x)}} \cdot \frac{\partial{h(x)}}{\partial{x}}.
$$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>‰æãÂ¶ÇÂú®ËÆ°ÁÆó‰∏ãÂºèÊó∂ÔºåÂ∞Ü$(Ax - b)$Áúã‰Ωú‰∏Ä‰∏™Êï¥‰ΩìÂèØÁÆÄÂåñËÆ°ÁÆóÔºö
$$
\begin{align*}
&amp;amp;\frac{\partial}{\partial{x}}(Ax - b)^{\mathsf{T}} W(Ax - b) \\
&amp;amp;= \frac{\partial{(Ax - b)}}{\partial{x}} \cdot 2W(Ax - b) \\
&amp;amp;= 2AW(Ax - b)
\end{align*}
$$&lt;/p>
&lt;/blockquote>
&lt;p>It is worth noting that the term &lt;strong>linear regression&lt;/strong> is often used to refer to a slightly more sophisticated model with one additional parameter &amp;ndash; an intercept term $b$. In this model&lt;/p>
&lt;p>$$
\tag{5.13}
\hat{y} = w^{\mathsf{T}}x + b
$$&lt;/p>
&lt;p>so the mapping from parameters to predictions is still a linear function but the mapping from features to predictions is now an &lt;strong>affine function&lt;/strong> (which is in the form of equation 5.13, it means that the plot of model&amp;rsquo;s predictions still looks like a line, but it need not pass though the origin). One can continue to use the model with only weights but augment $x$ with an extra entry that is always set to 1. Then the weight corresponding to the extra 1 entry plays the role of the bias parameter (i.e., the intercept term, $b$, a.k.a., the bias term).&lt;/p>
&lt;blockquote>
&lt;p>The intercept term $b$ is often called the &lt;strong>bias&lt;/strong> parameter of the affine transformation. This terminology derives from the point of view that the output of the transformation is biased toward being $b$ in the absence of any input.&lt;/p>
&lt;p>This term is different from the idea of a statistical bias, in which a statistical estimation algorithm&amp;rsquo;s expected estimate of a quantity is not equal to the true quantity.&lt;/p>
&lt;/blockquote>
&lt;h3 id="gradient-based-method" >
&lt;div>
&lt;a href="#gradient-based-method">
##
&lt;/a>
Gradient-based method
&lt;/div>
&lt;/h3>
&lt;p>Example: Linear Least Squares&lt;/p>
&lt;p>Suppose we want to find the value of $x$ that minimizes&lt;/p>
&lt;p>$$
\tag{4.21}
f(x) = \frac{1}{2}||Ax - b||^2_2
$$&lt;/p>
&lt;p>Specialized linear algebra algorithms can solve this problem efficiently; however, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work.&lt;/p>
&lt;p>First, we need to obtain the gradient (Recall the Chain Rule):&lt;/p>
&lt;p>$$
\tag{4.42}
\nabla_x f(x) = A^{\mathsf{T}} (Ax - b) = A^{\mathsf{T}}Ax - A^{\mathsf{T}}b
$$&lt;/p>
&lt;p>We can then follow this gradient downhill, taking small steps.&lt;/p>
&lt;hr>
&lt;p>Algorithm 4.1 An algorithm to minimize $f(x) = \frac{1}{2}||Ax - b||^2_2$ with respect to $x$ using gradient descent, starting from an arbitrary value of $x$.&lt;/p>
&lt;hr>
&lt;p>Set the step size ($\epsilon$, a.k.a., learning rate) and tolerance ($\delta$) to small, positive numbers.&lt;/p>
&lt;p>&lt;strong>while&lt;/strong> $||A^{\mathsf{T}}Ax - A^{\mathsf{T}}b||_2 &amp;gt; \delta$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>$x \leftarrow x - \epsilon (A^{\mathsf{T}}Ax - A^{\mathsf{T}}b)$&lt;/p>
&lt;p>&lt;strong>end while&lt;/strong>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>ÂÖ≥‰∫éÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï(Gradient Descent)&lt;/p>
&lt;p>Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊòØ‰∏ÄÁßçÂ∏∏Áî®ÁöÑ‰∏ÄÈò∂(first-order)‰ºòÂåñÊñπÊ≥ï, ÊòØÊ±ÇËß£Êó†Á∫¶Êùü‰ºòÂåñÈóÆÈ¢òÊúÄÁÆÄÂçï,ÊúÄÁªèÂÖ∏ÁöÑÊñπÊ≥ï‰πã‰∏Ä.&lt;/p>
&lt;p>ËÄÉËôëÊó†Á∫¶Êùü‰ºòÂåñÈóÆÈ¢ò$min_x f(x)$,ÂÖ∂‰∏≠$f(x)$‰∏∫ËøûÁª≠ÂèØÂæÆÂáΩÊï∞.Ëã•ËÉΩÊûÑÈÄ†‰∏Ä‰∏™Â∫èÂàó$x^0, x^1, x^2, \ldots$ Êª°Ë∂≥
$$
\tag{B.15}
\label{eq_ngd}
f(x^{(t+1)}) &amp;lt; f(x^{(t)}), t = 0,1,2,\ldots
$$
Âàô‰∏çÊñ≠ÊâßË°åËØ•ËøáÁ®ãÂç≥ÂèØÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊûÅÂ∞èÁÇπ.Ê¨≤Êª°Ë∂≥Âºè($\ref{eq_ngd}$),Ê†πÊçÆÊ≥∞ÂãíÂ±ïÂºèÊúâ
$$
\tag{B.16}
f(x + \Delta x) \simeq f(x) + \Delta x^{\mathsf{T}} \nabla f(x)
$$
‰∫éÊòØ,Ê¨≤Êª°Ë∂≥$f(x + \Delta x) &amp;lt; f(x)$,ÂèØÈÄâÊã©
$$
\tag{B.17}
\Delta x = - \epsilon \nabla f(x)
$$
ÂÖ∂‰∏≠Ê≠•Èïø(step size)$\epsilon$ÊòØ‰∏Ä‰∏™Â∞èÂ∏∏Êï∞.ËøôÂ∞±ÊòØÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï.&lt;/p>
&lt;p>Ëã•ÁõÆÊ†áÂáΩÊï∞$f(x)$Êª°Ë∂≥‰∏Ä‰∫õÊù°‰ª∂,ÂàôÈÄöËøáÈÄâÂèñÂêàÈÄÇÁöÑÊ≠•Èïø,Â∞±ËÉΩÁ°Æ‰øùÈÄöËøáÊ¢ØÂ∫¶‰∏ãÈôçÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊûÅÂ∞èÁÇπ.‰æãÂ¶Ç,Ëã•$f(x)$Êª°Ë∂≥L-LipschitzÊù°‰ª∂(‰∫¶Âç≥,ÂØπ‰∫é‰ªªÊÑè$x$,Â≠òÂú®Â∏∏Êï∞$L$‰ΩøÂæó$||\nabla f(x)|| \leq L$ÊàêÁ´ã),ÂàôÂ∞ÜÊ≠•ÈïøËÆæÁΩÆ‰∏∫$1/(2L)$Âç≥ÂèØÁ°Æ‰øùÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊûÅÂ∞èÁÇπ.ÂΩìÁõÆÊ†áÂáΩÊï∞ÊòØÂá∏ÂáΩÊï∞Êó∂,Â±ÄÈÉ®ÊûÅÂ∞èÁÇπÂ∞±ÊòØÂÖ®Â±ÄÊúÄÂ∞èÁÇπ,Ê≠§Êó∂,Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂèØÁ°Æ‰øùÊî∂ÊïõÂà∞ÂÖ®Â±ÄÊúÄ‰ºòËß£.&lt;/p>
&lt;p>ÂΩìÁõÆÊ†áÂáΩÊï∞$f(x)$‰∫åÈò∂ËøûÁª≠ÂèØÂæÆÊó∂,ÂèØÂ∞ÜÂºè($B.16$)ÊõøÊç¢ÊàêÊõ¥‰∏∫Á≤æÁ°ÆÁöÑ‰∫åÈò∂Ê≥∞ÂãíÂ±ïÂºè,ËøôÊ†∑Â∞±ÂæóÂà∞‰∫ÜÁâõÈ°øÊ≥ï(Newton&amp;rsquo;s method).ÁâõÈ°øÊ≥ïÊòØÂÖ∏ÂûãÁöÑ‰∫åÈò∂ÊñπÊ≥ï,ÂÖ∂Ëø≠‰ª£ËΩÆÊï∞ËøúÂ∞è‰∫éÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï.‰ΩÜÁâõÈ°øÊ≥ï‰ΩøÁî®‰∫Ü‰∫åÈò∂ÂØºÊï∞$\nabla^2 f(x)$ (second derivative),ÂÖ∂ÊØèËΩÆËø≠‰ª£‰∏≠Ê∂âÂèäÂà∞Êµ∑Ê£ÆÁü©Èòµ(Hessian matrix)ÁöÑÊ±ÇÈÄÜ,ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Áõ∏ÂΩìÈ´ò,Â∞§ÂÖ∂Âú®È´òÁª¥ÈóÆÈ¢ò‰∏≠Âá†‰πé‰∏çÂèØË°å.ÂÖ∂Ê¨°,ÁâõÈ°øÊ≥ï‰ªÖÈÄÇÁî®‰∫éÈôÑËøëÁÇπÊúâÂ±ÄÈÉ®ÊûÅÂ∞èÁÇπÁöÑÊÉÖÂÜµ(‰πüÂ∞±ÊòØ,Êµ∑Ê£ÆÁü©Èòµ‰∏∫Ê≠£ÂÆöÁü©Èòµ,‰πüÂ∞±ÊòØÊµ∑Ê£ÆÁü©ÈòµÊâÄÊúâÁöÑÁâπÂæÅÂÄºÈÉΩÊòØÊ≠£Êï∞),Ëã•ÈôÑËøëÁÇπÊòØÈûçÁÇπ(saddle point)ÂàôÁâõÈ°øÊ≥ïÂ§±Êïà.ÁÑ∂ËÄå,Ê¢ØÂ∫¶‰∏ãÈôçÂç¥‰∏ç‰ºöË¢´ÈûçÁÇπÂõ∞‰Ωè.Ëã•ËÉΩ‰ª•ËæÉ‰ΩéÁöÑËÆ°ÁÆó‰ª£‰ª∑ÂØªÊâæÊµ∑Ê£ÆÁü©ÈòµÁöÑËøë‰ººÈÄÜÁü©Èòµ,ÂàôÂèØ‰ª•ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄ,ËøôÂ∞±ÊòØÊãüÁâõÈ°øÊ≥ï(quai-Newton method).&lt;/p>
&lt;/blockquote>
&lt;h3 id="newtons-method" >
&lt;div>
&lt;a href="#newtons-method">
##
&lt;/a>
Newton&amp;rsquo;s method
&lt;/div>
&lt;/h3>
&lt;p>Sometimes we need to find all the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a &lt;strong>Jacobian matrix&lt;/strong>. Specifically, if we have a function $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$, then the Jacobian matrix $\mathbf{J} \in \mathbb{R}^{m \times n}$ of $f$ is defined such that $J_{i, j} = \frac{\partial}{\partial x_j}f(x)_i$.&lt;/p>
&lt;p>We are also sometimes interested in a derivative of a derivative. This is known as a &lt;strong>second derivative&lt;/strong>. For example, for a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$, the derivative with respect to $x_i$ of the derivative of $f$ with respect to $x_j$ is denoted as $\frac{\partial^2}{\partial x_i \partial x_j}f$.&lt;/p>
&lt;p>In a single dimension, we can denote $\frac{d^2}{d x^2}$ by $f&amp;rsquo;&amp;rsquo;(x)$. The second derivative tells us how the first derivative will change when we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. We can think of the second derivative as measuring &lt;strong>curvature&lt;/strong>.&lt;/p>
&lt;p>Suppose we have a quadratic function (or in practice it can be approximated well as quadratic, at least locally). If such a function has:&lt;/p>
&lt;ul>
&lt;li>Second derivative of zero: there is no curvature, it is a flat line, its value can be predict using only the gradient. If the gradient is 1, set the step size of $\epsilon$ along the negative gradient, then the cost function will decrease by $\epsilon$.&lt;/li>
&lt;li>Second derivative is negative: the function curves downward, decrease by more than $\epsilon$.&lt;/li>
&lt;li>Second derivative is positive: the function curves upward, decrease by less than $\epsilon$.&lt;/li>
&lt;/ul>
&lt;p>When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the &lt;strong>Hessian matrix&lt;/strong>. The Hessian matrix $H(f)(x)$ is defined such that&lt;/p>
&lt;p>$$
\tag{4.6}
H(f)(x)_{i, j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x).
$$&lt;/p>
&lt;p>Equivalently, the Hessian is the Jacobian of the gradient.&lt;/p>
&lt;p>The (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation to the function $f(x)$ around the current point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.8}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}}g + \frac{1}{2} (x - x^{(0)})^{\mathsf{T}}H(x - x^{(0)}),
$$&lt;/p>
&lt;p>where $g$ is the gradient and $H$ is the Hessian at $x^{(0)}$. If we use a learning rate of $\epsilon$, then the new point $x$ will be given by $x^{(0)} - \epsilon g$. Substituting this into our approximation, we obtain&lt;/p>
&lt;p>$$
\tag{4.9}
f(x^{(0)} - \epsilon g) \approx f(x^{(0)}) - \epsilon g^{\mathsf{T}}g + \frac{1}{2} \epsilon^2 g^{\mathsf{T}}Hg.
$$&lt;/p>
&lt;p>There are three terms here:&lt;/p>
&lt;ul>
&lt;li>the original value of the function&lt;/li>
&lt;li>the expected improvement due to the slope of the function&lt;/li>
&lt;li>and the correction we must apply to account for the curvature of the function&lt;/li>
&lt;/ul>
&lt;p>When this last term is too large, the gradient descent step can actually move uphill.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is zero or negative, the Taylor series approximation predicts that increase $\epsilon$ forever will decrease $f$ forever. In practice, the Taylor series is unlikely to remain accurate for large $\epsilon$, so one must resort to more heuristic choices of $\epsilon$ in this case.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is positive, solving for the optimal step size that decrease the Taylor series approximation of the function the most yields&lt;/p>
&lt;p>$$
\tag{4.10}
\epsilon^* = \frac{g^{\mathsf{T}}g}{g^{\mathsf{T}}Hg}.
$$&lt;/p>
&lt;p>In the worst case, when $g$ aligns with the eigenvector of $H$ corresponding to the maximal eigenvalue $\lambda_{max}$, then this optimal step size is given by $\frac{1}{\lambda_{max}}$. (The eigenvalues of the Hessian determine the scale of the learning rate, if the function we minimized can be approximated well by a quadratic function.)&lt;/p>
&lt;p>Using the eigendecomposition of the Hessian matrix, we can generalize the &lt;strong>second derivative test&lt;/strong> to multiple dimensions. At a critical point, where $\nabla_x f(x) = 0$, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum, or saddle point.&lt;/p>
&lt;ul>
&lt;li>when the Hessian is positive definite (all its eigenvalues are positive): local minimum.&lt;/li>
&lt;li>when the Hessian is negative definite (all its eigenvalues are negative): local maximum.&lt;/li>
&lt;li>the test is inconclusive whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero.&lt;/li>
&lt;/ul>
&lt;p>In multiple dimensions, there is a different second derivative for each direction at a single point. The &lt;strong>condition number&lt;/strong> of the Hessian at this point measures how much the second derivative differ from each other. When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly, Gradient descent is unaware of this change in the derivative, so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.&lt;/p>
&lt;p>Poor condition number also makes choosing a good step size difficult. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature.&lt;/p>
&lt;p>This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method for doing so is known as &lt;strong>Newton&amp;rsquo;s method&lt;/strong>. Newton&amp;rsquo;s method is based on using a second-order Taylor series expansion to approximate $f(x)$ near some point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.11}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}} \nabla_x f(x^{(0)}) + \frac{1}{2}(x - x^{(0)})^{\mathsf{T}}H(f)(x^{(0)})(x - x^{(0)})
$$&lt;/p>
&lt;p>If we solve for the critical point of this function, we obtain&lt;/p>
&lt;p>$$
\tag{4.12}
x^* = x^{(0)} - H(f)(x^{(0)})^{-1} \nabla_x f(x^{(0)})
$$&lt;/p>
&lt;p>When $f$ is a positive definite quadratic function, Newton&amp;rsquo;s method consist of applying equation 4.12 once to jump to the minimum of the function directly. When $f$ is not truly quadratic but can be locally approximated as a positive definite quadratic, Newton&amp;rsquo;s method consists of applying equation 4.12 multiple times. NOTE that Newton&amp;rsquo;s method is only appropriate when the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient is not attracted to saddle points unless the gradient points toward them.&lt;/p>
&lt;h2 id="part_ii-why-that--work" >
&lt;div>
&lt;a href="#part_ii-why-that--work">
#
&lt;/a>
Part_II: Why that Work
&lt;/div>
&lt;/h2>
&lt;h3 id="linear-regression-as-maximum-likelihood" >
&lt;div>
&lt;a href="#linear-regression-as-maximum-likelihood">
##
&lt;/a>
Linear Regression as Maximum Likelihood
&lt;/div>
&lt;/h3>
&lt;p>Previously, we motivated linear regression as an algorithm that learns to take an input $x$ and produce an output value $\hat{y}$. The mapping from $x$ to $\hat{y}$ is chosen to minimize mean squared error, a criterion that we introduced more or less arbitrarily. We now revisit linear regression from the point of view of maximum likelihood estimation.&lt;/p>
&lt;p>Instead of producing a single prediction $\hat{y}$, we now think of the model as producing a conditional distribution $p(y | x)$. We can imagine that with an infinitely large training set, we might see several training examples with the same input value $x$ but different values of $y$. The goal of the learning algorithm is now to fit the distribution $p(y | x)$ to all those different $y$ values that are all compatible with $x$.&lt;/p>
&lt;p>To derive the same linear regression algorithm we obtained before, we &lt;strong>define&lt;/strong> $p(y | x) = \mathcal{N}(y; \hat{y}(x; w), \sigma^2)$. In this example, we assume that the variance is fixed to some constant $\sigma^2$ chosen by user.&lt;/p>
&lt;p>Since the examples are assumed to be i.i.d., the conditional log-likelihood is given by&lt;/p>
&lt;p>$$
\begin{split}
&amp;amp;\sum^m_{i=1} log \ p(y^{(i)}|x^{(i)}; \theta) \\
&amp;amp;= -m \ log \ \sigma - \frac{m}{2} log(2 \pi) - \sum^m_{i=1} \frac{||\hat{y}^{(i)} - {y}^{(i)}||^2}{2 \sigma^2},
\end{split}
$$&lt;/p>
&lt;p>where $\hat{y}^{(i)}$ is the output of the linear regression on the $i$-th input $x^{(i)}$ and m is the number of the training examples. Comparing the log-likelihood with the mean squared error,&lt;/p>
&lt;p>$$
MSE_{train} = \frac{1}{m} \sum^m_{i=1} ||\hat{y}^{(i)} - {y}^{(i)}||^2,
$$&lt;/p>
&lt;p>we immediately see that maximizing the log-likelihood with respect to $w$ yields the same estimate of the parameters $w$ as does minimizing the mean squared error. The two criteria have different values but the same location of the optimum.&lt;/p>
&lt;p>This justifies the use of MSE as a maximum likelihood estimation procedure.&lt;/p>
&lt;h4 id="maximum-likelihood-estimation" >
&lt;div>
&lt;a href="#maximum-likelihood-estimation">
###
&lt;/a>
Maximum Likelihood Estimation
&lt;/div>
&lt;/h4>
&lt;p>Rather than guessing that some function might make a good estimator and then analyzing its bias and variance, we would like to have some principle from which we can derive specific functions that are good estimators for different models.&lt;/p>
&lt;p>The most common such principle is the maximum likelihood principle.&lt;/p>
&lt;p>Consider a set of m examples $\mathbb{X} = {x^{(1)}, \cdots, x^{(m)}}$ are i.i.d from true but unknown data-generating distribution $p_{data}(\mathbf{x})$.&lt;/p>
&lt;p>Let $p_{model}(\mathbf{x}; \mathbf{\theta})$ be a parametric family of probability distribution over the same space indexed by $\mathbf{\theta}$. In other words, $p_{model}({x}; \mathbf{\theta})$ maps any configuration $x$ to a real number estimating the true probability $p_{data}({x})$.&lt;/p>
&lt;p>The maximum likelihood estimator for $\mathbf{\theta}$ is then defined as&lt;/p>
&lt;p>$$
\begin{eqnarray}
\mathbf{\theta_{ML}}
\tag{5.56}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} {p_{model}(\mathbb{x}; \mathbf{\theta})} \\
\tag{5.57}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \prod^m_{i=1}{p_{model}(x^{(i)}; \mathbf{\theta})}
\end{eqnarray}
$$&lt;/p>
&lt;p>This product over many probabilities can be inconvenient for various reasons. Such as it&amp;rsquo;s prone to numerical underflow. We observe that taking the logarithm of the likelihood does not change its argmax but does conveniently transform a product into a sum:&lt;/p>
&lt;p>$$
\tag{5.58}
\mathbf{\theta_{ML}} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \sum^m_{i=1}{\text{log} \ p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>Because the argmax does not change when we rescale the cost function, we can divide by
$m$ to obtain a version of the criterion that is expressed as an expectation with
respect to the empirical distribution $\hat{p}_{data}$ defined by the training data:&lt;/p>
&lt;p>FIXME: \mathbb in equation(5.59, 5.60, 5.61) not work.&lt;/p>
&lt;p>$$
\tag{5.59}
\mathbf{\theta_{ML}} = {\underset{\mathbf{\theta}}{\operatorname{argmax}}}\
\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {\text{log} p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>One way to interpret maximum likelihood estimation is to view it as minimizing the
dissimilarity between the empirical distribution $\hat{p}_{data}$, defined by the
training set and the model distribution, with the degree of dissimilarity between the
two measured by the KL divergence. The KL divergence is given by&lt;/p>
&lt;p>$$
\tag{5.60}
D_{KL}(\hat{p}_{data} || p_{model}) = \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\log \hat{p}_{data}(x) - \log {p}_{model}(x)]}.
$$&lt;/p>
&lt;p>The term on the left is a function only of the data-generating process, not the model.
This means when we train the model to minimize the KL divergence, we need only minimize&lt;/p>
&lt;p>$$
\tag{5.61}
-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\text{log} \hat{p}_{data}(x)]},
$$&lt;/p>
&lt;p>which is of course the same as the maximization in equation 5.59.&lt;/p>
&lt;p>Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution and the model distribution. For example, MSE is the cross-entropy between the empirical distribution and a Gaussian model.&lt;/p>
&lt;p>We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution $\hat{p}_{data}$. While the optimal $\mathbf{\theta}$ is the same regardless of whatever we are maximizing the likelihood or minimizing the KL divergence, the values of the objective functions are different.&lt;/p>
&lt;p>In software, we often phrase both as minimizing a cost function.&lt;/p>
&lt;p>Maximum likelihood thus becomes minimization of the negative log-likelihood (NLL), or equivalently, minimization of the cross-entropy.&lt;/p>
&lt;p>ÂÖ≥‰∫éKLÊï£Â∫¶&lt;/p>
&lt;p>KLÊï£Â∫¶(Kullback-Leibler divergence), ‰∫¶Áß∞Áõ∏ÂØπÁÜµ(relative entropy)Êàñ‰ø°ÊÅØÊï£Â∫¶(information divergence), ÂèØÁî®‰∫éÂ∫¶Èáè‰∏§‰∏™Ê¶ÇÁéáÂàÜÂ∏É‰πãÈó¥ÁöÑÂ∑ÆÂºÇ. ÁªôÂÆö‰∏§‰∏™ËøûÁª≠ÂûãÊ¶ÇÁéáÂàÜÂ∏É$P$Âíå$Q$, ‰∫åËÄÖ‰πãÈó¥ÁöÑKLÊï£Â∫¶ÂÆö‰πâ‰∏∫&lt;/p>
&lt;p>$$
\label{eq_kld} \tag{C.34}
KL(P||Q) = \int^{\infty}_{-\infty} p(x) \text{log}\frac{p(x)}{q(x)} dx,
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠,$p(x)$Âíå$q(x)$ÂàÜÂà´ÊòØ$P$Âíå$Q$ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶ÂáΩÊï∞.&lt;/p>
&lt;p>KLÊï£Â∫¶Êª°Ë∂≥ÈùûË¥üÊÄß, Âç≥&lt;/p>
&lt;p>$$
\tag{C.35}
KL(P||Q) \geq 0,
$$&lt;/p>
&lt;p>ÂΩì‰∏î‰ªÖÂΩì$P=Q$Êó∂$KL(P||Q)=0$. ‰ΩÜÊòØ, KLÊï£Â∫¶‰∏çÊª°Ë∂≥ÂØπÁß∞ÊÄß, Âç≥&lt;/p>
&lt;p>$$
\tag{C.36}
KL(P||Q) \neq KL(Q||P),
$$&lt;/p>
&lt;p>Âõ†Ê≠§, KLÊï£Â∫¶‰∏çÊòØ‰∏Ä‰∏™Â∫¶Èáè(metric).&lt;/p>
&lt;p>Ëã•Â∞ÜKLÊï£Â∫¶ÁöÑÂÆö‰πâ($\ref{eq_kld}$)Â±ïÂºÄ, ÂèØÂæó&lt;/p>
&lt;p>$$
\begin{eqnarray}
KL(P||Q)
&amp;amp;=&amp;amp; \int^{\infty}_{-\infty} p(x)\log p(x)dx - \int^{\infty}_{-\infty} p(x)\log q(x)dx \\
\tag{C.37}
&amp;amp;=&amp;amp; -H(P) + H(P, Q),
\end{eqnarray}
$$&lt;/p>
&lt;p>ÂÖ∂‰∏≠$H(P)$‰∏∫ÁÜµ(entropy), $H(P,Q)$‰∏∫‰∫§ÂèâÁÜµ(cross-entropy).&lt;/p>
&lt;h3 id="bayesian-linear-regression" >
&lt;div>
&lt;a href="#bayesian-linear-regression">
##
&lt;/a>
Bayesian Linear Regression
&lt;/div>
&lt;/h3>
&lt;p>So far we have discussed &lt;strong>frequentist statistics&lt;/strong> and approaches based on estimating a single value of $\theta$, then making all predictions thereafter based on that one estimate. An other approach is to consider all possible values of $\theta$ when making a prediction. The latter is the domain of &lt;strong>Bayesian statistics&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>Freqentist: the true parameter value $\theta$ is fixed but unknown, while the point estimate $\hat{\theta}$ is a random variable on account of it being a function of the dataset (which is seen as random).&lt;/li>
&lt;li>Bayesian: the Bayesian uses probability to reflect degrees of certainty in states of knowledge. The dataset is directly observed and so is not random. On the other hand, the true parameter $\theta$ is unknown or uncertain and thus is represented as random variable.&lt;/li>
&lt;/ul>
&lt;p>Before observing the data, we represent our knowledge of $\theta$ using the &lt;strong>prior probability distribution&lt;/strong>, $p(\theta)$ (a.k.a., &amp;ldquo;the prior&amp;rdquo;). Generally, the machine learning practitioner selects a prior distribution that quite broad (i.e., with high entropy, such as uniform distribution) to reflect a high degree of uncertainty in the value of $\theta$ before observing any data.&lt;/p>
&lt;p>Now consider that we have a set of data samples {$x^{(1)}, \ldots, x^{(m)}$}. We can recover the effect of data on our belief about $\theta$ by combining the data likelihood $p(x^{(1)}, \ldots, x^{(m)} | \theta)$ with the prior via Bayes&amp;rsquo; rule:
$$
\tag{5.67}
p(\theta | x^{(1)}, \ldots, x^{(m)}) = \frac{p(x^{(1)}, \ldots, x^{(m)} | \theta) p(\theta)}{p(x^{(1)}, \ldots, x^{(m)})}
$$&lt;/p>
&lt;p>In the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation of the data usually causes the posterior to lose entropy and concentrate around a few highly likely values of the parameters.&lt;/p>
&lt;p>Bayesian estimation offers two important differences from MLE:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Unlike the MLE approach that makes predictions using a point estimate of $\theta$, the Bayesian approach is to make predictions using a full distribution over $\theta$. For example, after observing $m$ examples, the predicted distribution over the next data sample, $x^{(m+1)}$, is given by
$$
\tag{5.68}
p(x^{(m+1)} | x^{(1)}, \ldots, x^{(m)}) = \int p(x^{(m+1)} | \theta) p(\theta | x^{(1)}, \ldots, x^{(m)}) d \theta
$$
Here each value of $\theta$ with positive probability density contributes to the prediction of the next example, with the contribution weighted by the posterior density itself.&lt;/p>
&lt;p>After having observed {$x^{(1)}, \ldots, x^{(m)}$}, if we are still uncertain about the value of $\theta$, then this uncertainty is incorporated into any predictions we might make.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The prior has an influence by shifting probability mass density towards regions of the parameter space that are preferred a priori. In practice, the prior often expresses a preference for models that are simpler or more smooth.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Critics of the Bayesian approach identify the prior as a source of subjective human judgment affecting the predictions.&lt;/p>
&lt;p>Bayesian methods typically generalize much better when limited training data is available but typically suffer from high computational cost when the number of training examples is large.&lt;/p>
&lt;p>&lt;strong>$\color{Green}{\mathbf{Example}}$&lt;/strong>&lt;/p>
&lt;p>Here we consider the Bayesian estimation approach to learning the linear regression parameters. In linear regression, we learn a linear mapping from an input vector $x \in \mathbb{R}^n$ to predict the value of a scalar $y \in \mathbb{R}$. The prediction is parameterized by the vector $w \in \mathbb{R}^n$:
$$
\tag{5.69}
\hat{y} = w^{\mathsf{T}}x.
$$
Given a set of $m$ training samples ($X^{(train)}, y^{(train)}$), we can express the prediction of $y$ over the entire training set as
$$
\tag{5.70}
\hat{y}^{(train)} = X^{(train)}w.
$$
Expressed as a Gaussian conditional distribution on $y^{(train)}$, we have
$$
\begin{eqnarray}
p(y^{(train)} | X^{(train)}, w)
\tag{5.71}
&amp;amp;=&amp;amp; \mathcal{N}(y^{(train)} ; X^{(train)}w, I) \\
\tag{5.72}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(
- \frac{1}{2}(y^{(train)} - X^{(train)}w)^{\mathsf{T}} (y^{(train)} - X^{(train)}w)
\bigg),
\end{eqnarray}
$$
where we follow the standard MSE formulation in assuming that the Gaussian variance on $y$ is one.&lt;/p>
&lt;p>In what follows, to reduce the notational burden, we refer to ($X^{(train)}, y^{(train)}$) as simply ($X, y$).&lt;/p>
&lt;p>To determine the posterior distribution over the model parameter vector $w$, we first need to specify a prior distribution. For real-valued parameters it is common to use a Gaussian as a prior distribution,
$$
\tag{5.73}
p(w) = \mathcal{N}(w; \mu_0, \Lambda_0) \varpropto \text{exp} \bigg(
-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg),
$$
where $\mu_0$ and $\Lambda_0$ are the prior distribution mean vector and covariance matrix respectively. (We assume a diagonal covariance matrix $\Lambda_0 = diag(\lambda_0)$, unless there is a reason to use a particular covariance structure.)&lt;/p>
&lt;p>With the prior thus specified, we can now proceed in determining the &lt;strong>posterior&lt;/strong> distribution over the model parameters:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.74}
&amp;amp;\varpropto&amp;amp; p(y | X, w)p(w) \\
\tag{5.75}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(y - Xw)^{\mathsf{T}} (y - Xw) \bigg) \text{exp} \bigg(-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg) \\
\tag{5.76}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} \big( -2y^{\mathsf{T}}Xw + w^{\mathsf{T}}X^{\mathsf{T}}Xw + w^{\mathsf{T}} \Lambda_0^{-1}w - 2\mu_0^{\mathsf{T}}\Lambda_0^{-1}w \big) \bigg)
\end{eqnarray}
$$
We now define $\Lambda_m = (X^{\mathsf{T}}X + \Lambda_0^{-1})^{-1}$ and $\mu_m = \Lambda_m (X^{\mathsf{T}}y + \Lambda_0^{-1} \mu_0)$. Using these new variables, we find that the posterior may be rewritten as a Gaussian distribution:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.77}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) + \frac{1}{2} \mu_m^{\mathsf{T}}\Lambda_m^{-1}\mu_m \bigg) \\
\tag{5.78}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} (w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) \bigg)
\end{eqnarray}
$$
All terms that do not include the parameter vector $w$ have been omitted; they are implied by the fact that the distribution must be normalized to integrate to 1.&lt;/p>
&lt;p>Equation 3.23 shows how to normalize a multivariate Gaussian distribution:
$$
\tag{3.23}
\mathcal{N}(x; \mu, \Sigma) = \sqrt{\frac{1}{(2\pi)^n \text{det}(\Sigma)}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \Sigma^{-1}(x - \mu) \bigg).
$$
When we wish to evaluate the PDF several times for many different values of the parameters, the covariance is not a computationally efficient way to parametrize the distribution, since we need to invert $\Sigma$ to evaluate the PDF. We can instead use a &lt;strong>precision matrix $\beta$&lt;/strong>:
$$
\tag{3.24}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{\frac{\text{det}(\beta)}{(2\pi)^n}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \beta (x - \mu) \bigg).
$$&lt;/p>
&lt;h4 id="maximum-a-posteriori-map-estimation" >
&lt;div>
&lt;a href="#maximum-a-posteriori-map-estimation">
###
&lt;/a>
Maximum A Posteriori (MAP) Estimation
&lt;/div>
&lt;/h4>
&lt;p>While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter $\theta$, it is still often desirable to have a single point estimate. One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation.&lt;/p>
&lt;p>Rather than simply returning to the MLE, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate. One rational way to do this id to choose the &lt;strong>maximum a posteriori&lt;/strong> (MAP) point estimate. The MAP estimate chooses the point of maximal posterior probability ( or maximal probability density in the more common case of continuous $\theta$):
$$
\tag{5.79}
\theta_{MAP}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {p(\mathbf{\theta} | x)}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {\text{log} \ p(x | \mathbf{\theta})} + \text{log} \ p(\mathbf{\theta})
$$
We recognize, on the righthand side, $\text{log} \ p(x | \mathbf{\theta})$, that is, the standard log-likelihood term, and $\text{log} \ p(\mathbf{\theta})$, corresponding to the prior distribution.&lt;/p>
&lt;p>As an example, consider a linear regression model with a Gaussian prior on the weights $w$. If this prior is given by $\mathcal{N}(w; 0, \frac{1}{\lambda}I^2)$, then the log-prior term in equation 5.79 is proportional to the familiar $\lambda w^{\mathsf{T}}w$ weight decay penalty, plus a term that does not depend on $w$ and does not affect the learning process. MAP Bayesian inference with a Gaussian prior on the weights thus corresponds to weight decay.&lt;/p>
&lt;h3 id="mle-and-map-ÊÆäÈÄîÂêåÂΩí" >
&lt;div>
&lt;a href="#mle-and-map-%e6%ae%8a%e9%80%94%e5%90%8c%e5%bd%92">
##
&lt;/a>
MLE and MAP: ÊÆäÈÄîÂêåÂΩí
&lt;/div>
&lt;/h3>
&lt;h4 id="binary-variables" >
&lt;div>
&lt;a href="#binary-variables">
###
&lt;/a>
Binary Variables
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Coin flipping: heads = 1, tails = 0 with bias $\mu$
$$
p(X = 1 | \mu) = \mu
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Bernoulli Distribution
$$
Bern(x | \mu) = \mu^x \cdot (1 - \mu)^{1 - x} \\
\mathbf{E}[X] = \mu \\
var(X) = \mu \cdot (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>N coin flips: $X_1, \ldots, X_N$
$$
p(\Sigma_i X_i = m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Binomial Distribution
$$
p(m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
\mathbf{E}[\Sigma_i X_i] = N \mu \\
var[\Sigma_i X_i] = N \mu (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="the-bias-of-a-coin" >
&lt;div>
&lt;a href="#the-bias-of-a-coin">
###
&lt;/a>
The Bias of a Coin
&lt;/div>
&lt;/h4>
&lt;p>Suppose that we have a coin, and we would like to figure out what the probability is that it will flip up heads.&lt;/p>
&lt;ul>
&lt;li>How should we estimate the bias?&lt;/li>
&lt;/ul>
&lt;p>With these coin flips result: &lt;strong>[tail, head, tail, head, head]&lt;/strong>, our estimate of the bias is: 3/5 (&amp;ldquo;the frequency of heads&amp;rdquo;).&lt;/p>
&lt;ul>
&lt;li>
&lt;p>why is this a good estimate of the bias?&lt;/p>
&lt;p>- how good is this estimation?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$P(Heads) = \theta, \ P(Tails) = 1 - \theta$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Flips are i.i.d.&lt;/p>
&lt;ul>
&lt;li>Independent events&lt;/li>
&lt;li>Identically distributed according to Binomial distribution&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Our training data consists of $\alpha_H$ heads and $\alpha_T$ tails
$$
p(D | \theta) = \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="mle" >
&lt;div>
&lt;a href="#mle">
###
&lt;/a>
MLE
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Data: Observed set of $\alpha_H$ heads and $\alpha_T$ tails&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hypothesis: Coin flips follow a binomial distribution&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Learning: Find the &amp;ldquo;best&amp;rdquo; $\theta$&lt;/p>
&lt;p>Maximum Likelihood Estimation: Choose $\theta$ to maximize probability of $D$ given $\theta$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\hat{\theta}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;ul>
&lt;li>Set derivative to zero, and solve!&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\frac{d}{d\theta} \text{ln} \ P(D | \theta)
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}] \\
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\alpha_H \text{ln} \ \theta + \alpha_T \text{ln} (1 - \theta)] \\
&amp;amp;=&amp;amp; \alpha_H \frac{d}{d\theta} \text{ln} \ \theta + \alpha_T \frac{d}{d\theta} \text{ln} \ (1 - \theta) \\
&amp;amp;=&amp;amp; \frac{\alpha_H}{\theta} - \frac{\alpha_T}{1 - \theta} = 0 \\
\\
\Rightarrow \hat{\theta}_{MLE} &amp;amp;=&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;p>As we can see now, that&amp;rsquo;s exactly the &amp;ldquo;Frequency of the heads&amp;rdquo;! In other words, the frequency of heads is exactly the &lt;strong>maximum likelihood estimator&lt;/strong> for this problem.&lt;/p>
&lt;h4 id="map" >
&lt;div>
&lt;a href="#map">
###
&lt;/a>
MAP
&lt;/div>
&lt;/h4>
&lt;p>Suppose we have 5 coin flips all of which are heads, Our estimate of the bias is: ???&lt;/p>
&lt;ul>
&lt;li>MLE would give $\theta_{MLE} = 1$&lt;/li>
&lt;li>This event occurs with probability $1 / 2^5 = 1/32$ for a fair coin&lt;/li>
&lt;li>Are we willing to commit to such a strong conclusion with such little evidence?&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Priors&lt;/strong> are a Bayesian mechanism that allow us to take into account &amp;ldquo;prior&amp;rdquo; knowledge about our belief in the outcome. Rather than estimating a single $\theta$, consider a distribution over possible values of $\theta$ given the data:&lt;/p>
&lt;ul>
&lt;li>Without any data observed, our best guess of $\theta$ is obeyed a Beta(2, 2),&lt;/li>
&lt;li>After we see some data (such as observed flips:[tails, tails]), we update our prior to Beta(3, 2).&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Bayesian Learning&lt;/strong>
$$
\begin{eqnarray}
\tag{L1}
p(\theta | D) = \frac{p(D | \theta) \ p(\theta)}{p(D)} \\
\tag{L2} \label{eq_map}
\Rightarrow p(\theta | D) \varpropto p(D | \theta) \ p(\theta)
\end{eqnarray}
$$
where&lt;/p>
&lt;ul>
&lt;li>$p(\theta | D)$ is the posterior,&lt;/li>
&lt;li>$p(D | \theta)$ is the data likelihood,&lt;/li>
&lt;li>$p(\theta)$ is the prior,&lt;/li>
&lt;li>$p(D)$ is the normalization factor.&lt;/li>
&lt;/ul>
&lt;p>We update the prior according to the observed data to get the posterior by applying Bayes rule.&lt;/p>
&lt;p>&lt;strong>Picking Priors&lt;/strong>&lt;/p>
&lt;p>How do we pick a good prior distribution?&lt;/p>
&lt;ul>
&lt;li>Priors could represent expert domain knowledge&lt;/li>
&lt;li>Statisticians choose them to make the posterior distribution &amp;ldquo;nice&amp;rdquo; (conjugate priors, which makes the posterior the same form as the prior)&lt;/li>
&lt;/ul>
&lt;p>What is a good prior for the bias in the coin flipping problem?&lt;/p>
&lt;ul>
&lt;li>Truncated Gaussian (tough to work with)&lt;/li>
&lt;li>Beta distribution (works well for binary random variables)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Coin Flips with Beta Distribution&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Likelihood function: $p(D | \theta) = \theta^{\alpha_H} (1 - \theta)^{\alpha_T}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Prior: $p(\theta) = \frac{\theta^{\beta_H - 1} (1 - \theta)^{\beta_T - 1}}{B(\beta_H, \beta_T)} \sim Beta(\beta_H, \beta_T)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Posterior:
$$
\begin{eqnarray}
p(\theta | D)
&amp;amp;\varpropto&amp;amp; \theta^{\alpha_H}(1 - \theta)^{\alpha_T} \theta^{\beta_H - 1}(1 - \theta)^{\beta_T - 1} \\
&amp;amp;=&amp;amp; \theta^{\alpha_H + \beta_H - 1}(1 - \theta)^{\alpha_T + \beta_T - 1} \\
&amp;amp;=&amp;amp; Beta(\alpha_H + \beta_H, \alpha_T + \beta_T)
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>MAP Estimation&lt;/strong>&lt;/p>
&lt;p>Choosing $\theta$ to maximize the posterior distribution is called &amp;ldquo;maximum a posteriori (MAP)&amp;rdquo; estimation
$$
\theta_{MAP} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \ {p(\mathbf{\theta} | D)}
$$
The only difference between $\theta_{MLE}$ and $\theta_{MAP}$ is that one assumes a &lt;strong>uniform&lt;/strong> prior (MLE) and the other allows an arbitrary prior.&lt;/p>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>With uniform prior $p(\theta) \varpropto 1$, according to $\ref{eq_map}$, the posterior $p(\theta | D) \varpropto p(D | \theta)$.&lt;/p>
&lt;/blockquote>
&lt;p>Suppose we have 5 coin flips all of which are heads,&lt;/p>
&lt;ul>
&lt;li>
&lt;p>MLE would give $\theta_{MLE} = 1$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>MLE with a Beta(2, 2) prior gives $\theta_{MAP} = \frac{5 + 2 - 1}{5+2+0+2 - 2} = \frac{6}{7} \approx .857$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>As we see more data, the effect of the prior diminishes
$$
\begin{eqnarray}
\theta_{MAP}
&amp;amp;=&amp;amp; \frac{\alpha_H + \beta_H - 1}{\alpha_H + \beta_H + \alpha_T + \beta_T - 2} \\
\\
&amp;amp;\approx&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T} \ (\text{for large number of observations})
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="sample-complexity" >
&lt;div>
&lt;a href="#sample-complexity">
###
&lt;/a>
Sample Complexity
&lt;/div>
&lt;/h4>
&lt;p>How many coin flips do we need in order to guarantee that our learned parameter does not differ too much from the true parameter (with high probability)? Say, I want to know the coin parameter $\theta$, within $\epsilon = 0.1$ error with probability at least $1 - \delta = 0.95$.&lt;/p>
&lt;p>Using the Chernoff bound, we have
$$
p(|\theta_{true} - \theta_{MLE}| \geq \epsilon) \leq 2e^{-2N \epsilon^2} \\
\delta \geq 2e^{-2N \epsilon^2} \Rightarrow N \geq \frac{1}{2\epsilon^2} \text{ln} \frac{1}{\delta}
$$&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Convert each variable to standard units. The average of the products gives the correlation coefficient (may be more intuitively in the python code)&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Associated with a unit increase in $x$ there is some average change in $y$. The slope of the regression line estimates this change. The formula for the slope is $\frac{r \times SD_y}{SD_x}$. And the intercept of the regression line is just the predicted value for $y$ when $x$ is $0$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>