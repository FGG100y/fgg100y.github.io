<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Book Notes on fgg blog</title><link>/tags/book-notes/</link><description>fgg blog (Book Notes)</description><generator>Hugo -- gohugo.io</generator><language>zh</language><managingEditor>1522009317@qq.com
(fmh)</managingEditor><lastBuildDate>Fri, 07 Aug 2020 21:17:59 +0800</lastBuildDate><atom:link href="/tags/book-notes/index.xml" rel="self" type="application/rss+xml"/><item><title>Book Notes: Probability and Information Theory</title><link>/posts/lossfunctions/probability_information_theory/</link><pubDate>Fri, 07 Aug 2020 21:17:59 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/lossfunctions/probability_information_theory/</guid><description>&lt;h1 id="probability-and-information-theory" >
&lt;div>
&lt;a href="#probability-and-information-theory">
##
&lt;/a>
Probability and Information Theory
&lt;/div>
&lt;/h1>
&lt;p>Probability theory is a mathematical framework for representing uncertain statements. It provides a means of quantifying uncertainty as well as axioms (公理) for deriving new uncertain statements. In artificial intelligence applications, we use probability theory in two major ways:&lt;/p>
&lt;ul>
&lt;li>The laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using probability theory;&lt;/li>
&lt;li>We can use probability and statistics to theoretically analyze the behavior of proposed AI systems.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>While probability theory allows us to make uncertain statements and to reason in the presence of uncertainty, information theory enables us to quantify the amount of uncertainty in a probability distribution.&lt;/strong>&lt;/p>
&lt;h2 id="why-probability" >
&lt;div>
&lt;a href="#why-probability">
#
&lt;/a>
Why Probability?
&lt;/div>
&lt;/h2>
&lt;p>Machine learning must aways deal with uncertain quantities and sometimes stochastic (nondeterministic) quantities. Uncertainty and stochasticity can arise from many sources. There are three possible sources of uncertainty:&lt;/p>
&lt;ul>
&lt;li>Inherent stochasticity in the system being modeled. For example, most interpretations of quantum mechanics describe the dynamics of subatomic particles as being probabilistic.&lt;/li>
&lt;li>Incomplete observability. Even deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system. For example, there are three doors, two doors lead to a goat while a third leads to a car. The show contestant was asked to choose among three doors and wins the prize. The outcome given the contestant&amp;rsquo;s choice is deterministic, but from the contestant&amp;rsquo;s point of view, the outcome is uncertain.&lt;/li>
&lt;li>Incomplete modeling. When we use a model that must discard some of the information we have observed, the discarded information results in uncertainty in the model&amp;rsquo;s predictions.&lt;/li>
&lt;/ul>
&lt;p>While it should be clear that we need a means of representing and reasoning about uncertainty, it is not immediately obvious that probability theory can provide all the tools we want for AI applications. Probability theory was originally developed to analyze the frequencies of events. It is easy to see how probability theory can be used to study events like drawing a certain hand of cards in a poker game. These kinds of events are often repeatable. When we say that an outcome has a probability $p$ of occurring, it means that if we repeated the experiment (e.g., drawing a hand of cards, flipping a coin) infinitely many times, then proportion $p$ of the repetitions would result in that outcome. This kind of reasoning does not seem immediately applicable to propositions that are not repeatable. If a doctor analyzes a patient and says that the patient has a 40 percent chance of having the flu, this means something very different &amp;ndash; we cannot make infinitely many replicas of the patient, nor is there any reason to believe that different replicas of the patient would present with the same symptoms yet have varying underlying conditions. In the case of the doctor diagnosing the patient, we use probability to represent a &lt;strong>degree of belief&lt;/strong>, with $1$ indicating absolute certainty that the patient has the flu and $0$ indicating absolute certainty that the patient does not have the flu. The former kind of probability, related directly to the rates at which events occur, is known as &lt;strong>frequentist probability&lt;/strong>, while the latter, related to qualitative levels of certainty, is known as &lt;strong>Bayesian probability&lt;/strong>.&lt;/p>
&lt;p>Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions (命题) are implied to be true or false given the assumption that some other set of propositions is true of false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions.&lt;/p>
&lt;h2 id="random-variables" >
&lt;div>
&lt;a href="#random-variables">
#
&lt;/a>
Random Variables
&lt;/div>
&lt;/h2>
&lt;p>A &lt;strong>random variable&lt;/strong> is a variable that can take on different values randomly. On its own, a random variable is just a description of the states that are possible; it must be coupled with probability distribution that specifies how likely each of these states are.&lt;/p>
&lt;p>&lt;em>Quantitative&lt;/em> random variables may be discrete or continuous. This is not a hard-and-fast distinction, but it is a useful one. For a discrete variable, the values can only differ by fixed amounts. Family size is discrete. Two families can differ in size by 0 or 1 or 2, and so on. Nothing in between is possible. Age, on the other hand, is a continuous variable. This doesn&amp;rsquo;t refer to the fact that a person is continuously getting older; it just means that the difference in age between two people can be arbitrarily small &amp;ndash; a year, a month, a day, a hour, $\ldots$ And there are variables are &lt;em>qualitative&lt;/em> : examples are marital status (single, married, widowed, divorced, separated) and employment status (employed, unemployed, not in the labor force). Finally, the terms &lt;em>qualitative, quantitative, discrete&lt;/em>, and &lt;em>continuous&lt;/em> are also used to describe data &amp;ndash; qualitative data are collected on a qualitative variable, and so on.&lt;/p>
&lt;p>&lt;img alt="random variables" src="./images/stats_random_variables.png">&lt;/p>
&lt;h2 id="probability-distributions" >
&lt;div>
&lt;a href="#probability-distributions">
#
&lt;/a>
Probability Distributions
&lt;/div>
&lt;/h2>
&lt;p>A &lt;strong>probability distribution&lt;/strong> is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous.&lt;/p>
&lt;h3 id="discrete-variables-and-probability-mass-functions-pmf" >
&lt;div>
&lt;a href="#discrete-variables-and-probability-mass-functions-pmf">
##
&lt;/a>
Discrete Variables and Probability Mass Functions (PMF)
&lt;/div>
&lt;/h3>
&lt;p>A probability distribution over discrete variables may be described using a &lt;strong>Probability Mass Functions (PMF)&lt;/strong>. We typically denote probability mass functions with a capital $P$. Often we associate each random variable with different probability mass function and the reader must infer which PMF to use based on the identity of the random variable, rather than on the name of the function; $P(\bf{x})$ is usually not the same as $P(\bf{y})$.&lt;/p>
&lt;p>The PMF maps from a state of a random variable to the probability of that random variable taking on that state. The probability that $\mathbf{x} = x$ is denoted explicitly as $P(\mathbf{x}=x)$ or $P(x)$ in brevity, with a probability of $1$ indicating that $\mathbf{x} = x$ is certain and a probability of $0$ indicating that $\mathbf{x} = x$ is impossible. Sometimes we define a variable first, then use $\sim$ notation: $\mathbf{x} \sim P(\mathbf{x})$.&lt;/p>
&lt;p>PMF can act on many variables at the same time. Such a probability distribution over many variables is known as a &lt;strong>join probability distribution&lt;/strong>. $P(\mathbf{x} = x, \mathbf{y} = y)$ denotes the probability that $\mathbf{x}=x, \mathbf{y}=y$ simultaneously. We may also write $P(x, y)$ for brevity.&lt;/p>
&lt;p>To be a PMF on a random variable $\bf{x}$, a function $P$ must satisfy the following properties:&lt;/p>
&lt;ul>
&lt;li>The domain of $P$ must be the set of all possible states of $\bf{x}$.&lt;/li>
&lt;li>$\forall x \in \mathbf{x}, 0 \le P(x) \le 1$. An impossible event has probability $0$, and no state can be less probable than that. Likewise, an event that is guaranteed to happen has probability $1$, and no state can have a greater chance of occurring.&lt;/li>
&lt;li>$\sum_{x \in \mathbf{x}} P(x) = 1$. We refer to this property as being &lt;strong>normalized&lt;/strong>. Without this property, we could obtain probabilities greater than one by computing the probability of one of many events occurring.&lt;/li>
&lt;/ul>
&lt;p>For example, consider a single discrete random variable $\bf{x}$ with $k$ different states. We can place a &lt;strong>uniform distribution&lt;/strong> on $\bf{x}$ &amp;ndash; that is, make each of its states equally likely &amp;ndash; by setting its PMF to
$$
\tag{3.1}
P(\mathbf{x}=x_i) = {1 \over k}
$$
for all $i$. We can see that this fits the requirements for a probability mass function. The value $1 \over k$ is positive because $k$ is positive integer. We also see that
$$
\tag{3.2}
\sum_i P(\mathbf{x}=x_i) = \sum_i {1 \over k} = {k \over k} = 1,
$$
so the distribution is properly normalized.&lt;/p>
&lt;h3 id="continuous-variables-and-probability-density-functions-pdf" >
&lt;div>
&lt;a href="#continuous-variables-and-probability-density-functions-pdf">
##
&lt;/a>
Continuous Variables and Probability Density Functions (PDF)
&lt;/div>
&lt;/h3>
&lt;p>When working with continuous random variables, we describe probability distributions using a &lt;strong>Probability Density Functions (PDF)&lt;/strong> rather than a PMF. To be a Probability Density Function, a function $p$ must satisfy the following properties:&lt;/p>
&lt;ul>
&lt;li>The domain of $p$ must be the set of all possible states of $\bf{x}$.&lt;/li>
&lt;li>$\forall x \in \mathbf{x}, p(x) \ge 0$. Note that we do not require $p(x) \le 1$.&lt;/li>
&lt;li>$\int p(x)dx = 1$.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>A probability density function $p(x)$ does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume $\delta x$ is given by $p(x) \delta x$.&lt;/strong> Specifically, the probability that $x$ lies in some set $\mathbb{S}$ is given by the integral of density function $p(x)$ over that set. In the univariate example, the probability that $x$ lies in the interval $[a, b]$ is given by $\int_{[a, b]}p(x)dx$.&lt;/p>
&lt;p>For an example of a PDF corresponding to a specific probability density over a continuous random variable, consider a uniform distribution on an interval of the real numbers. We can do this with a function $u(x; a, b)$, where $a$ and $b$ are the endpoints of the interval, with $b &amp;gt; a$. The &amp;ldquo;$;$&amp;rdquo; notation means &amp;ldquo;parametrized by&amp;rdquo;; we consider $x$ to be the argument of the function, while $a$ and $b$ are parameters that define the function. To ensure that there is no probability mass outside the interval, we say $u(x; a, b) = 0\ \text{for all}\ x \notin [a, b]$. Within $[a, b]$, $u(x; a, b) = {1 \over b-a}$. we can see that this is non-negative everywhere. Additionally, it integrates to $1$. We often denote that $x$ follows the uniform distribution on $[a, b]$ by writing $\mathbf{x} \sim U(a, b)$.&lt;/p>
&lt;h3 id="marginal-probability" >
&lt;div>
&lt;a href="#marginal-probability">
##
&lt;/a>
Marginal Probability
&lt;/div>
&lt;/h3>
&lt;p>Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the &lt;strong>marginal probability distribution&lt;/strong>.&lt;/p>
&lt;p>For example, suppose we have discrete random variables $\bf{x}$ and $\bf{y}$ , and we know $P(\bf{x}, y)$. We can find $P(\bf{x})$ with &lt;strong>sum rule&lt;/strong>:
$$
\tag{3.3}
\forall x \in \mathbf{x}, P(\mathbf{x} = x) = \sum_y P(\mathbf{x}=x, \mathbf{y}=y) .
$$
The name &amp;ldquo;marginal probability&amp;rdquo; comes from the process of computing marginal probabilities on paper. When the values of $P(\bf{x}, y)$ are written in a grid with different values of $x$ in rows and different values of $y$ in columns, it is natural to sum across a row of the grid, then write $P(\bf{x})$ in the marginal of the paper just to the right of the row (see Table 4.1 from the book [Doing Bayesian Data Analysis]).&lt;/p>
&lt;p>&lt;img alt="discrete marginal probability" src="./images/DBDA_marginal_probability.png">&lt;/p>
&lt;p>For continuous variables, we need to use integration instead of summation:
$$
\tag{3.4}
p(x) = \int p(x, y)dy .
$$&lt;/p>
&lt;h3 id="conditional-probability" >
&lt;div>
&lt;a href="#conditional-probability">
##
&lt;/a>
Conditional Probability
&lt;/div>
&lt;/h3>
&lt;p>In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a &lt;strong>conditional probability&lt;/strong>. We denote the conditional probability that $\mathbf{y}=y$ given $\mathbf{x}=x$ as $P(\mathbf{y}=y | \mathbf{x}=x)$. This conditional probability can be computed with the formula
$$
\tag{3.5}
P(\mathbf{y}=y | \mathbf{x}=x) = \frac{P(\mathbf{y}=y , \mathbf{x}=x)}{P(\mathbf{x}=x)} .
$$
The conditional probability is only defined when $P(\mathbf{x}=x) &amp;gt; 0$. We cannot compute the conditional probability conditioned on an event that never happens.&lt;/p>
&lt;p>It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability that a person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change. Computing the consequences of an action is called making an &lt;strong>intervention query&lt;/strong>. Intervention queries are the domain of &lt;strong>causal modeling&lt;/strong>.&lt;/p>
&lt;h3 id="the-chain-rule-of-conditional-probability" >
&lt;div>
&lt;a href="#the-chain-rule-of-conditional-probability">
##
&lt;/a>
The Chain Rule of Conditional Probability
&lt;/div>
&lt;/h3>
&lt;p>Any join probability distribution over many random variables may be decomposed into conditional distribution over only one variable:
$$
\tag{3.6}
P(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}) = P(\mathbf{x}^{(1)}) \prod^n_{i=2} P(\mathbf{x}^{(i)} | \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(i-1)}) .
$$
This observation is known as the &lt;strong>chain rule&lt;/strong>, or &lt;strong>product rule&lt;/strong>, of probability. It follows immediately from the definition of conditional probability in equation 3.5.&lt;/p>
&lt;p>For example, applying the definition twice, we get
$$
\begin{eqnarray}
P(a, b, c) &amp;amp;=&amp;amp; P(a | b, c) P(b, c) \
\
P(b, c) &amp;amp;=&amp;amp; P(b | c) P(c) \
\
P(a, b, c) &amp;amp;=&amp;amp; P(a | b, c) P(b | c) P(c) .
\end{eqnarray}
$$&lt;/p>
&lt;h3 id="independence-and-conditional-independence" >
&lt;div>
&lt;a href="#independence-and-conditional-independence">
##
&lt;/a>
Independence and Conditional Independence
&lt;/div>
&lt;/h3>
&lt;p>Two random variables $\bf{x}$ and $\bf{y}$ are &lt;strong>independent&lt;/strong> if their probability distribution can be expressed as a product of two factors, one involving only $\bf{x}$ and one involving only $\bf{y}$ :
$$
\tag{3.7}
\forall x \in \mathbf{x}, y \in\mathbf{y}, p(\mathbf{x}=x, \mathbf{y}=y) = p(\mathbf{x}=x) p(\mathbf{y}=y) .
$$
Two random variables $\bf{x}$ and $\bf{y}$ are &lt;strong>conditional independent&lt;/strong> given a random variable $z$ if the conditional probability distribution over $\bf{x}$ and $\bf{y}$ factorizes in this way for every value of $z$ :
$$
\tag{3.8}
\forall x \in \mathbf{x}, y \in\mathbf{y}, z \in \mathbf{z},
p(\mathbf{x}=x, \mathbf{y}=y | \mathbf{z}=z) =
p(\mathbf{x}=x | \mathbf{z}=z) p(\mathbf{y}=y | \mathbf{z}=z) .
$$
We can denote &lt;strong>independent&lt;/strong> and &lt;strong>conditional independent&lt;/strong> with compact notation:&lt;/p>
&lt;ul>
&lt;li>$\bf{x} \perp y$ means that $\bf{x}$ and $\bf{y}$ are independent,&lt;/li>
&lt;li>$\bf{x} \perp y | z$ means that $\bf{x}$ and $\bf{y}$ are conditional independent given $z$.&lt;/li>
&lt;/ul>
&lt;h3 id="expectation-variance-and-covariance" >
&lt;div>
&lt;a href="#expectation-variance-and-covariance">
##
&lt;/a>
Expectation, Variance and Covariance
&lt;/div>
&lt;/h3>
&lt;p>The &lt;strong>expectation&lt;/strong>, or &lt;strong>expected value&lt;/strong>, of some function $f(x)$ with respect to a probability distribution $P(\mathbf{x})$ is the average, or mean value, that $f$ takes on when $x$ is drawn from $P$. For discrete variables this can be computed with summation:
$$
\tag{3.9}
\mathbb{E}&lt;em>{\mathbf{x} \sim P} [f(x)] = \sum_x P(x)f(x),
$$
while for continuous variables, it is computed with an integral:
$$
\tag{3.10}
\mathbb{E}&lt;/em>{\mathbf{x} \sim P} [f(x)] = \int p(x)f(x)dx,
$$
Expectations are linear, for example,
$$
\tag{3.11}
\mathbb{E}&lt;em>{\mathbf{x}} [\alpha f(x) + \beta g(x)] =
\alpha \mathbb{E}&lt;/em>{\mathbf{x}}[f(x)] + \beta \mathbb{E}_{\mathbf{x}}[g(x)] ,
$$
when $\alpha$ and $\beta$ are not dependent on $x$.&lt;/p>
&lt;p>The &lt;strong>variance&lt;/strong> gives a measure of how much the values of a function of a random variable $\bf{x}$ vary as we sample different values of $x$ from its probability distribution:
$$
\tag{3.12}
\text{Var}(f(x)) = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2] .
$$
When the variance is low, the values of $f(x)$ cluster near their expected value. The square root of the variance is known as the &lt;strong>standard deviation&lt;/strong>.&lt;/p>
&lt;p>The &lt;strong>covariance&lt;/strong> gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:
$$
\tag{3.13}
\text{Cov}(f(x), g(y)) = \mathbb{E} \bigg[\big(
f(x) - \mathbb{E}[f(x)])\ (g(y) - \mathbb{E}[g(y)]
\big)\bigg] .
$$
High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.&lt;/p>
&lt;ul>
&lt;li>If the sign of the covariance is positive, both variables tend to take on relatively high values simultaneously,&lt;/li>
&lt;li>If the sign of the covariance is negative, one variable tend to take on relatively high values and the other takes on a relatively low values and vice versa.&lt;/li>
&lt;/ul>
&lt;p>The notions of covariance and dependence are related but distinct concepts.&lt;/p>
&lt;ul>
&lt;li>Two independent variables have zero covariance, and two variables have nonzero covariance are dependent.&lt;/li>
&lt;li>Zero covariance means there must be no linear dependence, While independent also excludes nonlinear relationships between two variables.&lt;/li>
&lt;li>It is possible for two variables to be dependent but have zero covariance.&lt;/li>
&lt;/ul>
&lt;p>The &lt;strong>covariance matrix&lt;/strong> of a random vector $x \in \R^n$ is an $n \times n$ matrix, such that
$$
\tag{3.14}
\text{Cov}(\mathbf{x})_{i, j} = \text{Cov}(\mathbf{x}_i, \mathbf{x}_j) .
$$
The diagonal elements of the covariance give the variance:
$$
\tag{3.15}
\text{Cov}(\mathbf{x}_i, \mathbf{x}_i) = \text{Var}(\mathbf{x}_i) .
$$&lt;/p>
&lt;h3 id="common-probability-distribution" >
&lt;div>
&lt;a href="#common-probability-distribution">
##
&lt;/a>
Common Probability Distribution
&lt;/div>
&lt;/h3>
&lt;h4 id="bernoulli-distribution" >
&lt;div>
&lt;a href="#bernoulli-distribution">
###
&lt;/a>
Bernoulli Distribution
&lt;/div>
&lt;/h4>
&lt;p>The Bernoulli Distribution is a distribution over a single binary random variable. It is controlled by a single parameter $\phi \in [0, 1]$, which gives the probability of the random variable being equal to $1$. It has the following properties:
$$
\begin{eqnarray}
\tag{3.16}
P(\mathbf{x} = 1) &amp;amp;=&amp;amp; \phi \
\
\tag{3.17}
P(\mathbf{x} = 0) &amp;amp;=&amp;amp; 1 - \phi \
\
\tag{3.18}
P(\mathbf{x} = x) &amp;amp;=&amp;amp; \phi^{x} (1 - \phi)^{1-x} \
\
\tag{3.19}
\mathbb{E}&lt;em>\mathbf{x}[\mathbf{x}] &amp;amp;=&amp;amp; \phi \
\
\tag{3.20}
\text{Var}&lt;/em>\mathbf{x}(\mathbf{x}) &amp;amp;=&amp;amp; \phi (1 - \phi)
\end{eqnarray}
$$&lt;/p>
&lt;h4 id="gaussian-distribution" >
&lt;div>
&lt;a href="#gaussian-distribution">
###
&lt;/a>
Gaussian Distribution
&lt;/div>
&lt;/h4>
&lt;p>The most commonly used distribution over real numbers is the normal distribution, also known as the Gaussian distribution:
$$
\tag{3.21}
\mathcal{N}(x; \mu, \sigma^2) = \sqrt{{1 \over {2 \pi \sigma^2}}} \exp \bigg(-{1 \over {2 \sigma^2}} (x - \mu)^2 \bigg).
$$
The two parameters $\mu \in \R$ and $\sigma \in (0, \infin)$ control the normal distribution.&lt;/p>
&lt;p>When we evaluate the PDF, we need to square and invert $\sigma$. When we need to frequently evaluate the PDF with different parameter values, a more efficient way of parametrizing the distribution is to use a parameter $\beta \in (0, \infin)$ to control the &lt;strong>precision&lt;/strong>, or inverse variance, of the distribution:
$$
\tag{3.22}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{{\beta \over {2 \pi}}} \exp \bigg(-{1 \over 2} \beta (x - \mu)^2 \bigg).
$$
&lt;img alt="bell curve" src="./images/DL_gaussian_distribution.png">&lt;/p>
&lt;p>Normal distributions are a sensible choice for many applications, for two major reasons:&lt;/p>
&lt;ul>
&lt;li>First, many distributions we wish to model are truly close to being normal distributions. The &lt;strong>central limit theorem&lt;/strong> shows that the sum of many independent random variables is approximately normally distributed. This means that in practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior.&lt;/li>
&lt;li>Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model.&lt;/li>
&lt;/ul>
&lt;p>The normal distribution generalizes to $\R^n$, in which case it is known as the &lt;strong>multivariate normal distribution&lt;/strong>. It may be parametrized with a positive definite symmetric matrix $\Sigma$ :
$$
\tag{3.23}
\mathcal{N}(x; \mu, \Sigma) = \sqrt{{1 \over {(2 \pi)^n |\Sigma|}}}
\exp \bigg(-{1 \over 2} (x - \mu)^{\mathsf{T}} \Sigma^{-1} (x - \mu) \bigg).
$$
The parameter $\mu$ still gives the mean of the distribution, though now it is vector valued. The parameter $\Sigma$ gives the covariance matrix of the distribution. $|\Sigma|$ denotes the &lt;strong>determinant&lt;/strong> of $\Sigma$ .&lt;/p>
&lt;p>As in the univariate case, when we wish to evaluate the PDF multiple times with more efficient computation, we need to invert $\Sigma$ , and instead use a &lt;strong>precision matrix $\beta$&lt;/strong> :
$$
\tag{3.24}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{{|\beta| \over (2 \pi)^n}} \exp \bigg(-{1 \over 2} (x - \mu)^{\mathsf{T}} \beta (x - \mu) \bigg).
$$
We often fix the covariance matrix to be a diagonal matrix. An even simpler version is the &lt;strong>isotropic&lt;/strong> Gaussian distribution, whose covariance matrix is a scalar times the identity matrix.&lt;/p>
&lt;h4 id="exponential-and-laplace-distributions" >
&lt;div>
&lt;a href="#exponential-and-laplace-distributions">
###
&lt;/a>
Exponential and Laplace Distributions
&lt;/div>
&lt;/h4>
&lt;p>In the context of deep learning, we often want to have a probability distribution with a sharp point at $x = 0$. To accomplish this, we can use the &lt;strong>exponential distribution&lt;/strong>:
$$
\tag{3.25}
p(x; \lambda) = \lambda \mathbf{1}&lt;em>{x \ge 0} \exp(- \lambda x) .
$$
The exponential distribution uses the &lt;strong>indicator function&lt;/strong> $\mathbf{1}&lt;/em>{x \ge 0}$ to assign probability zero to all negative values of $x$.&lt;/p>
&lt;p>A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point $\mu$ is the &lt;strong>Laplace distribution&lt;/strong>
$$
\tag{3.26}
\text{Laplace}(x; \mu, \gamma) = {1 \over 2\gamma} \exp(- {|x - \mu| \over \gamma}).
$$&lt;/p>
&lt;h2 id="information-theory" >
&lt;div>
&lt;a href="#information-theory">
#
&lt;/a>
Information Theory
&lt;/div>
&lt;/h2>
&lt;p>Information theory is a branch of applied mathematics that resolves around quantifying how much information is present in a signal. In the context of machine learning, we can apply information theory to characterize probability distributions or quantify similarity between probability distributions.&lt;/p>
&lt;p>The basic intuition behind information theory is that learning that an unlikely event has has occurred is more informative than learning that a likely event has occurred. A message saying &amp;ldquo;the sun rose this morning&amp;rdquo; is so uninformative (to human being on the earth) as to be unnecessary to send, but a message saying &amp;ldquo;there are a solar eclipse this morning&amp;rdquo; is very informative.&lt;/p>
&lt;p>We would like to quantify information in a way that formalizes this intuition.&lt;/p>
&lt;ul>
&lt;li>Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.&lt;/li>
&lt;li>Less likely events should have higher information content.&lt;/li>
&lt;li>Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as head once.&lt;/li>
&lt;/ul>
&lt;p>To satisfy all three of these properties, we define the &lt;strong>self-information&lt;/strong> of an event $\mathbf{x} = x$ to be
$$
\tag{3.48}
I(x) = - \log P(x) .
$$
In this series, we always use log to mean the natural logarithm, with base $e$. Our definition of $I(x)$ is therefore written in units of &lt;strong>nats&lt;/strong>. One nat is the amount of information gained by observing an event of probability $1 \over e$. Other texts use base-2 logarithms and units called &lt;strong>bits&lt;/strong> or &lt;strong>shannons&lt;/strong>; information measured in bits is just a rescaling of information measured in nats.&lt;/p>
&lt;p>When $\bf{x}$ is continuous, an event with unit density still has zero information, despite not being an event that is guaranteed to occur.&lt;/p>
&lt;h3 id="entropy" >
&lt;div>
&lt;a href="#entropy">
##
&lt;/a>
Entropy
&lt;/div>
&lt;/h3>
&lt;p>Self-information deals only with a single outcome. We can quantify the amount of uncertainty in an entire probability distribution using the &lt;strong>Shannon entropy&lt;/strong>,
$$
\tag{3.49}
H(\mathbf{x})
= \mathbb{E}&lt;em>{\mathbf{x} \sim P}[I(x)]
= - \mathbb{E}&lt;/em>{\mathbf{x} \sim P}[\log P(x)],
$$
also denoted $H(P)$. In other words, the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives a lower bound on the number of nats needed on average to encode symbols drawn from a distribution $P$. Distributions that are nearly deterministic (where outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy. When $\bf{x}$ is continuous, the Shannon entropy is known as the &lt;strong>differential entropy&lt;/strong>.&lt;/p>
&lt;p>&lt;img alt="Shannon entropy" src="./images/DL_binary_Shannon_entropy.png">&lt;/p>
&lt;h3 id="kl-divergence" >
&lt;div>
&lt;a href="#kl-divergence">
##
&lt;/a>
KL divergence
&lt;/div>
&lt;/h3>
&lt;p>If we have two separate probability distributions $P(x)$ and $Q(x)$ over the same random variable $\bf{x}$, we can measure how different these two distributions are using the &lt;strong>Kullback-Leibler (KL) divergence&lt;/strong>:
$$
\tag{3.50} \label{eq_kld}
D_{KL}(P || Q)
= \mathbb{E}&lt;em>{\mathbf{x} \sim P} \bigg[\log {P(x) \over Q(x)} \bigg]
= \mathbb{E}&lt;/em>{\mathbf{x} \sim P}[\log P(x) - \log Q(x)] .
$$&lt;/p>
&lt;p>In the case of discrete variables, it is the extra amount of information (in bits or nats) needed to send a message containing symbols drawn from probability distribution $P$, when we use a code that was designed to minimize the length of messages drawn from probability distribution $Q$.&lt;/p>
&lt;p>The KL divergence has many useful properties, most notably being non-negative. The KL divergence is $0$ if and only if $P$ and $Q$ are the same distribution in the case of discrete variables, or equal &amp;ldquo;almost everywhere&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&amp;rdquo; in the case of continuous variables. Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions. It is not a true distance measure because it is not symmetric: $D_{KL}(P || Q) \ne D_{KL}(Q || P)$ for some $P$ and $Q$. This asymmetriy means that there are important consequences to the choice of whether to use $D_{KL}(P || Q)$ or $D_{KL}(Q || P)$. See figure 3.6 for more detail.&lt;/p>
&lt;p>&lt;img alt="KL divergence" src="./images/DL_kl_divergence.png">&lt;/p>
&lt;p>A quantity that is closely related to the KL divergence is the &lt;strong>cross-entropy&lt;/strong> $H(P, Q) = H(P) + D_{KL}(P||Q)$, which is similar to the KL divergence ($\ref{eq_kld}$) but lacking the term on the left:
$$
\tag{3.51}
H(P, Q) = - \mathbb{E}_{\mathbf{x} \sim P} \log Q(x) .
$$
Minimizing the cross-entropy with respect to $Q$ is equivalent to minimizing the KL divergence, because $Q$ does not participate in the omitted term.&lt;/p>
&lt;p>When computing many of these quantities, it is common to encounter expressions of the form $0\log0$. By convention, in the context of information theory, we treat these expressions as $\lim_{x \rightarrow 0} x \log x = 0$.&lt;/p>
&lt;h3 id="kl-散度" >
&lt;div>
&lt;a href="#kl-%e6%95%a3%e5%ba%a6">
##
&lt;/a>
KL 散度
&lt;/div>
&lt;/h3>
&lt;p>KL 散度，也称为相对熵(relative entropy)或信息散度(information divergence)，可用于度量两个概率分布之间的差异。给定两个(连续型)概率分布 $P$ 和 $Q$ ，二者之间的 KL 散度定义为
$$
\tag{C.34}
KL(P||Q) = \int^{\infin}_{-\infin} p(x) \log {p(x) \over q(x)} dx,
$$
其中，$p(x)$ 和 $q(x)$ 分别为 $P$ 和 $Q$ 的概率密度函数。KL 散度满足非负性，但不满足对称性，因此 KL 散度不是一个度量(metric)&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>。&lt;/p>
&lt;p>若将 KL 散度的定义展开，可得
$$
\begin{eqnarray}
KL(P||Q)
&amp;amp;=&amp;amp; \int^{\infin}&lt;em>{-\infin} p(x) \log {p(x)} dx - \int^{\infin}&lt;/em>{-\infin} p(x) \log {q(x)} dx \
\
\tag{C.37}
&amp;amp;=&amp;amp; -H(P) + H(P,Q)
\end{eqnarray}
$$
其中，$H(P)$ 为熵(entropy)，$H(P,Q)$ 为 $P$ 和 $Q$ 得交叉熵(cross-entropy)。在信息论中，熵 $H(P)$ 表示对来自 $P$ 的随机变量进行编码所需要的最小字节数，而交叉熵 $H(H,Q)$ 则表示使用基于 $Q$ 的编码对来自 $P$ 的变量进行编码所需要的 “额外” 的字节数；显然，额外字节数必然非负，当且仅当 $P = Q$ 时额外字节数为零。&lt;/p>
&lt;h2 id="density-estimation" >
&lt;div>
&lt;a href="#density-estimation">
#
&lt;/a>
Density Estimation
&lt;/div>
&lt;/h2>
&lt;h3 id="histogram" >
&lt;div>
&lt;a href="#histogram">
##
&lt;/a>
Histogram
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>&amp;ldquo;In a (standard) histogram, the areas of the blocks represent percentages.&amp;rdquo;&lt;/p>
&lt;p>&amp;ldquo;With the density scale on the vertical axis, the areas of the blocks come out in percent. The area under the histogram over an interval equals the percentage of cases in that interval. The total area under the histogram is 100%.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;p>A &lt;strong>histogram&lt;/strong> is a plot designed to show the distribution of values in a set of data. The values are first sorted, and then divided into a fixed number of equal-width bins. A plot is then drawn that shows the number of elements in each bin. (Note that the histograms in Figure 15.20 were not turn the counts into normalized probability density.)&lt;/p>
&lt;p>&lt;img alt="histograms" src="./images/ICPP_histograms_coin_flips.png">&lt;/p>
&lt;p>A histogram is a depiction of a &lt;strong>frequency distribution&lt;/strong>. It tells us how often a random variable has taken on a value in some range, e.g., how often the fraction of times a coin came up heads was between 0.4 and 0.5. It also provides information about the relative frequency of various ranges. For example, we can easily see that the fraction of heads falls between 0.4 and 0.5 far more frequently than it falls between 0.3 and 0.4 (see the one in the left in Figure 15.20). Notice that while the means in both plots (Figure 15.20) are about the same, the standard deviations are quite different. The spread of outcomes is much tighter when we flip the coin 1000 times per trail than when we flip the coin 100 times per trial&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>A &lt;strong>probability distribution&lt;/strong> captures the notion of relative frequency by giving the probability of a random value taking on a value within a range. Probability distributions fall into two groups: discrete probability distributions and continuous probability distributions, depending upon whether they define the probability distribution for a discrete or a continuous random variable.&lt;/p>
&lt;p>&lt;strong>Discrete probability distributions&lt;/strong> are easier to describe. Since there are a finite number of values that the variable can take on, the distribution can be described by simply listing the probability of each value.&lt;/p>
&lt;p>&lt;strong>Continuous probability distributions&lt;/strong> are trickier. Since there are an infinite number of possible values, the probability that a continuous random variable will take on a specific value is usually 0. For example, the probability that a car is traveling at exactly 81.3457283 miles per hour is probably 0. Mathematicians like to describe continuous probability distributions using a &lt;strong>Probability Density Function (PDF)&lt;/strong>. A &lt;strong>PDF&lt;/strong> describes the probability of a random variable lying between two values. Think of the PDF as defining a curve where the values on the x-axis lie between the minimum and maximum value of the random variable. Under the assumption that $x_1$ and $x_2$ lie in the domain of the random variable, the probability of the variable having a value between $x_1$ and $x_2$ is the area under the PDF curve between $x_1$ and $x_2$ .&lt;/p>
&lt;p>&lt;img alt="pdf" src="./images/ICPP_probability_density_functions.png">&lt;/p>
&lt;p>The &lt;code>random.random()&lt;/code> returns a value lies in interval [0, 1]. The area under the curve of PDF for &lt;code>random.random()&lt;/code> from 0 to 1 is 1. On the other hand, if we consider the area under the part of the curve between 0.2 and 0.4, it is 0.2 ($(0.4 - 0.2) \times 1.00$). Similarly, the area under the curve for &lt;code>random.random()+random.random()&lt;/code> from 0 to 2 is also 1, and the area under the curve from 0 to 1 is 0.5. Notice that the same length of interval has the same probability in PDF for &lt;code>random.random()&lt;/code> while some intervals are more probable than others in PDF for &lt;code>random.random()+random.random()&lt;/code>.&lt;/p>
&lt;h3 id="histogram-for-density-estimation" >
&lt;div>
&lt;a href="#histogram-for-density-estimation">
##
&lt;/a>
Histogram for density estimation
&lt;/div>
&lt;/h3>
&lt;p>Density estimation can be used of probability distributions (such Gaussian, Beta, Dirichlet distributions) having specific functional forms governed by a small number of parameters whose values are to be determined from a data set. This is called the &lt;em>parametric&lt;/em> approach to density modeling. An important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. For instance, if the process that generates the data is multimodal (多峰的), then this aspect of the distribution can never be captured by a Gaussian, which is necessarily unimodal&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We consider some &lt;em>nonparametric&lt;/em> approaches to density estimation that make few assumptions about the form of the distribution. Here we focus mainly on simple frequentist methods. Let&amp;rsquo;s start with a discussion of histogram methods for density estimation, we explore the properties of histogram density model, focusing on the case of a single continuous variable $x$.&lt;/p>
&lt;p>Standard histograms simply partition $x$ into distinct bins of with $\Delta_i$ and then count the number $n_i$ of observations of $x$ falling in bin $i$. In order to turn this count into a normalized probability density, we simply divide by the total number $N$ of observations and by the width $\Delta_i$ of the bins to obtain probability values for each bin given by
$$
\tag{2.241}
p_i = \frac{n_i}{N \Delta_i}
$$
for which it is easily seen that $\int p(x)dx = 1$. This gives a model for the density $p(x)$ that is constant over the width of each bin, and often the bins are chosen to have the same width $\Delta_i = \Delta$.&lt;/p>
&lt;p>&lt;img alt="histogram models" src="./images/PRML_histogram_models.png">&lt;/p>
&lt;p>From above figure, we can see that&lt;/p>
&lt;ul>
&lt;li>when $\Delta$ is very small (top right), the resulting density model is very spiky (lost a lot of structure of underlying distribution),&lt;/li>
&lt;li>if $\Delta$ is too large (bottom right), the result is a model that is too smooth (fails to capture the bimodal property of the green curve).&lt;/li>
&lt;/ul>
&lt;p>The best results are obtained for some intermediate value of $\Delta$ (middle right). In principle, a histogram density model is also dependent on the choice of edge location for the bins, though this is typically much less significant than the value of $\Delta$.&lt;/p>
&lt;p>Note that the histogram method has the property that once the histogram has been computed, the data set itself can be discarded, which can be advantageous if the data set is large. Also, the histogram approach is easily applied if the data points are arriving sequentially.&lt;/p>
&lt;p>In practice, the histogram technique can be useful for obtaining a quick visualization of data in one or two dimensions but is unsuited to most density estimation applications. The limitations are&lt;/p>
&lt;ul>
&lt;li>the estimated density has discontinuities that are due to the bin edges,&lt;/li>
&lt;li>in a space of high dimensionality, the number of bins (M) are exponential scaling with the number of dimension (D), which is $M^D$. The quantity of data needed to provide meaningful estimates of local probability density would be prohibitive.&lt;/li>
&lt;/ul>
&lt;p>Two widely used nonparametric techniques for density estimation, &lt;strong>kernel estimators&lt;/strong> and &lt;strong>nearest neighbors&lt;/strong>, which have better scaling with dimensionality than the simple histogram model.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>technical view of continuous variables [more advance mathematics knowledge required].&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>度量(metric)应满足四个基本性质：非负性($dist(x_i,x_j) \ge 0$)；同一性($dist(x_i, x_j)=0 \iff x_i=x_j$)；对称性($dist(x_i, x_j) = dist(x_j, x_i)$)；直递性($dist(x_i, x_j) \le dist(x_i, x_k) + dist(x_k, x_j)$)&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Recall that this is also an example of the law of the average.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Consider Gaussian Mixture Models in this case.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Book Notes: Deep-Forest Model</title><link>/posts/ml101/treebasedmodels/deepforest/</link><pubDate>Sat, 31 Aug 2019 14:11:27 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/treebasedmodels/deepforest/</guid><description>&lt;h1 id="deep-foresthttpsarxivorgabs170208835" >
&lt;div>
&lt;a href="#deep-foresthttpsarxivorgabs170208835">
##
&lt;/a>
&lt;a href="https://arxiv.org/abs/1702.08835">Deep Forest&lt;/a>
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>online paper, follow the link to all the details.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>In this paper, we extend our preliminary study which proposes the &lt;a href="https://fgg100y.github.io/">gcForest&lt;/a> (multi-Grained Cascade Forest) approach for constructing deep forest, a non-NN style deep model. This is a novel decision tree ensemble, with a cascade structure which enables representation learning by forests. Its representational learning ability can be further enhanced by multi-grained scanning, potentially enabling gcForest to be contextual or structural aware. The cascade levels can be automatically determined such that the model complexity can be determined in a data-dependent way rather than manually designed before training; this enables gcForest to work well even on small-scale data, and enables users to control training costs according to computational resource available. Moreover, the gcForest has much fewer hyper-parameters than DNNs. Even better news is that its performance is quite robust to hyper-parameter settings; our experiments show that in most cases, it is able to get excellent performance by using the default setting, even across different data from different domains.&lt;/p>
&lt;/blockquote>
&lt;h2 id="inspiration-from-dnns1" >
&lt;div>
&lt;a href="#inspiration-from-dnns1">
#
&lt;/a>
Inspiration from DNNs&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>
&lt;/div>
&lt;/h2>
&lt;p>It is widely recognized that the &lt;em>representation learning&lt;/em> ability is crucial for the success of deep neural networks. We believe&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> that the model complexity itself cannot explain the success of DNNs (e.g., large flat networks are not as successful as deep ones) and the &lt;em>layer-by-layer processing&lt;/em> is what really crucial for representation learning in DNNs. Figure 1 provides an illustration&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img alt="layer-by-layer_processing" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_crucial_of_representation_learning.png">&lt;/p>
&lt;p>Learning models, e.g., decision trees and Boosting machines, which also conduct layer-by-layer processing, why they are not as successful as DNNs? We believe that the most important distinguishing factor is that, in contrast to DNNs where new features are generated as illustrated in Figure 1, decision trees and Boosting machines always work on the original feature representation without creating new features during the learning process, or in other words, there is no in-model feature transformation. Moreover, DTs and Boosting machines can only have limited model complexity.&lt;/p>
&lt;p>Overall, we conjecture that behind the mystery of DNNs there are three crucial characteristics, i.e., layer-by-layer processing, in-model feature transformation, and sufficient model complexity. We will try to endow these characteristics to our non-NN style deep model.&lt;/p>
&lt;h2 id="inspiration-from-ensemble-learning" >
&lt;div>
&lt;a href="#inspiration-from-ensemble-learning">
#
&lt;/a>
Inspiration from Ensemble Learning
&lt;/div>
&lt;/h2>
&lt;p>It is well known that an ensemble (multiple learners are trained and combined) can usually achieve better generalization performance than single learners.&lt;/p>
&lt;p>To construct a good ensemble, the individual learners should be &lt;em>accurate&lt;/em> and &lt;em>diverse&lt;/em>. Combining only accurate learners is often inferior to combining some accurate learners with some relatively weaker ones, because the complementarity is more important than pure accuracy. Here is the equation derived from &lt;em>error-ambiguity decomposition&lt;/em>&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>:
$$
\tag{1}
E = \bar{E} - \bar{A},
$$
where $E$ denotes the error of an ensemble, $\bar{E}$ denotes the average error of individual classifiers in the ensemble, and $\bar{A}$ denotes the average &lt;em>ambiguity&lt;/em>, later called &lt;em>diversity&lt;/em>, among the individual classifiers. Eq. 1 reveals that, the more accurate and more diverse the individual classifiers, the better the ensemble. However, it could not be taken as an objective function for optimization, because the &lt;em>ambiguity&lt;/em> term is mathematically defined in the derivation and cannot be operated directly&lt;sup id="fnref1:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Actually, &lt;em>&amp;ldquo;what is diversity?&amp;rdquo;&lt;/em> remains the holy grail problem in ensemble learning.&lt;/p>
&lt;p>In practice, the basic strategy of diversity enhancement is to inject randomness based on some heuristics during the training process. Roughly speaking, there are four major category of mechanisms&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>data sample manipulation&lt;/strong>: which works by generating different data samples to trian individual learners.&lt;/p>
&lt;p>E.g., bootstrap sampling is exploited by Bagging; sequential importance sampling is adopted by AdaBoost.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>input feature manipulation&lt;/strong>: which works by generating different feature subspaces to train individual learners.&lt;/p>
&lt;p>E.g., the Random Subspace approach randomly picks a subset of features.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>learning parameter manipulation&lt;/strong>: which works by using different parameter settings of the base learning algorithm to generate diverse individual learners.&lt;/p>
&lt;p>E.g., different initial selections can be applied to individual neural networks; different split selections can be applied to individual decision trees.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>output representation manipulation&lt;/strong>: which works by using different output representations to generate diverse individual learners.&lt;/p>
&lt;p>E.g., the ECOC approach employs error-correcting output codes; the Flipping Output method randomly changes the labels of some training instances.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Note that, however, these mechanisms are not always effective. More information about ensemble learning can be found in the book Ensemble Methods&lt;sup id="fnref1:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Next we give you, the gcForest, which can be viewed as a decision tree ensemble approach that utilizes almost all categories of mechanisms for diversity enhancement.&lt;/p>
&lt;h2 id="the-gcforest-approach" >
&lt;div>
&lt;a href="#the-gcforest-approach">
#
&lt;/a>
The gcForest Approach
&lt;/div>
&lt;/h2>
&lt;p>First introduce the cascade forest structure, and then the multi-grained scanning, followed by the overall architecture.&lt;/p>
&lt;h3 id="cascade-forest-structure" >
&lt;div>
&lt;a href="#cascade-forest-structure">
##
&lt;/a>
Cascade Forest Structure
&lt;/div>
&lt;/h3>
&lt;p>Representation learning in DNNs mostly relies on the layer-by-layer processing of raw features. Inspired by this recognition, gcForest employs a cascade structure, as illustrated in Figure 2, where each level of cascade receives feature information processed by its preceding level, and outputs its processing result to the next level.&lt;/p>
&lt;p>&lt;img alt="cascade-forest structure" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_cascade_forest_structure.png">&lt;/p>
&lt;p>Each level is an ensemble of decision tree forests, i.e., an &lt;em>ensemble&lt;/em> of &lt;em>ensembles&lt;/em>. Here, we include different types of forests to encourage the &lt;em>diversity&lt;/em>, because diversity is crucial for ensemble construction.&lt;/p>
&lt;p>For simplicity, suppose that we use two completely-random tree forests and two random forests. Each completely-random tree forest contains 500 completely-random trees, generated by randomly selecting a feature for split at each node of the tree, and growing tree until pure leaf, i.e., each leaf node contains only the same class of instances. Similarly, each random forest contains 500 trees, by randomly selecting $\sqrt{d}$ number of features as candidate ($d$ is the number of input features) and choosing the one with the best &lt;em>gini&lt;/em> value for split. (Note that the number of trees in each forest is a hyper-parameter.)&lt;/p>
&lt;p>Given an instance, each forest will produce an estimate of class distribution, by counting the percentage of different classes of training examples at the leaf node where concerned instance falls, and then averaging across all trees in the same forest, as illustrated in Figure 3, where red color highlights paths along which the instance traverses to leaf nodes.&lt;/p>
&lt;p>&lt;img alt="class-vector generation" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_class-vector_generation.png">&lt;/p>
&lt;p>The estimated class distribution forms a class vector, which is then concatenated with the original feature vector to be input to the next level of cascade. For example, suppose there are three classes, then each of the four forests will produce a three-dimensional class vector; thus the next level of cascade will receive 12 ($= 3 \times 4$) augmented features.&lt;/p>
&lt;p>Note that here we take the simplest form of class vectors, i.e., the class distribution at the leaf nodes into which the concerned instance falls. The more complex form of class vectors can be constructed by getting more distributions such as class distribution of the parent nodes which express prior distribution, the sibling nodes which express complementary distribution, etc.&lt;/p>
&lt;p>To reduce the risk of over-fitting, class vector produced by each forest is generated by $k$-fold cross validation. In detail, each instance will be used as training data for $k - 1$ times, resulting $k - 1$ class vectors, which are then averaged to produce the final class vector as augmented features for the next level of cascade. After expanding a new level, the performance of the whole cascade can be estimated on validation set, and the training procedure will terminate if there is no significant performance gain; thus, the number of cascade levels is automatically determined. Note that the training error rather than cross validation error can also be used to control the cascade growth when the training cost is concerned or limited computation resource available.&lt;/p>
&lt;h3 id="multi-grained-scanning" >
&lt;div>
&lt;a href="#multi-grained-scanning">
##
&lt;/a>
Multi-Grained Scanning
&lt;/div>
&lt;/h3>
&lt;p>DNNs are powerful in handling feature relationships, e.g., convolutional-NN are effective on image data where spatial relationships among the raw pixels are critical; recurrent-NN are effective on sequence data where sequential relationships are critical. Inspired by this recognition, we enhance cascade forest with a procedure of multi-grained scanning.&lt;/p>
&lt;p>&lt;img alt="sliding-windows scanning" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_sliding-windows_scanning.png">&lt;/p>
&lt;p>As Figure 4 illustrates, sliding windows are used to scan the raw features. Suppose there are 400 raw features and a window size of 100 features is used. For sequence data, a 100-dimensional feature vector will be generated by sliding the window for one feature; in total 301 feature vectors are produced. If the raw features are with spacial relationships, such as a $20 \times 20$ panel of 400 image pixels, then a $10 \times 10$ window will produce 121 feature vectors. All feature vectors extracted from positive/negative training examples are regarded as positive/negative instances, which will then be used to generate class vectors like in Section [Cascade Forest Structure](###Cascade Forest Structure): the instance extracted from the same size of windows will be used to train a completely-random tree forest and a random forest, and then the class vectors are generated and concatenated as transformed features. As Figure 4 illustrates, suppose that there are 3 classes and a 100-dimensional window is used; then, 301 three-dimensional class vectors are produced by each forest, leading to a 1860-dimensional transformed feature vector corresponding to the original 400-dimensional raw feature vector.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>convolution operations&lt;/strong>: padding and strides&lt;/p>
&lt;p>when the input shape is $(n_h \times n_w)$, the &lt;em>convolution kernel&lt;/em>&amp;rsquo;s shape is $(k_h \times k_w)$,&lt;/p>
&lt;p>&lt;strong>with no padding and stride (default with $s_h = s_w = 1$)&lt;/strong>, then output shape will be:
$$
(n_h - k_h + 1, n_w - k_w + 1),
$$
&lt;strong>with padding (add $p_h$ rows and $p_w$ columns ) and stride $(s_h = s_w = 1)$,&lt;/strong> then output shape will be:
$$
(n_h - k_h + p_h + 1, n_w - k_w + p_w + 1),
$$
&lt;strong>with padding (add $p_h$ rows and $p_w$ columns ) and stride $(s_h, s_w)$,&lt;/strong> then output shape will be:
$$
\bigg((n_h-k_h+p_h+s_h)/s_h, (n_w-k_w+p_w+s_w)/s_w \bigg)
$$&lt;/p>
&lt;p>If we set $p_h=k_h-1$ and $p_w=k_w-1$, then the output shape will be simplified to:
$$
\bigg((n_h+s_h-1)/s_h, (n_w+s_w-1)/s_w \bigg)
$$&lt;/p>
&lt;p>Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be:
$$
\bigg((n_h/s_h)， (n_w/s_w)\bigg)
$$&lt;/p>
&lt;hr>
&lt;p>For the instances extracted from the windows, we simply assign them with the label of the original training example. Here, some label assignments are inherently incorrect. For example, suppose the original training example is a positive image about &amp;ldquo;car&amp;rdquo;; it is clearly that many extracted instances do not contain a car, and therefore, they are incorrectly labeled as positive. This is actually related to the Flipping Output method&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, a representative of output representation manipulation for ensemble diversity enhancement.&lt;/p>
&lt;p>Note that when transformed feature vectors are too long to be accommodated, feature sampling can be performed, e.g., by subsampling the instances generated by sliding window scanning, since completely-random trees do not rely on feature split selection whereas random forests are quite insensitive to inaccurate feature split selection. Such a feature sampling process is also related to the Random Subspace method&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, a representative of input feature manipulation for ensemble diversity enhancement.&lt;/p>
&lt;p>Figure 4 shows only one size of sliding window. By using multiple sizes of sliding windows, differently grained feature vectors will be generated, as show in Figure 5.&lt;/p>
&lt;p>&lt;img alt="multi-grained scanning" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\df_multi-grained_scanning.png">&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>DNNs, in a more technically view, is &amp;ldquo;multiple layers of parameterized differentiable nonlinear modules that can be trained by back-propagation.&amp;rdquo; Also note that back-propagation requires differentiability.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>There is no rigorous justification yet.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge, MA, 2016.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>A. Krogh and J. Vedelsby. Neural network ensembles, cross validation, and active learning. In G. Tesauro, D. S.Touretzky, and T. K. Leen, editors, Advances in Neural Information Processing Systems 7, pages 231{238. 1995.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>Z.-H. Zhou. Ensemble Methods: Foundations and Algorithms. CRC, Boca Raton, FL, 2012.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>L. Breiman. Randomizing outputs to increase prediction accuracy. Machine Learning, 40(3):113–120, 2000.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Analysis and Machine Intelligence, 20(8):832–844, 1998.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Book Notes: Tree-based Models</title><link>/posts/ml101/treebasedmodels/treemodels/</link><pubDate>Sat, 31 Aug 2019 11:11:27 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/ml101/treebasedmodels/treemodels/</guid><description>&lt;h1 id="tree-based-models" >
&lt;div>
&lt;a href="#tree-based-models">
##
&lt;/a>
Tree-based models
&lt;/div>
&lt;/h1>
&lt;h2 id="part-i-theorist-views" >
&lt;div>
&lt;a href="#part-i-theorist-views">
#
&lt;/a>
Part-I: Theorist views
&lt;/div>
&lt;/h2>
&lt;p>&lt;strong>基本术语和符号约定&lt;/strong>&lt;/p>
&lt;p>一般地，令 $D = {x_1, x_2, \ldots, x_m }$ 表示包含 $m$ 个示例的数据集，每个示例由 $d$ 个属性描述，则每个示例 $x_i = (x_{i1}, x_{i2}, \ldots, x_{id})$ 是 $d$ 维样本空间 $\mathcal{X}$ 的一个向量&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>，$x_i \in \mathcal{X}$, 其中 $x_{ij}$ 是 $x_i$ 在第 $j$ 个属性上的取值， $d$ 称为样本 $x_i$ 的“维数”（dimensionality）。&lt;/p>
&lt;p>要建立一个关于“预测(prediction)”的模型，单有示例数据（也称为样本，sample）还不行，我们还需要获得训练样本的“结果”信息，例如，一个描述西瓜的记录“（（色泽=青绿；根蒂=蜷缩；敲声=浊响），好瓜）”。这里，关于示例结果的信息，例如 “好瓜” ，称为 “标记(label)”；拥有了标记信息的示例，则称之为 &amp;ldquo;样例(example)&amp;quot;。&lt;/p>
&lt;p>一般地，用 $(x_i, y_i)$ 表示第 $i$ 个样例，其中 $y_i \in \mathcal{Y}$ 是示例 $x_i$ 的标记， $\mathcal{Y}$ 是所有标记的集合，亦称“标记空间(label space)”或“输出空间”。&lt;/p>
&lt;p>如果我们想要预测的是离散值，例如 “好瓜” “坏瓜”，此类学习任务称为 “分类(classification)”；如果要预测的是连续值， 例如西瓜的成熟度0.9，0.4，此类学习任务称为 “回归(regression)”。二分类(binary classification)任务中，通常令 $\mathcal{Y} = {-1, +1 }$ 或 $\mathcal{Y} = {0, 1 }$；对于多分类(multi-class classification), $|\mathcal{Y}| &amp;gt; 2$；对回归任务，$\mathcal{Y} = \R$，$\R$ 为实数集。&lt;/p>
&lt;h2 id="decision-tree" >
&lt;div>
&lt;a href="#decision-tree">
#
&lt;/a>
Decision Tree
&lt;/div>
&lt;/h2>
&lt;h3 id="决策树生成算法" >
&lt;div>
&lt;a href="#%e5%86%b3%e7%ad%96%e6%a0%91%e7%94%9f%e6%88%90%e7%ae%97%e6%b3%95">
##
&lt;/a>
决策树生成算法
&lt;/div>
&lt;/h3>
&lt;p>一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定测试序列。&lt;/p>
&lt;p>决策树学习的目的是为了产生一棵泛化性能强的决策树，亦即处理未见示例（unseen samples）的能力强的决策树。其基本流程遵循简单且直观的“分而治之”（divide-and-conquer）策略，如&lt;code>决策树学习基本算法&lt;/code>所示。&lt;/p>
&lt;hr>
&lt;p>&lt;code>决策树学习基本算法&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>输入&lt;/strong>： 训练集 $D = {(x_1, y_1), \dots, ({x_m, y_m}) }$; \
属性集 $A = {a_1, \ldots, a_d }$&lt;/p>
&lt;p>&lt;strong>过程&lt;/strong>： 函数 $\text{TreeGenerate}(D, A)$&lt;/p>
&lt;p>1: 生成结点 $\text{node}$;&lt;/p>
&lt;p>2: &lt;strong>if&lt;/strong> $D$ 中样本全属于同一类别 $C$ &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>3: 将 $\text{node}$ 标记为 $C$ 类叶结点；&lt;strong>return&lt;/strong>&lt;/p>
&lt;p>4: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>5: &lt;strong>if&lt;/strong> $A = \phi$ &lt;strong>OR&lt;/strong> $D$ 中样本在$A$ 上取值相同 &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>6: 将 $\text{node}$ 标记为叶结点，其类别标记为 $D$ 中样本数量最多的类；&lt;strong>return&lt;/strong>&lt;/p>
&lt;p>7: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>&lt;strong>8&lt;/strong>: 从 $A$ 中选择最优划分属性 $a_*$;&lt;/p>
&lt;p>9: &lt;strong>for&lt;/strong> 属性 $a_&lt;em>$ 的每一个值 $a_&lt;/em>^v$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>10: 为 $\text{node}$ 生成一个分支；令 $D_v$ 表示 $D$ 中在 $a_&lt;em>$ 上取值为 $a_&lt;/em>^v$ 的样本子集；&lt;/p>
&lt;p>11: &lt;strong>if&lt;/strong> $D_v$ 为空 &lt;strong>then&lt;/strong>&lt;/p>
&lt;p>12: 将分支结点标记为叶结点，其类别标记为 $D$ 中样本数量最多的类；&lt;strong>return&lt;/strong>&lt;/p>
&lt;p>13: &lt;strong>else&lt;/strong>&lt;/p>
&lt;p>14: 以 $\text{TreeGenerate}(D_v, A - a_*)$ 为分支结点&lt;/p>
&lt;p>15: &lt;strong>end if&lt;/strong>&lt;/p>
&lt;p>16: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>&lt;strong>输出&lt;/strong>： 以 $\text{node}$ 为根结点的一棵决策树&lt;/p>
&lt;hr>
&lt;p>显然，决策树的生成时一个递归过程，在&lt;code>决策树基本算法&lt;/code>中，有三种情形会导致递归返回：&lt;/p>
&lt;ol>
&lt;li>当前结点包含的样本全部属于同一类别 （无需进一步划分）&lt;/li>
&lt;li>当前属性集为空，或是所有样本在所有属性上取值相同 （无法进一步划分）&lt;/li>
&lt;li>当前结点包含的样本集合为空 （不能进一步划分）&lt;/li>
&lt;/ol>
&lt;p>在第2种情形下，我们把当前结点标记为叶结点，并将其类别设定为该结点中样本数量最多的类别；在第3种情形下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别，注意这两种情形处理实质不同：情形2中是利用当前结点的后验分布，而情形3中则是把父结点的样本分布作为当前结点的先验分布。&lt;/p>
&lt;blockquote>
&lt;p>《The hundred-Page Machine Learning》&lt;/p>
&lt;p>&lt;img alt="build tree the 1st split" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\looPagesML_dtree_build.png">&lt;/p>
&lt;p>The ID3 learning algorithm works as follows. Let $\cal{S}$ denotes a set of labeled examples. In the begining, the decision tree only has a start noed (root node) that contains all examples: $\mathcal{S} = {(\mathbb{x}&lt;em>i, y_i) }^N_i$. Start with a constant model $f&lt;/em>{ID3}$ :
$$
\tag{6}
f_{ID3} = {1 \over |\mathcal{S}|} \sum_{(\mathbb{x},y) \in \mathcal{S}} y .
$$
The prediction given by the above model, $f_{ID3}(\mathbb{x})$, would be the same for any input $\mathbb{x}$. The corresponding decision tree is shown in fig4(a).&lt;/p>
&lt;p>The we search through all features $j = 1, \ldots, D$ and all thresholds $t$, and split the set $\cal{S}$ into two subsets:&lt;/p>
&lt;ul>
&lt;li>$\mathcal{S}_{_} = {(\mathbb{x},y) | (\mathbb{x},y) \in \mathcal{S}, x^{(j)} &amp;lt; t }$ and&lt;/li>
&lt;li>$\mathcal{S}_{+} = {(\mathbb{x},y) | (\mathbb{x},y) \in \mathcal{S}, x^{(j)} \ge t }$ .&lt;/li>
&lt;/ul>
&lt;p>The new two subsets would go to two new leaf nodes (or inter nodes), and we evaluate, for all possible pairs $(j, t)$ how good the split with pieces $\mathcal{S}&lt;em>{_}$ and $\mathcal{S}&lt;/em>{+}$ is (see the followed section &lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/###划分选择">划分选择&lt;/a>). Finally, we pick the best such values $(j, t)$ for splitting $\cal{S}$ into $\mathcal{S}&lt;em>{_}$ and $\mathcal{S}&lt;/em>{+}$ , from two new leaf nodes, and continue recursively on $\mathcal{S}&lt;em>{_}$ and $\mathcal{S}&lt;/em>{+}$ (or quit if reach some criterion). A decision tree after one split is illustrated in fig4(b).&lt;/p>
&lt;/blockquote>
&lt;h3 id="划分选择" >
&lt;div>
&lt;a href="#%e5%88%92%e5%88%86%e9%80%89%e6%8b%a9">
##
&lt;/a>
划分选择
&lt;/div>
&lt;/h3>
&lt;p>决策树学习的关键是如何选择最优划分属性（&lt;code>决策树基本算法&lt;/code> 第8行）。一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高。&lt;/p>
&lt;h4 id="信息增益" >
&lt;div>
&lt;a href="#%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a">
###
&lt;/a>
信息增益
&lt;/div>
&lt;/h4>
&lt;p>“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k \ (k=1, \ldots, |\mathcal{Y}|)$，则 $D$ 信息熵定义为
$$
\tag{4.1}
\text{Ent}{(D)} = - \sum^{|\mathcal{Y}|}_{k=1} p_k \text{log}_2 p_k.
$$
$\text{Ent}(D)$ 的值越小，则 $D$ 的纯度越高。&lt;/p>
&lt;p>假定离散属性 $a$ 有 $V$ 个可能的取值 ${a^1, \ldots, a^V }$，若使用 $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$。我们根据式(4.1)计算出 $D^v$ 的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重 ${|D^v| \over |D|}$，即样本数越多的分支结点的影响越大，于是可以计算出用属性 $a$ 对样本集 $D$ 进行划分所获得的“信息增益(information gain)”
$$
\tag{4.2}
\text{Gain}(D, a) = \text{Ent}(D) - \sum^V_{v=1} {|D^v| \over |D|} \text{Ent}(D^v).
$$
一般而言，信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的“纯度提升”越大。因此，我们可以用信息增益来进行决策树的划分属性选择，即选择属性 $a_* = argmax_{(a \in A)} \text{Gain}(D, a)$。&lt;/p>
&lt;p>$\color{Green}{\bold{例子}}$&lt;/p>
&lt;p>&lt;img alt="xgs d2.0" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree_dataset1.png">&lt;/p>
&lt;p>以表4.1中的西瓜数据集2.0为例。该数据集包含17个训练样例，用以学习一棵能预测没有尝过的是不是好瓜的决策树。显然，分类的类别共两类（是好瓜，不是好瓜），$|\mathcal{Y}| = 2$。在决策树开始学习时，根结点包含 $D$ 中所有的样例，其中正例占 $p_1 = 8 / 17$ ，反例占 $p_1 = 9 / 17$。于是，根据式(4.1)可计算出根结点的信息熵为
$$
\text{Ent}(D) = - \sum^2_{k=1} p_k \text{log}_2 p_k = - \bigg({8 \over 17}\text{log}_2 {8 \over 17} + {9 \over 17}\text{log}_2 {9 \over 17} \bigg) \approx 0.998 .
$$
然后，我们要计算出当前属性集合｛色泽，根蒂，敲声，纹理，脐部，触感｝中每个属性的信息增益。以属性 “色泽” 为例，它有3个可能的取值：｛青绿，乌黑，浅白｝。若使用该属性对 $D$ 进行划分，则可得到3个子集，分别记为：$D^1 (色泽=青绿)，D^2 (色泽=乌黑)，D^3 (色泽=浅白）$。&lt;/p>
&lt;p>由表4.1可得，子集 $D^1$ 包含编号为｛1，4，6，10，13，17｝的6个样例，其中正例占 $p_1 = 3 / 6$ ，反例占 $p_2 = 3 / 6$；子集 $D^2$ 包含编号为｛2，3，7，8，9，15｝的6个样例，其中正例占 $p_1 = 4 / 6$ ，反例占 $p_2 = 2 / 6$；子集 $D^3$ 包含编号为｛5，11，12，14，16｝的5个样例，其中正例占 $p_1 = 1 / 5$ ，反例占 $p_2 = 4 / 5$。根据式(4.1)可计算出用 “色泽” 划分之后所得到的3个分支结点的信息熵为
$$
\begin{eqnarray}
\text{Ent}(D^1) &amp;amp;=&amp;amp; - \bigg({3 \over 6}\text{log}_2 {3 \over 6} + {3 \over 6}\text{log}_2 {3 \over 6} \bigg) = 1.000, \
\text{Ent}(D^2) &amp;amp;=&amp;amp; - \bigg({4 \over 6}\text{log}_2 {4 \over 6} + {2 \over 6}\text{log}_2 {2 \over 6} \bigg) = 0.918, \
\text{Ent}(D^3) &amp;amp;=&amp;amp; - \bigg({1 \over 5}\text{log}_2 {1 \over 5} + {4 \over 5}\text{log}&lt;em>2 {4 \over 5} \bigg) = 0.772, \
\end{eqnarray}
$$
于是，根据式(4.2)可计算出属性 “色泽” 的信息增益为
$$
\begin{eqnarray}
\text{Gain}(D, 色泽)
&amp;amp;=&amp;amp; \text{Ent}(D) - \sum^3&lt;/em>{v=1} {|D^v| \over |D|} \text{Ent}(D^v) \
&amp;amp;=&amp;amp; 0.998 - \bigg({6 \over 17} \times 1.000 + {6 \over 17} \times 0.918 + {5 \over 17} \times 0.772 \bigg) \
&amp;amp;=&amp;amp; 0.109 .
\end{eqnarray}
$$
类似的，我们可以计算出其他属性的信息增益：
$$
\text{Gain}(D, 根蒂) = 0.143;\text{Gain}(D, 敲声) = 0.141 \
\text{Gain}(D, 纹理) = 0.381;\text{Gain}(D, 脐部) = 0.289 \
\text{Gain}(D, 触感) = 0.006.\qquad \qquad \qquad \qquad \quad \ &lt;br>
$$
显然，属性 “纹理” 的信息增益最大，于是它被选为划分属性。图4.3给出了基于 “纹理” 对根结点进行划分的结果，各分支结点所包含的样例子集显示在结点中。&lt;/p>
&lt;p>&lt;img alt="tree first split" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree1.png">&lt;/p>
&lt;p>然后，决策树学习算法将对每个分支结点做进一步划分。以图4.3中第一个分支结点（“纹理=清晰”）为例，该结点包含的样例集合 $D^1$ 中有编号为 ｛1，2，3，4，5，6，8，10，15｝的9个样例，可用属性集合为 ｛色泽，根蒂，敲声，脐部，触感｝&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>。基于 $D^1$ 计算出各个属性的信息增益：
$$
\text{Gain}(D, 根蒂) = 0.458;\text{Gain}(D, 敲声) = 0.331 \
\text{Gain}(D, 色泽) = 0.043;\text{Gain}(D, 脐部) = 0.458 \
\text{Gain}(D, 触感) = 0.458.\qquad \qquad \qquad \qquad \quad \ &lt;br>
$$
“根蒂”、“脐部”、“触感” 3个属性均取得最大的信息增益，可任选其中之一作为划分属性。类似的，对每个分支结点进行上述操作，最终得到的决策树如图4.4所示。&lt;/p>
&lt;p>&lt;img alt="tree first split" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree2.png">&lt;/p>
&lt;h4 id="增益率" >
&lt;div>
&lt;a href="#%e5%a2%9e%e7%9b%8a%e7%8e%87">
###
&lt;/a>
增益率
&lt;/div>
&lt;/h4>
&lt;p>在上面的例子中，我们有意忽略了表4.1中的 “编号” 这一列。如果把 “编号” 也作为一个候选划分属性，则根据式(4.2)可计算出它的信息增益为$0.998$，远大于其他候选划分属性。这很容易理解：“编号” 将产生17个分支，每个分支结点仅包含一个样本，这些分支结点的纯度已达到最大。然而，这样的决策树显然不具有泛化能力，无法对新样本进行有效预测。&lt;/p>
&lt;p>实际上，信息增益准则对可取值数目较多的属性有所偏好。为减少这种偏好可能带来的不利影响，C4.5决策树算法使用 “增益率(gain ratio)” 来选择最优划分属性。采用与式4.2相同的符号表示，增益率定义为
$$
\tag{4.3}
\text{Gain_ratio}(D, a) = \frac{\text{Gain}(D, a)}{\text{IV}(a)},
$$
其中
$$
\tag{4.4}
\text{IV}(a) = - \sum^V_{v=1} {|D^v| \over |D|} \text{log}_2 {|D^v| \over |D|}
$$
称为属性 $a$ 的 “固有值(intrinsic value)”。属性 $a$ 的可能取值数目越多（即 $V$ 越大），则 ${\text{IV}(a)}$ 的值也越大。&lt;/p>
&lt;p>注：增益率准则对可取值数目较少的属性有所偏好，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用一个&lt;strong>启发式&lt;/strong>：先从候选划分属性中找出 &lt;em>信息增益&lt;/em> 高于平均水平的属性，再从中选择 &lt;em>增益率&lt;/em> 最大的。&lt;/p>
&lt;h4 id="基尼指数" >
&lt;div>
&lt;a href="#%e5%9f%ba%e5%b0%bc%e6%8c%87%e6%95%b0">
###
&lt;/a>
基尼指数
&lt;/div>
&lt;/h4>
&lt;p>CART(Classification and Regression Tree) 决策树使用基尼指数（Gini index）来选择划分属性。数据集D的纯度可用基尼指数来度量：
$$
\begin{eqnarray}
Gini(D)
&amp;amp;=&amp;amp; \sum^{|\mathcal{Y}|}&lt;em>{k=1} \sum&lt;/em>{k&amp;rsquo; \ne k} p_k p_{k&amp;rsquo;} \
&amp;amp;=&amp;amp; 1 - \sum^{|\mathcal{Y}|}&lt;em>{k=1} {p&lt;/em>{k}}^2.
\end{eqnarray}
$$
直观来说，$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此$Gini(D)$越小，则数据集$D$的纯度越高。&lt;/p>
&lt;p>属性 $a$ 的基尼指数定义为
$$
\text{Gini_index}(D, a) = \sum^{V}&lt;em>{v=1} \frac{|D^v|}{|D|}Gini(D^v)
$$
于是，我们候选属性集合$A$中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即$a^* = argmin&lt;/em>{(a \in A)} \text{Gini_index}(D, a)$.&lt;/p>
&lt;blockquote>
&lt;p>《hands-on Machine Learning with sklearn, Keras and tensorflow》&lt;/p>
&lt;p>The CART Training Algorithm&lt;/p>
&lt;p>&lt;strong>1. Classification Task&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Sklearn&lt;/strong> uses the CART algorithm to train Decision Tree (i.e., &amp;ldquo;growing&amp;rdquo; tree). The algorithm works by first splitting the training set into two subsets using a single feature $k$ and a threshold $t_k$ (e.g., &amp;ldquo;petal length $\le$ 2.45 cm&amp;rdquo; which is a feature in iris data). How does it choose $k$ and $t_k$ ? It searches for the pair ($k$, $t_k$) that produces the purest subsets (weighted by their size).
$$
\tag{6.2}
J(k, t_k) = {m_{left} \over m} G_{left} + {m_{right} \over m} G_{right}
$$
where&lt;/p>
&lt;ul>
&lt;li>$m_{left/right}$ is the number of instances in the left/right subset,&lt;/li>
&lt;li>$G_{left/right}$ measures the impurity of the left/right subset.&lt;/li>
&lt;/ul>
&lt;p>Equation 6.2 gives the cost function for classification task that the algorithm tries to minimize.&lt;/p>
&lt;p>Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (&lt;code>max_depth&lt;/code>), or if it cannot find a split that will reduce impurity. There are other additional stopping conditions hyperparameters such as &lt;code>min_samples_split&lt;/code>, &lt;code>min_samples_leaf&lt;/code>, &lt;code>min_weight_fraction_leaf&lt;/code>, and &lt;code>max_leaf_nodes&lt;/code>. Increasing &lt;code>min_*&lt;/code> hyperparameters or reducing &lt;code>max_*&lt;/code> hyperparameters will regularize the model.&lt;/p>
&lt;p>&lt;strong>2. Regresssion Task&lt;/strong>&lt;/p>
&lt;p>The CART algorithm works mostly the same as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE.
$$
\tag{6.2}
J(k, t_k) = {m_{left} \over m} \text{MSE}&lt;em>{left} + {m&lt;/em>{right} \over m} \text{MSE}_{right}
$$
where&lt;/p>
&lt;ul>
&lt;li>$MSE_{node} = \sum_{i \in node} (\hat{y}_{node} - y^{(i)})$ ,&lt;/li>
&lt;li>$\hat{y}&lt;em>{node} = {1 \over m&lt;/em>{node}} \sum_{i \in node} y^{(i)}$&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>3. Instability&lt;/strong>&lt;/p>
&lt;p>Decision Trees produce orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to trianing set rotation (The model on the right of figure 6-7 will not generalize well). Ony way to limit this problem is to use Principal Component Analysis (PCA), which often results in a better orientation of the training data.&lt;/p>
&lt;p>&lt;img alt="dtree instability" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_dtree_instability.png">&lt;/p>
&lt;p>More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Actually, since the training algorithm used by Sklearn is stochastic (means it randomly selects the set of features to evaluate at each node), it may produces very different models even on the same training data (unless you set the &lt;code>random_state&lt;/code> hyperparameter).&lt;/p>
&lt;/blockquote>
&lt;h3 id="剪枝处理" >
&lt;div>
&lt;a href="#%e5%89%aa%e6%9e%9d%e5%a4%84%e7%90%86">
##
&lt;/a>
剪枝处理
&lt;/div>
&lt;/h3>
&lt;p>剪枝 (pruning)是决策树学习算法对付 “过拟合” 的主要手段。在决策树学习过程中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，以致于把训练集自身的一些特点当作所有数据都具有的一般性质从而导致过拟合。因此，通过主动去掉一些分支来降低过拟合的风险。&lt;/p>
&lt;p>决策树剪枝的基本策略有 &lt;strong>预剪枝(prepruning)&lt;/strong> 和 &lt;strong>后剪枝(post-pruning)&lt;/strong> 。&lt;/p>
&lt;p>预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分，并将当前结点标记为叶结点。&lt;/p>
&lt;p>后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。&lt;/p>
&lt;p>&lt;strong>如何判断决策树泛化性能是否提升呢？&lt;/strong> 这可以使用常用的性能评估方法进行，如 “留出法”、“交叉验证法” 以及 “自助法” 等方法。&lt;/p>
&lt;p>预剪枝会使得决策树的很多分支没有 “展开”，这不仅能降低过拟合的风险，还会显著减少训练和测试的时间开销。但另一方面，有些分支的当前划分虽不能提升泛化性能（甚至可能导致泛化性能暂时下降），但在其基础上进行的后续划分却有可能使得泛化性能显著提高；预剪枝基于 “贪心” 本质禁止这些分支展开，这给预剪枝决策树带来欠拟合的风险。&lt;/p>
&lt;p>后剪枝决策树通常会比预剪枝决策树保留更多的分支。一般情形下，后剪枝决策树欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝决策树的训练时间开销则大得多。&lt;/p>
&lt;h3 id="连续值属性和缺失值" >
&lt;div>
&lt;a href="#%e8%bf%9e%e7%bb%ad%e5%80%bc%e5%b1%9e%e6%80%a7%e5%92%8c%e7%bc%ba%e5%a4%b1%e5%80%bc">
##
&lt;/a>
连续值属性和缺失值
&lt;/div>
&lt;/h3>
&lt;p>&lt;strong>连续值处理&lt;/strong>&lt;/p>
&lt;p>由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对结点进行划分。此时，&lt;strong>连续属性离散化&lt;/strong>技术可派上用场。最简单的策略是采用二分法(bi-partition)对连续属性进行处理，这正是C4.5决策树算法中采用的机制。&lt;/p>
&lt;p>给定样本集 $D$ 和连续属性 $a$，假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值，将这些取值从小到大进行排序，记为 {$a^1, \ldots, a^n$}。基于划分点 $t$ 可将 $D$ 分为子集 $D^-_t$ 和 $D^+&lt;em>t$ ，其中 $D^-&lt;em>t$ 包含哪些在属性 $a$ 上取值不大于 $t$ 的样本，而 $D^+&lt;em>t$ 则包含那些大于 $t$ 的样本。显然，对相邻的属性取值 $a^i$ 与 $a^{i+1}$ 来说，$t$ 在区间 [$a^i, a^{i+1}$) 中取任意值所产生的划分结果相同。因此，对连续属性 $a$，我们可考察包含 $n - 1$ 个元素的候选划分点集合
$$
\tag{4.7}
T_a = \bigg{{a^i + a^{i+1} \over 2} | 1 \le i \le n-1 \bigg},
$$
即把区间 [$a^i, a^{i+1}$) 的中位点作为候选划分点&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>。然后我们就可以像离散属性值一样来考虑这些划分点，选取最优的划分点进行样本集合的划分。例如，可对式4.2稍加改造：
$$
\begin{eqnarray}
\text{Gain}(D, a)
&amp;amp;=&amp;amp; \text{max}&lt;/em>{(t \in T_a)} \text{Gain}(D, a, t) \
\tag{4.8}
&amp;amp;=&amp;amp; \text{max}&lt;/em>{(t \in T_a)} \text{Ent}(D) - \sum&lt;/em>{\lambda \in {-, + }} {|D^{\lambda}_t| \over |D|} \text{Ent}(D^{\lambda}_t),
\end{eqnarray}
$$
其中，$\text{Gain}(D, a, t)$ 是样本集 $D$ 基于划分点 $t$ 二分后的信息增益。于是，我们就可选择使 $\text{Gain}(D, a, t)$ 最大化的划分点。&lt;/p>
&lt;p>&lt;strong>缺失值处理&lt;/strong>&lt;/p>
&lt;p>现实任务中常会遇到不完整样本，即样本的某些属性值缺失。在属性数目较多的情形下，往往会有大量样本出现缺失值。如果简单地放弃不完整样本，仅使用无缺失值的样本进行学习，显然是对数据信息的极大浪费。&lt;/p>
&lt;p>我们需要解决两个问题：&lt;/p>
&lt;ol>
&lt;li>如何在属性值缺失的情况下进行划分属性的选择？&lt;/li>
&lt;li>给定划分属性，如果样本在该属性上的值缺失，如何对样本进行划分？&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>The real handling approaches to missing data does not use data point with missing values in the evaluation of a split. However, when child nodes are created and trained, those instances are distributed somehow.&lt;/p>
&lt;p>I know about the following approaches to distribute the missing value instances to child nodes:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>simply ignoring the missing values (like ID3 and other old algorithms does) or treating the missing values as another category (in case of a nominal feature). Those approachs were used in the early stages of decision tree development.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>all goes to the node which already has the biggest number of instances (CART, but not its primary rule)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>distribute to all children, but with diminished weights, proportional with the number of instances from each child node (C4.5 and others)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>distribute randomly to only one single child node, eventually according with a categorical distribution (various implementations of C4.5 and CART for faster funing time)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>build, sort and use surrogates to distribute instances to a child node, where surrogates are input features which resembles best how the test feature send data instances to left or right child node (CART, if that fails, the majority rule is used)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This answer was copied from &lt;a href="https://stats.stackexchange.com/questions/96025/how-do-decision-tree-learning-algorithms-deal-with-missing-values-under-the-hoo">here&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="多变量决策树" >
&lt;div>
&lt;a href="#%e5%a4%9a%e5%8f%98%e9%87%8f%e5%86%b3%e7%ad%96%e6%a0%91">
##
&lt;/a>
多变量决策树
&lt;/div>
&lt;/h3>
&lt;p>如果我们把每个属性视为坐标空间中的一个坐标轴，则 $d$ 个属性描述的样本就对应了 $d$ 维空间中的一个数据点，对样本分类意味着在这个坐标空间中寻找不同样本之间的分类边界。&lt;/p>
&lt;p>决策树所形成的分类边界有一个明显的特点：轴平行(axis-parallel)，即它的分类边界由若干个与坐标轴平行的分段组成。&lt;/p>
&lt;p>&lt;img alt="data3a" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree_dataset3a.png">&lt;/p>
&lt;p>以表4.5中的西瓜数据$3.0 \alpha$为例，将它作为训练集学习得图4.10所示的决策树，其分类边界如图4.11所示。&lt;/p>
&lt;p>&lt;img alt="tree3a" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree3a2.png">&lt;/p>
&lt;p>显然，分类边界的每一段都是与坐标轴平行的。这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，如图4.12所示；此时的决策树会相当复杂，由于需要进行大量属性测试，预测时间开销会很大。&lt;/p>
&lt;p>&lt;img alt="tree3a3" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree3a3.png">&lt;/p>
&lt;p>如果能够使用斜的划分边界，如图4.12中的红色线段所示，则决策树模型将大为简化。&lt;/p>
&lt;p>&lt;strong>“多变量决策树”(multivariate decision tree)&lt;/strong> 就是能实现这样的 “斜划分” 甚至更复杂划分的决策树。以实现斜划分的多变量决策树为例，在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试；换言之，每个非叶结点是一个形如 $\sum^d_{i=1} w_i a_i = t$ 的线性分类器，其中 $w_i$ 是属性 $a_i$ 的权重， $w_i$ 和 $t$ 可以在该结点所含的样本集和属性集上学得&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>。于是，与传统的 “单变量决策树(univariate decision tree)” 不同，在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。&lt;/p>
&lt;p>例如对西瓜数据$3.0 \alpha$，我们可以学得图4.13这样的多变量决策树，其分类边界如图4.14所示。&lt;/p>
&lt;p>&lt;img alt="tree3a4" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\xgs_tree3a4.png">&lt;/p>
&lt;h3 id="阅读材料" >
&lt;div>
&lt;a href="#%e9%98%85%e8%af%bb%e6%9d%90%e6%96%99">
##
&lt;/a>
阅读材料
&lt;/div>
&lt;/h3>
&lt;p>在&lt;strong>信息增益、增益率、基尼指数&lt;/strong>之外，人们还设计了许多其他的准则用于决策树划分选择，然而有实验研究表明&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>，这些准则虽然对决策树的尺寸有较大影响，但对泛化性能的影响很有限；对信息增益和基尼指数进行的理论分析&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>也显示出，它们仅在 $2%$ 的情况下会有所不同。而剪枝方法和剪枝程度对决策树的泛化性能影响显著，有实验研究&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>表明，在数据带有噪声时，通过剪枝甚至可将决策树的泛化性能提高 $25%$。&lt;/p>
&lt;p>多变量决策树算法主要有 $OC1$&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>，$OC1$ 算法先贪心地寻找每个属性的最优权值，在局部优化的基础上再对分类边界进行随机扰动以试图找到更好的边界；Brodley and Utgoff&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> 则直接引入了线性分类器学习的最小二乘法。还有一些算法试图在决策树的叶结点上嵌入神经网络，以结合这两种学习机制的优势，例如 “感知机树(Perceptron tree)”&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup> 在每个叶结点上训练一个感知机，也有直接在叶结点上嵌入多层神经网络的模型&lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>。&lt;/p>
&lt;p>有一些决策树学习算法可进行 “增量学习(incremental learning)”，即在接收到新样本后可对已学得的模型进行调整，而不用完全重新学习。主要机制是通过调整分支路径上的划分属性次序来对树进行部分重构，代表性算法有ID4&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>、ID5R&lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>、ITI&lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>等。增量学习可有效降低每次接收到新样本后的训练时间开销，但多步增量学习后的模型会与基于全部数据训练而得的模型有较大差别。&lt;/p>
&lt;h2 id="集成学习ensemble" >
&lt;div>
&lt;a href="#%e9%9b%86%e6%88%90%e5%ad%a6%e4%b9%a0ensemble">
#
&lt;/a>
集成学习(Ensemble)
&lt;/div>
&lt;/h2>
&lt;p>根据个体学习器的生成方式，目前集成学习&lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>方法大致可分为两大类，&lt;/p>
&lt;ul>
&lt;li>个体学习器之间存在强依赖关系、必须串行生成的序列化方法，代表算法Boosting&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>;&lt;/li>
&lt;li>个体学习器之间不存在强依赖关系、可同时生成的并行化方法，代表算法Bagging和 “随机森林”.&lt;/li>
&lt;/ul>
&lt;p>想要得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然 “独立” 在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。&lt;/p>
&lt;p>给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果采样出的每个子集都完全不同，则意味着每个基学习器只用到了一小部分训练数据，甚至可能不足以进行有效学习，这就无法保证产出比较好的基学习器。为解决这个问题，我们可考虑使用互相有交叠的采样子集。&lt;/p>
&lt;p>&lt;img alt="hard voting classifier" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_majority_vote.png">&lt;/p>
&lt;p>&lt;em>Figure 7-2. Hard voting classifier predictions. Copy from the book《hands-on Machine Learning with sklearn, Keras and tensorflow》&lt;/em>&lt;/p>
&lt;h3 id="bagging" >
&lt;div>
&lt;a href="#bagging">
##
&lt;/a>
BAGGING
&lt;/div>
&lt;/h3>
&lt;p>Bagging (Bootstrap AGGregatING) 是并行式集成学习方法最著名的代表。从名字即可看出，它直接基于自助采样法&lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>。因此，我们知道初始训练集中约有 $63.2%$ 的样本出现在采样集中。我们可以采样出 $T$ 个含 $m$ 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。这就是Bagging的基本流程。&lt;/p>
&lt;p>在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务采用简单平均法。&lt;/p>
&lt;hr>
&lt;p>Bagging 算法&lt;/p>
&lt;hr>
&lt;p>&lt;strong>输入&lt;/strong>: 训练集 $D = {(x_1, y_1), \ldots, (x_m, y_m) }$;&lt;/p>
&lt;p>​ 基学习算法 $\mathcal{L}$;&lt;/p>
&lt;p>​ 训练轮数 $T$.&lt;/p>
&lt;p>&lt;strong>过程&lt;/strong>：&lt;/p>
&lt;p>1: &lt;strong>for&lt;/strong> $t = 1, \ldots, T$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>2: $h_t = \mathcal{L} (D, D_{bs})$&lt;/p>
&lt;p>3: &lt;strong>end for&lt;/strong>&lt;/p>
&lt;p>&lt;strong>输出&lt;/strong>: $H(x) = \text{argmax}&lt;em>{(y \in \mathcal{Y})} \sum^T&lt;/em>{t=1} \mathbf{I}(h_t(x) = y)$&lt;/p>
&lt;hr>
&lt;p>其中，$D_{bs}$ 是自助采样产生的样本分布。&lt;/p>
&lt;p>从偏差-方差分解的角度看，Bagging主要关注降低方差，因此因此它在不剪枝决策树、神经网络等易受到样本扰动的学习器上效用更为明显。&lt;/p>
&lt;blockquote>
&lt;p>《hands-on Machine Learning with sklearn, Keras and tensorflow》&lt;/p>
&lt;p>&lt;strong>Bagging and Pasting&lt;/strong>&lt;/p>
&lt;p>One way to get a diverse set of classifiers is to use very different training algorithms (such as SVMs, LR, DTs etc). Another approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed &lt;strong>with replacement&lt;/strong>, this method is called &lt;strong>bagging&lt;/strong>, when sampling is preformed &lt;strong>without replacement&lt;/strong>, it is called &lt;strong>pasting&lt;/strong>.&lt;/p>
&lt;p>In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictiors, but only bagging allows training instances to be sampled several times for the same predictor. This sampling and training process is represented in Figure 7-4.&lt;/p>
&lt;p>&lt;img alt="bagging pasting" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_bagging_pasting.png">&lt;/p>
&lt;p>Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions all predictors. The aggregation function is typically&lt;/p>
&lt;ul>
&lt;li>the &lt;em>statistical mode&lt;/em>&lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup> for classification, or&lt;/li>
&lt;li>the &lt;em>statistical average&lt;/em> for regression.&lt;/li>
&lt;/ul>
&lt;p>Generally, the net result is that the ensemble has a similar bias but a lower variance than single predictor trained on the original train set.&lt;/p>
&lt;/blockquote>
&lt;h4 id="随机森林-rf" >
&lt;div>
&lt;a href="#%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97-rf">
###
&lt;/a>
随机森林 RF
&lt;/div>
&lt;/h4>
&lt;p>随机森林是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集（假定有 $d$ 个属性）中选择一个最优属性；而在 RF 中，对基决策树的每个结点，先从该结点的属性集中随机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数 $k$ 控制了随机性的引入程度：&lt;/p>
&lt;ul>
&lt;li>$k = d$, 则基决策树的构建与传统决策树相同；&lt;/li>
&lt;li>$k = 1$, 则是随机选择一个属性用于划分；一般推荐 $k = \text{log}_2 d$.&lt;/li>
&lt;/ul>
&lt;p>可以看出，随机森林对Bagging只做了小改动，但是与Bagging中基学习器的 “多样性” 仅通过样本扰动而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异程度的增加而进一步提升。&lt;/p>
&lt;p>值得一提的是，随机森林的训练效率通常优于Bagging，因为在个体决策树的构建过程中，Bagging 使用的是 “确定型” 决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的 “随机型” 决策树则只需考察一个属性子集。&lt;/p>
&lt;blockquote>
&lt;p>《hands-on Machine Learning with sklearn, Keras and tensorflow》&lt;/p>
&lt;p>&lt;strong>1. Random Forest&lt;/strong>&lt;/p>
&lt;p>A Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with &lt;code>max_samples&lt;/code> set to the size of the training set. Instead of building a &lt;code>BaggingClassifier&lt;/code> and passing it a &lt;code>DecisionTreeClassifier&lt;/code>, you can instead use the &lt;code>RandomForestClassifier&lt;/code> class, which is more convenients and optimized for Decision Trees&lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup> (similarly, there is a &lt;code>RandomForestRegressor&lt;/code> class for regression tasks).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.ensemble &lt;span style="color:#ff6ac1">import&lt;/span> RandomForestClassifier
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rf_clf &lt;span style="color:#ff6ac1">=&lt;/span> RandomForestClassifier(n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">500&lt;/span>, max_leaf_nodes&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">16&lt;/span>, n_jobs&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rf_clf&lt;span style="color:#ff6ac1">.&lt;/span>fit(X_train, y_train)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> rf_clf&lt;span style="color:#ff6ac1">.&lt;/span>predict(X_test)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With a few exceptions, a &lt;code>RandomForestClassifier&lt;/code> has all the hypeparameters of a &lt;code>DecisionTreeClassifier&lt;/code> (to control how trees are grown), plus all the hypeparameters of a &lt;code>BaggingClassifier&lt;/code> to control the ensemble itself.&lt;/p>
&lt;p>&lt;strong>2. Extra-Trees&lt;/strong>&lt;/p>
&lt;p>When you are growing a tree in a Random Forest, at each node only a random subset of the features (the $k$ set) is considered for splitting. It is possible to make trees even more random by also using random thresholds (the $t_k$ value) for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).&lt;/p>
&lt;p>A forest with such extremely random trees is called an &lt;em>Extremely Randomized Trees&lt;/em> ensemble (or &lt;em>Extra-Trees&lt;/em> for short). Once again, this technique trades more bias for a lower variance. It also makes &lt;em>Extra-Trees&lt;/em> much faster to train than regular Random Forests&lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>It is hard to tell in advance whether a &lt;code>RandomForestClassifier&lt;/code> will preform better or worse than an &lt;code>ExtraTreesClassifier&lt;/code> . Generally, the only way to know is to try both and compare them using cross-validation (tuning the hyperparameters uisng grid search).&lt;/p>
&lt;/blockquote>
&lt;h3 id="boosting" >
&lt;div>
&lt;a href="#boosting">
##
&lt;/a>
BOOSTING
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>from 《A Gentle Introduction to Gradient Boosting》, &lt;a href="mailto:chengli@ccs.neu.edu">chengli@ccs.neu.edu&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>What is Gradient Boosting&lt;/strong>&lt;/p>
&lt;p style="text-align: center;color=green;font-size=20">
Gradient Boosting = Gradient Descent + Boosting
&lt;/p>
&lt;h4 id="adaboost" >
&lt;div>
&lt;a href="#adaboost">
###
&lt;/a>
AdaBoost
&lt;/div>
&lt;/h4>
&lt;p>&lt;img alt="adaboost" src="./images/adaboost.png">&lt;/p>
&lt;p>AdaBoost training:&lt;/p>
&lt;ul>
&lt;li>Fit an additive model (ensemble) $\sum_t \rho_t h_t(x)$ in a forward stage-wise manner.&lt;/li>
&lt;li>In each stage, introduce a weak learner to compensate the &lt;em>shortcomings&lt;/em> of existing weak learners.&lt;/li>
&lt;li>In AdaBoost, &lt;em>shortcomings&lt;/em> are identified by high-weight data points.&lt;/li>
&lt;/ul>
&lt;p>$$
H(x) = \sum_t \rho_t h_t(x)
$$
&lt;img alt="adaboost" src="./images/adaboost2.png">&lt;/p>
&lt;p style="text-align: center">
Figure: AdaBoost. Source: Figure 1.2 of [Schapire and Freund, 2012]
&lt;/p>
&lt;h4 id="gradient-boosting" >
&lt;div>
&lt;a href="#gradient-boosting">
###
&lt;/a>
Gradient Boosting
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>Fit an additive model (ensemble) $\sum_t \rho_t h_t(x)$ in a forward stage-wise manner.&lt;/li>
&lt;li>In each stage, introduce a weak learner to compensate the &lt;em>shortcomings&lt;/em> of existing weak learners.&lt;/li>
&lt;li>In Gradient Boosting, &lt;em>shortcomings&lt;/em> are identified by &lt;strong>gradients&lt;/strong>. (Recall that&lt;strong>hight-weight&lt;/strong> data points for adaboost)&lt;/li>
&lt;li>Both high-weight data points and gradients tell us how to improve our model.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>AdaBoost &amp;amp;&amp;amp; Gradient Boosting&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Invent Adaboost, the rst successful boosting algorithm
[Freund et al., 1996, Freund and Schapire, 1997]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Formulate Adaboost as gradient descent with a special loss
function[Breiman et al., 1998, Breiman, 1999]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Generalize Adaboost to Gradient Boosting in order to handle
a variety of loss functions
[Friedman et al., 2000, Friedman, 2001]&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="gradient-boosting-for-regression" >
&lt;div>
&lt;a href="#gradient-boosting-for-regression">
###
&lt;/a>
Gradient Boosting for Regression
&lt;/div>
&lt;/h4>
&lt;p>Given $D = {(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)}$, and the task is to fit a model $F(x)$ to minimize square loss.&lt;/p>
&lt;p>Suppose your friend wants to help you and gives you a model $F$. You check his model and find that the model is good but not perfect. There are some mistakes: $F(x_1)=0.8$, while $y_1=0.9$, $F(x_2)=1.8$, while $y_2=1.9$, and so on. How can you improve this model? With following rules:&lt;/p>
&lt;ul>
&lt;li>You are not allowed to remove anything from $F$ or change any parameter in $F$.&lt;/li>
&lt;li>You can add an additional model (regression tree) $h$ to $F$, so the new prediction will be $F(x) + h(x)$.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Simple solution:&lt;/strong>&lt;/p>
&lt;p>You wish to improve the model such that
$$
\begin{eqnarray}
F(x_1) + h(x_1) &amp;amp;=&amp;amp; y_1 \
F(x_2) + h(x_2) &amp;amp;=&amp;amp; y_2 \
\cdots \
F(x_n) + h(x_n) &amp;amp;=&amp;amp; y_n \
\end{eqnarray}
$$&lt;/p>
&lt;p>Or equivalently, you wish
$$
\begin{eqnarray}
h(x_1) &amp;amp;=&amp;amp; y_1 - F(x_1) \
h(x_2) &amp;amp;=&amp;amp; y_2 - F(x_2) \
\cdots \
h(x_n) &amp;amp;=&amp;amp; y_n - F(x_n) \
\end{eqnarray}
$$
Can any regression tree $h$ achieve this goal prefectly? Maybe not.&lt;/p>
&lt;p>But some regression tree might be able to do this approximately. But how?&lt;/p>
&lt;p>Just fit a regression tree $h$ to the &lt;strong>residuals&lt;/strong>&lt;sup id="fnref:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup> data :&lt;/p>
&lt;p>$(x_1,y_1 - F(x_1)), (x_2,y_2 - F(x_2)), \ldots, (x_n,y_n - F(x_n)),$&lt;/p>
&lt;p>which are the parts that existing model $F$ connot do well.&lt;/p>
&lt;p>The role of $h$ is to compensate the shortcoming of existing model $F$.&lt;/p>
&lt;p>If the the new model $F+h$ is still not satisfactory, we can add another regression tree $g$ to fit data:&lt;/p>
&lt;p>$(x_1,y_1 - F(x_1) - h(x_1)), (x_2,y_2 - F(x_2) - h(x_2)), \ldots, (x_n,y_n - F(x_n) - h(x_n)),$&lt;/p>
&lt;p>which are the parts that existing model $F+h$ connot do well.&lt;/p>
&lt;p>Repeat this process utill we are satisfied.&lt;/p>
&lt;p>Q: We are improving the predictions of training data, is the procedure also useful for test data?&lt;/p>
&lt;p>A: Yes! Because we are building a model, and the model can be applied to test data as well.&lt;/p>
&lt;p>Q: &lt;strong>How is this related to gradient descent?&lt;/strong>&lt;/p>
&lt;h4 id="relationship-to-the-gradient-descent" >
&lt;div>
&lt;a href="#relationship-to-the-gradient-descent">
###
&lt;/a>
Relationship to the Gradient Descent
&lt;/div>
&lt;/h4>
&lt;p>Minimize a function by moving in the opposite direction of the gradient.
$$
\theta_i := \theta_i - \rho{\partial{J} \over \partial{\theta_i}}
$$
&lt;img alt="gd" src="./images/gradient_descent.png">&lt;/p>
&lt;p>Figure: Gradient Descent. Source: &lt;a href="http://en.wikipedia.org/wiki/Gradient_descent">http://en.wikipedia.org/wiki/Gradient_descent&lt;/a>&lt;/p>
&lt;p>Recall that the task is to minimize the square loss, the loss function
$$
L(y, F(x)) = {1 \over 2} (y - F(x))^2
$$
And we want to minimize
$$
J = \sum_i L(y_i, F(x_i))
$$
by adjusting $F(x_i), F(x_2), \ldots, F(x_n)$.&lt;/p>
&lt;p>Notice that $F(x_i), F(x_2), \ldots, F(x_n)$ are just some numbers. We can treat $F(x_i)$ as parameters and take derivatives
$$
\frac{\partial{J}}{\partial{F(x_i)}}
= \frac{\partial{\sum_i L(y_i, F(x_i))}}{\partial{F(x_i)}}
= \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}}
= F(x_i) - y_i
$$
So we can interpret residuals as negative gradients:
$$
y_i - F(x_i) = - \frac{\partial{J}}{\partial{F(x_i)}}.
$$
And we get:
$$
\begin{eqnarray}
F(x_i) &amp;amp;:=&amp;amp; F(x_i) + h(x_i) \
F(x_i) &amp;amp;:=&amp;amp; F(x_i) + y_i - F(x_i) \
F(x_i) &amp;amp;:=&amp;amp; F(x_i) - 1 \frac{\partial{J}}{\partial{F(x_i)}} \
\end{eqnarray}
$$
This is exactly how the gradients update iteratively when $\rho=1$:
$$
\theta_i := \theta_i - \rho{\partial{J} \over \partial{\theta_i}}
$$
For regression with &lt;strong>square loss&lt;/strong>,
$$
\begin{eqnarray}
\text{residual} &amp;amp;\Leftrightarrow&amp;amp; \text{negative gradient} \
\text{fit h to residual} &amp;amp;\Leftrightarrow&amp;amp; \text{fit h to negative gradient} \
\text{update F based on residual} &amp;amp;\Leftrightarrow&amp;amp; \text{update F based on negative gradient} \
\end{eqnarray}
$$
So we are actually updating our model using &lt;strong>gradient descent&lt;/strong>!&lt;/p>
&lt;h4 id="loss-functions-for-regression-problem" >
&lt;div>
&lt;a href="#loss-functions-for-regression-problem">
###
&lt;/a>
Loss Functions for Regression Problem
&lt;/div>
&lt;/h4>
&lt;p>Square loss is:&lt;/p>
&lt;ol>
&lt;li>Easy to deal with mathematically, while&lt;/li>
&lt;li>Not robust to outliers.&lt;/li>
&lt;/ol>
&lt;p>The consequence is that it pay too much attention to outliers, and try hard to incorporate outliers into the model, leads to degrade the overall performance.&lt;/p>
&lt;p>Other commomly use loss functions are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Absolute loss (more robust to outliers):
$$
L(y, F) = |y - F|
$$
Negative gradient:
$$&lt;/p>
&lt;ul>
&lt;li>g(x_i)
= - \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}}
= \text{sign}(y_i - F(x_i))
$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Huber loss (more robust to outliers):
$$
L(y, F) = \left{
\begin{array}{ll}
{1 \over 2}(y - F)^2, &amp;amp;|y - F| \le \delta;&amp;amp; \
\delta(|y - F| - {\delta \over 2}), &amp;amp;|y - F| &amp;gt; \delta;&amp;amp; \
\end{array} \right.
$$
Negative gradient:
$$
\begin{eqnarray}&lt;/p>
&lt;ul>
&lt;li>g(x_i)
&amp;amp;=&amp;amp; - \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}} \
\
&amp;amp;=&amp;amp; \left{
\begin{array}{ll}
y_i - F(x_i), &amp;amp;|y_i - F(x_i)| \le \delta;&amp;amp; \
\delta\ \text{sign}(y_i - F(x_i)), &amp;amp;|y_i - F(x_i)| &amp;gt; \delta;&amp;amp; \
\end{array} \right.
\end{eqnarray}
$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>example:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: center">$y_i$&lt;/th>
&lt;th style="text-align: center">0.5&lt;/th>
&lt;th style="text-align: center">1.2&lt;/th>
&lt;th style="text-align: center">2&lt;/th>
&lt;th style="text-align: center">$5^*$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: center">$F(x_i)$&lt;/td>
&lt;td style="text-align: center">0.6&lt;/td>
&lt;td style="text-align: center">1.4&lt;/td>
&lt;td style="text-align: center">1.5&lt;/td>
&lt;td style="text-align: center">1.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">Square loss&lt;/td>
&lt;td style="text-align: center">0.005&lt;/td>
&lt;td style="text-align: center">0.02&lt;/td>
&lt;td style="text-align: center">0.125&lt;/td>
&lt;td style="text-align: center">5.445&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">Absolute loss&lt;/td>
&lt;td style="text-align: center">0.1&lt;/td>
&lt;td style="text-align: center">0.2&lt;/td>
&lt;td style="text-align: center">0.5&lt;/td>
&lt;td style="text-align: center">3.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: center">Huber loss ($\delta=0.5$)&lt;/td>
&lt;td style="text-align: center">0.005&lt;/td>
&lt;td style="text-align: center">0.02&lt;/td>
&lt;td style="text-align: center">0.125&lt;/td>
&lt;td style="text-align: center">1.525&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="regression-with-loss-function-l-general-procedure" >
&lt;div>
&lt;a href="#regression-with-loss-function-l-general-procedure">
###
&lt;/a>
Regression with loss function $L$: general procedure
&lt;/div>
&lt;/h4>
&lt;hr>
&lt;p>Give any differentiable loss function $L$,&lt;/p>
&lt;p>start with an initial model, say $F(x) = \frac{\sum^n_{i=1} y_i}{n}$,&lt;/p>
&lt;p>iterate until converge:&lt;/p>
&lt;p>​ calculate negative gradients $- g(x_i) = - \frac{\partial{L(y_i, F(x_i))}}{\partial{F(x_i)}}$,&lt;/p>
&lt;p>​ fit a regression tree $h$ to negative gradients $-g(x_i)$,&lt;/p>
&lt;p>​ $F := F + \rho h$&lt;/p>
&lt;hr>
&lt;p>In general, &lt;em>negative gradients&lt;/em> not equal to &lt;em>residuals&lt;/em>, and we should follow negative gradients rather than residuals because &lt;em>negative gradient&lt;/em> pays less attention to outliers.&lt;/p>
&lt;h4 id="summary-of-the-section" >
&lt;div>
&lt;a href="#summary-of-the-section">
###
&lt;/a>
Summary of the Section
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>Fit an additive model $F=\sum_t {\rho_t h_t}$ in a forward stage-wise manner.&lt;/li>
&lt;li>In each stage, introduce a new regression tree $h$ to compensate the shortcomings of existing model.&lt;/li>
&lt;li>The &lt;em>shortcomings&lt;/em> are identified by negative gradients.&lt;/li>
&lt;li>For any loss function, we can derive a gradient boosting algorithm.&lt;/li>
&lt;li>Absolute loss and Huber loss are more robust to outliers than square loss.&lt;/li>
&lt;/ul>
&lt;p>NOTE that the things not covered:&lt;/p>
&lt;p>How to choose a proper learning rate for each gradient boosting algorithm. See [Friedman, 2001]&lt;/p>
&lt;hr>
&lt;h4 id="gradient-boosting-for-classification" >
&lt;div>
&lt;a href="#gradient-boosting-for-classification">
###
&lt;/a>
Gradient Boosting for Classification
&lt;/div>
&lt;/h4>
&lt;p>&lt;strong>Problem:&lt;/strong>&lt;/p>
&lt;p>Recognize the given hand written capital letter. &lt;a href="http://archive.ics.uci.edu/ml/datasets/Letter+Recognition">dataset size: 20000 x 16&lt;/a>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Multi-class classification&lt;/p>
&lt;/li>
&lt;li>
&lt;p>26 classes. A, B, &amp;hellip;, Z&lt;/p>
&lt;/li>
&lt;li>
&lt;p>features:&lt;/p>
&lt;p>1 horizontal position of box;
2 vertical position of box;
3 width of box;
4 height of box;
5 total number on pixels;
6 mean x of on pixels in box;
7 mean y of on pixels in box;
8 mean x variance;&lt;/p>
&lt;p>9 mean y variance；&lt;/p>
&lt;p>10 mean x y correlation;&lt;/p>
&lt;p>11 mean of x * x * y;&lt;/p>
&lt;p>12 mean of x * y * y;&lt;/p>
&lt;p>13 mean edge count left to right;&lt;/p>
&lt;p>14 correlation of x-ege with y;&lt;/p>
&lt;p>15 mean edge count bottom to top;&lt;/p>
&lt;p>16 correlation of y-ege with x.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Feature Vector= (2; 1; 3; 1; 1; 8; 6; 6; 6; 6; 5; 9; 1; 7; 5; 10)
Label = G&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Model:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>26 score functions (our models): $F_A, F_B, \ldots, F_Z$.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$F_A(x)$ assigns a score for class A.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>scores are used to calculate probabilities:
$$
\begin{eqnarray}
P_A(x) &amp;amp;=&amp;amp; \frac{\exp(F_A(x))}{\sum^Z_{c=A}\exp(F_c(x))} \
\
P_B(x) &amp;amp;=&amp;amp; \frac{\exp(F_B(x))}{\sum^Z_{c=A}\exp(F_c(x))} \
\
\cdots \
\
P_Z(x) &amp;amp;=&amp;amp; \frac{\exp(F_Z(x))}{\sum^Z_{c=A}\exp(F_c(x))} \
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>predicted label = class that has the highest probability.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Loss function for each data point:&lt;/strong> step by step&lt;/p>
&lt;ol>
&lt;li>
&lt;p>turn the label $y_i$ into a (true) probability distribution $Y_c(x_i)$,&lt;/p>
&lt;p>For example: $y_5=\text{G}$,&lt;/p>
&lt;p>$Y_A(x_5)=0, Y_B(x_5)=0, \ldots, Y_G(x_5)=1, \ldots, Y_Z(x_5)=0$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>calculate the predicted probability distribution $P_c(x_i)$ based on the current model $F_A, F_B, \ldots, F_Z$.&lt;/p>
&lt;p>$P_A(x_5)=0.03, P_B(x_5)=0.05, \ldots, P_G(x_5)=0.3, \ldots, P_Z(x_5)=0.05$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>calculate the difference between the true probability distribution and the predicted distribution. One of the ways is to use KL-divergence to measure the difference.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>The Goal:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>minimize the total loss (KL-divergence)&lt;/li>
&lt;li>for each data point, we wish the predicted probability distribution to match the true probability distribution as closely as possible.&lt;/li>
&lt;li>we achieve this goal by adjusting our models $F_A, F_B, \ldots, F_Z$.&lt;/li>
&lt;/ul>
&lt;h4 id="gradient-boosting-for-classification-general-procedure" >
&lt;div>
&lt;a href="#gradient-boosting-for-classification-general-procedure">
###
&lt;/a>
Gradient Boosting for classification: general procedure
&lt;/div>
&lt;/h4>
&lt;hr>
&lt;p>start with initial models $F_A, F_B, \ldots, F_Z$.&lt;/p>
&lt;p>iterate until converge:&lt;/p>
&lt;p>​ calculate negative gradients for class A: $-g_A(x_i)= - \frac{\partial{L}}{\partial{F_A(x_i)}}$&lt;/p>
&lt;p>​ &amp;hellip;&lt;/p>
&lt;p>​ calculate negative gradients for class Z: $-g_Z(x_i)= - \frac{\partial{L}}{\partial{F_Z(x_i)}}$&lt;/p>
&lt;p>​ fit a regression tree $h_A$ to negative gradients $-g_A(x_i)$&lt;/p>
&lt;p>​ &amp;hellip;&lt;/p>
&lt;p>​ fit a regression tree $h_Z$ to negative gradients $-g_Z(x_i)$&lt;/p>
&lt;p>​ $F_A := F_A + \rho_A h_A$&lt;/p>
&lt;p>​ &amp;hellip;&lt;/p>
&lt;p>​ $F_Z := F_Z + \rho_Z h_Z$&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Classification VS Regression&lt;/strong>: The Differences&lt;/p>
&lt;ul>
&lt;li>$F_A, F_B, \ldots, F_Z$ &lt;strong>vs&lt;/strong> $F$&lt;/li>
&lt;li>a matrix of parameters to optimize &lt;strong>vs&lt;/strong> a column of parameters to optimize&lt;/li>
&lt;li>a matrix of gradients &lt;strong>vs&lt;/strong> a column of gradients&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;/blockquote>
&lt;h2 id="part-ii-engineering-views" >
&lt;div>
&lt;a href="#part-ii-engineering-views">
#
&lt;/a>
Part-II: Engineering views
&lt;/div>
&lt;/h2>
&lt;h2 id="sklearn-random-forest-model" >
&lt;div>
&lt;a href="#sklearn-random-forest-model">
#
&lt;/a>
Sklearn Random-forest model
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>sklearn&lt;span style="color:#ff6ac1">.&lt;/span>ensemble&lt;span style="color:#ff6ac1">.&lt;/span>RandomForestClassifier(n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">100&lt;/span>, &lt;span style="color:#ff6ac1">*&lt;/span>, criterion&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#39;gini&amp;#39;&lt;/span>, max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, min_samples_split&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>, min_samples_leaf&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, min_weight_fraction_leaf&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.0&lt;/span>, max_features&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#39;auto&amp;#39;&lt;/span>, max_leaf_nodes&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, min_impurity_decrease&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.0&lt;/span>, min_impurity_split&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, bootstrap&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>, oob_score&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">False&lt;/span>, n_jobs&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, random_state&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, verbose&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0&lt;/span>, warm_start&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">False&lt;/span>, class_weight&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>, ccp_alpha&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">0.0&lt;/span>, max_samples&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A random forest classifier.&lt;/p>
&lt;p>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code>max_samples&lt;/code> parameter if &lt;code>bootstrap=True&lt;/code> (default), otherwise the whole dataset is used to build each tree.&lt;/p>
&lt;h3 id="feature_importances_" >
&lt;div>
&lt;a href="#feature_importances_">
##
&lt;/a>
feature_importances_
&lt;/div>
&lt;/h3>
&lt;p>Sklearn measures a feature&amp;rsquo;s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node&amp;rsquo;s weight is equal to the number of training samples that are associated with it.&lt;/p>
&lt;p>&lt;code>feature_importances_&lt;/code> is a impurity-based feature importances.&lt;/p>
&lt;p>The higher, the more importance the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.&lt;/p>
&lt;p>Warning: impurity-based feature importance can be misleading for high cardinality features (many unique values). see [permutation_importance](###permutation importance) as an alternative.&lt;/p>
&lt;p>Tree-based models measure the feature importances based on the &lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/#MDI">mean decrease in impurity&lt;/a>. Impurity is quantified by the splitting criterion of the decision trees (Gini, Entropy(i.e., imformation gain) or Mean Square Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting. Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data (hold-out set, validation set, etc).&lt;/p>
&lt;p>Furthermore, impurity-based feature importance for trees are &lt;strong>strongly biased&lt;/strong> and &lt;strong>favor high cardinality features&lt;/strong> (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories. &lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/####增益率">(see this explanation)&lt;/a>.&lt;/p>
&lt;p>The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance:&lt;/p>
&lt;p>&lt;a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py">Permutation importance vs Random Forest Feature Importance (MDI)&lt;/a>&lt;/p>
&lt;h3 id="permutation-importance" >
&lt;div>
&lt;a href="#permutation-importance">
##
&lt;/a>
permutation importance
&lt;/div>
&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>sklearn&lt;span style="color:#ff6ac1">.&lt;/span>inspection&lt;span style="color:#ff6ac1">.&lt;/span>permutation_importance(estimator, X, y, &lt;span style="color:#ff6ac1">*&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scoring&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_repeats&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">5&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_jobs&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> random_state&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">None&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Permutation importance for feature evaluation.&lt;/p>
&lt;p>The &lt;code>estimator&lt;/code> is required to be a fitted estimator. &lt;code>X&lt;/code> can be the data set used to train the estimator or a hold-out set. The permutation importance of a feature is calculated as follows.&lt;/p>
&lt;p>First, a baseline metric, defined by [scoring](#Scoring Parameter), is evaluated on a (potentially different) dataset defined by &lt;code>X&lt;/code>. Next, a feature column from the validation set is permuted(&lt;a href="https://fgg100y.github.io/posts/ml101/treebasedmodels/treemodels/####How-it-work">How&lt;/a>) and the metric is evaluated again.&lt;/p>
&lt;p>The permutation importance is defined to be difference between the baseline metric and metric from permutating the feature column.&lt;/p>
&lt;hr>
&lt;p>Algorithm 1. Permutation importance&lt;/p>
&lt;hr>
&lt;p>Inputs: fitted predictive model $m$, tabular dataset (training or validation) $D$.&lt;/p>
&lt;p>Compute the reference score $s$ of the model $m$ on data $D$ (for instance the accuracy for a classifier or the $R^2$ for a regressor).&lt;/p>
&lt;p>&lt;strong>For&lt;/strong> each feature $j$ (column of $D$):&lt;/p>
&lt;p>​ &lt;strong>For&lt;/strong> each repetition $k$ in $1, \ldots, K$:&lt;/p>
&lt;p>​ Randomly shuffle column $j$ of dataset $D$ to generate a corrupted version of the data named $\tilde{D}_{k,j}$.&lt;/p>
&lt;p>​ Compute the score $s_{k,j}$ of model $m$ on corrupted data $\tilde{D}_{k,j}$.&lt;/p>
&lt;p>​ Compute importance $i_j$ for feature $f_j$ defined as:
$$
i_j = s - {1 \over K} \sum^K_{k=1}s_{k,j}.
$$&lt;/p>
&lt;hr>
&lt;h4 id="how-it-work" >
&lt;div>
&lt;a href="#how-it-work">
###
&lt;/a>
How it work
&lt;/div>
&lt;/h4>
&lt;p>Consider this: We want to predict a person&amp;rsquo;s height when they become 20 years old, using data that is available at age 10. Our data includes useful features (height at age 10), features with little predictive power (socks owned), as well as some other features we won&amp;rsquo;t focus on this explanation.&lt;/p>
&lt;p>&lt;strong>Permutation importance is calculated after a model has been fitted.&lt;/strong> so we won&amp;rsquo;t change the model or change what predictions we&amp;rsquo;d get for a given value of height, sock-count, etc.&lt;/p>
&lt;p>Instead we will ask the following question:&lt;/p>
&lt;p style="text-align:left;color:blue;">
"if I randomly shuffle a single column of the validation set data, leaving the target (or lable) and all other columns in place, how would that affect the accuracy of predictions in that now-shuffled data?"
&lt;/p>
&lt;p>Randomly re-ordering a single column should cause less accuracy predictions, since the resulting data no longer corresponds to anything observed in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling &lt;code>height at age 10&lt;/code> would cause terrible predictions while shuffling &lt;code>socks-owned&lt;/code> wouldn&amp;rsquo;t sufffer nearly as much.&lt;/p>
&lt;h3 id="misleading-values-on-strongly-correlated-features" >
&lt;div>
&lt;a href="#misleading-values-on-strongly-correlated-features">
##
&lt;/a>
Misleading values on strongly correlated features
&lt;/div>
&lt;/h3>
&lt;p>When two features are correlated and one of the feature is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important.&lt;/p>
&lt;p>One way to handle this is to cluster features that are correlated and only keep one feature from each cluster. This strategy is explored in the following example:&lt;/p>
&lt;p>&lt;a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py">Permutation Importance with Multicollinear or Correlated Features&lt;/a>.&lt;/p>
&lt;h3 id="does-modeling-with-random-forests-require-cross-validation" >
&lt;div>
&lt;a href="#does-modeling-with-random-forests-require-cross-validation">
##
&lt;/a>
Does modeling with Random Forests require cross-validation?
&lt;/div>
&lt;/h3>
&lt;p style="text-align:center;color:blue;">
"Random forests provide free cross-validation."
&lt;/p>
&lt;p>The &lt;code>RandomForestClassifier&lt;/code> is trained using &lt;em>bootstrap aggregation&lt;/em>, where each new tree is fit from a bootstrap sample of the training observations $z_i = (x_i, y_i)$. The out-of-bag (OOB) error is the average error for each $z_i$ calculated using predictions from the tress that do not contain $z_i$ in their respective bootstrap sample. This allows the &lt;code>RandomForestClassifier&lt;/code> to be fit and validated whilst being trained.&lt;/p>
&lt;blockquote>
&lt;p>By principle since it randomizes the variable selection during each tree split, it&amp;rsquo;s not prone to overfit unlike other models. However if you want to use CV using nfolds in sklearn you can still use the concept of hold-out set such as &lt;code>oob_score=True&lt;/code> which shows model performance with or without using CV.&lt;/p>
&lt;/blockquote>
&lt;h3 id="sklearn-boosting" >
&lt;div>
&lt;a href="#sklearn-boosting">
##
&lt;/a>
Sklearn Boosting
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>from book 《Hands-onML》.&lt;/p>
&lt;p>Boosting (original called &lt;em>hypothesis boosting&lt;/em>) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea most boosting methods is to train predictors sequentially, each trying to correct its predecessor. The most popular boosting methods by far are&lt;/p>
&lt;ul>
&lt;li>AdaBoost (short for Adaptive Boosting) and&lt;/li>
&lt;li>Gradient Boosting.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>1. AdaBoost&lt;/strong>&lt;/p>
&lt;p>One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard case. This technique used by AdaBoost.&lt;/p>
&lt;p>For example, when training an AdaBoost classifier, the algorithm first train a base classifier (such as a Decision Tree) and uses it to make predictions on the train set. The algorithm then increase the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on (see Figure 7-7).&lt;/p>
&lt;p>&lt;img alt="adaboost" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_adaboost.png">&lt;/p>
&lt;p>Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on their corresponding weighted training set.&lt;/p>
&lt;p>There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has ben trained and evaluated. As a result, it does not scale as well as bagging or pasting.&lt;/p>
&lt;p>&lt;strong>2. Gradient Boosting&lt;/strong>&lt;/p>
&lt;p>Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the &lt;em>residual errors&lt;/em> made by the previous predictor.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Gradient Tree Boosting for regression task,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># a.k.a., Gradient Boosted Regression Trees (GBRT)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.tree &lt;span style="color:#ff6ac1">import&lt;/span> DecisionTreeRegressor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg1 &lt;span style="color:#ff6ac1">=&lt;/span> DecisionTreeRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg1&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># train a second regressor on the residual errors made by the previous predictor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y2 &lt;span style="color:#ff6ac1">=&lt;/span> y &lt;span style="color:#ff6ac1">-&lt;/span> dtree_reg1&lt;span style="color:#ff6ac1">.&lt;/span>predict(X)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg2 &lt;span style="color:#ff6ac1">=&lt;/span> DecisionTreeRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg2&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y2)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># train a third regressor on the residual errors made by the previous predictor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y3 &lt;span style="color:#ff6ac1">=&lt;/span> y2 &lt;span style="color:#ff6ac1">-&lt;/span> dtree_reg2&lt;span style="color:#ff6ac1">.&lt;/span>predict(X)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg3 &lt;span style="color:#ff6ac1">=&lt;/span> DecisionTreeRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dtree_reg3&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y3)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ensemble contains three trees which makes predictions on a new instance simply by adding up the predictions of all the trees&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">sum&lt;/span>(tree&lt;span style="color:#ff6ac1">.&lt;/span>predict(X_new) &lt;span style="color:#ff6ac1">for&lt;/span> tree &lt;span style="color:#ff6ac1">in&lt;/span> (dtree_reg1, dtree_reg2, dtree_reg3))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A simpler way to train GBRT ensembles is to use sklearn &lt;code>GradientBoostingRegressor&lt;/code> class. Much like the &lt;code>RandomForestRegressor&lt;/code> class, it has hyperparameters to control the growth of Decision Trees.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.ensemble &lt;span style="color:#ff6ac1">import&lt;/span> GradientBoostingRegressor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt &lt;span style="color:#ff6ac1">=&lt;/span> GradientBoostingRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">3&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> learning_rate&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt&lt;span style="color:#ff6ac1">.&lt;/span>fit(X, y)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img alt="gbrt model fitting" src="d:\\JupyterNotebook\\fmhPlayground\\NotesDS\\treeBasedModels\\images\\hands-onML_ensemble_gradient_boosting.png">&lt;/p>
&lt;p>The &lt;code>learning_rate&lt;/code> hyperparameter scales the contribution of each tree.&lt;/p>
&lt;p>If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually better. This is a regularization technique called &lt;strong>shrinkage&lt;/strong>. In order to find the optimal number of trees, you can use early stopping which can simply implemented by setting&lt;code>warm_start=True&lt;/code> :&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.model_selection &lt;span style="color:#ff6ac1">import&lt;/span> train_test_split
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> sklearn.metrics &lt;span style="color:#ff6ac1">import&lt;/span> mean_square_error
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X_train, y_train, X_test, y_test &lt;span style="color:#ff6ac1">=&lt;/span> train_test_split(X, y)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt &lt;span style="color:#ff6ac1">=&lt;/span> GradientBoostingRegressor(max_depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> warm_start&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">early_stop_gbrt&lt;/span>(model, n_estimators&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">200&lt;/span>, n_rounds&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">5&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> train_data&lt;span style="color:#ff6ac1">=&lt;/span>(X_train, y_train),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> val_data&lt;span style="color:#ff6ac1">=&lt;/span>(X_test, y_test),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> min_val_error &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">float&lt;/span>(&lt;span style="color:#5af78e">&amp;#34;inf&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> error_going_up &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">for&lt;/span> n &lt;span style="color:#ff6ac1">in&lt;/span> &lt;span style="color:#ff5c57">range&lt;/span>(&lt;span style="color:#ff9f43">1&lt;/span>, n_estimators):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model&lt;span style="color:#ff6ac1">.&lt;/span>n_estimators &lt;span style="color:#ff6ac1">=&lt;/span> n
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model&lt;span style="color:#ff6ac1">.&lt;/span>fit(train_data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> model&lt;span style="color:#ff6ac1">.&lt;/span>predict(val_data[&lt;span style="color:#ff9f43">0&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> val_error &lt;span style="color:#ff6ac1">=&lt;/span> mean_square_error(val_data[&lt;span style="color:#ff9f43">1&lt;/span>], y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> val_error &lt;span style="color:#ff6ac1">&amp;lt;&lt;/span> min_val_error:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> min_val_error &lt;span style="color:#ff6ac1">=&lt;/span> val_error
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> error_going_up &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> error_going_up &lt;span style="color:#ff6ac1">+=&lt;/span> &lt;span style="color:#ff9f43">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> error_going_up &lt;span style="color:#ff6ac1">==&lt;/span> n_round:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">break&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gbrt &lt;span style="color:#ff6ac1">=&lt;/span> early_stop_gbrt(gbrt)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that there is an optimized implementation of Gradient Boosting out there called &amp;ldquo;XGBoost&amp;rdquo; which stands for Extreme Gradient Boosting, it is a popular Python library aimed to be extremely fast, scalable, and portable.&lt;/p>
&lt;/blockquote>
&lt;h3 id="plot-the-decision-tree" >
&lt;div>
&lt;a href="#plot-the-decision-tree">
##
&lt;/a>
Plot the decision tree
&lt;/div>
&lt;/h3>
&lt;p>To be continue &amp;hellip;&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>由 $d$ 个属性张成的 $d$ 维空间中，每个示例都可以在这个空间中找到自己的坐标位置，每个空间中的点对应一个坐标向量，因此：一个示例就是一个“特征向量”（feature vector）。&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>上一步的划分属性“纹理”，不再作为候选划分属性。&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Which is mainly due to the nature of how decision tree growed using greedy algorithm and easily overfiting.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>可将划分点设为该属性在训练集中出现的不大于中位点的最大值。由此，决策树使用的划分点都出现在训练集中。&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>待补充。&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>[Mingers, 1989b]&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>[Raileanu and Stoffel, 2004]&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>[Mingers, 1989a]&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>[Mruthy et al., 1994]&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10">
&lt;p>[Brodley and Utgoff, 1995]&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11">
&lt;p>[Utgoff, 1989b]&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12">
&lt;p>[Guo and Gelfand, 1992]&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13">
&lt;p>[Schlimmer and Fisher, 1986]&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14">
&lt;p>[Utgoff, 1989a]&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15">
&lt;p>[Utgoff et al., 1997]&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16">
&lt;p>集成学习(ensemble learning，a.k.a, multi-classifier system, committee-based learning)通过构建并结合多个学习器来完成学习任务。集成学习一般结构是，先产生一组 “个体学习器(individual learner)”，再用某种策略将它们结合起来。个体学习器通常由一个现有学习算法从训练数据产生，例如 C4.5决策树算法或BP神经网络算法等，此时，如果集成中只包含同种类型的个体学习器，如 “决策树集成”、“神经网络集成”等，则这样的集成是 “同质” 的(homogeneous)集成，同质集成中的个体学习器也称为 “基学习器(base learner)”，相应的学习算法称为 “基学习算法(base learning algorithm)”；反之，则是 “异质” 的(heterogenous)集成，这时个体学习器常称为 &amp;ldquo;组件学习器(component learner)&amp;rdquo; 或直接称为个体学习器。&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17">
&lt;p>Boosting 是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个机学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至学习器数目达到事先指定的值$T$，最终将这$T$个基学习器进行加权结合。&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18">
&lt;p>bootstrapping 采样是有放回的随机重复采样，样本在$m$次采样中始终不被采到的概率是$(1 - {1 \over m})^m$，取极限得到$lim_{(m \rightarrow \infin)} (1 - {1 \over m})^m \rightarrow {1 \over e} \approx 0.368$，即通过自助采样，初始数据集中约有36.8%的样本未出现在自助采样集中。自助法能从初始数据集中产生多个不同训练集，这对集成学习有很大好处。然而，自助法产生的数据集改变了初始数据集的分布而引入估计偏差，因此，在初始数据量足够时，留出法和交叉验证法更常用一些。&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19">
&lt;p>statistical mode: 众数。也就是频率最高的预测类别，与 hard voting classfifer 类似。&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20">
&lt;p>The &lt;code>BaggingClassifier&lt;/code> class remains useful if you want a bag of something other than Decision Trees.&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21">
&lt;p>Finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.&amp;#160;&lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:22">
&lt;p>residual is the difference between the predicted and the ground true.&amp;#160;&lt;a href="#fnref:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>