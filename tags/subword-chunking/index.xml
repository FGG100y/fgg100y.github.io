<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Subword Chunking on fgg blog</title><link>https://fgg100y.github.io/tags/subword-chunking/</link><description>Recent content in Subword Chunking on fgg blog</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Mon, 22 Apr 2024 15:41:56 +0800</lastBuildDate><atom:link href="https://fgg100y.github.io/tags/subword-chunking/index.xml" rel="self" type="application/rss+xml"/><item><title>Tokenization: BPE, Unigram and more</title><link>https://fgg100y.github.io/posts/llm_tokenization/</link><pubDate>Mon, 22 Apr 2024 15:41:56 +0800</pubDate><guid>https://fgg100y.github.io/posts/llm_tokenization/</guid><description>There is more than one way to tokenize a sentence word-level chunks/tokens
A big vocabulary is needed We combine words: what exactly constitutes a word (&amp;ldquo;bachelor of science&amp;rdquo;, or isolated words) Abbreviated words: &amp;ldquo;LOL&amp;rdquo;, &amp;ldquo;IMO&amp;rdquo;, are these collections of words or new words? Languages that don&amp;rsquo;t segment by spaces character-level chunks/tokens
Lack of meaning: Unlike words, characters don&amp;rsquo;t have any inherent meaning, model may lose the semantic-specific feature of words. Increased input computation Limits netword+k choices: It&amp;rsquo;s difficult to use architectures which process input sequentially since the input sequences will be much longer.</description></item></channel></rss>