<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Self-Attention on fgg blog</title><link>https://fgg100y.github.io/tags/self-attention/</link><description>Recent content in Self-Attention on fgg blog</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Fri, 26 Apr 2024 11:04:16 +0800</lastBuildDate><atom:link href="https://fgg100y.github.io/tags/self-attention/index.xml" rel="self" type="application/rss+xml"/><item><title>LLMs_interview_faq</title><link>https://fgg100y.github.io/posts/post_llms_faq/</link><pubDate>Fri, 26 Apr 2024 11:04:16 +0800</pubDate><guid>https://fgg100y.github.io/posts/post_llms_faq/</guid><description>01:简述GPT和BERT的区别 GPT (Decoder-only) 和 BERT (Encoder-only) 都是基于 Transformer 架构的自然语言处理模型，它们在设计上有一些显著区别：
任务类型 GPT 以生成文本为主要任务，其目标是生成与输入文本连贯和相关的文本。因此，GPT 通 常用于生成文本 (如：摘要总结，文本补充和chatbot)。 BERT 以理解文本为主要任务，其目标是从输入文本中提取语义信息。因此适用于各种文 本理解任务，如：情感分析、 文本分类、命名实体识别等下游任务。 预训练目标 GPT：单向语言建模。GPT通过自左向右的注意力机制来预测下一个单词，即根据上下文预 测下一个单词/词元是什么。 BERT：双向语言建模。BERT使用掩码语言建模（MLM）和下一句预测（NSP）两个任务，前 者在MLM任务中随机遮掩输入中的一些词语，模型需要预测这些被掩盖的词语是什么； NSP的任务是判断两个句子是否在原文中是前后连接的。 结构特点 GPT：Transformer-decoder的堆叠，仅使用自注意力机制 BERT：Transformer-encoder的堆叠，包含多层双向Transformer-encoder。在预训练阶段， BERT同时使用了自注意力机制和前馈神经网络。 模型微调 GPT：由于其生成式的特点，GPT在微调时通常将整个模型作为单独的序列生成任务进行微 调。 BERT：由于其双向表示的特点，BERT在微调时通常用于各种文本理解任务，微调时可以在 模型顶层添加适当的输出层来适应下游特定任务。 02:LLM中的因果语言建模与掩码语言建模有什么区别？ 因果语言建模（Causal Language Modeling）
在因果语言建模中，模型被要求根据输入序列的左侧内容来预测右侧的下一个词或标记。也就是 说，模型只能看到输入序列中已经生成的部分，而不能看到后续的内容。这种训练方式有助于模 型学习生成连贯和合理的文本，因为模型需要在生成每个词语时考虑上下文的信息，同时不能依 赖于未来的信息。GPT（Generative Pre-trained Transformer）就是以因果语言建模为基础的 模型。 掩码语言建模（Masked Language Modeling）：
在掩码语言建模中，模型被要求预测输入序列中一些被随机掩盖或掩码的词语。模型需要基于上 下文来预测这些被掩盖的词语是什么。 这种训练方式通常用于双向的语言理解任务，因为模型需要考虑上下文中的所有信息来预测被掩盖的词语。 BERT（Bidirectional Encoder Representations from Transformers）就是以掩码语言建模为基础的模型。 03:请简述Transformer基本原理 Transformer 是一种用于处理序列数据的深度学习模型，由谷歌团队于2017年提出，其主要原理包括 自注意力机制和位置编码。
自注意力机制： 允许模型在序列的任意两个位置间直接建立依赖关系，而不考虑它们之间的距离。具体就是将词 元线性转换为三个向量Q,K,V，然后将Q和K用来计算内积(相似度分数)并进行注意力缩放（scaled dot-product)，然后通过softmax归一化，得到每个词元相对于其他词元的注意力权重，然后用 注意力权重对向量V进行加权和计算得到“上下文向量”(context vector)，然后将上下文向量用 前馈网络（FFNN）进行变换，就得到编码器隐层输出。注意：自注意力机制中，每个输入词元的 context vector 以及后续的 hidden state，可以看成是相应的 Q 向量的函数，其他的如 K，V， 以及自注意力机制的参数对所有的 Q 都是恒定值。 + 多头注意力： 在多头注意力中，注意力机制被复制多次，并且每个注意力头都学习到一组不同的Q,K,V的 表示，然后将它们的输出拼接起来，再通过FFNN进行维度对齐。 - 复制注意力机制：原始输入序列会被用来计算多个注意力头（例如8个或16个头） - 独立学习：每个注意力头都会独立地学习一组Q，K，V的表示，也就是：每个注意力头都 有自己的权重矩阵，将输入序列转换为Q,K,V向量。 - 注意力计算：每个注意力头像单头注意力机制那样计算注意力分数和注意力权重。 - 拼接输出：将所有注意力头的输出拼接成一个向量，形成多头注意力的最终输出。这意味 着每个词元都会得到来自多个不同视角的表示，从而提高模型对输入序列的理解。 - 线性变换：拼接后的输出通过FFNN进行处理，维持输出维度以及融合不同注意力头的信息。 + narrow attn：Each attention head will get a chunk of the transformed data points (projections) to work with.</description></item></channel></rss>