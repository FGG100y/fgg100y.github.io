<!doctype html><html lang=en data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>Self-Attention - fgg blog</title>
<meta name=description content><link rel=alternate type=application/rss+xml href=https://fgg100y.github.io/tags/self-attention/index.xml title="fgg blog"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=https://fgg100y.github.io/css/style.min.d1bfddb74ce95e21195c88c4dc9ddc29a7292837af9c174852caf9c3556f3987.css integrity="sha256-0b/dt0zpXiEZXIjE3J3cKacpKDevnBdIUsr5w1VvOYc="><link rel=stylesheet href=https://fgg100y.github.io/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=https://fgg100y.github.io/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><script src=https://fgg100y.github.io/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class=social-icons><li><a href=https://github.com/fgg100y title=Github rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=https://fgg100y.github.io/index.xml title=RSS rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>Home</a>
<a href=https://fgg100y.github.io/posts/ title>Posts</a>
<a href=https://fgg100y.github.io/tags/ title>Tags</a>
<a href=https://fgg100y.github.io/about/ title>About</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><h1>Tag: Self-Attention</h1><div class=post-info><a href=/tags/>To all tags</a></div><article class="post-list h-feed post-card"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=/posts/post_llms_faq/>LLMs_interview_faq</a></h1></header></div><div class="content post-summary p-summary"># 01:简述GPT和BERT的区别 GPT (Decoder-only) 和 BERT (Encoder-only) 都是基于 Transformer 架构的自然语言处理模型，它们在设计上有一些显著区别：
任务类型 GPT 以生成文本为主要任务，其目标是生成与输入文本连贯和相关的文本。因此，GPT 通 常用于生成文本 (如：摘要总结，文本补充和chatbot)。 BERT 以理解文本为主要任务，其目标是从输入文本中提取语义信息。因此适用于各种文 本理解任务，如：情感分析、 文本分类、命名实体识别等下游任务。 预训练目标 GPT：单向语言建模。GPT通过自左向右的注意力机制来预测下一个单词，即根据上下文预 测下一个单词/词元是什么。 BERT：双向语言建模。BERT使用掩码语言建模（MLM）和下一句预测（NSP）两个任务，前 者在MLM任务中随机遮掩输入中的一些词语，模型需要预测这些被掩盖的词语是什么； NSP的任务是判断两个句子是否在原文中是前后连接的。 结构特点 GPT：Transformer-decoder的堆叠，仅使用自注意力机制 BERT：Transformer-encoder的堆叠，包含多层双向Transformer-encoder。在预训练阶段， BERT同时使用了自注意力机制和前馈神经网络。 模型微调 GPT：由于其生成式的特点，GPT在微调时通常将整个模型作为单独的序列生成任务进行微 调。 BERT：由于其双向表示的特点，BERT在微调时通常用于各种文本理解任务，微调时可以在 模型顶层添加适当的输出层来适应下游特定任务。 # 02:LLM中的因果语言建模与掩码语言建模有什么区别？ 因果语言建模（Causal Language Modeling）
在因果语言建模中，模型被要求根据输入序列的左侧内容来预测右侧的下一个词或标记。也就是 说，模型只能看到输入序列中已经生成的部分，而不能看到后续的内容。这种训练方式有助于模 型学习生成连贯和合理的文本，因为模型需要在生成每个词语时考虑上下文的信息，同时不能依 赖于未来的信息。GPT（Generative Pre-trained Transformer）就是以因果语言建模为基础的 模型。 掩码语言建模（Masked Language Modeling）：
在掩码语言建模中，模型被要求预测输入序列中一些被随机掩盖或掩码的词语。模型需要基于上 下文来预测这些被掩盖的词语是什么。 这种训练方式通常用于双向的语言理解任务，因为模型需要考虑上下文中的所有信息来预测被掩盖的词语。 BERT（Bidirectional Encoder Representations from Transformers）就是以掩码语言建模为基础的模型。 # 03:请简述Transformer基本原理 Transformer 是一种用于处理序列数据的深度学习模型，由谷歌团队于2017年提出，其主要原理包括 自注意力机制和位置编码。
## 自注意力机制： 允许模型在序列的任意两个位置间直接建立依赖关系，而不考虑它们之间的距离。具体就是将词 元线性转换为三个向量Q,K,V，然后将Q和K用来计算内积(相似度分数)并进行注意力缩放（scaled dot-product)，然后通过softmax归一化，得到每个词元相对于其他词元的注意力权重，然后用 注意力权重对向量V进行加权和计算得到“上下文向量”(context vector)，然后将上下文向量用 前馈网络（FFNN）进行变换，就得到编码器隐层输出。注意：自注意力机制中，每个输入词元的 context vector 以及后续的 hidden state，可以看成是相应的 Q 向量的函数，其他的如 K，V， 以及自注意力机制的参数对所有的 Q 都是恒定值。 + 多头注意力： 在多头注意力中，注意力机制被复制多次，并且每个注意力头都学习到一组不同的Q,K,V的 表示，然后将它们的输出拼接起来，再通过FFNN进行维度对齐。 - 复制注意力机制：原始输入序列会被用来计算多个注意力头（例如8个或16个头） - 独立学习：每个注意力头都会独立地学习一组Q，K，V的表示，也就是：每个注意力头都 有自己的权重矩阵，将输入序列转换为Q,K,V向量。 - 注意力计算：每个注意力头像单头注意力机制那样计算注意力分数和注意力权重。 - 拼接输出：将所有注意力头的输出拼接成一个向量，形成多头注意力的最终输出。这意味 着每个词元都会得到来自多个不同视角的表示，从而提高模型对输入序列的理解。 - 线性变换：拼接后的输出通过FFNN进行处理，维持输出维度以及融合不同注意力头的信息。 + narrow attn：Each attention head will get a chunk of the transformed data points (projections) to work with.</div><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-04-26>2024-04-26</time></div><a class="post-hidden-url u-url" href=https://fgg100y.github.io/posts/post_llms_faq/>https://fgg100y.github.io/posts/post_llms_faq/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me></a><div class=post-taxonomies><ul class=post-tags><li><a href=https://fgg100y.github.io/tags/transformer-architecture/>#Transformer Architecture</a></li><li><a href=https://fgg100y.github.io/tags/gpt/bert/>#GPT/BERT</a></li><li><a href=https://fgg100y.github.io/tags/self-attention/>#Self-Attention</a></li></ul></div></div></article></main><footer class="common-footer noselect"><ul class=language-select><li>English</li><li><a href=https://fgg100y.github.io/zh/>Chinese</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© 2024</div><div>Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me></a></p></footer></div></body></html>