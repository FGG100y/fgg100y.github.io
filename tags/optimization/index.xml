<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimization on fgg blog</title><link>/tags/optimization/</link><description>fgg blog (Optimization)</description><generator>Hugo -- gohugo.io</generator><language>zh</language><managingEditor>1522009317@qq.com
(fmh)</managingEditor><lastBuildDate>Fri, 30 Aug 2024 13:58:13 +0800</lastBuildDate><atom:link href="/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>expectation_maximization</title><link>/posts/optimazationmethods/em/expectation_maximization/</link><pubDate>Fri, 30 Aug 2024 13:58:13 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/optimazationmethods/em/expectation_maximization/</guid><description>&lt;p>概率模型有时既有 &lt;ruby>“观测变量”&lt;rt>（observable variable）&lt;/rt>&lt;/ruby> ，又含有
&lt;ruby>“隐变量或潜在变量”&lt;rt>（latent variable）&lt;/rt>&lt;/ruby> 。当存在后者时，不能直接使用
MLE这样的方法进行参数估计，需要引入EM进行参数估计。
EM算法就是含有隐变量的概率模型参数的最大似然估计法（或最大后验概率估计法）。&lt;/p>
&lt;h2 id="例子----三硬币模型" >
&lt;div>
&lt;a href="#%e4%be%8b%e5%ad%90----%e4%b8%89%e7%a1%ac%e5%b8%81%e6%a8%a1%e5%9e%8b">
#
&lt;/a>
例子 &amp;ndash; 三硬币模型
&lt;/div>
&lt;/h2>
&lt;p>假设有3枚硬币，分别记作 A, B, C。其抛出正面的概率分别是 $\pi$, $p$ 和 $q$ 。进行如下抛硬
币试验：先抛硬币A，根据其结果选择硬币B或者硬币C（正面选硬币B，反面选硬币A）；然后抛选出
的硬币，其抛硬币的结果，出现正面记作1，出现反面记作0；独立重复 $n$ 次试验。观测结果如下
（$n=10$）：&lt;/p>
&lt;pre>&lt;code>1,1,0,1,0,0,1,0,1,1
&lt;/code>&lt;/pre>
&lt;p>假设只能观察到抛硬币的结果，不能观测抛硬币过程。问：如何估计三枚硬币正面出现的概率，即三
硬币模型的参数。&lt;/p>
&lt;h3 id="建模" >
&lt;div>
&lt;a href="#%e5%bb%ba%e6%a8%a1">
##
&lt;/a>
建模
&lt;/div>
&lt;/h3>
&lt;p>三硬币模型可以表示为：&lt;/p>
&lt;p>$$
\begin{eqnarray}
P(y|\theta)
&amp;amp;=&amp;amp; \sum_{z} P(y, z | \theta) = \sum_{z} P(z|\theta)P(y|z,\theta) \\
&amp;amp;=&amp;amp; \pi p^{y}(1-p)^{1-y} + (1 - \pi) q^{y}(1-q)^{1-y}
\end{eqnarray}
$$&lt;/p>
&lt;p>其中，随机变量 $y$ 是观测变量，表示一次试验观测的结果是1或0；随机变量 $z$ 是隐变量，表示
未观测到的抛硬币A的结果；$\theta=(\pi, p, q)$ 是模型参数。这一模型是前述$n$次试验数据的
生成模型。&lt;/p>
&lt;p>将观测数据表示为 $Y = (Y_1, Y_2, \dots, Y_n)$，将未观测数据表示为 $Z = (Z_1, Z_2, \dots,
Z_n)$，则观测数据的&lt;strong>似然函数&lt;/strong>为&lt;/p>
&lt;p>$$
\begin{eqnarray}
P(Y|\theta)
&amp;amp;=&amp;amp; \sum_{z} P(Z|\theta)P(Y|Z,\theta) \\
&amp;amp;=&amp;amp; \prod^{n}_{j=1}[\pi p^{y_j}(1-p)^{1-y_j} + (1 - \pi) q^{y_j}(1-q)^{1-y_j}
\end{eqnarray}
$$&lt;/p>
&lt;p>求模型参数 $\theta=(\pi, p, q)$ 的极大似然估计，即&lt;/p>
&lt;p>$$
\hat{\theta} = \arg\max_{\theta} \log P(Y|\theta)
$$&lt;/p>
&lt;p>这个问题没有解析解，需要通过EM算法进行迭代求解。&lt;/p>
&lt;h3 id="em算法迭代步骤" >
&lt;div>
&lt;a href="#em%e7%ae%97%e6%b3%95%e8%bf%ad%e4%bb%a3%e6%ad%a5%e9%aa%a4">
##
&lt;/a>
EM算法迭代步骤
&lt;/div>
&lt;/h3>
&lt;p>EM算法通过迭代求 $L(\theta) = \log P(Y|\theta)$ 的最大似然估计。每次迭代包含两步：E步，求期望；M步，求最大化。&lt;/p>
&lt;hr>
&lt;p>EM算法步骤清单&lt;/p>
&lt;hr>
&lt;p>&lt;strong>输入&lt;/strong>：观测变量数据 $Y$，隐变量数据 $Z$，联合分布 $P(Y,Z|\theta)$，条件分布 $P(Z|Y, \theta)$；&lt;/p>
&lt;p>&lt;strong>输出&lt;/strong>：模型参数 $\theta$ 。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>选择参数初始值 $\theta^{(0)}$ ，开始迭代；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>E步：记 $\theta^{(i)}$ 为第$i$次迭代参数 $\theta$ 的估计值，在第$i+1$次迭代的E步，计算
$$
\begin{eqnarray}
Q(\theta, \theta^{(i)})
&amp;amp;=&amp;amp; E_{z} [\log P(Y,Z|\theta) | Y, \theta^{(i)}] \\
&amp;amp;=&amp;amp; \sum_{z} \log P(Y,Z|\theta) P(Z|Y, \theta^{(i)})
\end{eqnarray}
$$
这里，$P(Z|Y, \theta^{(i)}$ 是在给定观测数据 $Y$ 和当前的参数估计 $\theta^{(i)}$ 下隐变量数据 $Z$ 的条件概率分布；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>M步：求使得 $Q(\theta, \theta^{(i)})$ 最大化的 $\theta$ ，确定第 $i+1$ 次迭代的参数的估计值 $\theta^{(i)}$
$$
\theta^{(i)} = \arg\max_{\theta} Q(\theta, \theta^{(i)})
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>重复第2步和第3步，直到收敛。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr></description></item><item><title>maximum_likelihood_estimate</title><link>/posts/optimazationmethods/mle/maximum_likelihood_estimate/</link><pubDate>Fri, 30 Aug 2024 10:17:41 +0800</pubDate><author>1522009317@qq.com (fmh)</author><guid>/posts/optimazationmethods/mle/maximum_likelihood_estimate/</guid><description>&lt;p>最大似然估计（Maximum Likelihood Estimation, MLE）是一种常用参数估计方法，在统计学和机
器学习中被广泛使用。它的基本思想是找到一组参数值，使得观察到的数据出现的概率最大。&lt;/p>
&lt;h3 id="概念" >
&lt;div>
&lt;a href="#%e6%a6%82%e5%bf%b5">
##
&lt;/a>
概念
&lt;/div>
&lt;/h3>
&lt;p>假设我们有一组&lt;ruby>“独立同分布”&lt;rt>（independent identity distribution, i.i.d.）&lt;/rt>&lt;/ruby>
的观测数据 $X_1, X_2, \ldots, X_n$，这些数据来自某个概率分布族，其概率密度函数或概率质量
函数为 $f(x|\theta)$，其中$\theta$ 是未知参数。直观上看，最大似然估计的目标就是试图在
$\theta$ 所有可能的取值中，找到一个能使这一组观测数据出现的“可能性”最大的值。&lt;/p>
&lt;p>形式化地讲，给定观测数据 $x_1, x_2, \ldots, x_n$，最大似然估计 $\hat{\theta}$ 定义为：
$$
\hat{\theta} = \arg\max_\theta L(\theta|x_1, x_2, \ldots, x_n)
$$
其中 $L(\theta|x_1, x_2, \ldots, x_n)$ 称为似然函数，定义为：
$$
L(\theta|x_1, x_2, \ldots, x_n) = f(x_1, x_2, \ldots, x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)
$$&lt;/p>
&lt;p>啰唆多两句：所谓“似然”，和“概率”到底有什么区别呢？用抛硬币举例，区别在于：&lt;/p>
&lt;ul>
&lt;li>当我们谈论“概率”时，实际就是在问：已知模型（p=0.6，抛出正面的概率），则抛硬币10次中有7
次为正面的概率是多大？($P^{7}*(1-p)^{10-7}$)&lt;/li>
&lt;li>而谈论“似然”时，其实是反过来问：已经观察到抛10次硬币出现了7次正面，则硬币抛出正面的概
率是某个值的可能有多大？最大似然估计就是找到这样一个参数，此参数使得出现这样的数据（7
正面3反面）的可能最高。&lt;/li>
&lt;/ul>
&lt;h3 id="求解方法----直接求导法" >
&lt;div>
&lt;a href="#%e6%b1%82%e8%a7%a3%e6%96%b9%e6%b3%95----%e7%9b%b4%e6%8e%a5%e6%b1%82%e5%af%bc%e6%b3%95">
##
&lt;/a>
求解方法 &amp;ndash; &lt;strong>直接求导法&lt;/strong>：
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>写出似然函数 $L(\theta|x_1, x_2, \ldots, x_n)$。&lt;/li>
&lt;li>对似然函数取对数得到对数似然函数 $l(\theta|x_1, x_2, \ldots, x_n) = \log L(\theta|x_1, x_2, \ldots, x_n)$。&lt;/li>
&lt;li>对对数似然函数求导，通常情况下求一阶导数，并令其等于零来找到极值点。&lt;/li>
&lt;li>检查二阶导数或者直接通过直观判断是否为极大值点。&lt;/li>
&lt;/ul>
&lt;h4 id="示例有解析解" >
&lt;div>
&lt;a href="#%e7%a4%ba%e4%be%8b%e6%9c%89%e8%a7%a3%e6%9e%90%e8%a7%a3">
###
&lt;/a>
示例（有解析解）
&lt;/div>
&lt;/h4>
&lt;p>以正态分布为例，假设 $X_1, X_2, \ldots, X_n$ 服从均值为 $\mu$、方差为 $\sigma^2$ 的正态分布，那么似然函数为：
$$
L(\mu, \sigma^2|x_1, x_2, \ldots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)
$$&lt;/p>
&lt;p>对数似然函数为：
$$
l(\mu, \sigma^2|x_1, x_2, \ldots, x_n) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2
$$&lt;/p>
&lt;p>通过求导并设置为零可以得到最大似然估计值 $\hat{\mu}$ 和 $\hat{\sigma}^2$：
$$
\hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i
$$
$$
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \hat{\mu})^2
$$&lt;/p>
&lt;h3 id="求解方法----数值优化方法" >
&lt;div>
&lt;a href="#%e6%b1%82%e8%a7%a3%e6%96%b9%e6%b3%95----%e6%95%b0%e5%80%bc%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95">
##
&lt;/a>
求解方法 &amp;ndash; &lt;strong>数值优化方法&lt;/strong>：
&lt;/div>
&lt;/h3>
&lt;p>当直接求导法难以应用时（例如非线性问题、多维问题等），可以采用数值优化方法来寻找似然函数
的最大值。常见数值优化算法有：一阶导数：梯度下降法；二阶导数：牛顿-拉复生法、拟牛顿法等。&lt;/p>
&lt;h4 id="梯度下降法" >
&lt;div>
&lt;a href="#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95">
###
&lt;/a>
梯度下降法
&lt;/div>
&lt;/h4>
&lt;p>梯度下降法是一种迭代优化算法，它基于函数在某一点处的梯度方向，沿着梯度的反方向更新参数，直到达到一个局部最小值点。&lt;/p>
&lt;p>一般步骤：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>设置目标函数（aka, 损失函数）：在MLE中目标函数就是最大化对数似然（log-likelihood），
然而梯度下降法通常是求函数最小值，所以通常在MLE中最小化负数对数似然（negative
log-likelihood），这个只是出于数值计算方便的考虑，并且其等价于最大化对数似然。&lt;/p>
&lt;ul>
&lt;li>设 $\mathcal{l}(\theta)$ 为“对数似然”，则&lt;/li>
&lt;li>目标函数为 $-\mathcal{l}(\theta)$ 。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>计算梯度：计算负数对数似然的梯度（单维度时成为“导数”，多个维度时就叫“梯度”，实质都是
指变化率），也就是：梯度指示了对数似然函数中坡度最陡的方向,
$$
\nabla_{\theta}(\mathcal{l}(\theta)) = - \frac{\partial \mathcal{l}(\theta)}{\partial \theta}
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>迭代更新参数：梯度下降就是在坡度最陡峭的方向上，每次只前进一小步。如果将第t次迭代的参
数设为$\theta^{(t)}$，则参数更新的规则为：
$$
\theta{t+1} = \theta{(t)} - \eta \cdot \nabla_{\theta}(\mathcal{l}(\theta^{(t)}))
$$
其中，$\eta$ 是学习率，用于控制每次迭代前进的一小步到底有多小（通常设为 0.001等）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>收敛：这个迭代过程一直持续到参数的变化变得非常小（通常意味着梯度值接近0），也就是目标
函数值收敛于此最小值。（梯度下降过程一定是收敛的吗？不。远的不说，单把学习率的值调高，
就有可能导致发散。）&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="梯度下降法有许多变种包括但不限于" >
&lt;div>
&lt;a href="#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%9c%89%e8%ae%b8%e5%a4%9a%e5%8f%98%e7%a7%8d%e5%8c%85%e6%8b%ac%e4%bd%86%e4%b8%8d%e9%99%90%e4%ba%8e">
###
&lt;/a>
梯度下降法有许多变种，包括但不限于：
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>批量梯度下降（Batch Gradient Descent）：每次迭代时使用所有训练样本计算梯度。&lt;/li>
&lt;li>随机梯度下降（Stochastic Gradient Descent, SGD）：每次迭代仅使用一个训练样本计算梯度。&lt;/li>
&lt;li>小批量梯度下降（Mini-batch Gradient Descent）：每次迭代使用一小部分训练样本计算梯度。&lt;/li>
&lt;/ul></description></item></channel></rss>