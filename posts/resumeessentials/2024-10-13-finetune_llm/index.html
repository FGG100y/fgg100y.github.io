<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>finetune_llm - fgg blog</title>
<meta name=description content="你在简历里提到了微调，请你&mldr;"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">finetune_llm</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-10-13>2024-10-13</time></div><a class="post-hidden-url u-url" href=/posts/resumeessentials/2024-10-13-finetune_llm/>/posts/resumeessentials/2024-10-13-finetune_llm/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/sbert/>#SBERT</a></li><li><a href=/tags/bert/>#BERT</a></li><li><a href=/tags/finetuning/>#Finetuning</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#bge-embedding-finetuning>bge-embedding finetuning</a><ul><li><a href=#基础知识>基础知识</a></li><li><a href=#数据格式>数据格式</a></li><li><a href=#hard-negatives-方法>Hard Negatives 方法</a></li></ul></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><p>简历中提到对模型进行私域数据微调（finetuning）的，要能够说出所以然来。</p><h2 id=bge-embedding-finetuning><div><a href=#bge-embedding-finetuning>#
</a>bge-embedding finetuning</div></h2><ul><li>为什么要微调，要解决什么问题？ 适应和提高模型在下游任务的表现</li><li>微调和预训练分别指什么？主要区别是什么？ 预训练是“本科阶段学习”，微调是“研究生阶段专研”</li><li>SBERT是什么？哪些技术可以提高生成嵌入的质量？ SBERT是工具包，主要基于BERT等预训练模型来生成句子级别的嵌入向量</li></ul><h3 id=基础知识><div><a href=#%e5%9f%ba%e7%a1%80%e7%9f%a5%e8%af%86>##
</a>基础知识</div></h3><blockquote><p>注意BGE使用CLS的表征作为整个句子的表示，如果使用了错误的方式（如mean pooling)会导致效果很差。</p></blockquote><h4 id=bge-embedding-模型的使用><div><a href=#bge-embedding-%e6%a8%a1%e5%9e%8b%e7%9a%84%e4%bd%bf%e7%94%a8>###
</a>BGE-Embedding 模型的使用</div></h4><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sentence_transformers <span style=color:#ff6ac1>import</span> SentenceTransformer
</span></span><span style=display:flex><span>sentences_1 <span style=color:#ff6ac1>=</span> [<span style=color:#5af78e>&#34;样例数据-1&#34;</span>, <span style=color:#5af78e>&#34;样例数据-2&#34;</span>]
</span></span><span style=display:flex><span>sentences_2 <span style=color:#ff6ac1>=</span> [<span style=color:#5af78e>&#34;样例数据-3&#34;</span>, <span style=color:#5af78e>&#34;样例数据-4&#34;</span>]
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> SentenceTransformer(<span style=color:#5af78e>&#39;BAAI/bge-large-zh-v1.5&#39;</span>)
</span></span><span style=display:flex><span>embeddings_1 <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>encode(sentences_1, normalize_embeddings<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>embeddings_2 <span style=color:#ff6ac1>=</span> model<span style=color:#ff6ac1>.</span>encode(sentences_2, normalize_embeddings<span style=color:#ff6ac1>=</span><span style=color:#ff6ac1>True</span>)
</span></span><span style=display:flex><span>similarity <span style=color:#ff6ac1>=</span> embeddings_1 <span style=color:#ff6ac1>@</span> embeddings_2<span style=color:#ff6ac1>.</span>T
</span></span><span style=display:flex><span><span style=color:#ff5c57>print</span>(similarity)
</span></span></code></pre></div><p>原始BERT模型(<a href=/posts/resumeessentials/bert%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/>BERT论文精读笔记</a>)生成的是词级别</p><p>的嵌入，直接用于句子级别任务时，也只能使用[CLS] token的表征作为整个句子的嵌入向量，可能
无法很好的捕捉句子的全局语义。（BERT生成带有上下文语义的词嵌入表征）</p><p>SBERT：专门设计用于句子表示。SBERT 将句子通过 BERT 编码后，通常将输出的所有词的嵌入进行平均池化
（mean pooling）(也可以使用CLS pooling)，生成一个固定长度的句子级别向量表示。</p><p><a href=https://www.sbert.net/docs/sentence_transformer/usage/custom_models.html>Structure of Sentence Transformer Models</a></p><p>A Sentence Transformer model consists of a collection of modules (docs) that are executed sequentially. The most common architecture is a combination of a Transformer module, a Pooling module, and optionally, a Dense module and/or a Normalize module.</p><ul><li><p>Transformer: This module is responsible for processing the input text and generating
contextualized embeddings.</p></li><li><p>Pooling: This module reduces the dimensionality of the output from the Transformer
module by aggregating the embeddings. Common pooling strategies include <strong>mean
pooling</strong> and <strong>CLS pooling</strong>.</p></li><li><p>Dense: This module contains a linear layer that post-processes the embedding output
from the Pooling module.</p></li><li><p>Normalize: This module normalizes the embedding from the previous layer.</p></li></ul><h4 id=一般sbert-模型的使用><div><a href=#%e4%b8%80%e8%88%acsbert-%e6%a8%a1%e5%9e%8b%e7%9a%84%e4%bd%bf%e7%94%a8>###
</a>一般SBERT 模型的使用</div></h4><p>For example, the popular all-MiniLM-L6-v2 model can also be loaded by initializing the 3 specific modules that make up that model:</p><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff6ac1>from</span> sentence_transformers <span style=color:#ff6ac1>import</span> models, SentenceTransformer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transformer <span style=color:#ff6ac1>=</span> models<span style=color:#ff6ac1>.</span>Transformer(<span style=color:#5af78e>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>, max_seq_length<span style=color:#ff6ac1>=</span><span style=color:#ff9f43>256</span>)
</span></span><span style=display:flex><span>pooling <span style=color:#ff6ac1>=</span> models<span style=color:#ff6ac1>.</span>Pooling(transformer<span style=color:#ff6ac1>.</span>get_word_embedding_dimension(), pooling_mode<span style=color:#ff6ac1>=</span><span style=color:#5af78e>&#34;mean&#34;</span>)
</span></span><span style=display:flex><span>normalize <span style=color:#ff6ac1>=</span> models<span style=color:#ff6ac1>.</span>Normalize()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#ff6ac1>=</span> SentenceTransformer(modules<span style=color:#ff6ac1>=</span>[transformer, pooling, normalize])
</span></span></code></pre></div><h3 id=数据格式><div><a href=#%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f>##
</a>数据格式</div></h3><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{<span style=color:#ff6ac1>&#34;query&#34;</span>: <span style=color:#ff5c57>str</span>, <span style=color:#ff6ac1>&#34;pos&#34;</span>: <span style=color:#ff5c57>List</span>[<span style=color:#ff5c57>str</span>], <span style=color:#ff6ac1>&#34;neg&#34;</span>:<span style=color:#ff5c57>List</span>[<span style=color:#ff5c57>str</span>]}
</span></span></code></pre></div><blockquote><p>doc：如果没有负样本，可以从语料库随机抽取样本作为负样本。</p></blockquote><p>用私域数据进行微调，是因为选用的开源模型是基础款，对任务领域的数据的表征能力不足（区分度
不满足任务需求）。</p><p>构建正负样本的实际操作：
预处理：对各个问题类别（上报时选的类别）进行样本分组（如：工业污染、排水设施问题、生活垃圾等等）。
因为存在上报质量问题，因此对各个类别中不相符的样本进行手动过滤。</p><ul><li>正样本：某问题类别样本； （句子意义相近的句子）</li><li>负样本：其他问题类别样本。 （意义不相关的句子）</li></ul><h3 id=hard-negatives-方法><div><a href=#hard-negatives-%e6%96%b9%e6%b3%95>##
</a>Hard Negatives 方法</div></h3><p>Hard Negative 是指那些与输入句子在表面上非常相似但实际含义相差较大的句子。因为它们很难被
模型区分，所以在训练中使用这些难区分的负样本可以促使模型学习到更细致的语义差异。</p><p>降低过拟合和增加泛化能力：通过Hard Negative样本的对抗，模型需要学会捕捉更细腻的语义信息，
从而提高在实际应用中的表现，尤其是在对相似句子的区分能力上。</p><p>如何获取“困难负样本”？在FAISS检索结果中筛选。</p></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/resumeessentials/bert%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/>BERT模型论文精读笔记</a></div><div class="right pagination-item"><a href=/posts/resumeessentials/2024-10-09-hr%E8%B5%84%E9%9D%A2_faq/>HR资面_FAQ</a></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>