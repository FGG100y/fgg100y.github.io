<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>BERT模型论文精读笔记 - fgg blog</title>
<meta name=description content="(李沐-BERT论文精读的个人整理笔记)
BERT论文标题:
Pre-training预训练 of
Deep Bidirectional Transformers深度双向Transformer for
Language Understanding语言理解
预训练(pre-training)：先在一个大的数据集上训练一个模型(从零开始得到一组权重值W0)，这个模
型的主要任务是被用在其他任务（或下游任务上）进行训练(training：以W0初始化模型然后训练)
（以解决下游任务问题）。

BERT本身含义：Bidirectional Encoder Representations from Transformer，使用了 Transformers 模
型(Transformer论文精读)的编码编码器组件，学习一个双向的嵌入表示。与 ELMo 和 Generative Pre-trained Transformer 不同：

BERT 从无标注的文本中（jointly conditioning 联合左右的上下文信息）预训练词嵌入的双向表征。
pre-trained BERT 可以通过加一个输出层来 fine-tune，不需要对特定任务的做架构上的修改就
可以在在很多任务（问答、推理）有很不错的、state-of-the-art 的效果
GPT unidirectional，使用左边的上下文信息预测未来；BERT bidirectional，使用左右侧的上下文信息
ELMo based on RNNs, down-stream 任务需要调整架构
GPT, based on Transformers decoder, down-stream 任务只需要改最上层
BERT based on Transformers encoder, down-stream 任务只需要调整最上层
"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">BERT模型论文精读笔记</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-10-13>2024-10-13</time></div><a class="post-hidden-url u-url" href=/posts/resumeessentials/bert%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/>/posts/resumeessentials/bert%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/bert/>#BERT</a></li><li><a href>#论文精读笔记</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#bert-模型>BERT 模型</a><ul><li><a href=#模型架构主要调了三个参数>模型架构，主要调了三个参数：</a></li><li><a href=#模型输入输出>模型输入输出：</a></li><li><a href=#动态掩码策略>动态掩码策略：</a></li></ul></li><li><a href=#损失函数>损失函数</a></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><p>(李沐-BERT论文精读的个人整理笔记)</p><p>BERT论文标题:
<ruby>Pre-training<rt>预训练</rt></ruby> of
<ruby>Deep Bidirectional Transformers<rt>深度双向Transformer</rt></ruby> for
<ruby>Language Understanding<rt>语言理解</rt></ruby></p><pre><code>预训练(pre-training)：先在一个大的数据集上训练一个模型(从零开始得到一组权重值W0)，这个模
型的主要任务是被用在其他任务（或下游任务上）进行训练(training：以W0初始化模型然后训练)
（以解决下游任务问题）。
</code></pre><p>BERT本身含义：Bidirectional Encoder Representations from Transformer，使用了 Transformers 模
型(<a href=./Transformer%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/index.md>Transformer论文精读</a>)的编码编码器组件，学习一个双向的嵌入表示。与 ELMo 和 Generative Pre-trained Transformer 不同：</p><ul><li>BERT 从无标注的文本中（jointly conditioning 联合左右的上下文信息）预训练词嵌入的双向表征。</li><li>pre-trained BERT 可以通过加一个输出层来 fine-tune，不需要对特定任务的做架构上的修改就
可以在在很多任务（问答、推理）有很不错的、state-of-the-art 的效果</li><li>GPT unidirectional，使用左边的上下文信息预测未来；BERT bidirectional，使用左右侧的上下文信息</li><li>ELMo based on RNNs, down-stream 任务需要调整架构</li><li>GPT, based on Transformers decoder, down-stream 任务只需要改最上层</li><li>BERT based on Transformers encoder, down-stream 任务只需要调整最上层</li></ul><p>NLP 任务主要分两类：</p><ul><li>sentence-level tasks 句子级别的任务——情绪识别、语句相似计算、语义检索等；</li><li>token-level tasks 词级别的人物——NER (人名、街道名) 需要 fine-grained output</li></ul><p>标准语言模型是 unidirectional 单向的，基于神经网络的 ELMo 和 GPT 也都是单向的，就是从左
到右的架构，只能将输入的一个句子从左看到右，然后是预测下一个单词/词元。然而一些语言理解
任务，比如情感分类、QA，则从左看到右、从右看到左 都应该是合法的。如果能从两个方向看信息，
能提升模型解决这类任务的性能。</p><p>BERT 通过 MLM(Masked language model) 带掩码的语言模型 作为预训练的目标，来减轻标准语言模
型的单向约束：</p><p>MLM 带掩码的语言模型做什么呢？
每次随机选输入的词源 tokens, 然后 mask 它们，目标函数是预测被 masked 的词；类似挖空
填词、完形填空。</p><p>BERT 除了 MLM 还有什么？
NSP: next sentence prediction
判断两个句子是随机采样的 or 原文相邻，学习 sentence-level 的信息</p><p>总的来说：
ELMo 用了 bidirectional 信息，但架构是 RNN 比较老（无法并行计算）；
GPT 架构 Transformer 新（注意力机制大法好），但只用了 unidirectional 信息；
BERT = ELMo 的 bidirectional 信息 + GPT 的新架构 transformer。
毕竟语言任务很多并不是预测未来，而是完形填空。BERT结合这俩，并证明双向有用。</p><h2 id=bert-模型><div><a href=#bert-%e6%a8%a1%e5%9e%8b>#
</a>BERT 模型</div></h2><p>BERT 有两个任务：预训练 + 微调</p><p>pre-training: 使用 unlabeled data 训练</p><p>fine-tuning: 采用 Bert 模型，但是权重都是预训练期间得到的</p><ul><li>所有权重在微调的时候都会参与训练，用的是有标记的数据、</li><li>每一个下游任务都会常见一个 新的 BERT 模型，（由预训练参数初始化），但每一个下游任务会
根据自己任务的 labeled data 来微调自己的 BERT 模型</li></ul><p><img alt=IMG_BERT src=/posts/resumeessentials/bert%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/images/BERT_Pretrain_Finetune.png></p><p>下游任务：创建同样的 BERT 的模型，权重的初始化值来自于 预训练好 的权重。</p><p>MNLI, NER, SQuAD 下游任务有 自己的 labeled data, 对 BERT 继续训练，得到各个下游任务自己
的的 BERT 版本</p><h3 id=模型架构主要调了三个参数><div><a href=#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e4%b8%bb%e8%a6%81%e8%b0%83%e4%ba%86%e4%b8%89%e4%b8%aa%e5%8f%82%e6%95%b0>##
</a>模型架构，主要调了三个参数：</div></h3><ul><li>L: transform blocks 的个数</li><li>H: hidden size 隐藏层大小</li><li>A: 自注意力机制 multi-head 中 head 头的个数</li></ul><p>主要两个 size：</p><ul><li>调了 BERT_BASE （学习 1 亿参数，L=12, H=768, A=12）</li><li>BERT_LARGE（3.4 亿参数, L=24, H=1024, A=16）</li></ul><h3 id=模型输入输出><div><a href=#%e6%a8%a1%e5%9e%8b%e8%be%93%e5%85%a5%e8%be%93%e5%87%ba>##
</a>模型输入输出：</div></h3><ul><li>WordPiece 分词（tokenization）</li><li>输入序列构成：<ul><li>[CLS], [SEP]：特殊词元</li><li>序列开头永远是[CLS]，self-attn保证每个token都能联系到其他所有token</li><li>第一个句子后是[SEP]，用于区分两个句子</li></ul></li></ul><p><img alt=IMG_BERT_IO src=/posts/resumeessentials/bert%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/images/BERT_pretrain_io.png></p><p>预训练整体过程：</p><ol><li>每一个 token 进入 BERT 得到 这个 token 的 embedding 表示。</li><li>整体进 BERT 然后输出一个结果序列<ul><li>最后一个 transformer 块的输出，表示 这个词元 token 的 BERT 的表示。在后面再添加额
外的输出层，来得到特定任务想要的结果</li></ul></li></ol><p><img alt=IMG_input_emb src=/posts/resumeessentials/bert%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E7%AC%94%E8%AE%B0/images/BERT_input_embedding.png></p><p>在预训练第一步中得到token的embedding的过程：</p><ul><li>给定一个词元（token）</li><li>BERT input representation = token 本身的表示 + segment 句子的表示 + position embedding
位置表示<ul><li>token embedding：每一个 token 有对应的词向量</li><li>Segment：即到底是 A 句还是 B 句，通过学习得到</li><li>Position：是 token 在这个序列 sequence 中的位置信息。从 0 开始 1 2 3 4 -> N。其最
终值也是通过学习得到的（transformer 是给定的）</li><li>加起来的话每个向量的信息就会组合在一起</li></ul></li><li>从【一个词元的序列】得到【一个向量的序列】，最终可以进入 transformer 块</li></ul><h3 id=动态掩码策略><div><a href=#%e5%8a%a8%e6%80%81%e6%8e%a9%e7%a0%81%e7%ad%96%e7%95%a5>##
</a>动态掩码策略：</div></h3><p>BERT 模型中的 MLM（Masked Language Model） 任务确实引入了一个问题：在预训练阶段，输入序
列中的 15% 的单词被替换为 [MASK]，而在微调阶段，模型接收的是完整的自然语言句子，不再有
[MASK] 标记。因此，预训练和微调阶段模型看到的数据分布不一致，这可能会影响微调阶段模型的
表现。</p><p>为了解决这个问题，BERT 设计中采取了一些措施：</p><p>BERT 的设计者意识到了这个问题，因此在预训练的过程中引入了一种动态掩码策略。虽然 15% 的单
词被标记为需要预测的目标，但并不是所有这些单词都被替换为 [MASK]，具体的处理方式是：</p><pre><code>80% 的情况下：将目标单词替换为 [MASK]。
10% 的情况下：将目标单词替换为一个随机的其他单词。
10% 的情况下：保持目标单词不变。
</code></pre><p>这种设计的目的是让模型不仅仅习惯于看到 [MASK] 标记，还可以在预训练时学到更多关于真实单词
的上下文信息。通过这种混合策略，BERT 能够学会在存在 [MASK] 标记时预测缺失单词，同时也能
应对训练过程中未出现 [MASK] 的情况。</p><h2 id=损失函数><div><a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>#
</a>损失函数</div></h2><p>在 BERT 的预训练阶段，主要有两个任务，它们的损失函数与这两个任务紧密相关：</p><ul><li><p>MLM任务中，模型的目标是根据上下文预测被随机掩盖（15%）的单词。这个任务的损失函数是交叉
熵损失函数（cross-entropy loss），它用来衡量模型预测的单词与真实单词之间的差距。</p></li><li><p>NSP任务中，模型的目标是判断两个句子之间是否具有逻辑上的连续性，即模型要预测第二个句子
是否是第一个句子的下一个句子。NSP 任务同样使用交叉熵损失函数，用于计算模型预测的二分类
结果（连续/不连续）与真实标签之间的误差。</p></li></ul><p>因此，预训练阶段的总损失函数是 MLM 和 NSP 损失函数的加权和，模型通过这个损失函数学习到上
下文的语言信息。</p><p>在微调阶段，BERT 不再使用 MLM 或 NSP 任务，而是根据具体的下游任务定义新的损失函数。</p><ul><li><p>文本分类任务（如情感分析）：
微调阶段的文本分类任务通常基于 [CLS] token 的输出，使用这个向量来预测类别。此时的损
失函数依然是交叉熵损失函数，用于计算模型预测的类别分布与真实类别之间的差异。</p></li><li><p>序列标注任务（如命名实体识别 NER）：
在序列标注任务中，BERT 的每个 token 会被映射到一个标签，这类任务的损失函数也是交叉熵
损失，但它是针对每个 token 预测结果和实际标签之间的差异计算的。</p></li><li><p>问答任务（如 SQuAD）：
在问答任务中，BERT 会生成两个标量，一个表示答案的起始位置，另一个表示答案的结束位置。
对于这些位置预测，同样使用交叉熵损失函数。</p></li></ul><p>因此，微调阶段的损失函数完全取决于具体任务的性质，通常会选择合适的损失函数来优化任务的最终目标，而不再使用 MLM 和 NSP 的损失。</p></div></article><h3 class="read-next-title noselect"></h3><ul class="read-next-posts noselect"><li><a href=/posts/resumeessentials/2024-10-13-finetune_llm/>finetune_llm</a></li></ul><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/historicalsnippet/2024-10-16-%E6%9E%84%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/>构建二叉树</a></div><div class="right pagination-item"><a href=/posts/resumeessentials/2024-10-13-finetune_llm/>finetune_llm</a></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>