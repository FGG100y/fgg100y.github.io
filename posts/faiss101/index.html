<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>FAISS-IVFPQ - fgg blog</title><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Plain and Simple: IndexFlatL2 Given a set of vectors, we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index. Now, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels.
IndexFlatL2: simple but not scalable Partitioning the index: for speed when scale up Quantization: for more speed Inverted File Index (IVF) index The Inverted File Index (IVF) index consists of search scope reduction through clustering."><meta property="og:image" content><meta property="og:url" content="https://fgg100y.github.io/posts/faiss101/"><meta property="og:site_name" content="fgg blog"><meta property="og:title" content="FAISS-IVFPQ"><meta property="og:description" content="Plain and Simple: IndexFlatL2 Given a set of vectors, we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index. Now, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels.
IndexFlatL2: simple but not scalable Partitioning the index: for speed when scale up Quantization: for more speed Inverted File Index (IVF) index The Inverted File Index (IVF) index consists of search scope reduction through clustering."><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-22T11:15:40+08:00"><meta property="article:modified_time" content="2024-05-22T11:15:40+08:00"><meta property="article:tag" content="FAISS"><meta property="article:tag" content="IVF"><meta property="article:tag" content="乘积量化"><meta name=twitter:card content="summary"><meta name=twitter:title content="FAISS-IVFPQ"><meta name=twitter:description content="Plain and Simple: IndexFlatL2 Given a set of vectors, we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index. Now, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels.
IndexFlatL2: simple but not scalable Partitioning the index: for speed when scale up Quantization: for more speed Inverted File Index (IVF) index The Inverted File Index (IVF) index consists of search scope reduction through clustering."><script src=https://fgg100y.github.io/js/feather.min.js></script><link href=https://fgg100y.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://fgg100y.github.io/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://fgg100y.github.io/css/dark.726cd11ca6eb7c4f7d48eb420354f814e5c1b94281aaf8fd0511c1319f7f78a4.css media="(prefers-color-scheme: dark)"><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body><div class=content><header><div class=main><a href=https://fgg100y.github.io/>fgg blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main><article><div class=title><h1 class=title>FAISS-IVFPQ</h1><div class=meta>Posted on May 22, 2024</div></div><section class=body><h2 id=plain-and-simple-indexflatl2>Plain and Simple: IndexFlatL2</h2><blockquote><p>Given a set of vectors, we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index.
Now, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels.</p></blockquote><ul><li>IndexFlatL2: simple but not scalable</li><li>Partitioning the index: for speed when scale up</li><li>Quantization: for more speed</li></ul><h2 id=inverted-file-index-ivf-index>Inverted File Index (IVF) index</h2><p>The Inverted File Index (IVF) index consists of search scope reduction through clustering.</p><blockquote><p>Inverted File Index (IVF) The IVF is simply a technique for pre-filtering the dataset so that you don’t have to do an exhaustive search of all of the vectors. It’s pretty straightforward–you cluster the dataset ahead of time with k-means clustering to produce a large number (e.g., 100) of dataset partitions. Then, at query time, you compare your query vector to the partition centroids to find, e.g., the 10 closest clusters, and then you search against only the vectors in those partitions.</p></blockquote><p>Partitioning the index (clustering)</p><blockquote><p>Faiss allows us to add multiple steps that can optimize our search using many different methods. A popular approach is to partition the index into Voronoi cells.
We can imagine our vectors as each being contained within a Voronoi cell — when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid’s cell.
But there is a problem if our query vector lands near the edge of a cell — there’s a good chance that its closest other datapoint is contained within a neighboring cell.</p></blockquote><p>what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the nprobe value. With nprobe we can set the number of cells to search. I.e., Increasing nprobe increases our search scope.</p><p>进行聚类的结果，一方面可以极大提升查询速度，但另一方面，可能会造成落在聚类簇边缘的“query向量”只在本聚类簇内查找匹配的结果（实际上，它可能与邻近的聚类簇的其他向量更靠近），从而导致匹配质量的降低。
一个缓解这个问题的方法是：调整参数 nprobe. 通过增加 nprobe (增加用于匹配查询向量的邻近聚类簇数量）来提升匹配质量。（同时，也会增加查询耗时）</p><h2 id=product-quantization>Product Quantization</h2><blockquote><p>All of our indexes so far have stored our vectors as full (eg Flat) vectors. Now, in very large datasets this can quickly become a problem.
Fortunately, Faiss comes with the ability to compress our vectors using Product Quantization (PQ).
But, what is PQ? Well, we can view it as an additional approximation step with a similar outcome to our use of IVF. Where IVF allowed us to approximate by reducing the scope of our search, PQ approximates the distance/similarity calculation instead.
PQ achieves this approximated similarity operation by compressing the vectors themselves, which consists of three steps.</p></blockquote><ol><li>Original vector</li><li>Sliced sub-vector</li><li>slice clustering</li><li>centroid ID vector</li></ol><p>PQ（乘积量化）不是对嵌入向量空间进行降维，而是对向量本身进行压缩：</p><ul><li>01 向量分段，例如：1024 -> 128x8 (8个片段)；</li><li>02 如果数据量是50k，则从单个50k x 1024 的矩阵，变成 8个 50k x 128 的矩阵；</li><li>03 然后分别用k=256的k-means进行聚类，得到8组256个centroids；则每个原始向量可以用长度为8的向量进行表征（8组与各个向量片段最近的centroid的ID）；</li><li>04 查询向量（query）同样进行片段化，并找到各组的centroids，然后计算片段向量与centroid的距离，并保存为距离表（partial query subvector-to-centroid distances table)；</li><li>05 查询向量与数据向量的距离？将数据向量的centroid-ID向量，用于 partial-query-distance-table 的表查询（table lookup），就能得到对应的一系列距离，然后计算其总和L2距离；</li><li>06 将查询向量与所有数据向量的距离计算出来，排序，即可得到 top-k 最近距离，亦即 top-k 最近似结果 （实际就是 KNN 算法）。</li><li>07 进一步的优化查询耗时，就是在计算距离的时候，不是对所有数据向量，而是只针对局部数据向量进行计算（也就是 IVF + PQ）。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>m <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>  <span style=color:#75715e># number of centroid IDs in final compressed vectors</span>
</span></span><span style=display:flex><span>bits <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span> <span style=color:#75715e># number of bits in each centroid</span>
</span></span><span style=display:flex><span>nlist <span style=color:#f92672>=</span> <span style=color:#ae81ff>50</span>  <span style=color:#75715e># how many cells/blocks</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>quantizer <span style=color:#f92672>=</span> faiss<span style=color:#f92672>.</span>IndexFlatL2(d)  <span style=color:#75715e># we keep the same L2 distance flat index</span>
</span></span><span style=display:flex><span>index <span style=color:#f92672>=</span> faiss<span style=color:#f92672>.</span>IndexIVFPQ(quantizer, d, nlist, m, bits)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>index<span style=color:#f92672>.</span>train(sentence_embeddings)
</span></span><span style=display:flex><span>index<span style=color:#f92672>.</span>add(sentence_embeddings)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>index<span style=color:#f92672>.</span>nprobe <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>  <span style=color:#75715e># align to previous IndexIVFFlat nprobe value</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># %%time  # jupyterlab cell magic</span>
</span></span><span style=display:flex><span>D, I <span style=color:#f92672>=</span> index<span style=color:#f92672>.</span>search(xq, k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> I<span style=color:#f92672>.</span>tolist()[<span style=color:#ae81ff>0</span>]:
</span></span><span style=display:flex><span>    print(indata[i])  <span style=color:#75715e># sample of original texts/sentences</span>
</span></span></code></pre></div></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/faiss>FAISS</a></li><li><a href=/tags/ivf>IVF</a></li><li><a href=/tags/%E4%B9%98%E7%A7%AF%E9%87%8F%E5%8C%96>乘积量化</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/fgg100y/hugo-blog/ rel=me title=GitHub><i data-feather=github></i></a>
<a class=border></a></div><div class=footer-info>2024 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script>feather.replace()</script></div></body></html>