<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>maximum_likelihood_estimate - fgg blog</title>
<meta name=description content="MLE: 找到一组参数值，使得观察到的数据出现的概率最大。"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">maximum_likelihood_estimate</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-08-30>2024-08-30</time></div><a class="post-hidden-url u-url" href=/posts/optimazationmethods/mle/maximum_likelihood_estimate/>/posts/optimazationmethods/mle/maximum_likelihood_estimate/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/optimization/>#Optimization</a></li><li><a href=/tags/mle/>#MLE</a></li><li><a href=/tags/gd/sgd/>#GD/SGD</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#概念>概念</a></li><li><a href=#求解方法----直接求导法>求解方法 &ndash; <strong>直接求导法</strong>：</a></li><li><a href=#求解方法----数值优化方法>求解方法 &ndash; <strong>数值优化方法</strong>：</a></li></ul></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><p>最大似然估计（Maximum Likelihood Estimation, MLE）是一种常用参数估计方法，在统计学和机
器学习中被广泛使用。它的基本思想是找到一组参数值，使得观察到的数据出现的概率最大。</p><h3 id=概念><div><a href=#%e6%a6%82%e5%bf%b5>##
</a>概念</div></h3><p>假设我们有一组<ruby>“独立同分布”<rt>（independent identity distribution, i.i.d.）</rt></ruby>
的观测数据 $X_1, X_2, \ldots, X_n$，这些数据来自某个概率分布族，其概率密度函数或概率质量
函数为 $f(x|\theta)$，其中$\theta$ 是未知参数。直观上看，最大似然估计的目标就是试图在
$\theta$ 所有可能的取值中，找到一个能使这一组观测数据出现的“可能性”最大的值。</p><p>形式化地讲，给定观测数据 $x_1, x_2, \ldots, x_n$，最大似然估计 $\hat{\theta}$ 定义为：
$$
\hat{\theta} = \arg\max_\theta L(\theta|x_1, x_2, \ldots, x_n)
$$
其中 $L(\theta|x_1, x_2, \ldots, x_n)$ 称为似然函数，定义为：
$$
L(\theta|x_1, x_2, \ldots, x_n) = f(x_1, x_2, \ldots, x_n|\theta) = \prod_{i=1}^n f(x_i|\theta)
$$</p><p>啰唆多两句：所谓“似然”，和“概率”到底有什么区别呢？用抛硬币举例，区别在于：</p><ul><li>当我们谈论“概率”时，实际就是在问：已知模型（p=0.6，抛出正面的概率），则抛硬币10次中有7
次为正面的概率是多大？($P^{7}*(1-p)^{10-7}$)</li><li>而谈论“似然”时，其实是反过来问：已经观察到抛10次硬币出现了7次正面，则硬币抛出正面的概
率是某个值的可能有多大？最大似然估计就是找到这样一个参数，此参数使得出现这样的数据（7
正面3反面）的可能最高。</li></ul><h3 id=求解方法----直接求导法><div><a href=#%e6%b1%82%e8%a7%a3%e6%96%b9%e6%b3%95----%e7%9b%b4%e6%8e%a5%e6%b1%82%e5%af%bc%e6%b3%95>##
</a>求解方法 &ndash; <strong>直接求导法</strong>：</div></h3><ul><li>写出似然函数 $L(\theta|x_1, x_2, \ldots, x_n)$。</li><li>对似然函数取对数得到对数似然函数 $l(\theta|x_1, x_2, \ldots, x_n) = \log L(\theta|x_1, x_2, \ldots, x_n)$。</li><li>对对数似然函数求导，通常情况下求一阶导数，并令其等于零来找到极值点。</li><li>检查二阶导数或者直接通过直观判断是否为极大值点。</li></ul><h4 id=示例有解析解><div><a href=#%e7%a4%ba%e4%be%8b%e6%9c%89%e8%a7%a3%e6%9e%90%e8%a7%a3>###
</a>示例（有解析解）</div></h4><p>以正态分布为例，假设 $X_1, X_2, \ldots, X_n$ 服从均值为 $\mu$、方差为 $\sigma^2$ 的正态分布，那么似然函数为：
$$
L(\mu, \sigma^2|x_1, x_2, \ldots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)
$$</p><p>对数似然函数为：
$$
l(\mu, \sigma^2|x_1, x_2, \ldots, x_n) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2
$$</p><p>通过求导并设置为零可以得到最大似然估计值 $\hat{\mu}$ 和 $\hat{\sigma}^2$：
$$
\hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i
$$
$$
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \hat{\mu})^2
$$</p><h3 id=求解方法----数值优化方法><div><a href=#%e6%b1%82%e8%a7%a3%e6%96%b9%e6%b3%95----%e6%95%b0%e5%80%bc%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95>##
</a>求解方法 &ndash; <strong>数值优化方法</strong>：</div></h3><p>当直接求导法难以应用时（例如非线性问题、多维问题等），可以采用数值优化方法来寻找似然函数
的最大值。常见数值优化算法有：一阶导数：梯度下降法；二阶导数：牛顿-拉复生法、拟牛顿法等。</p><h4 id=梯度下降法><div><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95>###
</a>梯度下降法</div></h4><p>梯度下降法是一种迭代优化算法，它基于函数在某一点处的梯度方向，沿着梯度的反方向更新参数，直到达到一个局部最小值点。</p><p>一般步骤：</p><ol><li><p>设置目标函数（aka, 损失函数）：在MLE中目标函数就是最大化对数似然（log-likelihood），
然而梯度下降法通常是求函数最小值，所以通常在MLE中最小化负数对数似然（negative
log-likelihood），这个只是出于数值计算方便的考虑，并且其等价于最大化对数似然。</p><ul><li>设 $\mathcal{l}(\theta)$ 为“对数似然”，则</li><li>目标函数为 $-\mathcal{l}(\theta)$ 。</li></ul></li><li><p>计算梯度：计算负数对数似然的梯度（单维度时成为“导数”，多个维度时就叫“梯度”，实质都是
指变化率），也就是：梯度指示了对数似然函数中坡度最陡的方向,
$$
\nabla_{\theta}(\mathcal{l}(\theta)) = - \frac{\partial \mathcal{l}(\theta)}{\partial \theta}
$$</p></li><li><p>迭代更新参数：梯度下降就是在坡度最陡峭的方向上，每次只前进一小步。如果将第t次迭代的参
数设为$\theta^{(t)}$，则参数更新的规则为：
$$
\theta{t+1} = \theta{(t)} - \eta \cdot \nabla_{\theta}(\mathcal{l}(\theta^{(t)}))
$$
其中，$\eta$ 是学习率，用于控制每次迭代前进的一小步到底有多小（通常设为 0.001等）。</p></li><li><p>收敛：这个迭代过程一直持续到参数的变化变得非常小（通常意味着梯度值接近0），也就是目标
函数值收敛于此最小值。（梯度下降过程一定是收敛的吗？不。远的不说，单把学习率的值调高，
就有可能导致发散。）</p></li></ol><h4 id=梯度下降法有许多变种包括但不限于><div><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95%e6%9c%89%e8%ae%b8%e5%a4%9a%e5%8f%98%e7%a7%8d%e5%8c%85%e6%8b%ac%e4%bd%86%e4%b8%8d%e9%99%90%e4%ba%8e>###
</a>梯度下降法有许多变种，包括但不限于：</div></h4><ul><li>批量梯度下降（Batch Gradient Descent）：每次迭代时使用所有训练样本计算梯度。</li><li>随机梯度下降（Stochastic Gradient Descent, SGD）：每次迭代仅使用一个训练样本计算梯度。</li><li>小批量梯度下降（Mini-batch Gradient Descent）：每次迭代使用一小部分训练样本计算梯度。</li></ul></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/optimazationmethods/em/expectation_maximization/>expectation_maximization</a></div><div class="right pagination-item"><a href=/posts/clipmodel/clipasso/>clipasso</a></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>