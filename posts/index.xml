<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on fgg blog</title><link>https://fgg100y.github.io/posts/</link><description>Recent content in Posts on fgg blog</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Wed, 22 May 2024 11:15:40 +0800</lastBuildDate><atom:link href="https://fgg100y.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>FAISS-IVFPQ</title><link>https://fgg100y.github.io/posts/2024-05-22-faiss-ivfpq/</link><pubDate>Wed, 22 May 2024 11:15:40 +0800</pubDate><guid>https://fgg100y.github.io/posts/2024-05-22-faiss-ivfpq/</guid><description>Plain and Simple: IndexFlatL2 Given a set of vectors, we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index. Now, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels.
IndexFlatL2: simple but not scalable Partitioning the index: for speed when scale up Quantization: for more speed Inverted File Index (IVF) index The Inverted File Index (IVF) index consists of search scope reduction through clustering.</description></item><item><title>tech interview prepare (for my resume)</title><link>https://fgg100y.github.io/posts/notes4resume/</link><pubDate>Sat, 27 Apr 2024 22:35:53 +0800</pubDate><guid>https://fgg100y.github.io/posts/notes4resume/</guid><description>自我介绍 在自我介绍时，确保你提到的项目和技能与你申请的职位紧密相关，这样可以更好地展示你的专业 能力和对职位的适应性。同时，保持自信和热情，让面试官感受到你对工作和团队的承诺。
尊敬的面试官，您好！
我叫范明华，拥有6年在机器学习领域的工作经验。我于2017年硕士毕业于中山大学，专业是生态学， 这为我在实验设计、统计分析以及数据挖掘方面打下了扎实的基础。在过去的6年中，我一直致力于 将机器学习技术应用于实际问题，并取得了一些的成果。
我在目前公司担任高级数据挖掘工程师，期间我主导了多个机器学习项目的开发和交付，包括时序预 测模型， 图像识别/目标检测模型，以及基于大数据挖掘的普通机器学习模型等，并预研NLP以及语 音识别方面的技术, 同时也取得了授权的发明专利、软件著作、地方标准等成果。在这些项目的实战 中，不仅提升了我的技术深度，也锻炼了代码管理和团队领导能力。
在技术层面，我比较擅长结合业务流程开展半监督学习，并且有丰富的实践经验（包括在普通机器学 习和NLP领域）。我熟悉整个机器学习项目的开发流程，从项目调研、数据预处理、特征工程到模型 训练和部署，我都有深入的理解和实践。
除了技术专长，我还是一个注重团队合作和务实负责的人。我相信，我的专业技能和丰富经验，能够为贵公司带来直接的价值。
我对贵公司在机器学习/大模型应用/自然语言处理方面等方面的工作非常感兴趣，并且我相信我的背景和技能可以为贵公司的发展做出贡献。 我期待能够加入贵公司，并与团队一起解决更多有趣的技术挑战。
感谢您给我这次面试的机会，我期待在接下来的讨论中分享更多我的经验和想法。谢谢！
Project 01 &amp;ndash; NLP sklearn randomforest model 当谈到随机森林时，我们需要理解它的基础算法：决策树。随机森林是基于决策树的集成学习方法。所以，让我们首先来了解决策树的基本算法，然后再深入探讨随机森林。
1. 决策树算法: 1.1 CART算法 (Classification and Regression Trees): CART算法是一种用于构建分类和回归树的决策树算法。它通过对数据集递归地进行二分来构建决策树。具体步骤如下：
特征选择：对于分类问题，通常使用基尼指数（Gini index）或信息增益（Information Gain）来选择最佳的特征进行分裂；对于回归问题，通常使用平方误差来选择最佳的特征。
节点分裂：根据选择的特征，将数据集分成两部分，使得每个子集的样本属于同一类别（对于分类问题）或具有相似的回归值（对于回归问题）。
递归：对每个子集重复上述过程，直到满足停止条件，如达到最大深度、节点中样本数小于某个阈值或其他预定义条件。
剪枝：为了避免过拟合，可以对生成的树进行剪枝，即移除一些节点来简化树的结构。
2. 随机森林算法: 2.1 构建随机森林: 随机森林是通过构建多棵决策树并将它们集成起来来完成的。具体步骤如下：
随机抽样：从原始训练集中随机选择一部分样本（有放回抽样）来构建每棵决策树的训练集。这样可以保证每棵树的训练集略有差异，增加了模型的多样性。
随机特征选择：对于每棵树的每个节点，在选择分割特征时，随机选择一部分特征来进行评估。这样可以确保每棵树的分裂过程也有所差异。
独立构建：每棵树都是独立构建的，没有任何关联。这意味着可以并行地构建多棵树，提高了训练效率。
2.2 集成决策树: 构建多棵决策树后，随机森林采用不同的方式来集成它们的预测结果：
分类任务：采用投票的方式，即每棵树投票选择最终的类别。 回归任务：采用平均值的方式，即多棵树的预测结果取平均值。 总结: 随机森林是一种强大的机器学习方法，基于决策树的集成学习。通过利用决策树的随机性和集成策略，随机森林能够有效地应对分类和回归问题，并在许多实际应用中表现优异。
在 CART (Classification and Regression Trees) 算法中，节点的分裂依据是基于贪心算法。CART 算法通过贪心地选择每次分裂时能够最大程度减少不纯度（对于分类问题）或者最小化误差（对于回归问题）的特征来进行节点的分裂。这种贪心策略保证了在每个节点分裂时都选择了最优的特征来进行分裂。
节点分裂的依据： 对于分类问题： 在分类问题中，CART 算法通常使用以下两种方法作为节点分裂的依据：</description></item><item><title>LLMs_interview_faq</title><link>https://fgg100y.github.io/posts/post_llms_faq/</link><pubDate>Fri, 26 Apr 2024 11:04:16 +0800</pubDate><guid>https://fgg100y.github.io/posts/post_llms_faq/</guid><description>01:简述GPT和BERT的区别 GPT (Decoder-only) 和 BERT (Encoder-only) 都是基于 Transformer 架构的自然语言处理模型，它们在设计上有一些显著区别：
任务类型 GPT 以生成文本为主要任务，其目标是生成与输入文本连贯和相关的文本。因此，GPT 通 常用于生成文本 (如：摘要总结，文本补充和chatbot)。 BERT 以理解文本为主要任务，其目标是从输入文本中提取语义信息。因此适用于各种文 本理解任务，如：情感分析、 文本分类、命名实体识别等下游任务。 预训练目标 GPT：单向语言建模。GPT通过自左向右的注意力机制来预测下一个单词，即根据上下文预 测下一个单词/词元是什么。 BERT：双向语言建模。BERT使用掩码语言建模（MLM）和下一句预测（NSP）两个任务，前 者在MLM任务中随机遮掩输入中的一些词语，模型需要预测这些被掩盖的词语是什么； NSP的任务是判断两个句子是否在原文中是前后连接的。 结构特点 GPT：Transformer-decoder的堆叠，仅使用自注意力机制 BERT：Transformer-encoder的堆叠，包含多层双向Transformer-encoder。在预训练阶段， BERT同时使用了自注意力机制和前馈神经网络。 模型微调 GPT：由于其生成式的特点，GPT在微调时通常将整个模型作为单独的序列生成任务进行微 调。 BERT：由于其双向表示的特点，BERT在微调时通常用于各种文本理解任务，微调时可以在 模型顶层添加适当的输出层来适应下游特定任务。 02:LLM中的因果语言建模与掩码语言建模有什么区别？ 因果语言建模（Causal Language Modeling）
在因果语言建模中，模型被要求根据输入序列的左侧内容来预测右侧的下一个词或标记。也就是 说，模型只能看到输入序列中已经生成的部分，而不能看到后续的内容。这种训练方式有助于模 型学习生成连贯和合理的文本，因为模型需要在生成每个词语时考虑上下文的信息，同时不能依 赖于未来的信息。GPT（Generative Pre-trained Transformer）就是以因果语言建模为基础的 模型。 掩码语言建模（Masked Language Modeling）：
在掩码语言建模中，模型被要求预测输入序列中一些被随机掩盖或掩码的词语。模型需要基于上 下文来预测这些被掩盖的词语是什么。 这种训练方式通常用于双向的语言理解任务，因为模型需要考虑上下文中的所有信息来预测被掩盖的词语。 BERT（Bidirectional Encoder Representations from Transformers）就是以掩码语言建模为基础的模型。 03:请简述Transformer基本原理 Transformer 是一种用于处理序列数据的深度学习模型，由谷歌团队于2017年提出，其主要原理包括 自注意力机制和位置编码。
自注意力机制： 允许模型在序列的任意两个位置间直接建立依赖关系，而不考虑它们之间的距离。具体就是将词 元线性转换为三个向量Q,K,V，然后将Q和K用来计算内积(相似度分数)并进行注意力缩放（scaled dot-product)，然后通过softmax归一化，得到每个词元相对于其他词元的注意力权重，然后用 注意力权重对向量V进行加权和计算得到“上下文向量”(context vector)，然后将上下文向量用 前馈网络（FFNN）进行变换，就得到编码器隐层输出。注意：自注意力机制中，每个输入词元的 context vector 以及后续的 hidden state，可以看成是相应的 Q 向量的函数，其他的如 K，V， 以及自注意力机制的参数对所有的 Q 都是恒定值。 + 多头注意力： 在多头注意力中，注意力机制被复制多次，并且每个注意力头都学习到一组不同的Q,K,V的 表示，然后将它们的输出拼接起来，再通过FFNN进行维度对齐。 - 复制注意力机制：原始输入序列会被用来计算多个注意力头（例如8个或16个头） - 独立学习：每个注意力头都会独立地学习一组Q，K，V的表示，也就是：每个注意力头都 有自己的权重矩阵，将输入序列转换为Q,K,V向量。 - 注意力计算：每个注意力头像单头注意力机制那样计算注意力分数和注意力权重。 - 拼接输出：将所有注意力头的输出拼接成一个向量，形成多头注意力的最终输出。这意味 着每个词元都会得到来自多个不同视角的表示，从而提高模型对输入序列的理解。 - 线性变换：拼接后的输出通过FFNN进行处理，维持输出维度以及融合不同注意力头的信息。 + narrow attn：Each attention head will get a chunk of the transformed data points (projections) to work with.</description></item></channel></rss>