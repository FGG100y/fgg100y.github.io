<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on fgg blog</title><link>https://fgg100y.github.io/posts/</link><description>Recent content in Posts on fgg blog</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Sat, 27 Apr 2024 22:35:53 +0800</lastBuildDate><atom:link href="https://fgg100y.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>tech interview prepare (for my resume)</title><link>https://fgg100y.github.io/posts/notes4resume/</link><pubDate>Sat, 27 Apr 2024 22:35:53 +0800</pubDate><guid>https://fgg100y.github.io/posts/notes4resume/</guid><description>自我介绍 在自我介绍时，确保你提到的项目和技能与你申请的职位紧密相关，这样可以更好地展示你的专业 能力和对职位的适应性。同时，保持自信和热情，让面试官感受到你对工作和团队的承诺。
尊敬的面试官，您好！
我叫范明华，拥有6年在机器学习领域的工作经验。我于2017年硕士毕业于中山大学，专业是生态学， 这为我在实验设计、统计分析以及数据挖掘方面打下了扎实的基础。在过去的6年中，我一直致力于 将机器学习技术应用于实际问题，并取得了一些的成果。
我在目前公司担任高级数据挖掘工程师，期间我主导了多个机器学习项目的开发和交付，包括时序预 测模型， 图像识别/目标检测模型，以及基于大数据挖掘的普通机器学习模型等，并预研NLP以及语 音识别方面的技术, 同时也取得了授权的发明专利、软件著作、地方标准等成果。在这些项目的实战 中，不仅提升了我的技术深度，也锻炼了代码管理和团队领导能力。
在技术层面，我比较擅长结合业务流程开展半监督学习，并且有丰富的实践经验（包括在普通机器学 习和NLP领域）。我熟悉整个机器学习项目的开发流程，从项目调研、数据预处理、特征工程到模型 训练和部署，我都有深入的理解和实践。
除了技术专长，我还是一个注重团队合作和务实负责的人。我相信，我的专业技能和丰富经验，能够为贵公司带来直接的价值。
我对贵公司在机器学习/大模型应用/自然语言处理方面等方面的工作非常感兴趣，并且我相信我的背景和技能可以为贵公司的发展做出贡献。 我期待能够加入贵公司，并与团队一起解决更多有趣的技术挑战。
感谢您给我这次面试的机会，我期待在接下来的讨论中分享更多我的经验和想法。谢谢！
Project 01 &amp;ndash; NLP sklearn randomforest model 当谈到随机森林时，我们需要理解它的基础算法：决策树。随机森林是基于决策树的集成学习方法。所以，让我们首先来了解决策树的基本算法，然后再深入探讨随机森林。
1. 决策树算法: 1.1 CART算法 (Classification and Regression Trees): CART算法是一种用于构建分类和回归树的决策树算法。它通过对数据集递归地进行二分来构建决策树。具体步骤如下：
特征选择：对于分类问题，通常使用基尼指数（Gini index）或信息增益（Information Gain）来选择最佳的特征进行分裂；对于回归问题，通常使用平方误差来选择最佳的特征。
节点分裂：根据选择的特征，将数据集分成两部分，使得每个子集的样本属于同一类别（对于分类问题）或具有相似的回归值（对于回归问题）。
递归：对每个子集重复上述过程，直到满足停止条件，如达到最大深度、节点中样本数小于某个阈值或其他预定义条件。
剪枝：为了避免过拟合，可以对生成的树进行剪枝，即移除一些节点来简化树的结构。
2. 随机森林算法: 2.1 构建随机森林: 随机森林是通过构建多棵决策树并将它们集成起来来完成的。具体步骤如下：
随机抽样：从原始训练集中随机选择一部分样本（有放回抽样）来构建每棵决策树的训练集。这样可以保证每棵树的训练集略有差异，增加了模型的多样性。
随机特征选择：对于每棵树的每个节点，在选择分割特征时，随机选择一部分特征来进行评估。这样可以确保每棵树的分裂过程也有所差异。
独立构建：每棵树都是独立构建的，没有任何关联。这意味着可以并行地构建多棵树，提高了训练效率。
2.2 集成决策树: 构建多棵决策树后，随机森林采用不同的方式来集成它们的预测结果：
分类任务：采用投票的方式，即每棵树投票选择最终的类别。 回归任务：采用平均值的方式，即多棵树的预测结果取平均值。 总结: 随机森林是一种强大的机器学习方法，基于决策树的集成学习。通过利用决策树的随机性和集成策略，随机森林能够有效地应对分类和回归问题，并在许多实际应用中表现优异。
在 CART (Classification and Regression Trees) 算法中，节点的分裂依据是基于贪心算法。CART 算法通过贪心地选择每次分裂时能够最大程度减少不纯度（对于分类问题）或者最小化误差（对于回归问题）的特征来进行节点的分裂。这种贪心策略保证了在每个节点分裂时都选择了最优的特征来进行分裂。
节点分裂的依据： 对于分类问题： 在分类问题中，CART 算法通常使用以下两种方法作为节点分裂的依据：</description></item></channel></rss>