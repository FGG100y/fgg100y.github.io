<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on fgg blog</title><link>/posts/</link><description>fgg blog (Posts)</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 05 Aug 2024 15:29:29 +0800</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>pipe_table_and_md_to_pdf</title><link>/posts/thecli/md2pdf_using_pandoc/</link><pubDate>Mon, 05 Aug 2024 15:29:29 +0800</pubDate><guid>/posts/thecli/md2pdf_using_pandoc/</guid><description>&lt;h2 id="three-steps-to-produce-good-format-of-pdf-from-md" >
&lt;div>
&lt;a href="#three-steps-to-produce-good-format-of-pdf-from-md">
#
&lt;/a>
three steps to produce good format of .pdf from .md
&lt;/div>
&lt;/h2>
&lt;p>å¦‚æžœä½ çš„ markdown æ–‡æ¡£ä¸æ˜¯å…¨è‹±æ–‡ï¼Œå¦‚æžœä½ çš„è¡¨æ ¼æ˜¯ &lt;code>pipe table&lt;/code> ç±»åž‹ï¼ˆç®€å•æ¥è¯´å°±æ˜¯ç”¨ &amp;ldquo;|&amp;rdquo;
è¿™ä¸ªç¬¦å·æ¥å›´æˆï¼Œæ´‹äººå°±ç®¡å®ƒå« pipe tableï¼‰ï¼Œç„¶åŽè¡¨æ ¼ä¸­æŸäº›æ ¼å­ä¸­çš„å†…å®¹ç‰¹åˆ«é•¿ï¼Œå¦‚æžœå¯„å¸Œæœ›
ç›´æŽ¥ç”¨pandoc é»˜è®¤å‘½ä»¤è¿›è¡Œæ–‡æ¡£æ ¼å¼è½¬æ¢ï¼Œåˆ™å¤§æ¦‚çŽ‡ä¸ä¼šç«‹å³å¾—åˆ°â€œç¾Žè§‚â€çš„pdfæ–‡æ¡£ã€‚éœ€è¦è¿›è¡Œä¸€å®š
çš„å¤„ç†ï¼š&lt;/p>
&lt;ol>
&lt;li>æ”¾åœ¨ &lt;code>md&lt;/code> æ–‡æ¡£æœ€å¼€å¤´çš„å…ƒæ•°æ®ï¼ˆmeta dataï¼‰ï¼š&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>output:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pdf_document:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> latex_engine: xelatex
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> keep_tex: true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra_dependencies: [&amp;#34;fontspec&amp;#34;]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>header-includes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \usepackage{xeCJK}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \usepackage{tabularx}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \usepackage{fontspec}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \setmainfont{Noto Sans CJK SC}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - \setmonofont{Noto Sans Mono CJK SC}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>table_preamble.tex æ–‡ä»¶&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">\u&lt;/span>sepackage&lt;span style="color:#ff6ac1">{&lt;/span>xeCJK&lt;span style="color:#ff6ac1">}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">\u&lt;/span>sepackage&lt;span style="color:#ff6ac1">{&lt;/span>tabularx&lt;span style="color:#ff6ac1">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>å‘½ä»¤è¡Œå‘½ä»¤&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>pandoc your_cool.md -o document.pdf --pdf-engine&lt;span style="color:#ff6ac1">=&lt;/span>xelatex --include-in-header&lt;span style="color:#ff6ac1">=&lt;/span>table_preamble.tex
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- ![tb01](images/blog_test_tb01_Screenshot_20240805_175815.png)![tb02](images/blog_test_tb02_Screenshot_20240805_180004.png) -->
&lt;hr>
&lt;h2 id="table-format-expected" >
&lt;div>
&lt;a href="#table-format-expected">
#
&lt;/a>
table format expected
&lt;/div>
&lt;/h2>
&lt;div style="display: flex; justify-content: space-between;">
&lt;img src="images/blog_test_tb01_Screenshot_20240805_175815.png" alt="Image 1" style="width: 45%;"/>
&lt;img src="images/blog_test_tb02_Screenshot_20240805_180004.png" alt="Image 2" style="width: 45%;"/>
&lt;/div>
&lt;hr>
&lt;h2 id="table-rendering-testing" >
&lt;div>
&lt;a href="#table-rendering-testing">
#
&lt;/a>
table rendering testing
&lt;/div>
&lt;/h2>
&lt;p>è¡¨1: æµ‹è¯•è¡¨1 (pipe table syntax)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>| | time | describe | s2c | w2c |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| -- | ---- | ------- | -- | -- |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| 80891 | 2022-03-21 | xxåŒºxxxxxxxxxå—è·¯å—åä¸‰å··6å·æ—ï¼Œä¸€æš—æ¸ ç›´æŽ’æ±¡æ°´è‡³é›¨æ°´æ²Ÿï¼Œæ°´è´¨æµ‘æµŠï¼Œæ°”å‘³éš¾é—»ï¼Œåº•éƒ¨... | H | L |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| 9191 | 2020-10-07 | æ²³æ®µåº·é™‚è·¯5å·å¾€ä¸Šæ¸¸900ç±³æ²³åºŠæœ‰æ¼‚æµ®ç‰©,æŽ’å‡ºæ±¡æ°´ | H | L |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| 765 | 2022-05-10 | è”—åŸºæ°´é—¸å£æœ‰å¤§é‡æ¼‚æµ®ç‰©å’Œæ­»é±¼ | L | H |
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>time&lt;/th>
&lt;th>describe&lt;/th>
&lt;th>s2c&lt;/th>
&lt;th>w2c&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>80891&lt;/td>
&lt;td>2022-03-21&lt;/td>
&lt;td>xxåŒºxxxxxxxxxå—è·¯å—åä¸‰å··6å·æ—ï¼Œä¸€æš—æ¸ ç›´æŽ’æ±¡æ°´è‡³é›¨æ°´æ²Ÿï¼Œæ°´è´¨æµ‘æµŠï¼Œæ°”å‘³éš¾é—»ï¼Œåº•éƒ¨&amp;hellip;&lt;/td>
&lt;td>H&lt;/td>
&lt;td>L&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9191&lt;/td>
&lt;td>2020-10-07&lt;/td>
&lt;td>æ²³æ®µåº·é™‚è·¯5å·å¾€ä¸Šæ¸¸900ç±³æ²³åºŠæœ‰æ¼‚æµ®ç‰©,æŽ’å‡ºæ±¡æ°´&lt;/td>
&lt;td>H&lt;/td>
&lt;td>L&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>765&lt;/td>
&lt;td>2022-05-10&lt;/td>
&lt;td>è”—åŸºæ°´é—¸å£æœ‰å¤§é‡æ¼‚æµ®ç‰©å’Œæ­»é±¼&lt;/td>
&lt;td>L&lt;/td>
&lt;td>H&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>è¡¨2: æµ‹è¯•è¡¨2 (tabularx table syntax)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>\begin{tabularx}{\textwidth}{|X|X|X|}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\hline
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Header 1 &amp;amp; Header 2 &amp;amp; Header 3 \\
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\hline
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Long content that should wrap &amp;amp; éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ å—è·¯å—åä¸‰å··6å·æ—ï¼Œä¸€æš—æ¸ ç›´æŽ’æ±¡æ°´è‡³é›¨æ°´æ²Ÿï¼Œæ°´è´¨æµ‘æµŠï¼Œæ°”å‘³éš¾é—»ï¼Œåº•éƒ¨... &amp;amp; More content \\
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\hline
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\end{tabularx}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>\begin{tabularx}{\textwidth}{|X|X|X|}
\hline
Header 1 &amp;amp; Header 2 &amp;amp; Header 3 \
\hline
Long content that should wrap &amp;amp; éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ éžå¸¸é•¿çš„ä¸€æ®µè¯ å—è·¯å—åä¸‰å··6å·æ—ï¼Œä¸€æš—æ¸ ç›´æŽ’æ±¡æ°´è‡³é›¨æ°´æ²Ÿï¼Œæ°´è´¨æµ‘æµŠï¼Œæ°”å‘³éš¾é—»ï¼Œåº•éƒ¨&amp;hellip; &amp;amp; More content \
\hline
\end{tabularx}&lt;/p></description></item><item><title>matplotlibIssue</title><link>/posts/pyplot/matplotlibissue/</link><pubDate>Thu, 01 Aug 2024 17:58:28 +0800</pubDate><guid>/posts/pyplot/matplotlibissue/</guid><description>&lt;p>matplotlibæ˜¯ä¸ªå¥½å·¥å…·ï¼Œä½†è¿˜æ²¡å¥½é€ï¼Œåšå‡ºæ¥çš„å›¾ä¸­ä¸­æ–‡å˜æ¡†æ¡†ã€‚éœ€è¦é¢å¤–è®¾ç½®ã€‚&lt;/p>
&lt;ol>
&lt;li>èŽ·å– matplotlibrc&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># æŸ¥çœ‹matplotlibå®‰è£…åœ¨å“ªé‡Œï¼š&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python -c &lt;span style="color:#5af78e">&amp;#34;import matplotlib;print(matplotlib.__file__)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># your-venv/lib/python3.x/site-packages/matplotlib/mpl-data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># å¤åˆ¶ matplotlibrc åˆ°ç”¨æˆ·é…ç½®ç›®å½•&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mv your-venv/lib/python3.x/site-packages/matplotlib/mpl-data/matplotlibrc ~/.config/matplotlib/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>ä¿®æ”¹ matplotlibrc&lt;/li>
&lt;/ol>
&lt;p>ç¼–è¾‘å™¨ä¸­æ‰“å¼€ matplotlibrc æ–‡ä»¶ï¼Œæ‰¾åˆ°å¹¶ä¿®æ”¹ä»¥ä¸‹ä¸¤ä¸ªåœ°æ–¹ï¼š&lt;/p>
&lt;p>&amp;lsquo;#font.family: sans-serif&amp;rsquo; &amp;raquo; font.family: sans-serif
&amp;lsquo;#font.sans-serif: DejaVu Sans,&amp;hellip;&amp;rsquo; &amp;raquo; font.sans-serif: DejaVu Sans,&amp;hellip;&lt;/p>
&lt;p>ç„¶åŽæ·»åŠ ä¸­æ–‡å­—ä½“ï¼ˆttf æ–‡ä»¶çš„æ–‡ä»¶åï¼Œå¦‚ä¸‹æ–‡çš„ &lt;code>simfang&lt;/code> ä»¿å®‹å­—ä½“ï¼‰
font.sans-serif: &lt;code>simfang&lt;/code>, DejaVu Sans,&amp;hellip;&lt;/p>
&lt;p>æœ€åŽå°† &lt;code>simfang.ttf&lt;/code> å­—ä½“æ–‡ä»¶åŠ å…¥åˆ° &lt;code>mpl-data/fonts/ttf/&lt;/code> ç›®å½•ä¸­ã€‚&lt;/p>
&lt;p>æ³¨ï¼šæ›´å®¹æ˜“ç»´æŠ¤çš„ä¸€ç§é…ç½®æ–¹å¼åº”è¯¥æ˜¯è¿™æ ·ï¼š
ä½¿ç”¨ &lt;code>ln -s&lt;/code> å‘½ä»¤å°†ä¸ªäººå­—ä½“åº“ (&lt;code>.local/share/fonts/&lt;/code>) ä¸­çš„æŸä¸ªå­—ä½“æ˜ å°„åˆ°ä¸Šè¿°ç›®å½•&lt;/p>
&lt;p>æ³¨2ï¼šå¯èƒ½éœ€è¦åˆ é™¤matplotlibçš„ç¼“å­˜&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>rm -rf /home/user/.cache/matplotlib
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>&lt;code>mpl.rcParams&lt;/code> ä»£ç å—&lt;/li>
&lt;/ol>
&lt;p>åœ¨ä½œå›¾çš„æ¨¡å—ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> matplotlib &lt;span style="color:#ff6ac1">as&lt;/span> mpl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> matplotlib.pyplot &lt;span style="color:#ff6ac1">as&lt;/span> plt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ä¸­æ–‡å‡å·æ˜¾ç¤ºé—®é¢˜&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mpl&lt;span style="color:#ff6ac1">.&lt;/span>rcParams[&lt;span style="color:#5af78e">&amp;#39;axes.unicode_minus&amp;#39;&lt;/span>] &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ä¸­æ–‡å­—ç¬¦æ˜¾ç¤ºé—®é¢˜&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># åå­—ä¸Žttfæ–‡ä»¶çš„åå­—ä¸åŒ: simfang.ttf &amp;gt;&amp;gt; &amp;#39;FangSong&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mpl&lt;span style="color:#ff6ac1">.&lt;/span>rcParams[&lt;span style="color:#5af78e">&amp;#39;font.sans-serif&amp;#39;&lt;/span>] &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#39;FangSong&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>æœ€åŽï¼Œå¦‚æžœæœ‰å¤šä¸ª venv çŽ¯å¢ƒï¼Œéš¾é“è¦ä¸€ä¸ªä¸€ä¸ªåŽ»ä¿®æ”¹å—ï¼Ÿ
&lt;code>pipx&lt;/code> å¤§æ³•æ˜¯è§£è¯ã€‚&lt;/p></description></item><item><title>inspect_substrings_in_file_using_sed_awk_jq</title><link>/posts/cs101/awk_sed_cat_head_etc/awk_use_cases/</link><pubDate>Wed, 17 Jul 2024 19:12:45 +0800</pubDate><guid>/posts/cs101/awk_sed_cat_head_etc/awk_use_cases/</guid><description>&lt;p>åœ¨å‘½ä»¤è¡Œå·¥ä½œè¿œéžå®Œç¾Žï¼Œä½†å…·æœ‰æžé«˜çš„è‡ªç”±åº¦ã€‚æ¯”å¦‚ï¼Œä½ æƒ³è¦æŸ¥çœ‹æŸä¸ªæ–‡ä»¶å†…(è¿™é‡Œç»å¯¹ä¸æ˜¯æŒ‡
doc/docx è¿™ç±»æ€ªèƒŽï¼Œè€Œæ˜¯ text/csv/tsv/json ç­‰ç­‰æ­£å¸¸çš„æ–‡ä»¶) çš„æŸä¸€è¡Œçš„å†…å®¹ï¼Œå¦‚æžœé‚£ä¸€è¡Œæœ‰å¾ˆ
å¤šåˆ—ï¼Œè€Œå…¶ä¸­è¦æ˜¯æŸä¸€åˆ—çš„å†…å®¹ç‰¹åˆ«é•¿ï¼ˆå°±æ˜¯&lt;code>sed&lt;/code>å‡ºæ¥å æ®äº†æ•´ä¸ªå±å¹•è¿™ç§é•¿åº¦ï¼‰ï¼Œé‚£ä¹ˆï¼Œä½ æƒ³è¦å¯¹
è¿™ä¸ªæ–‡ä»¶çš„å†…å®¹æœ‰æ‰€äº†è§£ï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„è½¯ä»¶æ¥æ‰“å¼€æ­¤æ–‡ä»¶ï¼Œå†å¥½å¥½æ¬£èµä¹‹ã€‚æˆ–è€…ï¼Œå…¶å®žåœ¨å‘½ä»¤è¡Œ
å°±å¯ä»¥å‘¢ï¼Ÿ&amp;ndash; &lt;code>awk&lt;/code> äº†è§£ä¸€ä¸‹ã€‚&lt;/p>
&lt;p>&lt;img alt="is_value_missing" src="https://fgg100y.github.io/posts/cs101/awk_sed_cat_head_etc/awk_use_cases/images/awk-to-the-rescue.png">&lt;/p>
&lt;p>åƒæˆ‘å°±æ˜¯ç¢°ä¸Šè¿™ä¹ˆä¸ªæƒ…å†µï¼Œå°†å›¾ç‰‡è½¬æ¢æˆ base64 ç¼–ç çš„ byte-data åŽï¼Œåœ¨ Jupyterlab æ‰“å¼€å‘çŽ°
æœ‰äº›è¡Œæ€Žä¹ˆæ˜¯ç©ºè¡Œï¼Ÿï¼ˆé¦–å…ˆæˆ‘ä¸æ˜¯åŽ»è´¨ç–‘ä»£ç ï¼Œè€Œæ˜¯è´¨ç–‘â€œçœ¼è§ä¸ºå®žâ€ï¼‰&lt;/p>
&lt;p>æ‰€ä»¥ï¼Œå¿…é¡»è¦æŸ¥çœ‹æŸä¸€è¡Œçš„å†…å®¹ï¼Œå¹¶ä¸”è¦æˆªæ–­è¿‡é•¿çš„å­—ä¸²ï¼Œè€ä¼™è®¡ GPT4o å¾ˆå¿«ç»™å‡ºç­”æ¡ˆï¼š&lt;/p>
&lt;h2 id="tsv" >
&lt;div>
&lt;a href="#tsv">
#
&lt;/a>
TSV
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and truncate the 3rd column to 50 characters&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#5af78e">&amp;#39;NR==10 { $3=substr($3, 1, 50) &amp;#34;...&amp;#34;; print }&amp;#39;&lt;/span> &lt;span style="color:#ff5c57">FS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> &lt;span style="color:#ff5c57">OFS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> file.tsv
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>è¿™è¿˜æ²¡å®Œï¼Œå› ä¸º base64 ç¼–ç çš„ç¼˜æ•…ï¼ˆä»¥åŠè¾“å…¥å›¾ç‰‡æ¯”è¾ƒç›¸ä¼¼ï¼‰å¯¼è‡´ &lt;code>awk&lt;/code> åˆ°çš„ç»“æžœéƒ½æ˜¯ä¸€æ¨¡ä¸€æ ·
çš„èµ·å§‹å­—ä¸²ï¼Œè‡ªç„¶è€Œç„¶å°±æ€€ç–‘è¿™æ˜¯å·§åˆè¿˜æ˜¯é”™è¯¯ï¼Œæ‰€ä»¥ï¼Œå¿…é¡»æŸ¥çœ‹å­—ä¸²å°¾éƒ¨å†…å®¹æ˜¯å¦ä¹Ÿä¸€æ ·, è¿™å½“ç„¶
éš¾ä¸å€’ä¸€ç‚¹è„¾æ°”ä¹Ÿä¸ä¼šæœ‰çš„ GPT4o è€å“¥ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># print the last 50 characters of the string in the 3rd column of the 10th row&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>awk &lt;span style="color:#5af78e">&amp;#39;NR==10 { len=length($3); print substr($3, len-50, 50) }&amp;#39;&lt;/span> &lt;span style="color:#ff5c57">FS&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;\t&amp;#34;&lt;/span> file.tsv
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="jsonl" >
&lt;div>
&lt;a href="#jsonl">
#
&lt;/a>
JSONL
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and the field names of json object&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sed -n &lt;span style="color:#5af78e">&amp;#39;10p&amp;#39;&lt;/span> file.jsonl | jq &lt;span style="color:#5af78e">&amp;#39;keys&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># To print the 10th row and the first 10 chars of strings or 10 elements of array&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sed -n &lt;span style="color:#5af78e">&amp;#39;10p&amp;#39;&lt;/span> file.jsonl | jq &lt;span style="color:#5af78e">&amp;#39;.field_name[:10]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;p>è¯ç—¨å¤šä¸¤å¥ï¼š&lt;/p>
&lt;p>è‡³äºŽå‚æ•°ä»£è¡¨å•¥æ„æ€ï¼Œæˆ‘é€šå¸¸ä¸è®© GPT4o å¤šè´¹å”‡èˆŒã€‚&lt;/p>
&lt;p>ï¼ˆæ¯•ç«Ÿå°±ç®—è§£é‡Šäº†æˆ‘è¿˜æ˜¯ä¼šåŠ¨æ‰‹éªŒè¯ä¸€ç•ªï¼Œå†è€…ï¼Œæœ€å…³é”®æ˜¯æˆ‘è¿˜æ²¡å¼€é€š plus ä¼šå‘˜ï¼Œæ‰€ä»¥å¾—å¬æŸ¯æ™¯è…¾
ä»–è€å¦ˆçš„é‚£å¥åŠï¼šè¦çœç€ç‚¹ç”¨å“¦ :ï¼‰&lt;/p></description></item><item><title>wsl2_and_unicode</title><link>/posts/cs101/wsl2_and_unicode/</link><pubDate>Tue, 16 Jul 2024 21:27:30 +0800</pubDate><guid>/posts/cs101/wsl2_and_unicode/</guid><description>&lt;h2 id="å…³äºŽ-localeunicodejsondump-ä»¥åŠè¢«å†¤æž‰çš„-wsl2" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e-localeunicodejsondump-%e4%bb%a5%e5%8f%8a%e8%a2%ab%e5%86%a4%e6%9e%89%e7%9a%84-wsl2">
#
&lt;/a>
å…³äºŽ localeï¼Œunicodeï¼Œjson.dump() ä»¥åŠè¢«å†¤æž‰çš„ WSL2
&lt;/div>
&lt;/h2>
&lt;p>WSL2çš„ &lt;code>locale&lt;/code> ä¸€ç›´æ˜¯é»˜è®¤è®¾ç½®å€¼ &amp;ndash; &amp;ldquo;C.UTF-8&amp;rdquo;ã€‚ç„¶åŽä»Šå¤©ç”¨ python è§£æžè·¯å¾„æ–‡ä»¶ä¸­æ–‡åæ—¶ï¼Œ
æœ€ç»ˆä¿å­˜ä¸‹æ¥çš„ç«Ÿæ˜¯ unicode (&amp;quot;\u5c0f\u6728\u4ed4-\u7761\u7740\u4e862&amp;quot;)ï¼Œè¿™å€’æ˜¯ç ´å¤©è’ç¬¬ä¸€
æ¬¡ç¢°ä¸Š :&amp;rsquo;) ä¸ºäº†çœäº‹ï¼Œå¿…é¡»åŽ»æ‰¾ GPT é˜ä¸‹ã€‚&lt;/p>
&lt;p>æ›´æ–°ï¼šä¸‹æ–‡æ˜¯å®Œå…¨ç»•äº†è¿œè·¯äº†çš„ã€‚é€ æˆä¸Šè¿°æƒ…å†µçš„å®žé™…åŽŸå› åº”è¯¥æ˜¯ï¼š&lt;code>json.dump()&lt;/code> é»˜è®¤å‚æ•°ä¸­çš„
&lt;code>ensure_ascii=True&lt;/code>ï¼Œè¿™å°±å†³å®šäº†å†™å…¥æ–‡ä»¶çš„å­—ç¬¦ä¸ä¼šè¶…å‡ºASCIIå­—ç¬¦é›†ã€‚&lt;/p>
&lt;p>æ‰€ä»¥ï¼Œæ­£ç¡®çš„ä¿®æ”¹æ–¹å¼ä¸æ˜¯å¤§è´¹å‘¨ç« çš„æ”¹å†™ç³»ç»Ÿ &lt;code>locale&lt;/code> è€Œæ˜¯å°†å‚æ•°æ”¹ä¸º &lt;code>ensure_ascii=False&lt;/code>ã€‚&lt;/p>
&lt;p>å®žé™…ä¸Šï¼Œjson.dump() è¿™éƒ¨åˆ†çš„å‡½æ•°ä¹Ÿæ˜¯ GPT é˜ä¸‹å†™çš„ï¼Œå®ƒç†æ‰€å½“ç„¶åœ°è®¤ä¸ºæ˜¯åœ¨è‹±æ–‡è¯­å¢ƒä¸­ï¼Œä½¿ç”¨
é»˜è®¤çš„å‚æ•°ä¸ä¸ºè¿‡å§ã€‚ðŸ˜‚&lt;/p>
&lt;p>ä½† GPT4o æœ€å…ˆè¦æˆ‘ç¡®è®¤æ˜¯å¦æ˜¯ç»ˆç«¯çš„ç¼–ç é—®é¢˜ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># è¿™å°±æ˜¯åŽŸåŽŸæœ¬æœ¬ copy å¾—åˆ°çš„unicodeå­—ç¬¦ä¸²&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">echo&lt;/span> -e &lt;span style="color:#5af78e">&amp;#34;\u5c0f\u6728\u4ed4-\u7761\u7740\u4e862&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># output: å°æœ¨ä»”-ç¡ç€äº†2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>å…¶æ¬¡ï¼Œä½¿ç”¨ &lt;code>file -bi&lt;/code> å‘½ä»¤æŸ¥çœ‹æ–‡ä»¶çš„ç¼–ç æ ¼å¼ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>file -bi path/to/the/suspicious_file.jsonl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#output: application/json; charset=us-ascii&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>è¿™å°±æ„å‘³ç€ç¨‹åºåœ¨å†™å…¥æœ¬åœ°æ–‡ä»¶æ—¶ï¼Œåªä½¿ç”¨äº† ascii å­—ç¬¦ã€‚&lt;/p>
&lt;h2 id="å…³äºŽ-escape-sequences-åŠå…¶è§£æž" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e-escape-sequences-%e5%8f%8a%e5%85%b6%e8%a7%a3%e6%9e%90">
#
&lt;/a>
å…³äºŽ escape sequences åŠå…¶è§£æž
&lt;/div>
&lt;/h2>
&lt;p>ç„¶åŽ GPT4o å¾ˆè¯¦ç»†çš„å‘ŠçŸ¥åº”è¯¥å¦‚ä½•åœ¨å‘½ä»¤è¡Œç»ˆç«¯ç”Ÿæˆå’Œè®¾ç½® &amp;ldquo;zh_CN.UTF-8&amp;rdquo;ï¼Œä»¥åŠå¦‚ä½•åœ¨ python ä¸­è¿›è¡Œ
è½¬ä¹‰ç³»åˆ—çš„é‡ç¼–ç å’Œè§£ç æ¥æ˜¾ç¤ºä¸­æ–‡å­—ä¸²ï¼Œä½†å°è¯•ä¹‹åŽæ²¡æˆåŠŸã€‚åŽŸæ¥æ˜¯æˆ‘ç†è§£æœ‰è¯¯ã€‚&lt;/p>
&lt;p>GPT4o:
It appears there may be a misunderstanding regarding the purpose and usage of the
unicode-escape encoding. The unicode-escape codec is used to interpret escape sequences
in strings, but it does not process normal Unicode escape sequences (\uXXXX) that are
already present in a Unicode string.&lt;/p>
&lt;p>If you have a string that looks like a Unicode escape sequence and you need to convert
it into actual Unicode characters, you can use the unicode-escape codec. However, your
string needs to be encoded in a way that the codec understands.&lt;/p>
&lt;p>Here&amp;rsquo;s the corrected approach to convert a string with Unicode escape sequences into
actual Chinese characters:&lt;/p>
&lt;pre>&lt;code>Given String: You have a string with Unicode escape sequences.
Decode: Use unicode-escape to decode the string.
&lt;/code>&lt;/pre>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Given string with Unicode escape sequences&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>unicode_str &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u5c0f&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u6728&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u4ed4-&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u7761&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u7740&lt;/span>&lt;span style="color:#5af78e">\\&lt;/span>&lt;span style="color:#5af78e">u4e862&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Decode the Unicode escape sequences&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>decoded_str &lt;span style="color:#ff6ac1">=&lt;/span> unicode_str&lt;span style="color:#ff6ac1">.&lt;/span>encode(&lt;span style="color:#5af78e">&amp;#39;utf-8&amp;#39;&lt;/span>)&lt;span style="color:#ff6ac1">.&lt;/span>decode(&lt;span style="color:#5af78e">&amp;#39;unicode-escape&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(decoded_str)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Explanation&lt;/p>
&lt;pre>&lt;code>The string unicode_str contains Unicode escape sequences.
The encode('utf-8') part ensures the string is in a byte format.
The decode('unicode-escape') part interprets the byte format as Unicode escape sequences and converts them into actual characters.
&lt;/code>&lt;/pre>
&lt;p>This should correctly output the Chinese characters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>å°æœ¨ä»”-ç¡ç€äº†2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note&lt;/p>
&lt;p>Ensure that your string uses double backslashes (\uXXXX) if you&amp;rsquo;re defining it directly
in the script. This is necessary because a single backslash (\uXXXX) would be
interpreted as an actual Unicode character during the string definition phase, not as an
escape sequence.&lt;/p>
&lt;hr>
&lt;p>ä¸€ç‚¹ç‰¢éªšï¼šåˆå­¦äº†ä¸€äº›æ²¡ç”¨çš„çŸ¥è¯†ï¼Œæ¯•ç«Ÿï¼šé¦–å…ˆä½ ä¸ºä»€ä¹ˆè¦ç”¨WSLè¿™ç ´çƒ‚çŽ©æ„å„¿å‘¢ï¼Ÿï¼Ÿï¼Ÿ&lt;/p>
&lt;p>&lt;del>å› ä¸ºè¿™æœ¬æ¥æ˜¯ä¹°ç»™éžç»ˆç«¯çˆ±å¥½è€…äººå£«ä½¿ç”¨çš„ç”µè„‘&lt;/del>&lt;/p>
&lt;p>&lt;del>å¯äººå®¶æ¯•ç«Ÿä¹Ÿæ²¡æœ‰ç”¨ï¼Œè¿˜æ˜¯ä½ è‡ªä¸ªéœ¸å ç€ã€‚
å‰ä¸ä¹…å‘çŽ°å·¨ç¡¬åˆåœ¨ç³»ç»Ÿæ˜Žç«æ‰§ä»—åœ°æžå·çª¥æ´»åŠ¨ï¼ˆè®¾ç½®&amp;gt;éšç§å’Œå®‰å…¨æ€§&amp;gt;æ´»åŠ¨åŽ†å²è®°å½•ï¼‰å°±ä¿¡èª“æ—¦æ—¦è¯´
ç«‹åˆ»é©¬ä¸Šåˆ»ä¸å®¹ç¼“åºŸæŽ‰è¿™ä¸ªæ¶å¿ƒçŽ©æ„&lt;/del>&lt;/p>
&lt;p>&lt;del>æœ€è¿‘æ¯”è¾ƒå¿™å“”å“©ï¼Œæ–°å®‰è£…çš„2Tç›˜ä¸å¥½æ¸…ç†å§å•¦ï¼Œæ€»è§‰ç€äººå®¶æœ€ç»ˆè¦ç”¨å’•å™œ&lt;/del>&lt;/p>
&lt;p>æœ¬è´¨ä¸Šæ˜¯å› ä¸ºæç€é¼»å­è¿˜èƒ½å‹‰å¼ºç”¨ç”¨ï¼Œä½†ä¼°è®¡ä¹Ÿæ’‘ä¸äº†å¤šä¹…äº†ã€‚&lt;/p>
&lt;p>æ¯•ç«Ÿ Linux æ‰æ˜¯&lt;ruby>æ ¹&lt;rt>root&lt;/rt>&lt;/ruby>ï¼Œæ‰æœ‰&lt;ruby>å®¶&lt;rt>home/&lt;/rt>&lt;/ruby>çš„æ„Ÿè§‰ :)&lt;/p>
&lt;p>æ›´æ–°ï¼šè™½ç„¶å¯¼è‡´æœ¬æ–‡è¯žç”Ÿçš„åŽŸå› å‘ç”Ÿäº†æ ¹æœ¬æ€§æ”¹å˜ï¼ˆè¯¦è§æ–‡ç« å¼€å¤´çš„â€œæ›´æ–°â€å†…å®¹ï¼‰ä½†ä¸å½±å“â€œä¸€ç‚¹ç‰¢
éªšâ€ä¹‹åŽå’Œæœ¬æ›´æ–°ä¹‹å‰çš„ç‰¢éªšå†…å®¹ã€‚&lt;/p></description></item><item><title>poor_dad</title><link>/posts/personals/momentsoflife/poor_dad/</link><pubDate>Mon, 15 Jul 2024 21:47:50 +0800</pubDate><guid>/posts/personals/momentsoflife/poor_dad/</guid><description>&lt;p>åœ°é“å£å‡ºå£å¤„ã€‚ç«‹ç€çš„è¡ŒæŽç®±ä¸Šå„¿å­åŠè·ªç€å€¾å‘çˆ¸çˆ¸ï¼Œçˆ¸çˆ¸ä¸€åªæ‰‹æŠ“ç€è¡ŒæŽç®±æ†å­ï¼Œå¦ä¸€åªæ‰‹æ‹¿ç€æ‰‹æœºã€‚
å¥³å„¿åœ¨æ—è¾¹ä¹Ÿæƒ³å¾€è¡ŒæŽç®±ä¸ŠæŒ¤ï¼Œè¢«çˆ¸çˆ¸åˆ¶æ­¢ã€‚å¥³å„¿å¼€å§‹æœ‰å“­è…”ï¼Œçˆ¸çˆ¸æ²¡çœ‹å¥¹ï¼Œæ³¨æ„åŠ›å…¨åœ¨æ‰‹æœºä¸Šï¼Œ
å¤§çº¦æ˜¯åœ¨æ‰“è½¦ã€çœ‹åœ°å›¾ã€è”ç³»äººæˆ–è€…çŽ©æ¸¸æˆï¼Œä¸çŸ¥é“ã€‚å¥³å„¿ä¸‰ç•ªä¸¤æ¬¡ï¼Œè¿˜æ˜¯ä¸è¡Œï¼Œè¢«çˆ¸çˆ¸å¤§å£°å‘µæ–¥å±é™©ã€‚
åŽé¢å¥¹å¹²è„†å¾€åœ°ä¸Šå°±æ˜¯ä¸€åï¼Œèµ–å“­ç€ä¹±è¸¢åŒè…¿ã€‚èµ–å“­å£°é‡Œä¼ è¾¾çš„å¤§çº¦æ˜¯å§”å±ˆçš„ä¿¡å·ï¼ˆå‰ä¸ä¹…æˆ‘åˆšé¢†æ•™
è¿‡æˆ‘å¤§èˆ…å“¥ä»–é‚£å·®ä¸å¤šå¹´çºªå¤§çš„å¹¼ç¨šå›­å¥³å„¿çš„èµ–åœ°å¼å“­é—¹ï¼Œæ‰€ä»¥å¯¹è¿™ä¸ªä¿¡å·é¢‡ä¸ºç†Ÿæ‚‰ï¼‰ã€‚&lt;/p>
&lt;p>ç­‰æˆ‘èµ°ä¸‹åœ°é“å£ï¼Œå§”å±ˆä¿¡å·å·²ç»å˜æˆå¤šä¸ªå‡åŒ€é—´éš”çš„æ±‚å…³æ³¨ä¿¡å·äº†ã€‚&lt;/p>
&lt;hr>
&lt;p>ä¸€ç‚¹ç‰¢éªšï¼š
åŽæ¥æˆ‘å°±è¿›é—¸å£äº†ã€‚å°±ç®—æœ‰ï¼Œæˆ‘ä¹Ÿä¸å¯èƒ½è¾¨åˆ«å‡ºå›žåº”ä¿¡å·äº†ï¼›å°±ç®—æœ‰ï¼Œæˆ–è®¸å›žåº”çš„ä¹Ÿå¤ªè¿Ÿäº†ç‚¹ã€‚&lt;/p>
&lt;p>è¿™å¯èƒ½æ˜¯å°äº‹ä¸€æ¡©ï¼Œä¹Ÿå¯èƒ½æ˜¯å¤§äº‹ä¸€ä»¶ã€‚å› ä¸ºæˆ‘æ€»å¿ä¸ä½æƒ³èµ·å‰ä¸ä¹…çœ‹çš„é‚£éƒ¨å¯¼æ¼”æœ¬æ¥æƒ³æ‹è€çˆ¸ç»“æžœ
æ‹äº†ä»–å®¶é¾™å‡¤åŒèƒžèƒŽçš„çºªå½•ç‰‡ã€Šå§å§ã€‹(1996å¹´)ã€‚&lt;/p></description></item><item><title>python_module</title><link>/posts/cs101/python_module/</link><pubDate>Wed, 10 Jul 2024 16:19:58 +0800</pubDate><guid>/posts/cs101/python_module/</guid><description>&lt;h2 id="layout-of-python-installations" >
&lt;div>
&lt;a href="#layout-of-python-installations">
#
&lt;/a>
Layout of Python installations
&lt;/div>
&lt;/h2>
&lt;p>All paths in the table are relative to the installation root:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Files&lt;/th>
&lt;th>Windows&lt;/th>
&lt;th>Linux and macOS&lt;/th>
&lt;th>Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Interpreter&lt;/td>
&lt;td>python.exe&lt;/td>
&lt;td>bin/python3.x&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Standard library&lt;/td>
&lt;td>Lib and DLLs&lt;/td>
&lt;td>lib/python3.x&lt;/td>
&lt;td>Extension modules are located under DLLs on Windows. Fedora places the standard library under lib64 instead of lib.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Third-party packages&lt;/td>
&lt;td>Lib\site-packages&lt;/td>
&lt;td>lib/python3.x/site-packages&lt;/td>
&lt;td>Debian and Ubuntu put packages in dist-packages. Fedora places extension modules under lib64 instead of lib.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Entry-point scriptsa&lt;/td>
&lt;td>Scripts&lt;/td>
&lt;td>bin&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="the-interpreter" >
&lt;div>
&lt;a href="#the-interpreter">
#
&lt;/a>
The interpreter
&lt;/div>
&lt;/h2>
&lt;p>The Python interpreter ties the environment to three things:&lt;/p>
&lt;ul>
&lt;li>A specific version of the Python language (e.g., 2.x.y, 3.x.y)&lt;/li>
&lt;li>A specific implementation of Python (e.g., CPython, PyPy)&lt;/li>
&lt;li>A specific build of the interpreter (e.g., 32bit, 64bit, Intel or Apple)&lt;/li>
&lt;/ul>
&lt;p>Try this command (in Bash) to print metadata complied into the interpreter:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>python3 -m sysconfig
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="the-modules" >
&lt;div>
&lt;a href="#the-modules">
#
&lt;/a>
The modules
&lt;/div>
&lt;/h2>
&lt;p>Modules are containers of Python objects that you load via the &lt;code>import&lt;/code> statement.
Modules come in various forms and shapes:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Simple modules
In the simplest case, a &lt;code>module&lt;/code> is a single file containing Python source code. The
statement &lt;code>import string&lt;/code> executes the code in &lt;em>string.py&lt;/em> and binds the result to
the name &lt;code>string&lt;/code> in the local scope.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Packages
Directories with &lt;em>&lt;strong>init&lt;/strong>.py&lt;/em> files are known as &lt;em>packages&lt;/em> which allow you to
organize modules in a hierarchy. The statement &lt;code>import email.message&lt;/code> loads the
&lt;code>message&lt;/code> module from the &lt;code>email&lt;/code> package.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Namespace packages
Directories with modules but no &lt;em>&lt;strong>init&lt;/strong>.py&lt;/em> are known as &lt;em>namespace packages&lt;/em>. One
may use them to organize modules in a common namespace such as a company name (say
&lt;code>dsta.voicedsp&lt;/code> and &lt;code>dsta.vedioprepr&lt;/code>). Unlike with regular packages, one can
distribute each module in a namespace package separately.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Extension modules
Extension modules such as &lt;code>math&lt;/code> module, contain native code compiled from low-level
language like C. They are shared libraries with a special entry-point&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> that lets
you import them as modules from Python. People write them for performance reasons or
to make existing C libraries available as Python modules. Their names end in &lt;code>.pyd&lt;/code>
on Windows, &lt;code>.dylib&lt;/code> on macOS, and &lt;code>.so&lt;/code> on Linux.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Built-in modules
Some modules from the standard library, such as &lt;code>sys&lt;/code> and &lt;code>builtins&lt;/code> modules, are
compiled into the interpreter. The variable &lt;code>sys.builtin_module_names&lt;/code> lists all of
these modules.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Frozen modules
Some modules from the standard library are written in Python but have their
bytecode&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> embedding in the interpreter. Recent versions of Python freeze every
module that&amp;rsquo;s imported during interpreter startup, such as &lt;code>os&lt;/code> and &lt;code>io&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="python-virtual-environment" >
&lt;div>
&lt;a href="#python-virtual-environment">
#
&lt;/a>
Python virtual environment
&lt;/div>
&lt;/h2>
&lt;p>Python environments consist of an interpreter and modules. Virtual environments share
the interpreter and the standard library with their parent environment.&lt;/p>
&lt;p>&lt;img alt="py-venv" src="https://fgg100y.github.io/posts/cs101/python_module/images/py_python_and_venv.png">&lt;/p>
&lt;p>A Python environment can contain only a single version of each third-party package &amp;ndash;
if two projects require different versions of the same package, they canâ€™t be
installed side by side. Thatâ€™s why itâ€™s considered good practice to install every Python
application, and every project you work on, in a dedicated virtual environment.&lt;/p>
&lt;p>NOTE
&lt;span class="custom-en">
The term &lt;em>package&lt;/em> carries some ambiguity in the Python world. It refers both to modules
and to the artifacts used for distributing modules (aka &lt;em>distributions&lt;/em>).
&lt;/span>&lt;/p>
&lt;h2 id="the-module-path" >
&lt;div>
&lt;a href="#the-module-path">
#
&lt;/a>
The module path
&lt;/div>
&lt;/h2>
&lt;p>It&amp;rsquo;s helpful to look at entries of &lt;code>sys.path&lt;/code> to debugg the import error, naturally,
one may wonder where do the entries on &lt;code>sys.path&lt;/code> come from at first?&lt;/p>
&lt;p>When the interpreter starts up, it constructs the module path in two steps:&lt;/p>
&lt;ol>
&lt;li>It builds an initial module path which includes the standard library,&lt;/li>
&lt;li>It imports the &lt;code>site&lt;/code> module (from standard library) which extends the module path to
include the site packages from the current environment.&lt;/li>
&lt;/ol>
&lt;p>The locations on the initial module path fall into three categories, and they occur in
this order:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The current directory or the directory containing the Python scripts (if any)
The first item on &lt;code>sys.path&lt;/code> can be any of the following:&lt;/p>
&lt;ul>
&lt;li>If you ran &lt;code>python3 &amp;lt;script&amp;gt;&lt;/code>, the directory where the script is located&lt;/li>
&lt;li>If you ran &lt;code>python3 -m &amp;lt;module&amp;gt;&lt;/code>, the current directory&lt;/li>
&lt;li>Otherwise, the empty string, which also denotes the current directory
&lt;strong>Safty issue&lt;/strong>: having the working directory on &lt;code>sys.path&lt;/code> is quite unsafe, as an
attacker (or you, mistakenly) can override the standard library by placing Python
files in the victim&amp;rsquo;s directory. To avoid this,&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>Python-3.11 provides &lt;code>-P&lt;/code> option or the PYTHONSAFEPATH variable to omit the
current directory from &lt;code>sys.path&lt;/code>;&lt;/li>
&lt;li>Using a virtual environment.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>The locations in the PYTHONPATH environment variable (if set)
Avoid this mechanism for the same reasons as the current working directory and use a
virtual environment instead.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The locations of the standard library
The location of the standard library is not hardcoded in the interpreter. Rather,
Python looks for landmark files on the path to its own executable and uses them to
locate the current environment (&lt;code>sys.prefix&lt;/code>) and the Python installation
(&lt;code>sys.base_prefix&lt;/code>). One such landmark file is &lt;em>pyvenv.cfg&lt;/em>, which marks a virtual
environment and points to its parent installation via the &lt;code>home&lt;/code> key. Another
landmark file is &lt;em>os.py&lt;/em>, the file containing the standard &lt;code>os&lt;/code> module: Python uses
&lt;em>os.py&lt;/em> to discover the prefix outside a virtual environment and to locate the
standard library itself.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>For more glory details and interesting contents, go to the newest version (2024) of
&lt;em>&amp;ldquo;Hypermodern Python Tooling&amp;rdquo;&lt;/em> by Claudio Jolowicz.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>An entry-point script is an executable file in &lt;em>Scripts/&lt;/em> (Windows) or &lt;em>bin/&lt;/em>
(Linux and macOS) with a single purpose: it launches a Python application by
importing the module with its entry-point function and calling that function.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Bytecode is an intermediate representation of Python code that is
platform-independent and optimized for fast execution. The interpreter compiles pure
Python modules to bytecode when it loads them for the first time. Their names end in
&lt;code>.pyc&lt;/code> and they are cached in &lt;code>__pycache__&lt;/code> directories.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Add_and_test_custom_font</title><link>/posts/hugo101/2024-07-09-test_custom_font/</link><pubDate>Tue, 09 Jul 2024 21:53:56 +0800</pubDate><guid>/posts/hugo101/2024-07-09-test_custom_font/</guid><description>&lt;p>Linear regression:&lt;/p>
&lt;blockquote>
&lt;p>$\text{You&amp;rsquo;ve got to draw the line somewhere.}$&lt;/p>
&lt;/blockquote>
&lt;p>You&amp;rsquo;ve got to draw the line somewhere. (system-font)&lt;/p>
&lt;p>&lt;span class="custom-en">You&amp;rsquo;ve got to draw the line somewhere. (NerdFont 3270)&lt;/span>&lt;/p>
&lt;p>&lt;span class="custom-zh">ä¸­æ–‡ï¼šä½ å¿…é¡»åœ¨æŸä¸ªåœ°æ–¹æŠŠé‚£æ¡çº¿ç»™ç”»å‡ºæ¥ã€‚(äº¬è¯è€å®‹ä½“)&lt;/span>&lt;/p>
&lt;hr>
&lt;p>å®žä½œè¿‡ç¨‹ï¼š&lt;/p>
&lt;p>&lt;span class="custom-en">hugo version:
v0.128.2-de36c1a95d28595d8243fd8b891665b069ed0850+extended linux/amd64
BuildDate=2024-07-04T08:13:25Z VendorInfo=gohugoio&lt;/span>&lt;/p>
&lt;hr>
&lt;ol>
&lt;li>
&lt;p>å°†æœ¬åœ°å­—ä½“æ–‡ä»¶å¤åˆ¶åˆ° &lt;code>static/fonts/&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åœ¨ &lt;code>assets/css/&lt;/code> (æˆ– &lt;code>static/css/&lt;/code>) åˆ›å»ºæ–‡ä»¶ &lt;code>custom.css&lt;/code>:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-css" data-lang="css">&lt;span style="display:flex;">&lt;span>@&lt;span style="color:#ff6ac1">font-face&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#5af78e">&amp;#39;3270NerdFont&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">src&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">url&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;/fonts/3270NerdFont-Regular.ttf&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#ff6ac1">format&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;truetype&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-weight&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-style&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>@&lt;span style="color:#ff6ac1">font-face&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#5af78e">&amp;#39;KingHwa_OldSong&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">src&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">url&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;/fonts/KingHwa_OldSong.ttf&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#ff6ac1">format&lt;/span>&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;truetype&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-weight&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-style&lt;/span>&lt;span style="color:#ff6ac1">:&lt;/span> &lt;span style="color:#ff6ac1">normal&lt;/span>&lt;span style="color:#ff6ac1">;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.&lt;span style="color:#f3f99d">custom-en&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;3270NerdFont&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>.&lt;span style="color:#f3f99d">custom-zh&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">font-family&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;KingHwa_OldSong&amp;#39;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>æ›´å¤šå…³äºŽè‡ªå®šä¹‰å­—ä½“çš„è®¨è®ºï¼Œå¯å‚è€ƒè¿™é‡Œ: &lt;a href="https://discourse.gohugo.io/t/how-to-add-custom-fonts-and-not-depend-on-google/34267/3">https://discourse.gohugo.io/t/how-to-add-custom-fonts-and-not-depend-on-google/34267/3&lt;/a>&lt;/p>
&lt;ol start="3">
&lt;li>ä¿®æ”¹ &lt;code>config.toml&lt;/code>ï¼š&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[params]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> customCSS = [&lt;span style="color:#5af78e">&amp;#34;css/custom.css&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="4">
&lt;li>å¤åˆ¶ &lt;code>themes/layouts/partials/head.html&lt;/code> åˆ°æ ¹ç›®å½•&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>cp themes/layouts/partials/head.html ./layouts/partials/head.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Optional:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cp themes/layouts/partials/head-extra.html ./layouts/partials/head-extra.html
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>å¦‚æžœä¸»é¢˜è®¾è®¡å¾—è¶³å¤Ÿå¥½ï¼Œé€šå¸¸ä¼šç»™ä¸€ä»½ &lt;code>head-extra.html&lt;/code> ç”¨äºŽå®šåˆ¶åŒ–å†…å®¹ã€‚
æ­¤æ—¶ï¼Œåªéœ€è¦å°†ç›¸åº”é…ç½®åŠ å…¥åˆ° &lt;code>head-extra.html&lt;/code> æ–‡ä»¶ä¸­ï¼Œé¿å…å¯¹ &lt;code>head.html&lt;/code> çš„æ‰“æ‰°ã€‚&lt;/p>
&lt;p>å¢žåŠ çš„ç›¸åº”å†…å®¹æ˜¯ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;lt;!-- Other head content --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{- range .Site.Params.customCSS }}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;&lt;span style="color:#ff6ac1">link&lt;/span> &lt;span style="color:#57c7ff">rel&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;stylesheet&amp;#34;&lt;/span> &lt;span style="color:#57c7ff">href&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;{{ . | relURL }}&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{- end }}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="5">
&lt;li>ä½¿ç”¨è‡ªå®šä¹‰å­—ä½“&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>You&amp;#39;ve got to draw the line somewhere. (system-font)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;span class=&amp;#34;custom-en&amp;#34;&amp;gt;You&amp;#39;ve got to draw the line somewhere. (NerdFont 3270)&amp;lt;/span&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;span class=&amp;#34;custom-zh&amp;#34;&amp;gt;ä¸­æ–‡ï¼šä½ å¿…é¡»åœ¨æŸä¸ªåœ°æ–¹æŠŠé‚£æ¡çº¿ç»™ç”»å‡ºæ¥ã€‚(äº¬è¯è€å®‹ä½“)&amp;lt;/span&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>rime_trime</title><link>/posts/personals/momentsoflife/rime_trime/</link><pubDate>Sat, 06 Jul 2024 15:34:59 +0800</pubDate><guid>/posts/personals/momentsoflife/rime_trime/</guid><description>&lt;p>&lt;a href="https://github.com/iDvel/rime-ice">rime-ice&lt;/a> é›¾å‡‡æ‹¼éŸ³ + &lt;a href="https://github.com/osfans/trime">trime&lt;/a> åŒæ–‡è¾“å…¥æ³•&lt;/p>
&lt;p>&lt;img alt="IMG_trime_a_grimpse" src="https://fgg100y.github.io/posts/personals/momentsoflife/rime_trime/images/trime_theme_4in1.png">&lt;/p>
&lt;p>æ€Žä¹ˆåœ¨æ‰‹æœºä¸Šä½¿ç”¨è¿™ä¸ªè¾“å…¥æ³•ï¼Ÿæ¯”è¾ƒæ— è„‘çœåŠ›é‚£ç§&amp;hellip;
ç­‰ç­‰å…ˆï¼Œé™¤å¼€ä»€ä¹ˆâ€œæ„¤ä¸–å«‰ä¿—â€è¿™ç§å† å†•å ‚çš‡ï¼Œæˆ–è€…â€œæ‹’ç»é›·åŒâ€è¿™ç§ç¬‘æŽ‰å¤§ç‰™ï¼Œä»¥åŠâ€œå¬è¯´å®ƒå¾ˆéš¾æžæˆ‘éžè¦æžå®šå®ƒâ€è¿™ç§åæ‰§ç‹‚çš„ç†ç”±å¤–ï¼Œä¸ºä»€ä¹ˆç”¨è¿™ä¸ªï¼Ÿ
ðŸ‘‰ å¼€æºä¸‡å²ã€å¼€æºä¸‡å²ã€å¼€æºä¸‡å²ã€‚&lt;/p>
&lt;p>åº”è¯¥è¯´æ˜Žï¼šè¿™ç¯‡æ–‡ç« åŸºæœ¬æ˜¯æˆ‘è‡ªå·±çš„å¤‡å¿˜å½•ï¼Œæˆ‘è‡ªå·±å·²ç»ä½¿ç”¨è¿™äº›é…ç½®å¤šå¹´ã€‚
å¦‚æžœä½ æ˜¯ç¬¬ä¸€æ¬¡æŽ¥è§¦ rime/Trimeï¼Œç›´æŽ¥æŸ¥çœ‹ä»–ä»¬å®˜ç½‘æˆ–è€…æœ¬æ–‡çš„å‚è€ƒé“¾æŽ¥ï¼Œå¦åˆ™å¦‚æžœæŒ‰ç€æˆ‘çš„è¯´æ³•è¿›è¡Œæ“ä½œï¼Œå¤§æ¦‚çŽ‡ä½ ä¼šæžå¾—è‡ªå·±ç«å†’ä¸‰ä¸ˆã€‚&lt;/p>
&lt;hr>
&lt;p>æˆ‘çš„æ“ä½œï¼š&lt;/p>
&lt;p>1.1 æ‰‹æœºä¸Šä¸‹è½½ &lt;a href="https://f-droid.org/">F-Droid&lt;/a>ï¼Œåœ¨ F-Droid æœç´¢å¹¶ä¸‹è½½ Trime è¾“å…¥æ³• (ä¸­æ–‡åï¼šåŒæ–‡è¾“å…¥æ³•&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>)&lt;/p>
&lt;p>1.2 æ‰‹æœºåº”ç”¨æŽˆæƒç­‰å…¶ä»–æ“ä½œ&lt;/p>
&lt;p>2.1 ç”µè„‘ç«¯ä¸‹è½½ä»“åº“&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">cd&lt;/span> rime_hub
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>git clone https://github.com/iDvel/rime-ice.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>2.2 å°†æ‰‹æœºæŽ¥å…¥ç”µè„‘ï¼Œå°†ä»“åº“å¤åˆ¶åˆ°æ‰‹æœºç›®å½•è·¯å¾„ï¼ˆé€šå¸¸æ˜¯ä¸ŽDownload,DocumentsåŒçº§çš„è·¯å¾„ï¼‰ï¼Œå°†
å…¶åç§°ä¿®æ”¹ä¸º &lt;code>rime&lt;/code> (rime-iceä¹Ÿå¯ä»¥ï¼Œä½†è¦åšå‡ºç›¸åº”çš„é€‚é…å’Œè®¾ç½®)&lt;/p>
&lt;p>3.1 æ‰‹æœºç«¯æ‰“å¼€ Trime è¾“å…¥æ³•ï¼Œç‚¹å‡»å³ä¸Šè§’éƒ¨ç½²&lt;/p>
&lt;p>3.2 å¯èƒ½ä¼šæŠ¥é”™å¼¹å‡ºæ—¥å¿—çª—å£ï¼Œä½†ä¸ä¸€å®šå½±å“ä½¿ç”¨ï¼›å¦åˆ™ï¼ŒåŽ»å®˜ç½‘ç­‰åœ°æ–¹å¯»æ‰¾è§£å†³æ–¹æ¡ˆ&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>4.1 å¦‚æžœä¹Ÿåœ¨ç”µè„‘ç«¯ä½¿ç”¨ï¼Œå¯ä»¥å€ŸåŠ© Plum è¿›è¡Œå®‰è£…å’Œæ›´æ–°ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>git clone --depth&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span> https://github.com/rime/plum
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">cd&lt;/span> plum
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#å…¨é‡æ›´æ–°ï¼š&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/full
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#å±€éƒ¨æ›´æ–°ï¼š&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bash rime-install iDvel/rime-ice:others/recipes/all_dicts
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>5 å¤šç«¯åŒæ­¥ï¼šç”µè„‘ç«¯æ›´æ–°ï¼Œå¤åˆ¶åˆ°æ‰‹æœºç­‰å…¶ä»–ç»ˆç«¯ã€‚&lt;/p>
&lt;hr>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>åŒæ–‡è¾“å…¥æ³•ï¼šæŠŠ RIME è£…è¿› Android æ‰‹æœº &lt;a href="https://sspai.com/post/77499">https://sspai.com/post/77499&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Rime é…ç½®ï¼šé›¾å‡‡æ‹¼éŸ³ &lt;a href="https://dvel.me/posts/rime-ice/">https://dvel.me/posts/rime-ice/&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>èŒç§°è¯„å®¡</title><link>/posts/personals/engineering_career_ladder/</link><pubDate>Fri, 05 Jul 2024 22:40:23 +0800</pubDate><guid>/posts/personals/engineering_career_ladder/</guid><description>&lt;p>è™½ç„¶äººå®¶åªè¯„ä¼°è¿‘äº”å¹´çš„å·¥ä½œï¼Œä½†ç²—ç•¥ç®—èµ·æ¥è‡ªå·±å·¥ä½œéƒ½å¿«å…«å¹´äº†ï¼Œæ‰çˆ¬äº†ç¬¬äºŒä¸ªé˜¶æ¢¯ã€‚&lt;/p>
&lt;p>åŠè·¯å‡ºå®¶ç€å®žè´¹åŠ²ã€‚&lt;/p>
&lt;p>&lt;img alt="MidLevel" src="https://fgg100y.github.io/posts/personals/engineering_career_ladder/images/fmh_engineering_career_blur_extra_info.png">&lt;/p>
&lt;p>çªç„¶æƒ³èµ·ç»å…¸ç”µå½±ã€Šä¸‰æ¯›ä»Žå†›è®°ã€‹ç»“å°¾éƒ¨åˆ†çš„é¥±å«æ²§æ¡‘è€ŒåˆèŽ«åå–œæ„Ÿçš„é‚£ä¸€å¥å°è¯ï¼šå…«å¹´äº†&amp;hellip;&lt;/p>
&lt;p>æ€¥èºæ—¶ä¸å…æš—æš—å¥¢æœ›æœ‰äººï¼šâ€œçœ‹åœ¨æŠ€æœ¯çš„ä»½ä¸Šï¼Œæ‹‰å…„å¼Ÿæˆ‘ä¸€æŠŠã€‚â€ ðŸ˜‚&lt;/p>
&lt;p>ä½†è¯è¯´å›žæ¥ï¼Œâ€œå½“å¹¸ç¦æ¥æ•²é—¨â€çš„æ—¶å€™ï¼Œä½ æœ€å¥½æ˜¯æ°åœ¨å®¶ï¼Œåšå¥½å‡†å¤‡ã€‚&lt;/p>
&lt;p>è¦ä¸ç„¶çœŸæœ‰äººæ‹‰ä½ ä¸€æŠŠæ—¶ï¼Œç»“æžœå´å¼„æˆçƒ‚æ³¥æ‰¶ä¸ä¸Šå¢™ã€‚ææ€•æ²¡æœ‰æ¯”è¿™æ›´ç³Ÿç³•çš„äº†ã€‚&lt;/p></description></item><item><title>so_long_and_no_thanks</title><link>/posts/personals/momentsoflife/so_long/</link><pubDate>Thu, 04 Jul 2024 20:03:19 +0800</pubDate><guid>/posts/personals/momentsoflife/so_long/</guid><description>&lt;p>å¦‚æžœä½ çœ‹è¿‡ã€Šé“¶æ²³ç³»æ¼«æ¸¸æŒ‡å—ã€‹ï¼Œåº”è¯¥è®°å¾—åœ°çƒç”Ÿç‰©æ™ºæ…§æŽ’ç¬¬ä¸€çš„æ˜¯æµ·è±šï¼ˆäººç±»åªæŽ’ç¬¬ä¸‰ï¼‰ðŸ˜‚&lt;/p>
&lt;p>ç¦»åˆ«æ—¶æœ¬æ¥åº”è¯¥æ˜¯æ¥ä¸€å¥ï¼š&lt;/p>
&lt;blockquote>
&lt;p>â€œSo long! And thanks for all the fish.&amp;quot;&lt;/p>
&lt;/blockquote>
&lt;p>ä½†å®žé™…æƒ…å†µå´æ˜¯ï¼š&lt;/p>
&lt;p>&lt;img alt="no_thanks" src="https://fgg100y.github.io/posts/personals/momentsoflife/so_long/images/dophine.jpg">&lt;/p></description></item><item><title>Short_time_Fourier_Transform</title><link>/posts/dsp101/2024-07-03-short_time_fourier_transform/</link><pubDate>Wed, 03 Jul 2024 11:25:37 +0800</pubDate><guid>/posts/dsp101/2024-07-03-short_time_fourier_transform/</guid><description>&lt;p>Sine Wave Signal&lt;/p>
&lt;p>An audio signal, y(t), composed of exactly one sine wave, can be completely described by
the parameters $t, A, f$ and $\phi$,
$$
y(t) = A \sin(2 \pi f t + \phi)
$$
where $t$ represents time in seconds, $A$ is the wave&amp;rsquo;s amplitude (unit-less), $f$ is
its frequency in Hz, and $\phi$ is its phase offset in radians (i.e., where in the cycle
the wave is at $t=0$). If $t \ne 0$, then the sine wave appears shifted in time by
$\frac{\phi}{2 \pi f}$, where negative values mean &amp;ldquo;delay&amp;rdquo; and positive &amp;ldquo;advance&amp;rdquo; it.&lt;/p>
&lt;p>Fourier Series&lt;/p>
&lt;blockquote>
&lt;p>Our old pal Fourier told us that any sound can be represented as an infinite summation
of sine waves each with their own amplitudes, frequencies, and phase offsets. This means
that any sound we hear can be represented as many, many tuples of $t, A, f, \phi$.&lt;/p>
&lt;/blockquote>
&lt;p>Time-Frequency representation&lt;/p>
&lt;p>A Time-Frequency representation is a 2 dimensional matrix that represents the frequency
contents of an audio signal over time.&lt;/p>
&lt;p>We can visualize a TF Representation using a heatmap, which has time along the x-axis
and frequency along the y-axis. Each &lt;em>TF bin&lt;/em> (entry in heatmap) in the heatmap
represents the &lt;strong>amplitude&lt;/strong> of the signal at that particular time and frequency.
If there is no color bar, it is usually safe to assume that brighter colors indicate
higher amplitudes than darker colors.&lt;/p>
&lt;p>&lt;img alt="TFrepr" src="images/dsp_tf_representation.png">&lt;/p>
&lt;p>Short-time Fourier Transform (STFT)&lt;/p>
&lt;p>An STFT is calculated from a waveform representation by computing a discrete Fourier
transform (DFT) of a small, moving window across the duration of the window. The
location of each entry in an STFT determines its time (x-axis) and frequency (y-axis).
The absolute value of a TF bin |$X(t,f)$| at time t and frequency f determines the amount
of energy heard from frequency $f$ at time $t$.&lt;/p>
&lt;p>Importantly, each bin in our STFT is complex, meaning each entry contains both a
magnitude component and a phase component. Both components are needed to convert an STFT
matrix back to a waveform by &lt;em>inverse STFT&lt;/em> so that we may hear it.&lt;/p>
&lt;p>&lt;img alt="STFT" src="images/dsp_stft_process.png">&lt;/p>
&lt;p>Window Types&lt;/p>
&lt;p>The window type determines the shape of the short-time window that will segment the
audio into short segments before applying the DFT. The shape of this window will affect
which frequencies get emphasized or attenuated in the DFT. There are many types of
&lt;a href="https://docs.scipy.org/doc/scipy/reference/signal.windows.html">window functions&lt;/a>.&lt;/p>
&lt;p>&lt;img alt="windows" src="images/dsp_window_types.png">&lt;/p></description></item><item><title>Discrete_Fourier_Transform</title><link>/posts/dsp101/2024-07-03-discrete_fourier_transform/</link><pubDate>Wed, 03 Jul 2024 09:46:45 +0800</pubDate><guid>/posts/dsp101/2024-07-03-discrete_fourier_transform/</guid><description>&lt;p>An analysis problem, which is equivalent to the &lt;strong>DFT&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>Given a signal, how to find the amplitude and phase offset of its frequency
components?
&lt;/code>&lt;/pre>
&lt;p>A synthesis problem, which is equivalent to the &lt;strong>inverse DFT&lt;/strong>:&lt;/p>
&lt;pre>&lt;code>Given a set of frequency components and their amplitudes, how can we construct a
signal?
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>DFT&lt;/strong> and &lt;strong>Inverse DFT&lt;/strong> in code&lt;/p>
&lt;p>The &lt;code>analyze()&lt;/code> computes something very close to the DFT, with one difference: The
conventional definition of DFT does not divide by &lt;code>N&lt;/code> (highlighted line below):&lt;/p>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">analyze&lt;/span>(ys):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> amps &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>conj()&lt;span style="color:#ff6ac1">.&lt;/span>transpose()&lt;span style="color:#ff6ac1">.&lt;/span>dot(ys) &lt;span style="color:#ff6ac1">/&lt;/span> N
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> amps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">dft&lt;/span>(ys): &lt;span style="color:#78787e"># return same result as np.fft.fft()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> amps &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>conj()&lt;span style="color:#ff6ac1">.&lt;/span>transpose()&lt;span style="color:#ff6ac1">.&lt;/span>dot(ys)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> amps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">idft&lt;/span>(amps): &lt;span style="color:#78787e"># inverse DFT&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> N &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(amps)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> synthesis_matrix(N)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ys &lt;span style="color:#ff6ac1">=&lt;/span> M&lt;span style="color:#ff6ac1">.&lt;/span>dot(amps) &lt;span style="color:#ff6ac1">/&lt;/span> N
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> ys &lt;span style="color:#78787e"># ys: value of the signal at each time step&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">synthesis_matrix&lt;/span>(N):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ts &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(N) &lt;span style="color:#ff6ac1">/&lt;/span> N &lt;span style="color:#78787e"># ts: the sequence of times where the signal should be evaluated&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fs &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>arange(N) &lt;span style="color:#78787e"># fs: the list of frequencies&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>outer(ts, fs) &lt;span style="color:#78787e"># args: outer product of ts and fs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> M &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>exp(&lt;span style="color:#ff9f43">1&lt;/span>j &lt;span style="color:#ff6ac1">*&lt;/span> PI2 &lt;span style="color:#ff6ac1">*&lt;/span> args) &lt;span style="color:#78787e"># PI2 = np.pi * 2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>ResNet</title><link>/posts/resnet/</link><pubDate>Sat, 29 Jun 2024 09:17:15 +0800</pubDate><guid>/posts/resnet/</guid><description>&lt;p>æ€ç»´å®žéªŒï¼š
åœ¨æµ…å±‚ç½‘ç»œç»“æž„çš„åŸºç¡€ä¸Šï¼ˆæ¯”å¦‚20å±‚ï¼‰ï¼Œå¾€åŽé¢ç›´æŽ¥æ·»åŠ æ›´å¤šçš„åŒæ˜ å°„éšå±‚ï¼ˆidentity layersï¼‰ï¼Œ
å¾—åˆ°çš„æ·±å±‚ç½‘ç»œï¼ˆæ¯”å¦‚50å±‚ï¼‰ç†è®ºä¸Šæ•ˆæžœåº”è¯¥ä¸ä¼šå˜å·®ã€‚ä½†å®žéªŒç»“æžœè¯´æ˜Žï¼Œå®ƒçœŸä¼šå˜å·®ã€‚æ„å‘³ç€ï¼š
SGDç®—æ³•æ— æ³•æ‰¾åˆ°ä½¿å¾—æ›´æ·±å±‚ç½‘ç»œæ€§èƒ½ä¸å˜å·®çš„å‚æ•°ã€‚&lt;/p>
&lt;p>æ®‹å·®ç½‘ç»œæž¶æž„å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚&lt;/p>
&lt;h2 id="why-what-and-how" >
&lt;div>
&lt;a href="#why-what-and-how">
#
&lt;/a>
Why, What, and How
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>Deeper neural networks are more difficult to train. We present a residual learning
framework to ease the training of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learning residual functions with
reference to the layer inputs, instead of learning unreferenced functions.&lt;/p>
&lt;/blockquote>
&lt;p>è®­ç»ƒä¸€ä¸ªæ·±åº¦ï¼ˆè¶³å¤Ÿæ·±çš„ï¼‰ç¥žç»ç½‘ç»œæ˜¯ä¸€ä»¶å¾ˆéš¾çš„äº‹æƒ…ï¼ˆ2015å¹´ï¼‰ã€‚
ä½¿ç”¨â€œæ®‹å·®â€ç¥žç»ç½‘ç»œæž¶æž„å¯ä»¥æ›´å®¹æ˜“åœ°è®­ç»ƒè¶³å¤Ÿæ·±çš„ç¥žç»ç½‘ç»œã€‚
â€œæ®‹å·®â€æž¶æž„å°±æ˜¯æŠŠè¿™äº›ä¸­é—´å±‚ä½œä¸ºä¸€ä¸ªå­¦ä¹ è¾“å…¥ä¸Žè¾“å‡ºçš„æ®‹å·®çš„å‡½æ•°ã€‚&lt;/p>
&lt;p>&lt;img alt="ResNet Block" src="images/resnet_learning_block.png">&lt;/p>
&lt;p>å°±æ˜¯è¯´ï¼šå¢žåŠ çš„éšå±‚åŽ»å­¦ä¹  $h(x) - x$ï¼ˆæ®‹å·®ï¼‰è€Œä¸æ˜¯ $h(x)$ï¼Œè€Œè¾“å‡ºçš„æ˜¯å½“å‰éšå±‚çš„è¾“å‡ºåŠ ä¸Šå‰
ä¸€å±‚çš„è¾“å‡º $x$ ï¼ˆåŒæ—¶ä¹Ÿæ˜¯å½“å‰å±‚çš„è¾“å…¥ï¼‰ã€‚&lt;/p>
&lt;h2 id="æ®‹å·®ç½‘ç»œç»“æž„" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84">
#
&lt;/a>
æ®‹å·®ç½‘ç»œç»“æž„
&lt;/div>
&lt;/h2>
&lt;p>æ®‹å·®è¿žæŽ¥ï¼ˆaka, shortcut connectionï¼‰åœ¨è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ç›¸åŒæƒ…å†µä¸‹å¯ä»¥ç›´æŽ¥ä½¿ç”¨å¦‚ä¸‹ç»“æž„å—ï¼š&lt;/p>
&lt;p>$$
y = F(x, {W_i}) + x.
$$&lt;/p>
&lt;p>å½“æƒ³è¦å¢žåŠ ç»´åº¦æ—¶ï¼Œæœ‰ä¸¤ç§æ–¹å¼æ¥å¯¹é½è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>æ®‹å·®è¿žæŽ¥è¿›è¡ŒåŒæ˜ å°„æ“ä½œï¼Œç„¶åŽå¯¹å¢žåŠ çš„ç»´åº¦è¿›è¡Œè¡¥é›¶æ“ä½œï¼ˆpadding zeros)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ®‹å·®è¿žæŽ¥è¿›è¡Œçº¿æ€§æŠ•å½±æ“ä½œ $y = F(x, {W_i}) + W_s x.$ æ¥å¯¹é½ç»´åº¦ï¼ˆä½¿ç”¨ 1x1 å·ç§¯å®žçŽ°ï¼‰&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>ä¸¤ç§æƒ…å†µä¸‹ï¼Œå½“æ®‹å·®è¿žæŽ¥çš„æ˜¯ä¸åŒå°ºå¯¸çš„ç‰¹å¾å›¾ï¼ˆfeature mapï¼‰æ—¶ï¼Œä½¿ç”¨æ­¥å¹…ä¸º2çš„å·ç§¯æ“ä½œã€‚
ï¼ˆé€šé“æ•°ç¿»å€ï¼Œåˆ™é«˜å®½å‡åŠï¼Œå› æ­¤ä½¿ç”¨æ­¥å¹…=2ï¼ˆstride=2ï¼‰çš„å·ç§¯æ¥ä¿è¯ç»´æ•°å¯¹é½ï¼‰&lt;/p>
&lt;p>&lt;img alt="bottlenet-block" src="https://fgg100y.github.io/posts/resnet/images/resnet_bottlenet_building_block.png">&lt;/p>
&lt;p>&lt;img alt="resnet-arch" src="https://fgg100y.github.io/posts/resnet/images/resnet_arch_for_imagenet.png">&lt;/p>
&lt;h2 id="æ®‹å·®è¿žæŽ¥ä¸ºä»€ä¹ˆæœ‰ç”¨" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e7%94%a8">
#
&lt;/a>
æ®‹å·®è¿žæŽ¥ä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿ
&lt;/div>
&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Easier Gradient Flow&lt;/p>
&lt;ul>
&lt;li>ç¼“è§£æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸é—®é¢˜&lt;/li>
&lt;li>æ®‹å·®è¿žæŽ¥ä½¿å¾—æ¢¯åº¦æ›´ç¨³å®š&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>æ›´å®¹æ˜“å­¦ä¹ åŒæ˜ å°„å‡½æ•°&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æå‡æœ€ä¼˜åŒ–å’Œæ”¶æ•›æ•ˆæžœ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ›´å¥½çš„ç‰¹å¾ä¼ æ’­ï¼ˆfeature propagationï¼‰&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="æ­¤æ®‹å·®éžå½¼æ®‹å·®resnet-vs-gdbt" >
&lt;div>
&lt;a href="#%e6%ad%a4%e6%ae%8b%e5%b7%ae%e9%9d%9e%e5%bd%bc%e6%ae%8b%e5%b7%aeresnet-vs-gdbt">
#
&lt;/a>
æ­¤æ®‹å·®éžå½¼æ®‹å·®ï¼ˆResNet VS GDBTï¼‰
&lt;/div>
&lt;/h2>
&lt;p>GBDTï¼ˆæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼‰ä¸­çš„æ®‹å·®ï¼ˆresidualï¼‰æ˜¯æŒ‡é¢„æµ‹å€¼ä¸ŽçœŸå®žå€¼ä¹‹é—´çš„å·®å€¼ã€‚&lt;/p>
&lt;p>ResNet ä¸­çš„æ®‹å·®ï¼ˆresidualï¼‰ï¼Œæ˜¯ç¥žç»ç½‘ç»œå°è¯•åŽ»å­¦ä¹ ç‰¹å¾å›¾ï¼ˆfeature mapï¼‰çš„æ®‹å·®ï¼ˆæ˜¯ç½‘ç»œå—è¾“å‡ºä¸Žè¾“å…¥ä¹‹é—´çš„å·®å€¼ï¼‰ã€‚&lt;/p>
&lt;p>Paper: &lt;a href="https://arxiv.org/pdf/1512.03385">https://arxiv.org/pdf/1512.03385&lt;/a>&lt;/p></description></item><item><title>knowledge_distillation2</title><link>/posts/2024-06-28-knowledge_distillation2/</link><pubDate>Fri, 28 Jun 2024 11:25:43 +0800</pubDate><guid>/posts/2024-06-28-knowledge_distillation2/</guid><description>&lt;h2 id="generalized-knowledge-distillation-gkd" >
&lt;div>
&lt;a href="#generalized-knowledge-distillation-gkd">
#
&lt;/a>
Generalized Knowledge Distillation (GKD)
&lt;/div>
&lt;/h2>
&lt;p>æ³›åŒ–çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§æ”¹è¿›çš„çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨è‡ªå›žå½’åºåˆ—æ¨¡åž‹ä¸­é‡åˆ°çš„åˆ†
å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒå’ŒæŽ¨ç†é˜¶æ®µä¹‹é—´ã€‚ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•é€šå¸¸åŸºäºŽå›ºå®šçš„è¾“å‡ºåºåˆ—é›†è¿›è¡Œï¼Œ
è¿™äº›åºåˆ—æˆ–è€…æ˜¯æ•™å¸ˆæ¨¡åž‹ç”Ÿæˆçš„ï¼Œæˆ–è€…æ˜¯åŸºäºŽçœŸå®žæ•°æ®çš„æ ‡ç­¾ã€‚ç„¶è€Œï¼Œè¿™å¯¼è‡´å­¦ç”Ÿæ¨¡åž‹åœ¨æŽ¨ç†æ—¶ç”Ÿæˆ
çš„åºåˆ—å¯èƒ½ä¸Žè®­ç»ƒæ—¶è§åˆ°çš„åºåˆ—åˆ†å¸ƒä¸åŒï¼Œä»Žè€Œå½±å“äº†å­¦ç”Ÿæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p>
&lt;p>GKD ä¸å†å±€é™äºŽå›ºå®šè¾“å‡ºåºåˆ—çš„è®­ç»ƒï¼Œè€Œæ˜¯å…è®¸å­¦ç”Ÿæ¨¡åž‹åœ¨å…¶è‡ªæˆ‘ç”Ÿæˆçš„åºåˆ—ä¸Šè¿›è¡Œå­¦ä¹ ï¼ŒåŒæ—¶åˆ©ç”¨
æ•™å¸ˆæ¨¡åž‹æä¾›çš„åé¦ˆã€‚&lt;/p>
&lt;h2 id="gkd-æŠ€æœ¯è¦ç‚¹" >
&lt;div>
&lt;a href="#gkd-%e6%8a%80%e6%9c%af%e8%a6%81%e7%82%b9">
#
&lt;/a>
GKD æŠ€æœ¯è¦ç‚¹
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>åŠ¨æ€åºåˆ—ç”Ÿæˆä¸Žåé¦ˆï¼šGKD åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šæ ¹æ®å­¦ç”Ÿæ¨¡åž‹çš„è‡ªæˆ‘ç”Ÿæˆåºåˆ—è¿›è¡Œå­¦ä¹ ã€‚è¿™æ„å‘³ç€å­¦
ç”Ÿæ¨¡åž‹åœ¨å°è¯•ç”Ÿæˆåºåˆ—æ—¶ï¼Œå¦‚æžœç”Ÿæˆé”™è¯¯æˆ–ä¸ç†æƒ³çš„åºåˆ—ï¼Œå®ƒå¯ä»¥ä»Žæ•™å¸ˆæ¨¡åž‹é‚£é‡ŒèŽ·å¾—ç›´æŽ¥åé¦ˆï¼Œ
å­¦ä¹ å¦‚ä½•çº æ­£è¿™äº›é”™è¯¯ã€‚è¿™ç§æœºåˆ¶èƒ½å¤Ÿå‡å°‘è®­ç»ƒå’ŒæŽ¨ç†è¿‡ç¨‹ä¸­çš„åˆ†å¸ƒå·®å¼‚ï¼Œæé«˜å­¦ç”Ÿæ¨¡åž‹å¯¹å„ç§è¾“
å…¥åºåˆ—çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>çµæ´»çš„æŸå¤±å‡½æ•°é€‰æ‹©ï¼šä¸Žä»…é‡‡ç”¨å‰å‘KLæ•£åº¦çš„ä¼ ç»Ÿç›‘ç£å¼çŸ¥è¯†è’¸é¦ä¸åŒï¼ŒGKD æä¾›äº†çµæ´»æ€§ï¼Œå¯ä»¥
åœ¨å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡åž‹ä¹‹é—´ä½¿ç”¨æ›¿ä»£çš„æŸå¤±å‡½æ•°ã€‚å½“å­¦ç”Ÿæ¨¡åž‹éš¾ä»¥ç²¾ç¡®æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡åž‹çš„æ¦‚çŽ‡åˆ†å¸ƒæ—¶ï¼Œè¿™
ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é‡‡ç”¨Jensen-Shannonæ•£åº¦ç­‰æŒ‡æ ‡æ¥è¡¡é‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œè¿™æœ‰åŠ©äºŽå­¦ç”Ÿ
æ¨¡åž‹åœ¨è¡¨è¾¾åŠ›æœ‰é™çš„æƒ…å†µä¸‹æ›´å¥½åœ°å­¦ä¹ ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>é›†æˆå¼ºåŒ–å­¦ä¹ ç²¾è°ƒï¼šGKD è¿˜ä¿ƒè¿›äº†çŸ¥è¯†è’¸é¦ä¸ŽåŸºäºŽå¼ºåŒ–å­¦ä¹ çš„è¯­è¨€æ¨¡åž‹å¾®è°ƒçš„æ— ç¼æ•´åˆã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="åŠ¨æ€åºåˆ—ç”Ÿæˆ" >
&lt;div>
&lt;a href="#%e5%8a%a8%e6%80%81%e5%ba%8f%e5%88%97%e7%94%9f%e6%88%90">
##
&lt;/a>
åŠ¨æ€åºåˆ—ç”Ÿæˆ
&lt;/div>
&lt;/h3>
&lt;p>åœ¨ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦ä¸­ï¼Œå­¦ç”Ÿæ¨¡åž‹é€šå¸¸åŸºäºŽé¢„å…ˆå®šä¹‰å¥½çš„å›ºå®šæ•°æ®é›†æˆ–ç”±æ•™å¸ˆæ¨¡åž‹ç”Ÿæˆçš„è¾“å‡ºåºåˆ—è¿›è¡Œ
è®­ç»ƒã€‚è€ŒåŠ¨æ€åºåˆ—ç”Ÿæˆåˆ™æ‰“ç ´äº†è¿™ä¸€é™åˆ¶ï¼Œå®ƒè®©å­¦ç”Ÿæ¨¡åž‹è‡ªå·±ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå­¦ä¹ ã€‚
è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>åˆå§‹åŒ–å­¦ç”Ÿæ¨¡åž‹ï¼šé¦–å…ˆï¼Œå­¦ç”Ÿæ¨¡åž‹éœ€è¦å…·å¤‡ä¸€å®šçš„ç”Ÿæˆèƒ½åŠ›ï¼Œé€šå¸¸é€šè¿‡åˆæ­¥çš„ç›‘ç£å¼é¢„è®­ç»ƒæˆ–å¾®è°ƒ
ï¼ˆSupervised Fine-Tuning, SFTï¼‰æ¥å®žçŽ°ï¼Œç¡®ä¿å®ƒå¯ä»¥ç”Ÿæˆè´¨é‡å°šå¯çš„åºåˆ—ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è‡ªæˆ‘ç”Ÿæˆåºåˆ—ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­¦ç”Ÿæ¨¡åž‹æŽ¥æ”¶åˆ°è¾“å…¥åŽï¼Œä¼šæ ¹æ®å½“å‰çš„ç­–ç•¥ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚è¿™äº›åº
åˆ—åæ˜ äº†å­¦ç”Ÿæ¨¡åž‹å½“å‰çš„èƒ½åŠ›å’Œç†è§£ï¼Œå¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å®Œç¾Žçš„éƒ¨åˆ†ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="åé¦ˆæœºåˆ¶" >
&lt;div>
&lt;a href="#%e5%8f%8d%e9%a6%88%e6%9c%ba%e5%88%b6">
##
&lt;/a>
åé¦ˆæœºåˆ¶
&lt;/div>
&lt;/h3>
&lt;p>ä¸€æ—¦å­¦ç”Ÿæ¨¡åž‹ç”Ÿæˆäº†è¾“å‡ºåºåˆ—ï¼ŒæŽ¥ä¸‹æ¥çš„å…³é”®æ­¥éª¤æ˜¯èŽ·å–æ•™å¸ˆæ¨¡åž‹çš„åé¦ˆï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>æ•™å¸ˆæ¨¡åž‹è¯„ä¼°ï¼šæ•™å¸ˆæ¨¡åž‹ä¼šå¯¹å­¦ç”Ÿç”Ÿæˆçš„åºåˆ—è¿›è¡Œè¯„ä¼°ï¼Œé€šå¸¸é€šè¿‡æ¯”è¾ƒæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡åž‹åœ¨æ¯ä¸ªç”Ÿæˆ
æ­¥éª¤ä¸Šå¯¹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚çŽ‡åˆ†å¸ƒã€‚è¿™æ¶‰åŠåˆ°è®¡ç®—ä¸¤è€…çš„æ¦‚çŽ‡åˆ†å¸ƒä¹‹é—´çš„æŸç§å·®å¼‚åº¦é‡ï¼Œå¦‚KLæ•£åº¦ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æŒ‡å¯¼æ€§å­¦ä¹ ï¼šæ ¹æ®æ•™å¸ˆæ¨¡åž‹çš„åé¦ˆï¼Œå­¦ç”Ÿæ¨¡åž‹è°ƒæ•´å…¶å‚æ•°ï¼Œç›®çš„æ˜¯ä½¿è‡ªå·±åœ¨ç”Ÿæˆç±»ä¼¼åºåˆ—æ—¶èƒ½æ›´æŽ¥
è¿‘æ•™å¸ˆæ¨¡åž‹çš„è¡¨çŽ°ã€‚GKD å…è®¸é€‰æ‹©ä¸åŒçš„å·®å¼‚åº¦é‡ï¼ˆdivergenceï¼‰æ¥ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è¿­ä»£ä¼˜åŒ–ï¼šéšç€å­¦ç”Ÿæ¨¡åž‹ä¸æ–­ä»Žè‡ªæˆ‘ç”Ÿæˆçš„é”™è¯¯ä¸­å­¦ä¹ ï¼Œå®ƒçš„ç”Ÿæˆèƒ½åŠ›é€æ­¥æå‡ï¼Œç”Ÿæˆçš„åºåˆ—è´¨é‡
ä¹Ÿéšä¹‹æé«˜ï¼Œå½¢æˆäº†ä¸€ä¸ªæ­£å‘å¾ªçŽ¯ã€‚æ­¤è¿‡ç¨‹å¯ä»¥è§†ä¸ºä¸€ç§æ¨¡ä»¿å­¦ä¹ ï¼ˆImitation Learningï¼‰ä¸Žå¼ºåŒ–
å­¦ä¹ ï¼ˆReinforcement Learningï¼‰çš„ç»“åˆï¼Œå…¶ä¸­å­¦ç”Ÿæ¨¡åž‹é€šè¿‡è¿­ä»£åœ°æŽ¢ç´¢å’Œä¿®æ­£å…¶è¡Œä¸ºï¼Œä»¥é€¼è¿‘æ•™
å¸ˆæ¨¡åž‹çš„è¡¨çŽ°ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>å‚è€ƒæ–‡çŒ®ï¼šhttps://arxiv.org/pdf/2306.13649&lt;/p></description></item><item><title>knowledge_distillation</title><link>/posts/2024-06-27-knowledge_distillation/</link><pubDate>Thu, 27 Jun 2024 11:34:18 +0800</pubDate><guid>/posts/2024-06-27-knowledge_distillation/</guid><description>&lt;p>çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å°†å¤§åž‹ã€å¤æ‚çš„æ¨¡åž‹ï¼ˆç§°ä¸ºæ•™å¸ˆ
æ¨¡åž‹ï¼ŒTeacher Modelï¼‰çš„çŸ¥è¯†â€œè’¸é¦â€åˆ°å°åž‹ã€ç®€æ´çš„æ¨¡åž‹ï¼ˆç§°ä¸ºå­¦ç”Ÿæ¨¡åž‹ï¼ŒStudent Modelï¼‰ä¸­ï¼Œä»Ž
è€Œå®žçŽ°æ¨¡åž‹åŽ‹ç¼©å’ŒåŠ é€Ÿï¼ŒåŒæ—¶å°½å¯èƒ½ä¿æŒåŽŸå§‹æ¨¡åž‹çš„æ€§èƒ½ã€‚è¿™ä¸€æŠ€æœ¯ä½¿å¾—æ¨¡åž‹å¯ä»¥åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡
ä¸Šé«˜æ•ˆè¿è¡Œï¼Œå¦‚æ‰‹æœºæˆ–åµŒå…¥å¼è®¾å¤‡ã€‚&lt;/p>
&lt;blockquote>
&lt;p>The method works by incorporating an additional loss into the traditional cross entropy
loss, which is based on the softmax output of the teacher network. The assumption is
that the output activations of a properly trained teacher network carry additional
information that can be leveraged by a student network during training.&lt;/p>
&lt;/blockquote>
&lt;h2 id="åŸºæœ¬åŽŸç†" >
&lt;div>
&lt;a href="#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86">
#
&lt;/a>
åŸºæœ¬åŽŸç†
&lt;/div>
&lt;/h2>
&lt;p>çŸ¥è¯†è½¬ç§»ï¼šæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è®©å­¦ç”Ÿæ¨¡åž‹æ¨¡ä»¿æ•™å¸ˆæ¨¡åž‹çš„è¾“å‡ºè¡Œä¸ºï¼Œä¸ä»…åŒ…æ‹¬ç¡¬åˆ†ç±»æ ‡ç­¾ï¼Œè¿˜æœ‰è½¯æ¦‚çŽ‡
åˆ†å¸ƒï¼ˆsoftmaxæ¦‚çŽ‡ï¼‰ï¼Œè¿™æ ·å¯ä»¥ä¼ é€’æ›´å¤šå…³äºŽæ•°æ®åˆ†å¸ƒçš„ä¿¡æ¯ã€‚è½¯æ ‡ç­¾ç›¸æ¯”ç¡¬æ ‡ç­¾å«æœ‰æ›´å¤šå…³äºŽæ•°
æ®ä¸ç¡®å®šæ€§åŠç±»é—´å…³ç³»çš„ä¿¡æ¯ï¼Œæœ‰åŠ©äºŽå­¦ç”Ÿæ¨¡åž‹å­¦ä¹ æ›´ç»†è…»çš„å†³ç­–è¾¹ç•Œã€‚&lt;/p>
&lt;p>ç‰¹å¾è’¸é¦ï¼šé™¤äº†è¾“å‡ºå±‚çš„çŸ¥è¯†å¤–ï¼Œæ³›åŒ–çŸ¥è¯†è’¸é¦è¿˜å¯ä»¥æ¶‰åŠä¸­é—´å±‚ç‰¹å¾çš„å­¦ä¹ ï¼Œå³å­¦ç”Ÿæ¨¡åž‹è¯•å›¾å­¦ä¹ 
æ•™å¸ˆæ¨¡åž‹çš„é«˜å±‚ç‰¹å¾è¡¨ç¤ºã€‚è¿™æœ‰åŠ©äºŽæå‡å­¦ç”Ÿæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºå®ƒå­¦ä¼šäº†å¦‚ä½•ä»Žè¾“å…¥æ•°æ®ä¸­æå–
æ›´æœ‰ç”¨çš„ç‰¹å¾ã€‚&lt;/p>
&lt;p>å…³ç³»è’¸é¦ï¼šå¼ºè°ƒä¿æŒæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡åž‹å¯¹äºŽè¾“å…¥æ ·æœ¬é—´å…³ç³»çš„ç†è§£ä¸€è‡´æ€§ã€‚è¿™æ„å‘³ç€å­¦ç”Ÿæ¨¡åž‹ä¸ä»…è¦å­¦
ä¼šå•ä¸ªæ ·æœ¬çš„å¤„ç†ï¼Œè¿˜è¦ç†è§£æ ·æœ¬ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ï¼Œè¿™å¯¹äºŽä¸€äº›éœ€è¦ç†è§£å¤æ‚ä¸Šä¸‹æ–‡çš„ä»»åŠ¡å°¤ä¸ºé‡è¦ã€‚&lt;/p>
&lt;p>&lt;strong>çŸ¥è¯†è’¸é¦è¿‡ç¨‹&lt;/strong>ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>è®­ç»ƒæ•™å¸ˆæ¨¡åž‹&lt;/strong>ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºè®­ç»ƒä¸€ä¸ªé«˜æ€§èƒ½çš„æ·±åº¦ç¥žç»ç½‘ç»œï¼ˆæ•™å¸ˆæ¨¡
åž‹ï¼‰ã€‚è¿™ä¸ªæ¨¡åž‹å¯èƒ½åŒ…å«æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿ä¸ªå‚æ•°ï¼Œä½†å®ƒåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨çŽ°éžå¸¸å‡ºè‰²ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ç”Ÿæˆè½¯æ ‡ç­¾&lt;/strong>ï¼šæ•™å¸ˆæ¨¡åž‹åœ¨å¯¹è¾“å…¥æ•°æ®è¿›è¡Œé¢„æµ‹æ—¶ï¼Œä¸ä»…ä»…ç»™å‡ºæœ€ç»ˆçš„åˆ†ç±»ç»“æžœï¼Œè¿˜ä¼šç»™å‡ºå„
ç±»åˆ«çš„æ¦‚çŽ‡åˆ†å¸ƒï¼ˆé€šå¸¸é€šè¿‡softmaxå±‚èŽ·å¾—ï¼‰ã€‚è¿™äº›æ¦‚çŽ‡åˆ†å¸ƒè¢«ç§°ä¸ºâ€œè½¯æ ‡ç­¾â€ï¼Œå®ƒä»¬åŒ…å«äº†é¢å¤–çš„
ä¿¡æ¯ï¼Œæ¯”å¦‚ç±»åˆ«çš„ä¸ç¡®å®šæ€§ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>è®­ç»ƒå­¦ç”Ÿæ¨¡åž‹&lt;/strong>ï¼šæŽ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ•™å¸ˆæ¨¡åž‹çš„è½¯æ ‡ç­¾å’Œå®žé™…çš„ç¡¬æ ‡ç­¾ï¼ˆå³æ•°æ®çš„çœŸå®žç±»åˆ«ï¼‰æ¥è®­
ç»ƒå­¦ç”Ÿæ¨¡åž‹ã€‚å­¦ç”Ÿæ¨¡åž‹çš„æž¶æž„è®¾è®¡å¾—æ›´ç®€å•ï¼Œå‚æ•°é‡è¿œå°äºŽæ•™å¸ˆæ¨¡åž‹ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­¦ç”Ÿæ¨¡åž‹ä¸
ä»…è¦å­¦ä¹ æ¨¡ä»¿ç¡¬æ ‡ç­¾ï¼Œè¿˜è¦é€šè¿‡æŸå¤±å‡½æ•°ï¼ˆå¦‚KLæ•£åº¦æˆ–äº¤å‰ç†µï¼‰å°½é‡æŽ¥è¿‘æ•™å¸ˆæ¨¡åž‹çš„è½¯æ ‡ç­¾è¾“å‡ºã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ¸©åº¦å‚æ•°è°ƒæ•´&lt;/strong>ï¼šåœ¨ç”Ÿæˆè½¯æ ‡ç­¾æ—¶ï¼Œæœ‰æ—¶ä¼šå¼•å…¥ä¸€ä¸ªâ€œæ¸©åº¦â€å‚æ•°æ¥è°ƒæ•´æ¦‚çŽ‡åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦ã€‚
é«˜æ¸©å¯ä»¥ä½¿è½¯æ ‡ç­¾æ›´åŠ å¹³æ»‘ï¼Œä¿ƒè¿›å­¦ç”Ÿæ¨¡åž‹å­¦ä¹ åˆ°æ•™å¸ˆæ¨¡åž‹çš„å†³ç­–è¾¹ç•Œï¼›ä½Žæ¸©åˆ™ä½¿å¾—è½¯æ ‡ç­¾æŽ¥è¿‘
ç¡¬æ ‡ç­¾ï¼Œä½†å¯èƒ½ä¼šä¸¢å¤±æ•™å¸ˆæ¨¡åž‹çš„ä¸€äº›ç»†å¾®å†³ç­–ä¿¡æ¯ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="çŸ¥è¯†è’¸é¦çš„ä¸‰ç§ä»£ç å®žçŽ°" >
&lt;div>
&lt;a href="#%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f%e7%9a%84%e4%b8%89%e7%a7%8d%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0">
#
&lt;/a>
çŸ¥è¯†è’¸é¦çš„ä¸‰ç§ä»£ç å®žçŽ°ï¼š
&lt;/div>
&lt;/h2>
&lt;ol>
&lt;li>çŸ¥è¯†è½¬ç§»ï¼šåŸºäºŽ softmax output è½¯æ ‡ç­¾&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the teacher model - do not save gradients here as we do not change the teacher&amp;#39;s weights&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> teacher_logits &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#Soften the student logits by applying softmax first and log() second&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_targets &lt;span style="color:#ff6ac1">=&lt;/span> nn&lt;span style="color:#ff6ac1">.&lt;/span>functional&lt;span style="color:#ff6ac1">.&lt;/span>softmax(teacher_logits &lt;span style="color:#ff6ac1">/&lt;/span> T, dim&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_prob &lt;span style="color:#ff6ac1">=&lt;/span> nn&lt;span style="color:#ff6ac1">.&lt;/span>functional&lt;span style="color:#ff6ac1">.&lt;/span>log_softmax(student_logits &lt;span style="color:#ff6ac1">/&lt;/span> T, dim&lt;span style="color:#ff6ac1">=-&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper &amp;#34;Distilling the knowledge in a neural network&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soft_targets_loss &lt;span style="color:#ff6ac1">=&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>sum(soft_targets &lt;span style="color:#ff6ac1">*&lt;/span> (soft_targets&lt;span style="color:#ff6ac1">.&lt;/span>log() &lt;span style="color:#ff6ac1">-&lt;/span> soft_prob)) &lt;span style="color:#ff6ac1">/&lt;/span> soft_prob&lt;span style="color:#ff6ac1">.&lt;/span>size()[&lt;span style="color:#ff9f43">0&lt;/span>] &lt;span style="color:#ff6ac1">*&lt;/span> (T&lt;span style="color:#ff6ac1">**&lt;/span>&lt;span style="color:#ff9f43">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> soft_target_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> soft_targets_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>ç‰¹å¾è’¸é¦ï¼šåŸºäºŽ hidden state ä½™å¼¦ç›¸ä¼¼åº¦&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the teacher model and keep only the hidden representation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _, teacher_hidden_representation &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits, student_hidden_representation &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hidden_rep_loss &lt;span style="color:#ff6ac1">=&lt;/span> cosine_loss(student_hidden_representation, teacher_hidden_representation, target&lt;span style="color:#ff6ac1">=&lt;/span>torch&lt;span style="color:#ff6ac1">.&lt;/span>ones(inputs&lt;span style="color:#ff6ac1">.&lt;/span>size(&lt;span style="color:#ff9f43">0&lt;/span>))&lt;span style="color:#ff6ac1">.&lt;/span>to(device))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> hidden_rep_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> hidden_rep_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>ç‰¹å¾è’¸é¦ï¼šåŸºäºŽ Intermediate regressor MSEæŸå¤±&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Again ignore teacher logits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">with&lt;/span> torch&lt;span style="color:#ff6ac1">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _, teacher_feature_map &lt;span style="color:#ff6ac1">=&lt;/span> teacher(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Forward pass with the student model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>student_logits, regressor_feature_map &lt;span style="color:#ff6ac1">=&lt;/span> student(inputs)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hidden_rep_loss &lt;span style="color:#ff6ac1">=&lt;/span> mse_loss(regressor_feature_map, teacher_feature_map)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Calculate the true label loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>label_loss &lt;span style="color:#ff6ac1">=&lt;/span> ce_loss(student_logits, labels)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Weighted sum of the two losses&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss &lt;span style="color:#ff6ac1">=&lt;/span> feature_map_weight &lt;span style="color:#ff6ac1">*&lt;/span> hidden_rep_loss &lt;span style="color:#ff6ac1">+&lt;/span> ce_loss_weight &lt;span style="color:#ff6ac1">*&lt;/span> label_loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>loss&lt;span style="color:#ff6ac1">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>optimizer&lt;span style="color:#ff6ac1">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>å…·ä½“å‚è€ƒï¼šhttps://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html?highlight=distill&lt;/p></description></item><item><title>mixup_beyond_ERM</title><link>/posts/2024-06-27-mixup_beyond_erm/</link><pubDate>Thu, 27 Jun 2024 09:42:34 +0800</pubDate><guid>/posts/2024-06-27-mixup_beyond_erm/</guid><description>&lt;h1 id="empirical-risk-minimazation-erm" >
&lt;div>
&lt;a href="#empirical-risk-minimazation-erm">
##
&lt;/a>
Empirical Risk Minimazation (ERM)
&lt;/div>
&lt;/h1>
&lt;p>ç»éªŒé£Žé™©æœ€å°åŒ–, Empirical Risk Minimazation principle (Vapnik, 1998)&lt;/p>
&lt;ol>
&lt;li>åŸºäºŽERMè®­ç»ƒæ¨¡åž‹ï¼šäº¦å³åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šå­¦ä¹ ä»¥æœ€å°åŒ–å…¶å¹³å‡è¯¯å·®ã€‚&lt;/li>
&lt;li>å½“å‰SOTAæ¨¡åž‹çš„å‚æ•°é‡éšç€è®­ç»ƒæ•°æ®é›†è§„æ¨¡å¢žå¤§è€Œçº¿æ€§å¢žåŠ ã€‚&lt;/li>
&lt;/ol>
&lt;p>è€Œç»å…¸VCå­¦ä¹ ç†è®ºï¼ˆlearning theory, Vapnik &amp;amp; Chervonenkis, 1971ï¼‰è¡¨æ˜Žï¼šåªè¦å­¦ä¹ å™¨çš„å‚æ•°é‡
ä¸éšç€è®­ç»ƒæ ·æœ¬æ•°é‡å¢žåŠ ï¼Œåˆ™åŸºäºŽERMå­¦ä¹ ä¸€å®šä¼šæ”¶æ•›(convergence, i.e., good generalization
to new data)ã€‚äº¦å³ï¼šæ¨¡åž‹çš„å¤æ‚åº¦ï¼ˆå‚æ•°é‡è§„æ¨¡ï¼‰ç›¸å¯¹äºŽè®­ç»ƒæ•°æ®è§„æ¨¡åº”è¯¥æ˜¯å›ºå®šçš„æˆ–è€…å˜åŠ¨ä¸å¤§ã€‚&lt;/p>
&lt;p>è¿™å°±é€ æˆäº†è¿™æ ·çš„çŸ›ç›¾ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ç»å…¸å­¦ä¹ ç†è®ºè®¤ä¸ºï¼šæƒ³è¦ERMæœ‰æ•ˆï¼Œåˆ™æ¨¡åž‹çš„å‚æ•°é‡åº”è¯¥ä¿æŒä¸€å®šå¤§å°ï¼Œè€Œä¸æ˜¯éšç€è®­ç»ƒæ•°æ®é›†è§„æ¨¡å¢žåŠ è€Œå¢žåŠ ï¼›&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ç„¶è€Œåœ¨å®žé™…ä»»åŠ¡ä¸­ï¼šSOTAæ¨¡åž‹çš„å‚æ•°é‡æ˜¯ä¸Žè®­ç»ƒæ•°æ®é‡ä¿æŒçº¿æ€§å¢žåŠ çš„ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>å®žé™…ä¸Šï¼Œä¸€æ–¹é¢ERMå…è®¸å¤§çš„ç¥žç»ç½‘ç»œæ¨¡åž‹â€œè®°ä½â€è®­ç»ƒæ ·æœ¬ï¼ˆå³ä½¿ç”¨äº†å¾ˆå¼ºçš„æ­£åˆ™åŒ–çº¦æŸï¼Œ
strong regularization)ï¼Œå¦ä¸€æ–¹é¢ERMè®­ç»ƒå¾—åˆ°çš„æ¨¡åž‹åœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„æ ·æœ¬ä¸Šçš„é¢„æµ‹ç»“æžœå·®å¼‚å·¨
å¤§ï¼ˆå³ä½¿è¢«é¢„æµ‹çš„æ ·æœ¬ä»…ä»…å‘ç”Ÿäº†ç›¸å¯¹å¾®å°çš„æ”¹å˜ï¼Œadversarial examplesï¼‰ã€‚&lt;/p>
&lt;p>é‚£ä¹ˆï¼Œæœ‰æ²¡æœ‰åˆ«çš„æ¨¡åž‹è®­ç»ƒæœ€ä¼˜åŒ–å‡†åˆ™å‘¢ï¼Ÿ-&amp;gt; Vicinal Risk Minimization (VRM).&lt;/p>
&lt;blockquote>
&lt;p>In VRM, human knowledge is required to describe a vicinity or neighborhood around each
example in the training data. Then, additional virtual examples can be drawn from the
vicinity distribution of the training examples to enlarge the support of the training
distribution.&lt;/p>
&lt;/blockquote>
&lt;p>ä¾‹å¦‚ï¼Œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå›¾åƒå¢žå¼ºé€šå¸¸åŒ…æ‹¬è½»å¾®çš„æ—‹è½¬ã€ç¿»è½¬ã€ç¼©æ”¾ç­‰æ“ä½œï¼Œè¿™å…¶å®žæ˜¯å›¾åƒçš„è¿‘é‚»
é›†ï¼ˆvicinity setï¼‰ã€‚è¿™äº›æ•°æ®å¢žå¼ºé€šå¸¸æå‡äº†æ¨¡åž‹çš„æ³›åŒ–æ€§èƒ½ã€‚ä½†è¿™äº›æ“ä½œé€šå¸¸ä¹Ÿæ˜¯æ•°æ®ä¾èµ–çš„ã€‚&lt;/p>
&lt;p>Mixup æä¾›äº†ä¸€ç§æ•°æ®æ— å…³ï¼ˆdata-agnosticï¼‰çš„å¢žå¼ºæ–¹æ³•ï¼Œå®ƒæž„é€ è™šæ‹Ÿæ ·æœ¬çš„æ–¹å¼å¦‚ä¸‹ï¼š&lt;/p>
&lt;p>$$
\begin{eqnarray}
\tilde{x} &amp;amp;=&amp;amp; \lambda x_i + (1 - \lambda) x_j, \text{where } x_i, x_j \text{ are raw input vectors} \\
\tilde{y} &amp;amp;=&amp;amp; \lambda y_i + (1 - \lambda) y_j, \text{where } y_i, y_j \text{ are one-hot label encoding}
\end{eqnarray}
$$&lt;/p>
&lt;p>$(x_i, y_i)$ å’Œ $(x_j, y_j)$ æ˜¯ä»Žè®­ç»ƒé›†ä¸­éšæœºæŠ½å–çš„ä¸¤ä¸ªæ ·ä¾‹ã€‚$\lambda \in [0, 1]$ã€‚&lt;/p>
&lt;p>å› æ­¤ï¼Œmixupé€šè¿‡ç»“åˆå…ˆéªŒçŸ¥è¯†æ‰©å±•äº†è®­ç»ƒåˆ†å¸ƒï¼Œå³ç‰¹å¾å‘é‡çš„çº¿æ€§æ’å€¼åº”å¯¼è‡´ç›¸å…³ç›®æ ‡çš„çº¿æ€§æ’å€¼ã€‚
mixupå¯ä»¥ç”¨å‡ è¡Œä»£ç å®žçŽ°ï¼Œå¹¶ä¸”åªå¼•å…¥æœ€å°çš„è®¡ç®—å¼€é”€ã€‚&lt;/p>
&lt;p>Mixup: beyond_ERM, &lt;a href="https://arxiv.org/pdf/1710.09412">https://arxiv.org/pdf/1710.09412&lt;/a>&lt;/p></description></item><item><title>è·¯ç”±å™¨wifiè¿žä¸ä¸Š_ä½†æ‰‹æœºæµé‡å¯ä»¥</title><link>/posts/2024-06-16-wifi%E8%BF%9E%E4%B8%8D%E4%B8%8A_%E4%BD%86%E6%89%8B%E6%9C%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E4%BB%A5/</link><pubDate>Sun, 16 Jun 2024 10:07:12 +0800</pubDate><guid>/posts/2024-06-16-wifi%E8%BF%9E%E4%B8%8D%E4%B8%8A_%E4%BD%86%E6%89%8B%E6%9C%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E4%BB%A5/</guid><description>&lt;p>Qï¼šè¯·é—®Wifiæ— æ³•ç¿»å¢™æˆ–è®¿é—®æŸäº›ç½‘ç«™ï¼Œä½†ç”¨æ‰‹æœºæµé‡å´å¯ä»¥æ˜¯ä»€ä¹ˆåŽŸå› ï¼Ÿ&lt;/p>
&lt;p>A:
å…ˆè¯´è¯´ä½ çš„wifiå’Œ5gçŽ¯å¢ƒä¸‹ç¿»å¢™çš„é—®é¢˜ï¼Œæˆ‘çŒœæµ‹ä¸‹ä½ å®¶é‡Œçš„å®½å¸¦æ˜¯ä¸æ˜¯ç§»åŠ¨çš„ï¼Œæ‰‹æœºæµé‡ç”¨çš„æ˜¯è”é€š
æˆ–è€…ç”µä¿¡ï¼Œæ€»ä¹‹ï¼Œå®¶åº­å®½å¸¦å’Œ5gä¸æ˜¯ä¸€å®¶ispã€‚ å¦‚æžœæ˜¯çš„è¯ï¼Œè¿™ä¸ªé—®é¢˜å°±æ¯”è¾ƒå¥½ç†è§£ï¼Œä¸åŒispä¸åŒ
çš„åœ°åŒºéƒ½ä¼šæœ‰è‡ªå·±çš„ä¸€å¥—é»‘åå•æœºåˆ¶ã€‚ æ‰€ä»¥å¯èƒ½ä½ çš„å®¶å®½ispç›‘æµ‹ä½ çš„ä¸Šç½‘æµé‡æœ‰ä¸æ­£å¸¸ï¼Œæˆ–è€…å¤§æ•°
æ®è®¤ä¸ºä½ çš„ç¿»å¢™æœåŠ¡å™¨å¾ˆå¯ç–‘ï¼ˆæœªå¿…æ˜¯ä½ é€ æˆçš„ï¼Œå› ä¸ºæœ‰å¾ˆå¤šäººåœ¨ç”¨ï¼‰ï¼ŒäºŽæ˜¯å¼€å¯äº†å±è”½ï¼Œä½†æ˜¯å¦ä¸€
å®¶æ‰‹æœºispå¹¶æ²¡æœ‰è§¦å‘é»‘åå•åˆ¤å®šï¼Œæ‰€ä»¥æœªå±è”½ã€‚ ç§»åŠ¨çš„é»‘åå•ä¸€èˆ¬æ˜¯æœ€æ¿€è¿›çš„ï¼Œå·ç§°å¢™ä¸­å¢™ï¼Œä¸è¿‡
ä¸åŒåœ°åŒºçš„åŒä¸€æœåŠ¡å•†æ”¿ç­–ä¹Ÿä¼šæœ‰å¾ˆå¤§å·®åˆ«ï¼Œä¸å¥½è¯´è°å°±ä¸€å®šå¥½ï¼Œè°å°±ä¸€å®šå·®ã€‚æ‰€ä»¥è§£å†³æ–¹æ³•å°±æ˜¯æ¢
ä¸ªç¿»å¢™æœåŠ¡å™¨å°±å¯ä»¥äº†ã€‚&lt;/p>
&lt;p>&lt;del>å†è¯´è¯´æœºåœºé—®é¢˜ï¼Œæœºåœºå¦‚æžœè¦æ‰¾çš„è¯åƒåƒä¸‡ï¼Œä¸å¤¸å¼ çš„è¯´å°å­¦ç”Ÿéƒ½èƒ½å¼€æœºåœºï¼Œæ‰€ä»¥ï¼Œå¦‚æžœçœŸ
è¦æ˜¯é€‰çš„è¯ï¼Œå¦‚æžœä½ ä¸å¤ªæ‡‚æŠ€æœ¯ï¼Œå°±é€‰å¤§æœºåœºå¥½äº†ï¼Œå¯èƒ½è´µä¸€äº›ï¼Œä½†æ˜¯ç¨³å®šæ€§æ˜¯æœ‰ä¿éšœçš„ã€‚
å¦‚æžœæ‡‚æŠ€æœ¯ï¼Œå°±ä¸å¦‚æœºåœºï¼ˆæˆ–è½¬å‘æœåŠ¡ï¼‰+è‡ªå»ºæœåŠ¡å™¨ç»“åˆçš„æ–¹å¼ï¼Œè¿™æ ·å°±èƒ½å…¼é¡¾é€Ÿåº¦å’Œéš
ç§å®‰å…¨ã€‚ å‡ ä¹Žæ‰€æœ‰çš„æœºåœºéƒ½ä¼šæœ‰å®¡æŸ¥æœºåˆ¶ï¼Œæœ€èµ·ç ä»–éœ€è¦ç›‘æŽ§æµé‡æ¥è®¡è´¹ï¼Œåªæ˜¯å®¡æŸ¥ç¨‹åº¦
å’Œéšç§æ•°æ®å¤„ç†ä¸¥è°¨åº¦æœ‰åŒºåˆ«è€Œå·²ï¼Œçœ‹å•†å®¶è‰¯å¿ƒäº†ã€‚æˆ‘ä¸ªäººæ²¡ç”¨è¿‡æœºåœºï¼Œåªèƒ½è¯´ä¸¤ä¸ªæˆ‘æ‰€çŸ¥
é“çš„æ¯”è¾ƒå¤§çš„ï¼Œjustmysocksï¼Œdler cloudï¼Œé­…å½±æžé€Ÿã€‚ å¦å¤–ï¼Œåˆ«å¤ªè¿·ä¿¡é‚£äº›æœºåœºæŽ¨èï¼ŒåŸº
æœ¬éƒ½æ˜¯æŽé¥­ã€‚ å¯¹äº†ï¼Œè¿˜æœ‰ä¸ªæˆ‘è§‰å¾—ä½ å¯ä»¥è¯•ä¸‹ï¼Œå«Shiliewã€‚ ç†ç”±æœ‰2ä¸ªï¼Œè¿™ä¸ªappçš„ä½œè€…
å°±æ˜¯å¼€æºç¿»å¢™è½¯ä»¶brookçš„ä½œè€…ï¼Œåœ¨app storeé‡Œçœ‹ç€æƒé™å¾ˆå¹²å‡€ï¼Œå¦å¤–è¿™ä¸ªä½œè€…åå­—å«
txthinkingï¼Œæœ‰ç‚¹è‡´æ•¬ç¼–ç¨‹éšæƒ³çš„æ„æ€ï¼Œæ„Ÿè§‰ä»–åº”è¯¥æœ€èµ·ç ä¸çº¢å§ã€‚ã€‚ã€‚å“ˆå“ˆå“ˆ ä½†æ˜¯è¿™
ä¸œè¥¿å¥½ç”¨ä¸å¥½ç”¨æˆ‘å°±ä¸çŸ¥é“äº†ã€‚&lt;/del>&lt;/p>
&lt;p>æœ€åŽæˆ‘å†è¯´ä¸‹å…³äºŽ@han_chineseçš„è§‚ç‚¹ï¼Œç§»åŠ¨æ•°æ®çš„å®¡æŸ¥æ˜¯æœ€å¼ºçš„ï¼Œå› ä¸ºç§»åŠ¨èœ‚çªæ•°æ®æ˜¯éœ€è¦ç²¾ç¡®è®¡
è´¹çš„ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œæ¯ä¸ªæœåŠ¡å•†éƒ½æœ‰å¾ˆå¤šå…æµé‡æœåŠ¡ï¼Œæ‰€ä»¥éœ€è¦è¯†åˆ«æµé‡å†…å®¹æ¥åˆ¤æ–­è®¡è´¹ä¸Žå¦ã€‚æ›´ç”šçš„
æ˜¯ï¼Œç§»åŠ¨æ•°æ®æµé‡éƒ½ä¼šè¢«è‡ªåŠ¨æ ‡è®°ä¸Šæœ¬æœºç”µè¯å·ç ï¼Œä»–ä»¬è¿˜ä¸“é—¨æžäº†apiæ¥å–è¿™ç§æœåŠ¡ã€‚ç®€å•æ¥è®²ï¼Œ
æ¯”å¦‚ä½ ç”¨æµé‡ä¸Šäº¬ä¸œï¼Œä½ ä¸éœ€è¦è¾“å…¥ä½ çš„ç”µè¯å·ç æ¥æ³¨å†Œæˆ–è€…ç™»å½•ï¼Œäº¬ä¸œå°±å·²ç„¶çŸ¥æ™“äº†ï¼Œå› ä¸ºäº¬ä¸œä¹°
äº†ispçš„apiæŽ¥å£ï¼Œåªéœ€è¦åœ¨ç½‘é¡µä¸­åŠ å…¥å‡ è¡Œä»£ç æ¥è°ƒç”¨è¿™ä¸ªapiï¼Œä½ ä¸€æ—¦ç”¨æ‰‹æœºæµé‡è®¿é—®è¿™ä¸ªç½‘é¡µï¼Œ
ç”µè¯å·ç å°±ä¼šè¿”å›žç»™äº¬ä¸œã€‚ æ‰€ä»¥æ— è®ºç¿»å¢™ä¸Žå¦ï¼Œéƒ½å»ºè®®åœ¨ä½¿ç”¨æ•°æ®æµé‡ä¸Šç½‘æ—¶å¼€å¯vpnæœåŠ¡æ¥åŠ å¯†å’Œ
ä¸­è½¬æ•°æ®ã€‚ å…³äºŽä¹°å›½å¤–ç”µè¯å¡æ¥ç¿»å¢™ï¼ŒåŒæ ·ä¹Ÿæ˜¯ä¸é è°±çš„ï¼Œå› ä¸ºå¤–å›½çš„simå¡æ‹¿åˆ°ä¸­å›½æ¥èƒ½ç”¨ï¼Œåªæ˜¯
æ¼«æ¸¸äº†è€Œå·²ï¼Œæ¼«æ¸¸çš„æ„æ€å°±æ˜¯ï¼Œå›½å¤–å’Œå›½å†…çš„ispç­¾äº†åˆåŒï¼Œèƒ½è®©å„è‡ªçš„ç”¨æˆ·ç§Ÿç”¨è‡ªå·±è¿™è¾¹çš„åŸºç«™çº¿
è·¯ç­‰è®¾å¤‡æ¥ä¿è¯è·¨å›½ä½¿ç”¨ï¼Œæ‰€ä»¥å³ä½¿ä¹°äº†google-fiæ¥ä¸­å›½ä½¿ç”¨ï¼Œå’Œä½ ç”¨ä¸­å›½çš„simå¡åœ¨ä¸­å›½ä½¿ç”¨ï¼Œéƒ½
æ˜¯é€šè¿‡ä¸­å›½ispçš„åŸºç«™çº¿è·¯å’ŒæœåŠ¡å™¨ã€‚å”¯ä¸€åŒºåˆ«å°±æ˜¯google-fiçš„æ‰€æœ‰è®¿é—®éƒ½ä¼šè¢«åˆ—å…¥ç™½åå•ï¼Œä½†æ˜¯æ•°
æ®æµé‡çš„å®¡è®¡æ˜¯ä¸€æ ·çš„ã€‚ä½ æ‹¿google-fiæ¥ä¸Špincongï¼Œç§»åŠ¨æœåŠ¡å•†æ˜¯çŸ¥é“ï¼Œåªæ˜¯ä¸é˜»æ‹¦è€Œå·²ï¼Œè¿™æ ·ä¸€
å®šç¨‹åº¦ä¸Šæ›´åŠ å±é™©ã€‚&lt;/p>
&lt;p>@han_chinese:
ä¸­åœ‹é›»ä¿¡å¸‚å ´çš„ç§»å‹•æ•¸æ“šç¶²çµ¡ä½¿ç”¨çš„æ˜¯è¤‡é›œçš„å¤šé‡ä¼æ¥­IPv4-NATï¼ŒåŠ ä¹‹ï¼Œç§»å‹•æ•¸æ“šç¶²çµ¡æ¥­å‹™å¾ˆè¤‡é›œã€‚æ‰€ä»¥ï¼ŒGFWå°ç§»å‹•æ•¸æ“šç¶²è·¯çš„å®¡æŸ¥è¼ƒå¼±ã€‚
å»ºè­°ç›´æŽ¥ä½¿ç”¨åœ‹å¤–æ•¸æ“šæ¥­å‹™ï¼Œåƒæ˜¯Google-Fiï¼Œä½¿ç”¨ä¸å—GFWå½±éŸ¿çš„ç§»å‹•æ•¸æ“šç¶²çµ¡ã€‚&lt;/p>
&lt;p>&lt;a href="https://pincong.rocks/question/45451">ç‚¹è¿™é‡ŒæŸ¥é˜…ä¿¡æ¯æ¥æº&lt;/a>&lt;/p></description></item><item><title>calculate_gpu_vram_for_llama3-70B</title><link>/posts/2024-06-13-calculate_gpu_vram_for_llama3-70b/</link><pubDate>Thu, 13 Jun 2024 16:11:47 +0800</pubDate><guid>/posts/2024-06-13-calculate_gpu_vram_for_llama3-70b/</guid><description>&lt;p>How many GPUs do I need to be able to serve Llama 70B? In order to answer that, you need
to know how much GPU memory will be required by the Large Language Model.&lt;/p>
&lt;p>The formula is simple:&lt;/p>
&lt;p>$$
M=\frac{(P * 4B)}{(32/Q)} * 1.2
$$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Symbol&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M&lt;/td>
&lt;td>GPU memory expressed in Gigabyte&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>P&lt;/td>
&lt;td>The amount of parameters in the model. E.g. a 7B model has 7 billion parameters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>4 bytes, expressing the bytes used for each parameter&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>There are 32 bits in 4 bytes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Q&lt;/td>
&lt;td>The amount of bits that should be used for loading the model. E.g. 16 bits, 8 bits or 4 bits.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.2&lt;/td>
&lt;td>Represents a 20% overhead of loading additional things in GPU memory.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Now let&amp;rsquo;s try out some examples.
GPU memory required for serving Llama 70B&lt;/p>
&lt;p>Let&amp;rsquo;s try it out for Llama 70B that we will load in 16 bit. The model has 70 billion parameters.&lt;/p>
&lt;p>$$
\frac{70 * 4bytes}{32/16} * 1.2=168GB
$$&lt;/p>
&lt;p>That&amp;rsquo;s quite a lot of memory. A single A100 80GB wouldn&amp;rsquo;t be enough, although 2x A100
80GB should be enough to serve the Llama 2 70B model in 16 bit mode.&lt;/p></description></item><item><title>lean_principles</title><link>/posts/mlteam101/lean_principles/</link><pubDate>Mon, 03 Jun 2024 17:59:09 +0800</pubDate><guid>/posts/mlteam101/lean_principles/</guid><description>&lt;p>Principle 1: Identify value&lt;/p>
&lt;pre>&lt;code>Determine what is most valuable to the customer and focus on maximizing that value.
&lt;/code>&lt;/pre>
&lt;p>Principle 2: Map the value stream&lt;/p>
&lt;pre>&lt;code>Identify the steps in the process that add value and eliminate those that do not.
&lt;/code>&lt;/pre>
&lt;p>Principle 3: Create flow&lt;/p>
&lt;pre>&lt;code>Streamline the process to create a smooth and continuous flow of work.
&lt;/code>&lt;/pre>
&lt;p>Principle 4: Establish pull&lt;/p>
&lt;pre>&lt;code>Use customer demand to trigger production and avoid overproduction.
&lt;/code>&lt;/pre>
&lt;p>Principle 5: Continuous improvement&lt;/p>
&lt;pre>&lt;code>Continuously strive for improvement and eliminate waste in all areas of the value
chain.
&lt;/code>&lt;/pre>
&lt;p>Value stream mapping (principle 2) is a tool that lets us visually represent all the
steps and resources involved in delivering a unit of value (e.g., a product feature) to
customers. Teams can use this tool to identify waste, work toward eliminating waste,
and improve the flow of value (principle 3).&lt;/p>
&lt;p>To map your team or productâ€™s value stream, you can follow these steps:&lt;/p>
&lt;ol>
&lt;li>Identify the product or service being mapped. This could be a single product or1.
an entire process.&lt;/li>
&lt;li>Identify the current state map. Create a visual representation of the current2.
process, including all steps and materials (including time and labor) involved
from raw materials to finished product.&lt;/li>
&lt;li>Identify value-added and non-value-added activities. Determine which steps add3.
value to the product or service and which do not.&lt;/li>
&lt;li>Identify waste. Look for areas of overproduction, waiting, defects, overprocessâ€4.
ing, excess inventory, unnecessary motion, excess transport, unnecessary use of
raw materials, and unnecessary effort.&lt;/li>
&lt;li>Create a future state map. Based on the analysis of the current state map, redesign5.
the process to eliminate waste and create a more efficient flow of materials and
information.&lt;/li>
&lt;li>Implement changes. Put the redesigned process into practice and continuously6.
monitor and improve (principle 5).&lt;/li>
&lt;/ol></description></item><item><title>EffectiveML 01: Delivering successful ML projects</title><link>/posts/mlteam101/feedback_loops_and_times_to_feedback/</link><pubDate>Mon, 03 Jun 2024 17:12:37 +0800</pubDate><guid>/posts/mlteam101/feedback_loops_and_times_to_feedback/</guid><description>&lt;p>Table: Comparison of feedback mechanisms and time-to-feedback in high- and low-effectiveness environments&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Task&lt;/th>
&lt;th>High-effectiveness environment&lt;/th>
&lt;th>Low-effectiveness environment&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Testing if code changes worked as expected&lt;/td>
&lt;td>Automated testing (~ seconds to minutes)&lt;/td>
&lt;td>Manual testing (~ minutes to hours)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Testing if training pipeline works end to end&lt;/td>
&lt;td>Training smoke test (~ 1 minute)&lt;/td>
&lt;td>Full model training (~ minutes to hours, depending on model architecture)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Getting feedback on code changes&lt;/td>
&lt;td>Pair programming (~ seconds to minutes)&lt;/td>
&lt;td>Pull request reivews (~ hours to days)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Understanding if application is working as expected in production&lt;/td>
&lt;td>Monitoring in production (~ seconds - as it happens)&lt;/td>
&lt;td>Customer complaints (~ days, or longer if not directly reported)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>How teams can move from a low-effectiveness environment to a high-effectiveness environment?
MLOps is not enough (MLOps and ML platforms arenâ€™t going to write comprehensive tests
for you, talk to users for you, or reduce the negative impacts of team silos for you.)&lt;/p>
&lt;p>Delivering successful ML projects requires a multiâ€disciplinary approach across these
five disciplines: product, software engineering, data, ML, and delivery:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>software engineering:&lt;/p>
&lt;ul>
&lt;li>code design&lt;/li>
&lt;li>automated testing&lt;/li>
&lt;li>refactoring&lt;/li>
&lt;li>dependency management&lt;/li>
&lt;li>code editor efficency&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>data:&lt;/p>
&lt;ul>
&lt;li>data quality&lt;/li>
&lt;li>data engineering&lt;/li>
&lt;li>data security and privacy&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Machine learning:&lt;/p>
&lt;ul>
&lt;li>ML techniques&lt;/li>
&lt;li>model evaluation&lt;/li>
&lt;li>ML governance&lt;/li>
&lt;li>MLOps&lt;/li>
&lt;li>CI/CD&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Product:&lt;/p>
&lt;ul>
&lt;li>product thinking&lt;/li>
&lt;li>responsible AI&lt;/li>
&lt;li>human-centered AI&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Delivery:&lt;/p>
&lt;ul>
&lt;li>lean delivery&lt;/li>
&lt;li>team topologies&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>tech interview prepare (for my resume)</title><link>/posts/notes4resume/</link><pubDate>Mon, 27 May 2024 20:43:25 +0800</pubDate><guid>/posts/notes4resume/</guid><description>&lt;h1 id="è‡ªæˆ‘ä»‹ç»" >
&lt;div>
&lt;a href="#%e8%87%aa%e6%88%91%e4%bb%8b%e7%bb%8d">
##
&lt;/a>
è‡ªæˆ‘ä»‹ç»
&lt;/div>
&lt;/h1>
&lt;blockquote>
&lt;p>åœ¨è‡ªæˆ‘ä»‹ç»æ—¶ï¼Œç¡®ä¿ä½ æåˆ°çš„é¡¹ç›®å’ŒæŠ€èƒ½ä¸Žä½ ç”³è¯·çš„èŒä½ç´§å¯†ç›¸å…³ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°å±•ç¤ºä½ çš„ä¸“ä¸š
èƒ½åŠ›å’Œå¯¹èŒä½çš„é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œä¿æŒè‡ªä¿¡å’Œçƒ­æƒ…ï¼Œè®©é¢è¯•å®˜æ„Ÿå—åˆ°ä½ å¯¹å·¥ä½œå’Œå›¢é˜Ÿçš„æ‰¿è¯ºã€‚&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>å°Šæ•¬çš„é¢è¯•å®˜ï¼Œæ‚¨å¥½ï¼&lt;/p>
&lt;p>æˆ‘å«èŒƒæ˜ŽåŽï¼Œæ‹¥æœ‰6å¹´åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„å·¥ä½œç»éªŒã€‚æˆ‘äºŽ2017å¹´ç¡•å£«æ¯•ä¸šäºŽä¸­å±±å¤§å­¦ï¼Œä¸“ä¸šæ˜¯ç”Ÿæ€å­¦ï¼Œ
åœ¨æ¤è¢«åŠ¨æ€å’Œæ¼”æ›¿æ–¹é¢çš„ç ”ç©¶å·¥ä½œï¼Œä¸ºæˆ‘åœ¨å®žéªŒè®¾è®¡ã€ç»Ÿè®¡åˆ†æžä»¥åŠæ•°æ®æŒ–æŽ˜æ–¹é¢æ‰“ä¸‹äº†æ‰Žå®žçš„åŸºç¡€ã€‚
è¿‡åŽ»çš„6å¹´é‡Œï¼Œæˆ‘åœ¨å·¥ä½œä¸­ä¸€ç›´è‡´åŠ›äºŽå°†æœºå™¨å­¦ä¹ æŠ€æœ¯åº”ç”¨äºŽè§£å†³å®žé™…é—®é¢˜ï¼Œå¹¶å–å¾—äº†ä¸€äº›çš„æˆæžœã€‚&lt;/p>
&lt;p>æˆ‘åœ¨ç›®å‰å…¬å¸æ‹…ä»»é«˜çº§æ•°æ®æŒ–æŽ˜å·¥ç¨‹å¸ˆï¼ŒæœŸé—´æˆ‘ä¸»å¯¼äº†å¤šä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®çš„å¼€å‘å’Œäº¤ä»˜ã€‚
åœ¨æŠ€æœ¯å±‚é¢ï¼Œæˆ‘æ¯”è¾ƒæ“…é•¿ç»“åˆæ•°æ®çŽ°çŠ¶æ¥å†³å®šæœºå™¨å­¦ä¹ æˆ–æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æ­é…è§£å†³å®žé™…é—®é¢˜ï¼Œ
ç†Ÿæ‚‰æ•´ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®çš„å¼€å‘æµç¨‹ï¼Œä»Žé¡¹ç›®è°ƒç ”ã€æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹åˆ°æ¨¡åž‹è®­ç»ƒå’Œéƒ¨ç½²ï¼Œ
æˆ‘éƒ½æœ‰æ·±å…¥çš„ç†è§£å’Œå®žè·µã€‚åœ¨è¿™äº›é¡¹ç›®çš„å®žæˆ˜ä¸­ï¼Œä¸ä»…æå‡äº†ä¸ªäººçš„æŠ€æœ¯æ·±åº¦å’Œå¸¦é˜Ÿèƒ½åŠ›ï¼Œ
åŒæ—¶ä¹Ÿå–å¾—äº†æŽˆæƒçš„å‘æ˜Žä¸“åˆ©ã€è½¯ä»¶è‘—ä½œã€åœ°æ–¹æ ‡å‡†ç­‰æˆæžœã€‚&lt;/p>
&lt;p>é™¤äº†æŠ€æœ¯ä¸“é•¿ï¼Œæˆ‘è¿˜æ˜¯ä¸€ä¸ªæ³¨é‡å›¢é˜Ÿåˆä½œå’ŒåŠ¡å®žè´Ÿè´£çš„äººã€‚æˆ‘ç›¸ä¿¡ï¼Œæˆ‘çš„ä¸“ä¸šæŠ€èƒ½å’Œä¸°å¯Œç»éªŒï¼Œ
èƒ½å¤Ÿä¸ºè´µå…¬å¸å¸¦æ¥ç›´æŽ¥çš„ä»·å€¼ã€‚&lt;/p>
&lt;p>æˆ‘å¯¹è´µå…¬å¸åœ¨æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ æ–¹é¢çš„å·¥ä½œæ„Ÿå…´è¶£ï¼Œæˆ‘æœŸå¾…èƒ½å¤ŸåŠ å…¥è´µå…¬å¸ï¼Œå¹¶ä¸Žå›¢é˜Ÿä¸€èµ·è§£å†³æ›´å¤šæœ‰è¶£çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚&lt;/p>
&lt;p>æ„Ÿè°¢æ‚¨ç»™æˆ‘è¿™æ¬¡é¢è¯•çš„æœºä¼šï¼Œæˆ‘æœŸå¾…åœ¨æŽ¥ä¸‹æ¥çš„è®¨è®ºä¸­åˆ†äº«æ›´å¤šæˆ‘çš„ç»éªŒå’Œæƒ³æ³•ã€‚è°¢è°¢ï¼&lt;/p>
&lt;hr>
&lt;h1 id="project-01----nlp" >
&lt;div>
&lt;a href="#project-01----nlp">
##
&lt;/a>
Project 01 &amp;ndash; NLP
&lt;/div>
&lt;/h1>
&lt;h2 id="sklearn-randomforest-model" >
&lt;div>
&lt;a href="#sklearn-randomforest-model">
#
&lt;/a>
sklearn randomforest model
&lt;/div>
&lt;/h2>
&lt;p>å½“è°ˆåˆ°éšæœºæ£®æž—æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£å®ƒçš„åŸºç¡€ç®—æ³•ï¼šå†³ç­–æ ‘ã€‚éšæœºæ£®æž—æ˜¯åŸºäºŽå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ æ–¹æ³•ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬é¦–å…ˆæ¥äº†è§£å†³ç­–æ ‘çš„åŸºæœ¬ç®—æ³•ï¼Œç„¶åŽå†æ·±å…¥æŽ¢è®¨éšæœºæ£®æž—ã€‚&lt;/p>
&lt;h3 id="1-å†³ç­–æ ‘ç®—æ³•" >
&lt;div>
&lt;a href="#1-%e5%86%b3%e7%ad%96%e6%a0%91%e7%ae%97%e6%b3%95">
##
&lt;/a>
1. å†³ç­–æ ‘ç®—æ³•:
&lt;/div>
&lt;/h3>
&lt;h4 id="11-cartç®—æ³•-classification-and-regression-trees" >
&lt;div>
&lt;a href="#11-cart%e7%ae%97%e6%b3%95-classification-and-regression-trees">
###
&lt;/a>
1.1 CARTç®—æ³• (Classification and Regression Trees):
&lt;/div>
&lt;/h4>
&lt;p>CARTç®—æ³•æ˜¯ä¸€ç§ç”¨äºŽæž„å»ºåˆ†ç±»å’Œå›žå½’æ ‘çš„å†³ç­–æ ‘ç®—æ³•ã€‚å®ƒé€šè¿‡å¯¹æ•°æ®é›†é€’å½’åœ°è¿›è¡ŒäºŒåˆ†æ¥æž„å»ºå†³ç­–æ ‘ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ç‰¹å¾é€‰æ‹©&lt;/strong>ï¼šå¯¹äºŽåˆ†ç±»é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨åŸºå°¼æŒ‡æ•°ï¼ˆGini indexï¼‰æˆ–ä¿¡æ¯å¢žç›Šï¼ˆInformation Gainï¼‰æ¥é€‰æ‹©æœ€ä½³çš„ç‰¹å¾è¿›è¡Œåˆ†è£‚ï¼›å¯¹äºŽå›žå½’é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨å¹³æ–¹è¯¯å·®æ¥é€‰æ‹©æœ€ä½³çš„ç‰¹å¾ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>èŠ‚ç‚¹åˆ†è£‚&lt;/strong>ï¼šæ ¹æ®é€‰æ‹©çš„ç‰¹å¾ï¼Œå°†æ•°æ®é›†åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œä½¿å¾—æ¯ä¸ªå­é›†çš„æ ·æœ¬å±žäºŽåŒä¸€ç±»åˆ«ï¼ˆå¯¹äºŽåˆ†ç±»é—®é¢˜ï¼‰æˆ–å…·æœ‰ç›¸ä¼¼çš„å›žå½’å€¼ï¼ˆå¯¹äºŽå›žå½’é—®é¢˜ï¼‰ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>é€’å½’&lt;/strong>ï¼šå¯¹æ¯ä¸ªå­é›†é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼Œå¦‚è¾¾åˆ°æœ€å¤§æ·±åº¦ã€èŠ‚ç‚¹ä¸­æ ·æœ¬æ•°å°äºŽæŸä¸ªé˜ˆå€¼æˆ–å…¶ä»–é¢„å®šä¹‰æ¡ä»¶ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>å‰ªæž&lt;/strong>ï¼šä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå¯ä»¥å¯¹ç”Ÿæˆçš„æ ‘è¿›è¡Œå‰ªæžï¼Œå³ç§»é™¤ä¸€äº›èŠ‚ç‚¹æ¥ç®€åŒ–æ ‘çš„ç»“æž„ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="2-éšæœºæ£®æž—ç®—æ³•" >
&lt;div>
&lt;a href="#2-%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e7%ae%97%e6%b3%95">
##
&lt;/a>
2. éšæœºæ£®æž—ç®—æ³•:
&lt;/div>
&lt;/h3>
&lt;h4 id="21-æž„å»ºéšæœºæ£®æž—" >
&lt;div>
&lt;a href="#21-%e6%9e%84%e5%bb%ba%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97">
###
&lt;/a>
2.1 æž„å»ºéšæœºæ£®æž—:
&lt;/div>
&lt;/h4>
&lt;p>éšæœºæ£®æž—æ˜¯é€šè¿‡æž„å»ºå¤šæ£µå†³ç­–æ ‘å¹¶å°†å®ƒä»¬é›†æˆèµ·æ¥æ¥å®Œæˆçš„ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>éšæœºæŠ½æ ·&lt;/strong>ï¼šä»ŽåŽŸå§‹è®­ç»ƒé›†ä¸­éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬ï¼ˆæœ‰æ”¾å›žæŠ½æ ·ï¼‰æ¥æž„å»ºæ¯æ£µå†³ç­–æ ‘çš„è®­ç»ƒé›†ã€‚è¿™æ ·å¯ä»¥ä¿è¯æ¯æ£µæ ‘çš„è®­ç»ƒé›†ç•¥æœ‰å·®å¼‚ï¼Œå¢žåŠ äº†æ¨¡åž‹çš„å¤šæ ·æ€§ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>éšæœºç‰¹å¾é€‰æ‹©&lt;/strong>ï¼šå¯¹äºŽæ¯æ£µæ ‘çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œåœ¨é€‰æ‹©åˆ†å‰²ç‰¹å¾æ—¶ï¼Œéšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç‰¹å¾æ¥è¿›è¡Œè¯„ä¼°ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ£µæ ‘çš„åˆ†è£‚è¿‡ç¨‹ä¹Ÿæœ‰æ‰€å·®å¼‚ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ç‹¬ç«‹æž„å»º&lt;/strong>ï¼šæ¯æ£µæ ‘éƒ½æ˜¯ç‹¬ç«‹æž„å»ºçš„ï¼Œæ²¡æœ‰ä»»ä½•å…³è”ã€‚è¿™æ„å‘³ç€å¯ä»¥å¹¶è¡Œåœ°æž„å»ºå¤šæ£µæ ‘ï¼Œæé«˜äº†è®­ç»ƒæ•ˆçŽ‡ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="22-é›†æˆå†³ç­–æ ‘" >
&lt;div>
&lt;a href="#22-%e9%9b%86%e6%88%90%e5%86%b3%e7%ad%96%e6%a0%91">
###
&lt;/a>
2.2 é›†æˆå†³ç­–æ ‘:
&lt;/div>
&lt;/h4>
&lt;p>æž„å»ºå¤šæ£µå†³ç­–æ ‘åŽï¼Œéšæœºæ£®æž—é‡‡ç”¨ä¸åŒçš„æ–¹å¼æ¥é›†æˆå®ƒä»¬çš„é¢„æµ‹ç»“æžœï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;strong>åˆ†ç±»ä»»åŠ¡&lt;/strong>ï¼šé‡‡ç”¨æŠ•ç¥¨çš„æ–¹å¼ï¼Œå³æ¯æ£µæ ‘æŠ•ç¥¨é€‰æ‹©æœ€ç»ˆçš„ç±»åˆ«ã€‚&lt;/li>
&lt;li>&lt;strong>å›žå½’ä»»åŠ¡&lt;/strong>ï¼šé‡‡ç”¨å¹³å‡å€¼çš„æ–¹å¼ï¼Œå³å¤šæ£µæ ‘çš„é¢„æµ‹ç»“æžœå–å¹³å‡å€¼ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="æ€»ç»“" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93">
##
&lt;/a>
æ€»ç»“:
&lt;/div>
&lt;/h3>
&lt;p>éšæœºæ£®æž—æ˜¯ä¸€ç§å¼ºå¤§çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºŽå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ ã€‚é€šè¿‡åˆ©ç”¨å†³ç­–æ ‘çš„éšæœºæ€§å’Œé›†æˆç­–ç•¥ï¼Œéšæœºæ£®æž—èƒ½å¤Ÿæœ‰æ•ˆåœ°åº”å¯¹åˆ†ç±»å’Œå›žå½’é—®é¢˜ï¼Œå¹¶åœ¨è®¸å¤šå®žé™…åº”ç”¨ä¸­è¡¨çŽ°ä¼˜å¼‚ã€‚&lt;/p>
&lt;p>åœ¨ CART (Classification and Regression Trees) ç®—æ³•ä¸­ï¼ŒèŠ‚ç‚¹çš„åˆ†è£‚ä¾æ®æ˜¯åŸºäºŽè´ªå¿ƒç®—æ³•ã€‚CART ç®—æ³•é€šè¿‡è´ªå¿ƒåœ°é€‰æ‹©æ¯æ¬¡åˆ†è£‚æ—¶èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦å‡å°‘ä¸çº¯åº¦ï¼ˆå¯¹äºŽåˆ†ç±»é—®é¢˜ï¼‰æˆ–è€…æœ€å°åŒ–è¯¯å·®ï¼ˆå¯¹äºŽå›žå½’é—®é¢˜ï¼‰çš„ç‰¹å¾æ¥è¿›è¡ŒèŠ‚ç‚¹çš„åˆ†è£‚ã€‚è¿™ç§è´ªå¿ƒç­–ç•¥ä¿è¯äº†åœ¨æ¯ä¸ªèŠ‚ç‚¹åˆ†è£‚æ—¶éƒ½é€‰æ‹©äº†æœ€ä¼˜çš„ç‰¹å¾æ¥è¿›è¡Œåˆ†è£‚ã€‚&lt;/p>
&lt;h3 id="èŠ‚ç‚¹åˆ†è£‚çš„ä¾æ®" >
&lt;div>
&lt;a href="#%e8%8a%82%e7%82%b9%e5%88%86%e8%a3%82%e7%9a%84%e4%be%9d%e6%8d%ae">
##
&lt;/a>
èŠ‚ç‚¹åˆ†è£‚çš„ä¾æ®ï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="å¯¹äºŽåˆ†ç±»é—®é¢˜" >
&lt;div>
&lt;a href="#%e5%af%b9%e4%ba%8e%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98">
###
&lt;/a>
å¯¹äºŽåˆ†ç±»é—®é¢˜ï¼š
&lt;/div>
&lt;/h4>
&lt;p>åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼ŒCART ç®—æ³•é€šå¸¸ä½¿ç”¨ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ä½œä¸ºèŠ‚ç‚¹åˆ†è£‚çš„ä¾æ®ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>åŸºå°¼æŒ‡æ•° (Gini index)&lt;/strong>ï¼šåŸºå°¼æŒ‡æ•°è¡¡é‡äº†ä»Žä¸€ä¸ªæ•°æ®é›†ä¸­éšæœºæŠ½å–ä¸¤ä¸ªæ ·æœ¬ï¼Œå®ƒä»¬ç±»åˆ«ä¸ä¸€è‡´çš„æ¦‚çŽ‡ã€‚å…·ä½“åœ°ï¼Œå¯¹äºŽä¸€ä¸ªèŠ‚ç‚¹ $t$ï¼ŒåŸºå°¼æŒ‡æ•°å¯ä»¥è®¡ç®—ä¸ºï¼š&lt;/p>
&lt;p>$ Gini(t) = 1 - \sum_{i=1}^{c} p(i|t)^2 $&lt;/p>
&lt;p>å…¶ä¸­ï¼Œ$c$ æ˜¯ç±»åˆ«çš„æ•°é‡ï¼Œ$p(i|t)$ æ˜¯åœ¨èŠ‚ç‚¹ $t$ ä¸­å±žäºŽç±»åˆ« $i$ çš„æ ·æœ¬çš„æ¯”ä¾‹ã€‚é€‰æ‹©èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦é™ä½ŽåŸºå°¼æŒ‡æ•°çš„ç‰¹å¾æ¥è¿›è¡ŒèŠ‚ç‚¹åˆ†è£‚ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ä¿¡æ¯å¢žç›Š (Information Gain)&lt;/strong>ï¼šä¿¡æ¯å¢žç›Šè¡¡é‡äº†åœ¨æŸä¸ªç‰¹å¾çš„æ¡ä»¶ä¸‹ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºä¸åŒç±»åˆ«åŽï¼Œå¸¦æ¥çš„ä¸ç¡®å®šæ€§å‡å°‘çš„ç¨‹åº¦ã€‚å…·ä½“åœ°ï¼Œå¯¹äºŽä¸€ä¸ªèŠ‚ç‚¹ $t$ å’Œä¸€ä¸ªç‰¹å¾ $A$ï¼Œä¿¡æ¯å¢žç›Šå¯ä»¥è®¡ç®—ä¸ºï¼š&lt;/p>
&lt;p>$ IG(t, A) = H(t) - \sum_{v \in Values(A)} \frac{|t_v|}{|t|} \cdot H(t_v) $&lt;/p>
&lt;p>å…¶ä¸­ï¼Œ$H(t)$ æ˜¯èŠ‚ç‚¹ $t$ çš„ç†µï¼Œ$Values(A)$ æ˜¯ç‰¹å¾ $A$ çš„å–å€¼é›†åˆï¼Œ$t_v$ æ˜¯åœ¨ç‰¹å¾ $A$ ä¸Šå–å€¼ä¸º $v$ çš„æ ·æœ¬é›†åˆï¼Œ$H(t_v)$ æ˜¯æ ·æœ¬é›†åˆ $t_v$ çš„ç†µã€‚é€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–ä¿¡æ¯å¢žç›Šçš„ç‰¹å¾æ¥è¿›è¡ŒèŠ‚ç‚¹åˆ†è£‚ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="å¯¹äºŽå›žå½’é—®é¢˜" >
&lt;div>
&lt;a href="#%e5%af%b9%e4%ba%8e%e5%9b%9e%e5%bd%92%e9%97%ae%e9%a2%98">
###
&lt;/a>
å¯¹äºŽå›žå½’é—®é¢˜ï¼š
&lt;/div>
&lt;/h4>
&lt;p>åœ¨å›žå½’é—®é¢˜ä¸­ï¼ŒCART ç®—æ³•é€šå¸¸ä½¿ç”¨å¹³æ–¹è¯¯å·® (Mean Squared Error, MSE) ä½œä¸ºèŠ‚ç‚¹åˆ†è£‚çš„ä¾æ®ã€‚é€‰æ‹©èƒ½å¤Ÿæœ€å°åŒ–èŠ‚ç‚¹åˆ†è£‚åŽæ ·æœ¬çš„å¹³æ–¹è¯¯å·®çš„ç‰¹å¾æ¥è¿›è¡Œåˆ†è£‚ã€‚&lt;/p>
&lt;h3 id="æ€»ç»“-1" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-1">
##
&lt;/a>
æ€»ç»“ï¼š
&lt;/div>
&lt;/h3>
&lt;p>CART ç®—æ³•åœ¨èŠ‚ç‚¹åˆ†è£‚æ—¶é‡‡ç”¨è´ªå¿ƒç®—æ³•ï¼Œé€‰æ‹©èƒ½å¤Ÿæœ€å¤§ç¨‹åº¦å‡å°‘ä¸çº¯åº¦ï¼ˆåˆ†ç±»é—®é¢˜ï¼‰æˆ–è€…æœ€å°åŒ–è¯¯å·®ï¼ˆå›žå½’é—®é¢˜ï¼‰çš„ç‰¹å¾æ¥è¿›è¡Œåˆ†è£‚ã€‚è¿™ç§è´ªå¿ƒç­–ç•¥ä¿è¯äº†æ¯æ¬¡åˆ†è£‚éƒ½é€‰æ‹©äº†æœ€ä¼˜çš„ç‰¹å¾ï¼Œä»¥æž„å»ºå‡ºå°½å¯èƒ½ç®€å•ä¸”æœ‰æ•ˆçš„å†³ç­–æ ‘ã€‚&lt;/p>
&lt;p>å½“è°ˆåˆ°åŸºäºŽå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ æ—¶ï¼Œé™¤äº†éšæœºæ£®æž—ï¼Œè¿˜æœ‰ä¸€ç§é‡è¦çš„æ–¹æ³•æ˜¯æå‡æ ‘ï¼ˆBoostingï¼‰ã€‚æå‡æ ‘æ˜¯ä¸€ç§è¿­ä»£çš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¸²è¡Œåœ°æž„å»ºä¸€ç³»åˆ—å†³ç­–æ ‘æ¥é€æ­¥æå‡æ¨¡åž‹çš„æ€§èƒ½ã€‚ä¸‹é¢æˆ‘ä¼šè¯¦ç»†ä»‹ç»æå‡æ ‘çš„åŽŸç†å’Œå®žçŽ°æ–¹å¼ã€‚&lt;/p>
&lt;h3 id="æå‡æ ‘çš„åŽŸç†" >
&lt;div>
&lt;a href="#%e6%8f%90%e5%8d%87%e6%a0%91%e7%9a%84%e5%8e%9f%e7%90%86">
##
&lt;/a>
æå‡æ ‘çš„åŽŸç†ï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="1-åŸºæœ¬æ€æƒ³" >
&lt;div>
&lt;a href="#1-%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3">
###
&lt;/a>
1. åŸºæœ¬æ€æƒ³ï¼š
&lt;/div>
&lt;/h4>
&lt;p>æå‡æ ‘çš„åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡è®­ç»ƒä¸€ç³»åˆ—å¼±å­¦ä¹ å™¨ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰ï¼Œç„¶åŽå°†å®ƒä»¬ç»„åˆèµ·æ¥æž„æˆä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡åž‹ã€‚æ¯ä¸ªå¼±å­¦ä¹ å™¨éƒ½ä¸“æ³¨äºŽçº æ­£ä¹‹å‰æ¨¡åž‹çš„é”™è¯¯ï¼Œå› æ­¤åœ¨æž„å»ºè¿‡ç¨‹ä¸­ä¼šå…³æ³¨ä¹‹å‰æ¨¡åž‹é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ã€‚&lt;/p>
&lt;h4 id="2-ç®—æ³•æµç¨‹" >
&lt;div>
&lt;a href="#2-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b">
###
&lt;/a>
2. ç®—æ³•æµç¨‹ï¼š
&lt;/div>
&lt;/h4>
&lt;p>æå‡æ ‘çš„ç®—æ³•æµç¨‹å¦‚ä¸‹ï¼š&lt;/p>
&lt;ol>
&lt;li>åˆå§‹åŒ–æ¨¡åž‹ä¸ºä¸€ä¸ªå¸¸æ•°å€¼ï¼Œé€šå¸¸ä¸ºç›®æ ‡å˜é‡çš„å‡å€¼ï¼ˆå¯¹äºŽå›žå½’é—®é¢˜ï¼‰æˆ–è€…æ˜¯ç±»åˆ«çš„å…ˆéªŒæ¦‚çŽ‡ï¼ˆå¯¹äºŽåˆ†ç±»é—®é¢˜ï¼‰ã€‚&lt;/li>
&lt;li>è¿­ä»£åœ°è®­ç»ƒå†³ç­–æ ‘ï¼Œæ¯æ¬¡è®­ç»ƒéƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„å¼±å­¦ä¹ å™¨ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œç®—æ³•ä¼šè®¡ç®—å½“å‰æ¨¡åž‹çš„æ®‹å·®ï¼ˆå¯¹äºŽå›žå½’é—®é¢˜ï¼‰æˆ–è€…æ¢¯åº¦ï¼ˆå¯¹äºŽåˆ†ç±»é—®é¢˜ï¼‰ï¼Œç„¶åŽè®­ç»ƒä¸€ä¸ªæ–°çš„å†³ç­–æ ‘æ¥æ‹Ÿåˆè¿™äº›æ®‹å·®æˆ–è€…æ¢¯åº¦ã€‚&lt;/li>
&lt;li>å°†æ–°ç”Ÿæˆçš„å†³ç­–æ ‘åŠ åˆ°æ¨¡åž‹ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„å­¦ä¹ çŽ‡æ¥ç¼“è§£æ¯æ£µæ ‘çš„å½±å“ã€‚&lt;/li>
&lt;li>é‡å¤è¿­ä»£æ­¥éª¤2å’Œæ­¥éª¤3ï¼Œç›´åˆ°è¾¾åˆ°é¢„å…ˆè®¾å®šçš„è¿­ä»£æ¬¡æ•°æˆ–è€…æ¨¡åž‹çš„æ€§èƒ½è¾¾åˆ°æŸä¸ªé˜ˆå€¼ä¸ºæ­¢ã€‚&lt;/li>
&lt;/ol>
&lt;h4 id="3-åŠ æ³•æ¨¡åž‹" >
&lt;div>
&lt;a href="#3-%e5%8a%a0%e6%b3%95%e6%a8%a1%e5%9e%8b">
###
&lt;/a>
3. åŠ æ³•æ¨¡åž‹ï¼š
&lt;/div>
&lt;/h4>
&lt;p>æå‡æ ‘çš„æœ€ç»ˆæ¨¡åž‹æ˜¯ä¸€ä¸ªåŠ æ³•æ¨¡åž‹ï¼Œå³å¤šä¸ªå¼±å­¦ä¹ å™¨çš„åŠ æƒæ±‚å’Œã€‚é€šè¿‡è¿­ä»£è®­ç»ƒï¼Œæ¯ä¸ªå¼±å­¦ä¹ å™¨éƒ½ä¼šå¯¹æ¨¡åž‹è¿›è¡Œä¸€å®šçš„ä¿®æ­£ï¼Œæœ€ç»ˆç»„åˆèµ·æ¥æž„æˆä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡åž‹ã€‚&lt;/p>
&lt;h3 id="å®žçŽ°" >
&lt;div>
&lt;a href="#%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
å®žçŽ°ï¼š
&lt;/div>
&lt;/h3>
&lt;p>æå‡æ ‘çš„å®žçŽ°é€šå¸¸é‡‡ç”¨æ¢¯åº¦æå‡ç®—æ³•ï¼ˆGradient Boostingï¼‰ï¼Œå…¶ä¸­æœ€å¸¸è§çš„æ˜¯æ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGradient Boosting Decision Treesï¼ŒGBDTï¼‰ã€‚&lt;/p>
&lt;p>GBDT ç®—æ³•çš„å…³é”®æ­¥éª¤åŒ…æ‹¬è®¡ç®—æ®‹å·®æˆ–è€…æ¢¯åº¦ã€è®­ç»ƒå†³ç­–æ ‘ä»¥æ‹Ÿåˆæ®‹å·®æˆ–è€…æ¢¯åº¦ã€ç¡®å®šå­¦ä¹ çŽ‡ç­‰ã€‚GBDT é€šè¿‡ä¸æ–­åœ°è¿­ä»£è®­ç»ƒå†³ç­–æ ‘æ¥é€æ­¥ä¼˜åŒ–æ¨¡åž‹ï¼Œç›´åˆ°è¾¾åˆ°ä¸€å®šçš„è¿­ä»£æ¬¡æ•°æˆ–è€…è¾¾åˆ°ä¸€å®šçš„æ€§èƒ½æŒ‡æ ‡ã€‚&lt;/p>
&lt;h3 id="æ€»ç»“-2" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-2">
##
&lt;/a>
æ€»ç»“ï¼š
&lt;/div>
&lt;/h3>
&lt;p>æå‡æ ‘æ˜¯ä¸€ç§åŸºäºŽå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£è®­ç»ƒä¸€ç³»åˆ—å†³ç­–æ ‘æ¥é€æ­¥æå‡æ¨¡åž‹çš„æ€§èƒ½ã€‚ç›¸è¾ƒäºŽéšæœºæ£®æž—ï¼Œæå‡æ ‘é€šå¸¸ä¼šäº§ç”Ÿæ›´åŠ ç²¾ç¡®çš„é¢„æµ‹ï¼Œä½†éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”å¯¹å¼‚å¸¸å€¼å’Œå™ªå£°æ•°æ®æ›´æ•æ„Ÿã€‚&lt;/p>
&lt;h2 id="mlp-model" >
&lt;div>
&lt;a href="#mlp-model">
#
&lt;/a>
MLP model
&lt;/div>
&lt;/h2>
&lt;p>MLPï¼Œå³å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMultilayer Perceptronï¼‰ï¼Œæ˜¯ä¸€ç§åŸºæœ¬çš„äººå·¥ç¥žç»ç½‘ç»œæ¨¡åž‹ã€‚å®ƒç”±å¤šå±‚ç¥žç»å…ƒç»„æˆï¼Œæ¯ä¸€å±‚éƒ½ä¸Žä¸‹ä¸€å±‚å…¨è¿žæŽ¥ã€‚MLPæ˜¯ä¸€ç§å‰é¦ˆç¥žç»ç½‘ç»œï¼Œæ„å‘³ç€ä¿¡æ¯åªèƒ½ä»Žè¾“å…¥å±‚å‘è¾“å‡ºå±‚ä¼ é€’ï¼Œä¸ä¼šå­˜åœ¨å¾ªçŽ¯è¿žæŽ¥ã€‚&lt;/p>
&lt;p>ä¸‹é¢æ˜¯MLPçš„ä¸€äº›åŸºæœ¬åŽŸç†ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>ç¥žç»å…ƒï¼ˆPerceptronï¼‰&lt;/strong>ï¼š
åœ¨MLPä¸­ï¼Œæ¯ä¸ªç¥žç»å…ƒéƒ½æ˜¯ä¸€ä¸ªç®€å•çš„è®¡ç®—å•å…ƒã€‚å®ƒæŽ¥æ”¶æ¥è‡ªå‰ä¸€å±‚çš„è¾“å…¥ä¿¡å·ï¼Œå°†è¿™äº›ä¿¡å·åŠ æƒæ±‚å’Œï¼Œå¹¶é€šè¿‡æ¿€æ´»å‡½æ•°äº§ç”Ÿè¾“å‡ºã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>å¤šå±‚ç»“æž„&lt;/strong>ï¼š
MLPç”±å¤šä¸ªå±‚ç»„æˆï¼Œå…¸åž‹çš„MLPåŒ…æ‹¬è¾“å…¥å±‚ã€è‡³å°‘ä¸€ä¸ªéšè—å±‚å’Œè¾“å‡ºå±‚ã€‚è¾“å…¥å±‚æŽ¥æ”¶åŽŸå§‹æ•°æ®ï¼Œéšè—å±‚å¯¹è¾“å…¥æ•°æ®è¿›è¡Œç‰¹å¾æå–å’Œè½¬æ¢ï¼Œè¾“å‡ºå±‚ç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ç»“æžœã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æƒé‡å’Œåç½®&lt;/strong>ï¼š
åœ¨MLPä¸­ï¼Œæ¯ä¸ªè¿žæŽ¥éƒ½æœ‰ä¸€ä¸ªç›¸å…³è”çš„æƒé‡ï¼Œç”¨äºŽæŽ§åˆ¶ä¿¡å·ä¼ é€’çš„å¼ºåº¦å’Œæ–¹å‘ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªç¥žç»å…ƒè¿˜æœ‰ä¸€ä¸ªåç½®ï¼Œç”¨äºŽè°ƒæ•´ç¥žç»å…ƒçš„æ¿€æ´»é˜ˆå€¼ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ¿€æ´»å‡½æ•°&lt;/strong>ï¼š
åœ¨ç¥žç»å…ƒä¸­ï¼Œæ¿€æ´»å‡½æ•°å†³å®šäº†ç¥žç»å…ƒè¾“å‡ºçš„éžçº¿æ€§å…³ç³»ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬Sigmoidã€ReLUï¼ˆRectified Linear Unitï¼‰ã€tanhç­‰ï¼Œå®ƒä»¬åœ¨ä¸åŒæƒ…å†µä¸‹å…·æœ‰ä¸åŒçš„ä¼˜åŠ¿ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>å‰å‘ä¼ æ’­&lt;/strong>ï¼š
åœ¨MLPä¸­ï¼Œæ•°æ®ä»Žè¾“å…¥å±‚å¼€å§‹ä¼ æ’­ï¼Œç»è¿‡ä¸€ç³»åˆ—çš„åŠ æƒæ±‚å’Œå’Œæ¿€æ´»å‡½æ•°å¤„ç†ï¼Œä¸€ç›´ä¼ æ’­åˆ°è¾“å‡ºå±‚ï¼Œç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ç»“æžœã€‚è¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºå‰å‘ä¼ æ’­ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>åå‘ä¼ æ’­&lt;/strong>ï¼š
åå‘ä¼ æ’­æ˜¯MLPä¸­ç”¨äºŽè®­ç»ƒæ¨¡åž‹çš„å…³é”®æ­¥éª¤ã€‚å®ƒåˆ©ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰çš„æ¢¯åº¦ï¼Œå¹¶æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°ï¼Œä»Žè€Œæœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æŸå¤±å‡½æ•°&lt;/strong>ï¼š
æŸå¤±å‡½æ•°ç”¨äºŽè¡¡é‡æ¨¡åž‹é¢„æµ‹ç»“æžœä¸ŽçœŸå®žæ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚å¸¸è§çš„æŸå¤±å‡½æ•°åŒ…æ‹¬å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€äº¤å‰ç†µç­‰ï¼Œé€‰æ‹©é€‚å½“çš„æŸå¤±å‡½æ•°å–å†³äºŽå…·ä½“çš„é—®é¢˜ç±»åž‹ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ä¼˜åŒ–ç®—æ³•&lt;/strong>ï¼š
åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡åž‹å‚æ•°ã€‚å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•åŒ…æ‹¬éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€Adamã€RMSpropç­‰ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>æ€»çš„æ¥è¯´ï¼ŒMLPé€šè¿‡å¤šå±‚çš„éžçº¿æ€§å˜æ¢æ¥å­¦ä¹ è¾“å…¥æ•°æ®çš„å¤æ‚ç‰¹å¾è¡¨ç¤ºï¼Œé€šè¿‡åå‘ä¼ æ’­ç®—æ³•ä¸æ–­è°ƒæ•´æ¨¡åž‹å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œä»Žè€Œå®žçŽ°å¯¹æ•°æ®çš„åˆ†ç±»ã€å›žå½’ç­‰ä»»åŠ¡ã€‚&lt;/p>
&lt;p>åå‘ä¼ æ’­ç®—æ³•æ˜¯ç”¨äºŽè®­ç»ƒç¥žç»ç½‘ç»œçš„å…³é”®ç®—æ³•ä¹‹ä¸€ï¼Œå®ƒé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦æ¥æ›´æ–°æ¨¡åž‹å‚æ•°ï¼Œä»Žè€Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿé€æ¸ä¼˜åŒ–ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ä¸‹é¢æˆ‘å°†ä»‹ç»åå‘ä¼ æ’­ç®—æ³•çš„è®¡ç®—è¿‡ç¨‹ï¼Œä»¥åŠé“¾å¼æ³•åˆ™å¦‚ä½•ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚&lt;/p>
&lt;h3 id="åå‘ä¼ æ’­ç®—æ³•çš„è®¡ç®—è¿‡ç¨‹" >
&lt;div>
&lt;a href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95%e7%9a%84%e8%ae%a1%e7%ae%97%e8%bf%87%e7%a8%8b">
##
&lt;/a>
åå‘ä¼ æ’­ç®—æ³•çš„è®¡ç®—è¿‡ç¨‹ï¼š
&lt;/div>
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>å‰å‘ä¼ æ’­&lt;/strong>ï¼š
é¦–å…ˆï¼Œé€šè¿‡å‰å‘ä¼ æ’­è®¡ç®—æ¨¡åž‹çš„è¾“å‡ºã€‚å°†è¾“å…¥æ•°æ®è¾“å…¥åˆ°ç½‘ç»œä¸­ï¼ŒæŒ‰ç…§ç½‘ç»œç»“æž„é€å±‚è®¡ç®—æ¯ä¸ªç¥žç»å…ƒçš„è¾“å‡ºï¼Œå¹¶å°†è¾“å‡ºä¼ é€’ç»™ä¸‹ä¸€å±‚ï¼Œç›´è‡³ç”Ÿæˆæœ€ç»ˆçš„é¢„æµ‹ç»“æžœã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>è®¡ç®—æŸå¤±&lt;/strong>ï¼š
ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—æ¨¡åž‹çš„é¢„æµ‹å€¼ä¸ŽçœŸå®žæ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œå¾—åˆ°æŸå¤±å€¼ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>åå‘ä¼ æ’­&lt;/strong>ï¼š
ä»Žè¾“å‡ºå±‚å¼€å§‹ï¼Œåˆ©ç”¨é“¾å¼æ³•åˆ™é€å±‚è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚æ¢¯åº¦è¡¨ç¤ºäº†æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„å˜åŒ–çŽ‡ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è°ƒæ•´å‚æ•°æ‰èƒ½ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>å‚æ•°æ›´æ–°&lt;/strong>ï¼š
æ ¹æ®è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ï¼Œåˆ©ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚éšæœºæ¢¯åº¦ä¸‹é™ï¼‰æ¥æ›´æ–°æ¨¡åž‹å‚æ•°ã€‚é€šå¸¸ï¼Œå‚æ•°æ›´æ–°çš„æ­¥é•¿ï¼ˆå­¦ä¹ çŽ‡ï¼‰æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œéœ€è¦æ ¹æ®å®žé™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="é“¾å¼æ³•åˆ™å¦‚ä½•ç”¨æ¥è®¡ç®—æ¢¯åº¦" >
&lt;div>
&lt;a href="#%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99%e5%a6%82%e4%bd%95%e7%94%a8%e6%9d%a5%e8%ae%a1%e7%ae%97%e6%a2%af%e5%ba%a6">
##
&lt;/a>
é“¾å¼æ³•åˆ™å¦‚ä½•ç”¨æ¥è®¡ç®—æ¢¯åº¦ï¼š
&lt;/div>
&lt;/h3>
&lt;p>é“¾å¼æ³•åˆ™æ˜¯å¾®ç§¯åˆ†ä¸­çš„åŸºæœ¬åŽŸç†ï¼Œç”¨äºŽè®¡ç®—å¤åˆå‡½æ•°çš„å¯¼æ•°ã€‚åœ¨åå‘ä¼ æ’­ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨é“¾å¼æ³•åˆ™æ¥è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¨¡åž‹å‚æ•°çš„æ¢¯åº¦ï¼Œä»Žè€Œå®žçŽ°å‚æ•°çš„æ›´æ–°ã€‚&lt;/p>
&lt;p>å‡è®¾æœ‰ä¸€ä¸ªå¤åˆå‡½æ•° $z = f(g(x))$ï¼Œå…¶ä¸­ $x$ æ˜¯è¾“å…¥ï¼Œ$g(x)$ æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œ$f(x)$ æ˜¯å¦ä¸€ä¸ªå‡½æ•°ã€‚æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œ$z$ å¯¹ $x$ çš„å¯¼æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š&lt;/p>
&lt;p>$$
\frac{dz}{dx} = \frac{dz}{dg} \cdot \frac{dg}{dx}
$$&lt;/p>
&lt;p>åœ¨ç¥žç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æŸå¤±å‡½æ•° $L$ è§†ä¸º $z$ï¼Œæ¨¡åž‹çš„å‚æ•°è§†ä¸º $x$ï¼Œå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„æ¯ä¸€å±‚è¾“å‡ºè§†ä¸º $g$ï¼Œæ¿€æ´»å‡½æ•°è§†ä¸º $f$ã€‚
åˆ©ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥é€å±‚è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼Œç„¶åŽæ ¹æ®ä¼˜åŒ–ç®—æ³•æ›´æ–°å‚æ•°ï¼Œä½¿å¾—æŸå¤±å‡½æ•°é€æ¸å‡å°ã€‚&lt;/p>
&lt;p>æ€»çš„æ¥è¯´ï¼Œåå‘ä¼ æ’­ç®—æ³•é€šè¿‡åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ï¼Œå®žçŽ°äº†é«˜æ•ˆçš„ç¥žç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å¤æ‚çš„æ•°æ®è¡¨ç¤ºã€‚&lt;/p>
&lt;p>äº¤å‰ç†µï¼ˆCross-Entropyï¼‰æ˜¯åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šåˆ†ç±»ä»»åŠ¡ä¸­ã€‚å®ƒçš„åŽŸç†å’Œä¸ºä»€ä¹ˆæœ‰ç”¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‡ ç‚¹è¿›è¡Œè¯¦ç»†è§£é‡Šï¼š&lt;/p>
&lt;h3 id="1-æŸå¤±å‡½æ•°çš„ä½œç”¨" >
&lt;div>
&lt;a href="#1-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e7%9a%84%e4%bd%9c%e7%94%a8">
##
&lt;/a>
1. æŸå¤±å‡½æ•°çš„ä½œç”¨ï¼š
&lt;/div>
&lt;/h3>
&lt;p>æŸå¤±å‡½æ•°ç”¨äºŽè¡¡é‡æ¨¡åž‹é¢„æµ‹ç»“æžœä¸ŽçœŸå®žæ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚ä¼˜åŒ–æ¨¡åž‹çš„ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿäº§ç”Ÿä¸ŽçœŸå®žæ ‡ç­¾ç›¸åŒ¹é…çš„é¢„æµ‹ç»“æžœã€‚&lt;/p>
&lt;h3 id="2-äº¤å‰ç†µçš„å®šä¹‰" >
&lt;div>
&lt;a href="#2-%e4%ba%a4%e5%8f%89%e7%86%b5%e7%9a%84%e5%ae%9a%e4%b9%89">
##
&lt;/a>
2. äº¤å‰ç†µçš„å®šä¹‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>å¯¹äºŽå¤šåˆ†ç±»ä»»åŠ¡ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°çš„æ•°å­¦å®šä¹‰å¦‚ä¸‹ï¼š&lt;/p>
&lt;p>$$
\text{Cross-Entropy}(y, \hat{y}) = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)
$$&lt;/p>
&lt;p>å…¶ä¸­ï¼Œ$y$ æ˜¯çœŸå®žæ ‡ç­¾çš„æ¦‚çŽ‡åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªone-hotç¼–ç çš„å‘é‡ï¼‰ï¼Œ$\hat{y}$ æ˜¯æ¨¡åž‹é¢„æµ‹çš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œ$N$ æ˜¯ç±»åˆ«çš„æ•°é‡ã€‚
è¯¥æŸå¤±å‡½æ•°ç”¨äºŽè¡¡é‡çœŸå®žæ ‡ç­¾ä¸Žæ¨¡åž‹é¢„æµ‹ç»“æžœä¹‹é—´çš„å·®å¼‚ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">binary_cross_entropy&lt;/span>(y_true, y_pred):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_true &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_true)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Clip predictions to avoid log(0)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>clip(y_pred, &lt;span style="color:#ff9f43">1e-15&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> &lt;span style="color:#ff9f43">1e-15&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cross_entropy &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span>np&lt;span style="color:#ff6ac1">.&lt;/span>mean(y_true &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(y_pred) &lt;span style="color:#ff6ac1">+&lt;/span> (&lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> y_true) &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(&lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> y_pred))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> cross_entropy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_true &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#ff9f43">0.9&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>, &lt;span style="color:#ff9f43">0.8&lt;/span>, &lt;span style="color:#ff9f43">0.7&lt;/span>, &lt;span style="color:#ff9f43">0.2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(binary_cross_entropy(y_true, y_pred))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">multiclass_cross_entropy&lt;/span>(y_true, y_pred):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_true &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_true)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(y_pred)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># Clip predictions to avoid log(0)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_pred &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>clip(y_pred, &lt;span style="color:#ff9f43">1e-15&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span> &lt;span style="color:#ff9f43">1e-15&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cross_entropy &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff6ac1">-&lt;/span>np&lt;span style="color:#ff6ac1">.&lt;/span>mean(np&lt;span style="color:#ff6ac1">.&lt;/span>sum(y_true &lt;span style="color:#ff6ac1">*&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>log(y_pred), axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> cross_entropy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_true &lt;span style="color:#ff6ac1">=&lt;/span> [[&lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>], [&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>], [&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">1&lt;/span>]] &lt;span style="color:#78787e"># how to get these? One-hot encoding.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> [[&lt;span style="color:#ff9f43">0.9&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>], [&lt;span style="color:#ff9f43">0.1&lt;/span>, &lt;span style="color:#ff9f43">0.8&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>], [&lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.05&lt;/span>, &lt;span style="color:#ff9f43">0.9&lt;/span>]] &lt;span style="color:#78787e"># how to get these? Softmax.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(multiclass_cross_entropy(y_true, y_pred))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Softmax: Get the y_pred from model output raw scores (i.e., logits)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">softmax&lt;/span>(logits):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exp_logits &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>exp(logits &lt;span style="color:#ff6ac1">-&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>max(logits, axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, keepdims&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> exp_logits &lt;span style="color:#ff6ac1">/&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>sum(exp_logits, axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>, keepdims&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Example logits (raw scores) outputted by a model&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logits &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([[&lt;span style="color:#ff9f43">2.0&lt;/span>, &lt;span style="color:#ff9f43">1.0&lt;/span>, &lt;span style="color:#ff9f43">0.1&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ff9f43">0.5&lt;/span>, &lt;span style="color:#ff9f43">2.5&lt;/span>, &lt;span style="color:#ff9f43">1.5&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ff9f43">1.2&lt;/span>, &lt;span style="color:#ff9f43">0.7&lt;/span>, &lt;span style="color:#ff9f43">2.1&lt;/span>]])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Compute the softmax probabilities&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_pred &lt;span style="color:#ff6ac1">=&lt;/span> softmax(logits)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-åŽŸç†è§£é‡Š" >
&lt;div>
&lt;a href="#3-%e5%8e%9f%e7%90%86%e8%a7%a3%e9%87%8a">
##
&lt;/a>
3. åŽŸç†è§£é‡Šï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>ä¿¡æ¯è®ºè§’åº¦&lt;/strong>ï¼š
äº¤å‰ç†µæŸå¤±å‡½æ•°æºè‡ªä¿¡æ¯è®ºä¸­çš„ä¿¡æ¯ç†µæ¦‚å¿µã€‚ä¿¡æ¯ç†µç”¨äºŽè¡¡é‡ä¸€ä¸ªéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§ï¼Œè€Œäº¤å‰ç†µåˆ™è¡¡é‡ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚å½“çœŸå®žæ ‡ç­¾å’Œæ¨¡åž‹é¢„æµ‹çš„åˆ†å¸ƒè¶ŠæŽ¥è¿‘æ—¶ï¼Œäº¤å‰ç†µè¶Šå°ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ¢¯åº¦ä¸‹é™ä¼˜åŒ–&lt;/strong>ï¼š
äº¤å‰ç†µæŸå¤±å‡½æ•°åœ¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¿‡ç¨‹ä¸­å…·æœ‰è‰¯å¥½çš„æ€§è´¨ã€‚å®ƒçš„å¯¼æ•°ç›¸å¯¹ç®€å•ï¼Œè®¡ç®—èµ·æ¥æ›´åŠ é«˜æ•ˆï¼Œè€Œä¸”å½“æ¨¡åž‹çš„é¢„æµ‹ç»“æžœä¸ŽçœŸå®žæ ‡ç­¾çš„å·®å¼‚è¾ƒå¤§æ—¶ï¼Œæ¢¯åº¦ä¹Ÿä¼šå˜å¾—æ›´å¤§ï¼Œä»Žè€ŒåŠ é€Ÿæ¨¡åž‹å‚æ•°çš„æ›´æ–°ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>é€‚ç”¨äºŽå¤šåˆ†ç±»ä»»åŠ¡&lt;/strong>ï¼š
äº¤å‰ç†µæŸå¤±å‡½æ•°ç‰¹åˆ«é€‚ç”¨äºŽå¤šåˆ†ç±»ä»»åŠ¡ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¡é‡å¤šä¸ªç±»åˆ«ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶ä¸”åœ¨æ¨¡åž‹ä¼˜åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿå¼•å¯¼æ¨¡åž‹æ›´å¿«åœ°æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="4-ä¸ºä»€ä¹ˆæœ‰ç”¨" >
&lt;div>
&lt;a href="#4-%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e7%94%a8">
##
&lt;/a>
4. ä¸ºä»€ä¹ˆæœ‰ç”¨ï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>æ¢¯åº¦ä¿¡æ¯&lt;/strong>ï¼š
äº¤å‰ç†µæŸå¤±å‡½æ•°æä¾›äº†ä¸°å¯Œçš„æ¢¯åº¦ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡åž‹å¯ä»¥æ›´å¿«åœ°å­¦ä¹ åˆ°æ­£ç¡®çš„é¢„æµ‹ç»“æžœã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>é€‚ç”¨æ€§å¹¿æ³›&lt;/strong>ï¼š
äº¤å‰ç†µæŸå¤±å‡½æ•°é€‚ç”¨äºŽå¤šåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨å®žé™…åº”ç”¨ä¸­è¡¨çŽ°è‰¯å¥½ï¼Œå› æ­¤æˆä¸ºäº†è®¸å¤šåˆ†ç±»ä»»åŠ¡çš„é¦–é€‰æŸå¤±å‡½æ•°ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ä¸Žæ¦‚çŽ‡ç›¸å…³&lt;/strong>ï¼š
äº¤å‰ç†µæŸå¤±å‡½æ•°ç›´æŽ¥ä¸Žæ¦‚çŽ‡åˆ†å¸ƒç›¸å…³ï¼Œæ›´ç¬¦åˆä»»åŠ¡çš„æœ¬è´¨ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æŒ‡å¯¼æ¨¡åž‹å­¦ä¹ æ•°æ®çš„åˆ†å¸ƒæƒ…å†µã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>æ€»çš„æ¥è¯´ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°é€šè¿‡è¡¡é‡æ¨¡åž‹é¢„æµ‹ç»“æžœä¸ŽçœŸå®žæ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œæä¾›äº†æœ‰æ•ˆçš„ä¼˜åŒ–ç›®æ ‡ï¼Œå¹¶åœ¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¿‡ç¨‹ä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼Œå› æ­¤è¢«å¹¿æ³›åº”ç”¨äºŽåˆ†ç±»ä»»åŠ¡ä¸­ã€‚&lt;/p>
&lt;h2 id="bertroberta-model" >
&lt;div>
&lt;a href="#bertroberta-model">
#
&lt;/a>
BERT/RoBERTa model
&lt;/div>
&lt;/h2>
&lt;p>BERT(Bidirectional Encoder Representations from Transformers):
BERTæ˜¯ä¸€ç§åŒå‘çš„ï¼ˆBidirectionalï¼‰æ¨¡åž‹ï¼Œè¿™æ„å‘³ç€å®ƒèƒ½å¤ŸåŒæ—¶è€ƒè™‘åˆ°ä¸€ä¸ªå•è¯å·¦è¾¹å’Œå³è¾¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ä½¿å¾—BERTåœ¨ç†è§£å¥å­è¯­å¢ƒæ—¶æ¯”ä¹‹å‰çš„æ¨¡åž‹æ›´ä¸ºå¼ºå¤§ã€‚
BERTæ¨¡åž‹çš„é¢„è®­ç»ƒè¿‡ç¨‹æ˜¯é€šè¿‡æŽ©ç›–è¾“å…¥æ–‡æœ¬ä¸­çš„ä¸€éƒ¨åˆ†è¯æ±‡ï¼ˆMasked Language Modelï¼ŒMLMï¼‰å’Œé¢„æµ‹å¥å­æ˜¯å¦è¿žç»­ï¼ˆNext Sentence Predictionï¼ŒNSPï¼‰æ¥å®Œæˆçš„ã€‚
BERTä»¥â€œtransformerâ€ä¸ºåŸºç¡€ï¼Œè¿™æ˜¯ä¸€ç§è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰æœºåˆ¶çš„ç¥žç»ç½‘ç»œç»“æž„ï¼Œå®ƒèƒ½å¤Ÿåœ¨è€ƒè™‘åˆ°è¾“å…¥åºåˆ—çš„æ‰€æœ‰ä½ç½®ä¹‹é—´å»ºç«‹å…³è”ï¼Œä»Žè€Œæ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ã€‚&lt;/p>
&lt;p>RoBERTaæ˜¯Facebook AIæå‡ºçš„ä¸€ç§æ”¹è¿›çš„é¢„è®­ç»ƒè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡åž‹ï¼Œå®ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå»ºç«‹åœ¨BERTçš„åŸºç¡€ä¸Šï¼Œä½†é€šè¿‡ä¸€ç³»åˆ—çš„æ”¹è¿›ï¼Œä½¿å…¶åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šè¡¨çŽ°æ›´ä¼˜ç§€ã€‚&lt;/p>
&lt;p>RoBERTaçš„ä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>åŠ¨æ€æŽ©ç ç­–ç•¥ï¼ˆDynamic Maskingï¼‰ï¼šRoBERTaåœ¨é¢„è®­ç»ƒæ—¶é‡‡ç”¨äº†åŠ¨æ€æŽ©ç ç­–ç•¥ï¼Œå³åœ¨æ¯ä¸ªè®­ç»ƒè¿­ä»£ä¸­å¯¹è¾“å…¥å¥å­è¿›è¡ŒéšæœºåŒ–å¤„ç†ï¼Œè€Œä¸æ˜¯å›ºå®šåœ°åœ¨å¥å­ä¸­éšæœºæŽ©ç ã€‚è¿™ä½¿å¾—æ¨¡åž‹æ›´å¥½åœ°å­¦ä¹ å¥å­ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ›´é•¿çš„è®­ç»ƒæ—¶é—´å’Œæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼šRoBERTaä½¿ç”¨äº†æ›´å¤§çš„æ‰¹æ¬¡å¤§å°å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œä»¥æé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åŽ»é™¤NSPï¼ˆNext Sentence Predictionï¼‰ä»»åŠ¡ï¼šRoBERTaä¸å†ä½¿ç”¨BERTä¸­çš„NSPä»»åŠ¡ï¼Œè€Œæ˜¯ä¸“æ³¨äºŽMLMï¼ˆMasked Language Modelï¼‰ä»»åŠ¡ï¼Œè¿™ä½¿å¾—æ¨¡åž‹æ›´å¥½åœ°ç†è§£è¾“å…¥æ–‡æœ¬ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼šRoBERTaä½¿ç”¨äº†æ›´å¤šçš„æ–‡æœ¬æ•°æ®æ¥è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™æœ‰åŠ©äºŽæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>æ€»çš„æ¥è¯´ï¼ŒRoBERTaæ˜¯å¯¹BERTæ¨¡åž‹çš„ä¸€ç§ä¼˜åŒ–å’Œæ”¹è¿›ï¼Œå®ƒåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ¯”BERTæ›´å¥½çš„æ€§èƒ½ã€‚&lt;/p>
&lt;p>&lt;strong>æ›´å¤šç»†èŠ‚ï¼Œåº”è¯¥åŽ»æŸ¥æ‰¾å°å¤§æŽå®æ¯…è€å¸ˆçš„è¯¾ç¨‹ï¼Œéžå¸¸ç²¾å½©çš„è®²è§£ã€‚&lt;/strong>&lt;/p>
&lt;h2 id="faiss-indexivfpq" >
&lt;div>
&lt;a href="#faiss-indexivfpq">
#
&lt;/a>
FAISS (indexIVFPQ)
&lt;/div>
&lt;/h2>
&lt;p>åœ¨æˆ‘çš„å¦ä¸€ç¯‡åšå®¢å•ç‹¬æœ‰ä»‹ç»ï¼Œå¹¶å¸¦æœ‰ä»£ç ç¤ºä¾‹ã€‚&lt;/p>
&lt;h2 id="bertopic" >
&lt;div>
&lt;a href="#bertopic">
#
&lt;/a>
BERTopic
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>clustering&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>UMAP/PCA&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>c-TF-IDF&lt;/li>
&lt;/ul>
&lt;p>BERTopic æ˜¯ä¸€ä¸ªåŸºäºŽBERTçš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·ï¼Œç”¨äºŽä¸»é¢˜å»ºæ¨¡ä»»åŠ¡ã€‚BERTopicç»“åˆäº†BERTçš„å¼ºå¤§è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›å’Œä¸»é¢˜å»ºæ¨¡çš„æ€æƒ³ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä¸Šå¿«é€Ÿã€å‡†ç¡®åœ°æå–ä¸»é¢˜ä¿¡æ¯ã€‚&lt;/p>
&lt;p>BERTopicçš„å·¥ä½œåŽŸç†å¦‚ä¸‹ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>æ–‡æœ¬å‘é‡åŒ–&lt;/strong>ï¼šé¦–å…ˆï¼ŒBERTopicä½¿ç”¨é¢„è®­ç»ƒçš„BERTæ¨¡åž‹æ¥å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡è¡¨ç¤ºã€‚è¿™äº›å‘é‡æ•æ‰äº†è¾“å…¥æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä¸”é€šå¸¸èƒ½å¤Ÿæ›´å¥½åœ°åæ˜ æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ä¸»é¢˜å‘çŽ°&lt;/strong>ï¼šæŽ¥ä¸‹æ¥ï¼ŒBERTopicä½¿ç”¨èšç±»ç®—æ³•ï¼ˆä¾‹å¦‚DBSCANæˆ–HDBSCANï¼‰å¯¹æ–‡æœ¬å‘é‡è¿›è¡Œèšç±»ï¼Œä»¥å‘çŽ°æ½œåœ¨çš„ä¸»é¢˜ã€‚èšç±»ç®—æ³•å°†æ–‡æœ¬å‘é‡åˆ†ç»„ä¸ºå…·æœ‰ç›¸ä¼¼ä¸»é¢˜çš„ç°‡ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ä¸»é¢˜å…³é”®è¯æå–&lt;/strong>ï¼šå¯¹äºŽæ¯ä¸ªå‘çŽ°çš„ä¸»é¢˜ç°‡ï¼ŒBERTopicè¿˜å¯ä»¥æå–å…³é”®è¯æ¥æè¿°è¯¥ä¸»é¢˜ã€‚è¿™äº›å…³é”®è¯é€šå¸¸æ˜¯ç°‡ä¸­æœ€å…·ä»£è¡¨æ€§çš„è¯è¯­ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ä¸»é¢˜çš„å†…å®¹ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ä¸»é¢˜å¯è§†åŒ–&lt;/strong>ï¼šæœ€åŽï¼ŒBERTopicå¯ä»¥å°†å‘çŽ°çš„ä¸»é¢˜å¯è§†åŒ–ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°äº†è§£æ–‡æœ¬æ•°æ®ä¸­çš„ä¸»é¢˜ç»“æž„ã€‚é€šå¸¸ï¼Œå¯è§†åŒ–ç»“æžœä¼šä»¥ç°‡çš„å½¢å¼å±•ç¤ºï¼Œæ¯ä¸ªç°‡ä»£è¡¨ä¸€ä¸ªä¸»é¢˜ï¼Œç°‡å†…çš„æ–‡æœ¬åˆ™è¡¨ç¤ºè¯¥ä¸»é¢˜çš„å…·ä½“å†…å®¹ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>å…³äºŽæœ€ä½³å®žè·µï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>è°ƒæ•´æ¨¡åž‹å‚æ•°&lt;/strong>ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚å’Œæ•°æ®ç‰¹ç‚¹ï¼Œè°ƒæ•´BERTopicçš„å‚æ•°ï¼Œä¾‹å¦‚èšç±»ç®—æ³•çš„å‚æ•°ã€ä¸»é¢˜æ•°é‡ç­‰ï¼Œä»¥èŽ·å¾—æ›´å¥½çš„ç»“æžœã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>é¢„å¤„ç†æ–‡æœ¬æ•°æ®&lt;/strong>ï¼šåœ¨ä½¿ç”¨BERTopicä¹‹å‰ï¼Œå¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†æ˜¯å¾ˆé‡è¦çš„ï¼Œä¾‹å¦‚åŽ»é™¤åœç”¨è¯ã€è¿›è¡Œè¯å¹²åŒ–æˆ–è¯å½¢è¿˜åŽŸç­‰ï¼Œä»¥å‡å°‘å™ªéŸ³å¯¹ä¸»é¢˜å»ºæ¨¡çš„å½±å“ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ç†è§£ä¸»é¢˜ç»“æžœ&lt;/strong>ï¼šå¯¹äºŽæ¯ä¸ªå‘çŽ°çš„ä¸»é¢˜ï¼Œä»”ç»†æŸ¥çœ‹ä¸»é¢˜ç°‡ä¸­çš„æ–‡æœ¬ï¼Œå¹¶è€ƒè™‘å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œå…±æ€§ï¼Œä»¥ç¡®ä¿ä¸»é¢˜çš„åˆç†æ€§å’Œå¯è§£é‡Šæ€§ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ä¸Žé¢†åŸŸçŸ¥è¯†ç»“åˆ&lt;/strong>ï¼šåœ¨è§£é‡Šå’Œåˆ©ç”¨ä¸»é¢˜ç»“æžœæ—¶ï¼Œç»“åˆé¢†åŸŸçŸ¥è¯†ä¼šæ›´æœ‰å¸®åŠ©ã€‚é€šè¿‡æ·±å…¥äº†è§£é¢†åŸŸä¸“ä¸šæœ¯è¯­å’Œç›¸å…³æ¦‚å¿µï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°ç†è§£å’Œè§£é‡Šä¸»é¢˜çš„å«ä¹‰ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>é€šè¿‡åˆç†åœ°ä½¿ç”¨BERTopicå·¥å…·ï¼Œå¹¶ç»“åˆé€‚å½“çš„æ•°æ®é¢„å¤„ç†å’Œæ¨¡åž‹è°ƒä¼˜ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å®ŒæˆåŸºäºŽè‡ªç„¶è¯­è¨€çš„ä¸»é¢˜å»ºæ¨¡ä»»åŠ¡ã€‚&lt;/p>
&lt;p>å½“ç„¶ï¼Œè®©æˆ‘æ›´è¯¦ç»†åœ°è§£é‡Šä¸€ä¸‹è¿™äº›æŠ€æœ¯ã€‚&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>UMAPï¼ˆUniform Manifold Approximation and Projectionï¼‰&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>UMAPæ˜¯ä¸€ç§éžçº¿æ€§é™ç»´ç®—æ³•ï¼Œæ—¨åœ¨å°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°ä½Žç»´ç©ºé—´ä»¥è¿›è¡Œå¯è§†åŒ–æˆ–è¿›ä¸€æ­¥åˆ†æžã€‚&lt;/li>
&lt;li>UMAPç›¸å¯¹äºŽä¼ ç»Ÿçš„é™ç»´æŠ€æœ¯ï¼ˆå¦‚t-SNEï¼‰å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œä¿æŒå…¨å±€æ•°æ®ç»“æž„çš„èƒ½åŠ›ã€‚å®ƒèƒ½å¤Ÿä¿æŒæ›´å¤šçš„å±€éƒ¨ç»“æž„ï¼ŒåŒæ—¶åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šçš„è®¡ç®—æ•ˆçŽ‡æ›´é«˜ã€‚&lt;/li>
&lt;li>åœ¨BERTopicä¸­ï¼ŒUMAPå¸¸ç”¨äºŽå¯¹BERTå‘é‡åŒ–çš„æ–‡æœ¬æ•°æ®è¿›è¡Œé™ç»´ï¼Œä»¥ä¾¿è¿›è¡Œæ›´å¥½çš„å¯è§†åŒ–æˆ–è¿›ä¸€æ­¥çš„åˆ†æžã€‚é™ç»´åŽçš„æ•°æ®å¯ä»¥æ›´å®¹æ˜“åœ°è¢«äººç±»ç†è§£å’Œè§£é‡Šã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>c-TF-IDFï¼ˆClass-based TF-IDFï¼‰&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>c-TF-IDFæ˜¯ä¸€ç§æ”¹è¿›çš„TF-IDFï¼ˆTerm Frequency-Inverse Document Frequencyï¼‰æ–¹æ³•ï¼Œç”¨äºŽä»Žæ–‡æœ¬æ•°æ®ä¸­æå–å…³é”®è¯ã€‚&lt;/li>
&lt;li>ä¸Žä¼ ç»Ÿçš„TF-IDFç›¸æ¯”ï¼Œc-TF-IDFè€ƒè™‘äº†å•è¯åœ¨ä¸åŒä¸»é¢˜ä¸­çš„é‡è¦æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯åœ¨æ•´ä¸ªæ–‡æœ¬é›†åˆä¸­çš„é‡è¦æ€§ã€‚è¿™ä½¿å¾—c-TF-IDFèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ–‡æœ¬ä¸­çš„ä¸»é¢˜ç›¸å…³ä¿¡æ¯ã€‚&lt;/li>
&lt;li>åœ¨BERTopicä¸­ï¼Œc-TF-IDFå¸¸ç”¨äºŽä»Žæ¯ä¸ªå‘çŽ°çš„ä¸»é¢˜ç°‡ä¸­æå–å…³é”®è¯ï¼Œä»¥æè¿°è¯¥ä¸»é¢˜çš„å†…å®¹ã€‚è¿™äº›å…³é”®è¯é€šå¸¸æ˜¯ç°‡ä¸­æœ€å…·ä»£è¡¨æ€§çš„è¯è¯­ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ä¸»é¢˜çš„å«ä¹‰å’Œå†…å®¹ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>HDBSCANï¼ˆHierarchical Density-Based Spatial Clustering of Applications with Noiseï¼‰&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>HDBSCANæ˜¯ä¸€ç§å¯†åº¦èšç±»ç®—æ³•ï¼Œæ—¨åœ¨å‘çŽ°æ•°æ®ä¸­çš„é«˜å¯†åº¦åŒºåŸŸï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆç°‡ã€‚&lt;/li>
&lt;li>ä¸Žä¼ ç»Ÿçš„åŸºäºŽè·ç¦»çš„èšç±»ç®—æ³•ï¼ˆå¦‚Kå‡å€¼ï¼‰ä¸åŒï¼ŒHDBSCANä¸éœ€è¦é¢„å…ˆæŒ‡å®šç°‡çš„æ•°é‡ï¼Œå› æ­¤æ›´é€‚ç”¨äºŽå‘çŽ°å…·æœ‰ä¸åŒå¤§å°å’Œå½¢çŠ¶çš„ç°‡çš„æ•°æ®ã€‚&lt;/li>
&lt;li>åœ¨BERTopicä¸­ï¼ŒHDBSCANé€šå¸¸ä¸ŽUMAPä¸€èµ·ä½¿ç”¨ï¼Œç”¨äºŽå¯¹BERTå‘é‡åŒ–çš„æ–‡æœ¬æ•°æ®è¿›è¡Œèšç±»ã€‚HDBSCANèƒ½å¤Ÿè¯†åˆ«å‡ºå…·æœ‰ä¸åŒä¸»é¢˜çš„æ–‡æœ¬ç°‡ï¼Œä»Žè€Œå¸®åŠ©ç”¨æˆ·å‘çŽ°æ–‡æœ¬æ•°æ®ä¸­çš„ä¸»é¢˜ç»“æž„ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>ç»¼ä¸Šæ‰€è¿°ï¼ŒUMAPç”¨äºŽå°†é«˜ç»´æ–‡æœ¬å‘é‡é™ç»´åˆ°ä½Žç»´ç©ºé—´ï¼Œä»¥ä¾¿äºŽå¯è§†åŒ–å’Œåˆ†æžï¼›c-TF-IDFç”¨äºŽä»Žæ¯ä¸ªä¸»é¢˜ç°‡ä¸­æå–å…³é”®è¯ä»¥æè¿°ä¸»é¢˜å†…å®¹ï¼›HDBSCANç”¨äºŽå‘çŽ°æ–‡æœ¬æ•°æ®ä¸­çš„æ½œåœ¨ä¸»é¢˜ç°‡ã€‚è¿™äº›æŠ€æœ¯çš„ç»“åˆä½¿å¾—BERTopicèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œè‡ªç„¶è¯­è¨€ä¸»é¢˜å»ºæ¨¡ï¼Œå¹¶ä»Žæ–‡æœ¬æ•°æ®ä¸­æå–æœ‰æ„ä¹‰çš„ä¸»é¢˜ä¿¡æ¯ã€‚&lt;/p>
&lt;h1 id="project-02----ml" >
&lt;div>
&lt;a href="#project-02----ml">
##
&lt;/a>
Project 02 &amp;ndash; ML
&lt;/div>
&lt;/h1>
&lt;h2 id="self-training--gausianmixtruemodels" >
&lt;div>
&lt;a href="#self-training--gausianmixtruemodels">
#
&lt;/a>
self-training &amp;amp; GausianMixtrueModels
&lt;/div>
&lt;/h2>
&lt;p>åŠç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„æ•°æ®æ¥è¿›è¡Œæ¨¡åž‹è®­ç»ƒã€‚ç›¸æ¯”äºŽåªä½¿ç”¨æœ‰æ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒåŠç›‘ç£å­¦ä¹ å¯ä»¥åˆ©ç”¨æ›´å¤šçš„æ— æ ‡ç­¾æ•°æ®ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„æ€§èƒ½ã€‚&lt;/p>
&lt;p>self-trainingï¼ˆè‡ªè®­ç»ƒï¼‰æ˜¯åŠç›‘ç£å­¦ä¹ ä¸­çš„ä¸€ç§å¸¸è§æŠ€æœ¯ï¼Œå…¶åŸºæœ¬æ€æƒ³æ˜¯åˆ©ç”¨å·²ç»è®­ç»ƒå¥½çš„æ¨¡åž‹å¯¹æ— æ ‡ç­¾æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œç„¶åŽå°†ç½®ä¿¡åº¦è¾ƒé«˜çš„é¢„æµ‹ç»“æžœä½œä¸ºä¼ªæ ‡ç­¾ï¼Œå°†è¿™äº›ä¼ªæ ‡ç­¾çš„æ•°æ®ä¸Žæœ‰æ ‡ç­¾æ•°æ®ä¸€èµ·é‡æ–°è®­ç»ƒæ¨¡åž‹ã€‚&lt;/p>
&lt;p>å…·ä½“æ¥è¯´ï¼Œself-trainingçš„æ­¥éª¤å¦‚ä¸‹ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>åˆ©ç”¨æœ‰æ ‡ç­¾æ•°æ®è®­ç»ƒä¸€ä¸ªåˆå§‹æ¨¡åž‹ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ä½¿ç”¨è¯¥æ¨¡åž‹å¯¹æ— æ ‡ç­¾æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶é€‰å–ç½®ä¿¡åº¦è¾ƒé«˜çš„é¢„æµ‹ç»“æžœä½œä¸ºä¼ªæ ‡ç­¾ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å°†ä¼ªæ ‡ç­¾çš„æ•°æ®ä¸ŽåŽŸæœ‰çš„æœ‰æ ‡ç­¾æ•°æ®åˆå¹¶ï¼Œé‡æ–°è®­ç»ƒæ¨¡åž‹ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>é‡å¤æ­¥éª¤2å’Œæ­¥éª¤3ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆæ¯”å¦‚è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€æ¨¡åž‹æ€§èƒ½ä¸å†æå‡ç­‰ï¼‰ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>self-trainingçš„å…³é”®åœ¨äºŽå¦‚ä½•é€‰æ‹©ç½®ä¿¡åº¦è¾ƒé«˜çš„é¢„æµ‹ç»“æžœä½œä¸ºä¼ªæ ‡ç­¾ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®ä¸€ä¸ªé˜ˆå€¼æ¥ç­›é€‰ç½®ä¿¡åº¦è¾ƒé«˜çš„é¢„æµ‹ç»“æžœï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ¨¡åž‹çš„è¾“å‡ºæ¦‚çŽ‡æ¥ä½œä¸ºç½®ä¿¡åº¦çš„åº¦é‡ã€‚&lt;/p>
&lt;p>ä½¿ç”¨self-trainingæŠ€æœ¯å¯ä»¥å¸®åŠ©æ‰©å……æ ·æœ¬ï¼Œæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡è®°æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè‡ªè®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šå¼•å…¥å™ªå£°ï¼Œå› æ­¤éœ€è¦ä»”ç»†è°ƒèŠ‚å‚æ•°å’Œç›‘æŽ§æ¨¡åž‹æ€§èƒ½ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆå’Œæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚&lt;/p>
&lt;p>å¦‚æžœèšç±»ç°‡å†…çš„æ ‡ç­¾å¹¶ä¸ä¸€è‡´ï¼Œè€Œä¸”ç±»åˆ«æ•°ç›®è¾ƒä¸ºç›¸åŒï¼Œè¿™ç§æƒ…å†µå¯èƒ½ä¼šå¯¼è‡´ä¸€äº›æ··ä¹±ï¼Œå› ä¸ºæ— æ³•ç®€å•åœ°é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§çš„æ ‡ç­¾æ¥ä¸ºæ•´ä¸ªèšç±»ç°‡çš„æ ·æœ¬æ‰“æ ‡ç­¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ç§å¤„ç†æ–¹å¼ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>æŠ•ç¥¨æœºåˆ¶&lt;/strong>ï¼š&lt;/p>
&lt;ul>
&lt;li>å¯¹äºŽæ¯ä¸ªèšç±»ç°‡ï¼Œå¯ä»¥é‡‡ç”¨æŠ•ç¥¨æœºåˆ¶æ¥é€‰æ‹©æ ‡ç­¾ã€‚å³ï¼Œç»Ÿè®¡èšç±»ç°‡ä¸­æ ·æœ¬çš„çœŸå®žæ ‡ç­¾æˆ–è€…å·²æœ‰çš„ä¼ªæ ‡ç­¾ï¼Œé€‰æ‹©å‡ºçŽ°é¢‘çŽ‡æœ€é«˜çš„æ ‡ç­¾ä½œä¸ºæ•´ä¸ªèšç±»ç°‡çš„æ ‡ç­¾ã€‚&lt;/li>
&lt;li>å¦‚æžœæœ‰å¤šä¸ªæ ‡ç­¾å‡ºçŽ°é¢‘çŽ‡ç›¸åŒï¼Œå¯ä»¥éšæœºé€‰æ‹©å…¶ä¸­ä¸€ä¸ªæˆ–è€…é‡‡ç”¨ä¸€äº›å…¶ä»–çš„ç­–ç•¥æ¥è§£å†³å†²çªã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ ‡ç­¾èžåˆ&lt;/strong>ï¼š&lt;/p>
&lt;ul>
&lt;li>å¯¹äºŽèšç±»ç°‡å†…çš„æ ·æœ¬ï¼Œå¯ä»¥å°†å…¶çœŸå®žæ ‡ç­¾æˆ–è€…å·²æœ‰çš„ä¼ªæ ‡ç­¾è¿›è¡Œèžåˆã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è®¡ç®—èšç±»ç°‡ä¸­æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼Œç„¶åŽæ ¹æ®æƒé‡å¯¹å¤šä¸ªæ ‡ç­¾è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œå¾—åˆ°ä¸€ä¸ªç»¼åˆçš„æ ‡ç­¾ã€‚&lt;/li>
&lt;li>è¿™ç§æ–¹æ³•å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£å†³æ ‡ç­¾å†²çªçš„é—®é¢˜ï¼Œä½†éœ€è¦è°¨æ…Žè®¾è®¡æƒé‡è®¡ç®—çš„æ–¹æ³•ï¼Œä»¥é¿å…ç»™ä¸å¤ªå‡†ç¡®çš„æ ‡ç­¾è¿‡å¤šçš„æƒé‡ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>è¿›ä¸€æ­¥åˆ†æž&lt;/strong>ï¼š&lt;/p>
&lt;ul>
&lt;li>å¯¹äºŽé‚£äº›æ ‡ç­¾å†²çªè¾ƒä¸ºä¸¥é‡çš„èšç±»ç°‡ï¼Œå¯ä»¥è¿›ä¸€æ­¥åˆ†æžå…¶æ ·æœ¬ç‰¹å¾æˆ–è€…æ•°æ®åˆ†å¸ƒï¼Œå°è¯•æ‰¾åˆ°æ›´åˆé€‚çš„æ ‡ç­¾é€‰æ‹©ç­–ç•¥ã€‚&lt;/li>
&lt;li>å¯ä»¥è€ƒè™‘ä½¿ç”¨ä¸€äº›æ•°æ®æŒ–æŽ˜æŠ€æœ¯æˆ–è€…é¢†åŸŸçŸ¥è¯†æ¥å¸®åŠ©è§£å†³è¿™ä¸ªé—®é¢˜ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>åŠç›‘ç£å­¦ä¹ æ–¹æ³•&lt;/strong>ï¼š&lt;/p>
&lt;ul>
&lt;li>é™¤äº†é«˜æ–¯æ··åˆèšç±»ï¼Œè¿˜å¯ä»¥å°è¯•å…¶ä»–åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå¦‚å›¾åŠç›‘ç£å­¦ä¹ ï¼ˆGraph-based Semi-Supervised Learningï¼‰ã€æ ‡ç­¾ä¼ æ’­ç®—æ³•ï¼ˆLabel Propagationï¼‰ã€è‡ªè®­ç»ƒï¼ˆSelf-trainingï¼‰ç­‰ã€‚&lt;/li>
&lt;li>è¿™äº›æ–¹æ³•å¯èƒ½ä¼šæ›´æœ‰æ•ˆåœ°å¤„ç†æ ‡ç­¾å†²çªçš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨æ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œå…³è”æ€§ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©é€‚åˆçš„æ–¹æ³•æ¥å¤„ç†æ ‡ç­¾å†²çªé—®é¢˜ï¼ŒåŒæ—¶éœ€è¦æ³¨æ„ç›‘æŽ§æ¨¡åž‹çš„æ€§èƒ½å’Œæ•ˆæžœï¼ŒåŠæ—¶è°ƒæ•´å’Œä¼˜åŒ–ç®—æ³•ã€‚&lt;/p>
&lt;h2 id="xgboost" >
&lt;div>
&lt;a href="#xgboost">
#
&lt;/a>
XGBoost
&lt;/div>
&lt;/h2>
&lt;p>XGBoostï¼Œå…¨ç§°ä¸ºâ€œeXtreme Gradient Boostingâ€ï¼Œæ˜¯ä¸€ç§é«˜æ•ˆçš„é›†æˆå­¦ä¹ ç®—æ³•ï¼Œå±žäºŽæ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boosting Treeï¼‰çš„ä¸€ç§å®žçŽ°ã€‚å®ƒåœ¨æœºå™¨å­¦ä¹ ç«žèµ›ä¸­éžå¸¸æµè¡Œï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨å„ç§ç±»åž‹çš„æ•°æ®é›†ä¸Šå–å¾—ä¼˜ç§€çš„æ€§èƒ½ï¼Œå¹¶ä¸”ç›¸å¯¹äºŽå…¶ä»–ç®—æ³•ï¼Œå®ƒé€šå¸¸æ›´å®¹æ˜“è°ƒæ•´å‚æ•°ä»¥èŽ·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚&lt;/p>
&lt;p>ä»¥ä¸‹æ˜¯ XGBoost æ¨¡åž‹çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œä¼˜åŠ¿ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>é›†æˆå­¦ä¹ &lt;/strong>ï¼šXGBoost æ˜¯ä¸€ç§é›†æˆå­¦ä¹ ç®—æ³•ï¼Œå®ƒé€šè¿‡ç»„åˆå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰æ¥æž„å»ºä¸€ä¸ªå¼ºå¤§çš„æ¨¡åž‹ã€‚æ¯ä¸ªå†³ç­–æ ‘éƒ½æ˜¯æ ¹æ®å‰ä¸€ä¸ªæ ‘çš„é”™è¯¯è¿›è¡Œè®­ç»ƒï¼Œä»¥é€æ­¥å‡å°‘æ¨¡åž‹çš„æ®‹å·®ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ¢¯åº¦æå‡ç®—æ³•&lt;/strong>ï¼šXGBoost ä½¿ç”¨æ¢¯åº¦æå‡ç®—æ³•æ¥è®­ç»ƒæ¨¡åž‹ã€‚è¯¥ç®—æ³•é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥ä¼˜åŒ–æ¨¡åž‹ï¼Œä»Žè€Œä½¿æ¨¡åž‹åœ¨æ¯ä¸€æ­¥éƒ½æ›´åŠ è´´è¿‘çœŸå®žå€¼ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ­£åˆ™åŒ–&lt;/strong>ï¼šXGBoost æä¾›äº†å¯¹æ¨¡åž‹è¿›è¡Œæ­£åˆ™åŒ–çš„é€‰é¡¹ï¼ŒåŒ…æ‹¬ L1 å’Œ L2 æ­£åˆ™åŒ–ï¼Œä»¥åŠæŽ§åˆ¶æ ‘çš„å¤æ‚åº¦çš„å‚æ•°ã€‚è¿™æœ‰åŠ©äºŽå‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ”¯æŒå¹¶è¡ŒåŒ–&lt;/strong>ï¼šXGBoost å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¹¶è¡Œè®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šä¹Ÿèƒ½å¤Ÿå¿«é€Ÿåœ°è®­ç»ƒæ¨¡åž‹ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ç‰¹å¾é‡è¦æ€§è¯„ä¼°&lt;/strong>ï¼šXGBoost å¯ä»¥è®¡ç®—ç‰¹å¾çš„é‡è¦æ€§ï¼Œä»Žè€Œå¸®åŠ©ç”¨æˆ·äº†è§£å“ªäº›ç‰¹å¾å¯¹æ¨¡åž‹çš„é¢„æµ‹æœ€ä¸ºå…³é”®ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>çµæ´»æ€§&lt;/strong>ï¼šXGBoost å¯ä»¥ç”¨äºŽåˆ†ç±»é—®é¢˜ã€å›žå½’é—®é¢˜ä»¥åŠæŽ’åºé—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒç±»åž‹çš„æ•°æ®é›†ä¸Šè¿›è¡Œåº”ç”¨ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>æ€»çš„æ¥è¯´ï¼ŒXGBoost æ˜¯ä¸€ç§å¼ºå¤§ä¸”çµæ´»çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œé€‚ç”¨äºŽå„ç§ç±»åž‹çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨å®žè·µä¸­è¡¨çŽ°å‡ºè‰²ã€‚&lt;/p>
&lt;p>æ¢¯åº¦æå‡ç®—æ³•ï¼ˆGradient Boosting Algorithmï¼‰æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å°†å¤šä¸ªå¼±å­¦ä¹ å™¨ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰ä¸²è”èµ·æ¥ï¼Œé€æ­¥å‡å°‘æ¨¡åž‹çš„æ®‹å·®æ¥æž„å»ºä¸€ä¸ªå¼ºå¤§çš„é¢„æµ‹æ¨¡åž‹ã€‚æ¢¯åº¦æå‡ç®—æ³•é€šè¿‡æ¢¯åº¦ä¸‹é™çš„æ€æƒ³ï¼Œä¸æ–­ä¼˜åŒ–æ¨¡åž‹ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚&lt;/p>
&lt;p>ä¸‹é¢æ˜¯æ¢¯åº¦æå‡ç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>åˆå§‹åŒ–æ¨¡åž‹&lt;/strong>ï¼šæ¢¯åº¦æå‡ç®—æ³•é€šå¸¸ä»Žä¸€ä¸ªç®€å•çš„æ¨¡åž‹å¼€å§‹ï¼Œä¾‹å¦‚ç”¨ä¸€ä¸ªå¸¸æ•°æ¥æ‹Ÿåˆæ•°æ®çš„å¹³å‡å€¼ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>è¿­ä»£ä¼˜åŒ–&lt;/strong>ï¼šæŽ¥ä¸‹æ¥ï¼Œç®—æ³•è¿­ä»£åœ°æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š&lt;/p>
&lt;ul>
&lt;li>è®¡ç®—æ®‹å·®ï¼šä½¿ç”¨å½“å‰æ¨¡åž‹å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è®¡ç®—å®žé™…å€¼ä¸Žé¢„æµ‹å€¼ä¹‹é—´çš„æ®‹å·®ã€‚&lt;/li>
&lt;li>æ‹Ÿåˆæ®‹å·®ï¼šæž„å»ºä¸€ä¸ªæ–°çš„å¼±å­¦ä¹ å™¨ï¼ˆå¦‚å†³ç­–æ ‘ï¼‰ï¼Œä»¥æ‹Ÿåˆæ®‹å·®ã€‚è¿™æ„å‘³ç€æ–°çš„å­¦ä¹ å™¨ä¼šå°è¯•çº æ­£ä¸Šä¸€ä¸ªæ¨¡åž‹çš„é”™è¯¯ã€‚&lt;/li>
&lt;li>æ›´æ–°æ¨¡åž‹ï¼šå°†æ–°çš„å­¦ä¹ å™¨ä¸Žå‰é¢çš„æ¨¡åž‹ç»„åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡åž‹ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>åœæ­¢æ¡ä»¶&lt;/strong>ï¼šå½“è¾¾åˆ°é¢„å…ˆè®¾å®šçš„è¿­ä»£æ¬¡æ•°ï¼Œæˆ–è€…å½“æ¨¡åž‹çš„æ€§èƒ½æ»¡è¶³æŸä¸ªç‰¹å®šçš„æ ‡å‡†æ—¶ï¼Œåœæ­¢è¿­ä»£ã€‚&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>ä¸Žæ¢¯åº¦ä¸‹é™ç®—æ³•çš„å…³ç³»åœ¨äºŽï¼Œæ¢¯åº¦æå‡ç®—æ³•ä¹Ÿåˆ©ç”¨äº†æ¢¯åº¦çš„ä¿¡æ¯æ¥ä¼˜åŒ–æ¨¡åž‹ã€‚ä½†ä¸¤è€…ä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºŽä¼˜åŒ–çš„å¯¹è±¡å’Œä¼˜åŒ–æ–¹å‘ã€‚åœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œä¼˜åŒ–çš„å¯¹è±¡æ˜¯æŸå¤±å‡½æ•°æœ¬èº«ï¼Œè€Œä¼˜åŒ–çš„æ–¹å‘æ˜¯æ²¿ç€æŸå¤±å‡½æ•°æ¢¯åº¦çš„åæ–¹å‘ã€‚è€Œåœ¨æ¢¯åº¦æå‡ç®—æ³•ä¸­ï¼Œä¼˜åŒ–çš„å¯¹è±¡æ˜¯æŸå¤±å‡½æ•°çš„æ®‹å·®ï¼Œä¼˜åŒ–çš„æ–¹å‘æ˜¯ä½¿æ®‹å·®æœ€å°åŒ–çš„æ–¹å‘ã€‚&lt;/p>
&lt;p>æ€»çš„æ¥è¯´ï¼Œæ¢¯åº¦æå‡ç®—æ³•æ˜¯ä¸€ç§å¼ºå¤§çš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¸æ–­è¿­ä»£ä¼˜åŒ–æ¨¡åž‹ä»¥å‡å°‘æ®‹å·®ï¼Œä»Žè€Œæž„å»ºä¸€ä¸ªå¼ºå¤§çš„é¢„æµ‹æ¨¡åž‹ã€‚å®ƒåˆ©ç”¨äº†æ¢¯åº¦ä¿¡æ¯æ¥æŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ï¼Œä½†ä¸Žæ¢¯åº¦ä¸‹é™ç®—æ³•ç›¸æ¯”ï¼Œå®ƒçš„ä¼˜åŒ–ç›®æ ‡å’Œä¼˜åŒ–æ–¹å‘æœ‰æ‰€ä¸åŒã€‚&lt;/p>
&lt;h2 id="onnx-dockerpodman-and-restful-api" >
&lt;div>
&lt;a href="#onnx-dockerpodman-and-restful-api">
#
&lt;/a>
ONNX, Docker/Podman, and restful-api
&lt;/div>
&lt;/h2>
&lt;p>å½“ä½ å¸Œæœ›ä½¿ç”¨ Docker æ¥éƒ¨ç½²æœåŠ¡ï¼Œå¹¶ä¸”æž„å»º Flask RESTful API æ¥æä¾›å¯¹ ONNX æ¨¡åž‹çš„æŽ¨ç†æœåŠ¡æ—¶ï¼Œä½ å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š&lt;/p>
&lt;h3 id="1-å‡†å¤‡-flask-restful-api-ä»£ç " >
&lt;div>
&lt;a href="#1-%e5%87%86%e5%a4%87-flask-restful-api-%e4%bb%a3%e7%a0%81">
##
&lt;/a>
1. å‡†å¤‡ Flask RESTful API ä»£ç 
&lt;/div>
&lt;/h3>
&lt;p>ä½ éœ€è¦åˆ›å»ºä¸€ä¸ª Flask åº”ç”¨ç¨‹åºï¼Œç¼–å†™ä»£ç ä»¥åŠ è½½ ONNX æ¨¡åž‹å¹¶æä¾› RESTful API æ¥è¿›è¡ŒæŽ¨ç†ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> flask &lt;span style="color:#ff6ac1">import&lt;/span> Flask, request
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">from&lt;/span> flask_restful &lt;span style="color:#ff6ac1">import&lt;/span> Api, Resource
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> onnxruntime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>app &lt;span style="color:#ff6ac1">=&lt;/span> Flask(__name__)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>api &lt;span style="color:#ff6ac1">=&lt;/span> Api(app)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">class&lt;/span> &lt;span style="color:#f3f99d">ModelInference&lt;/span>(Resource):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> __init__(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">super&lt;/span>(ModelInference, self)&lt;span style="color:#ff6ac1">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff6ac1">.&lt;/span>session &lt;span style="color:#ff6ac1">=&lt;/span> onnxruntime&lt;span style="color:#ff6ac1">.&lt;/span>InferenceSession(&lt;span style="color:#5af78e">&amp;#34;your_model.onnx&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">def&lt;/span> &lt;span style="color:#57c7ff">post&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> data &lt;span style="color:#ff6ac1">=&lt;/span> request&lt;span style="color:#ff6ac1">.&lt;/span>json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_data &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array(data[&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>])&lt;span style="color:#ff6ac1">.&lt;/span>astype(np&lt;span style="color:#ff6ac1">.&lt;/span>float32)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output &lt;span style="color:#ff6ac1">=&lt;/span> self&lt;span style="color:#ff6ac1">.&lt;/span>session&lt;span style="color:#ff6ac1">.&lt;/span>run([], {&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>: input_data})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">return&lt;/span> {&lt;span style="color:#5af78e">&amp;#34;output&amp;#34;&lt;/span>: output&lt;span style="color:#ff6ac1">.&lt;/span>tolist()}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>api&lt;span style="color:#ff6ac1">.&lt;/span>add_resource(ModelInference, &lt;span style="color:#5af78e">&amp;#34;/predict&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">if&lt;/span> __name__ &lt;span style="color:#ff6ac1">==&lt;/span> &lt;span style="color:#5af78e">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app&lt;span style="color:#ff6ac1">.&lt;/span>run(debug&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">True&lt;/span>, host&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;0.0.0.0&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-åˆ›å»º-dockerfile" >
&lt;div>
&lt;a href="#2-%e5%88%9b%e5%bb%ba-dockerfile">
##
&lt;/a>
2. åˆ›å»º Dockerfile
&lt;/div>
&lt;/h3>
&lt;p>åˆ›å»ºä¸€ä¸ª Dockerfile æ¥æž„å»º Docker é•œåƒã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">FROM&lt;/span>&lt;span style="color:#5af78e"> python:3.9-slim&lt;/span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">WORKDIR&lt;/span>&lt;span style="color:#5af78e"> /app&lt;/span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">COPY&lt;/span> requirements.txt .&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">RUN&lt;/span> pip install --no-cache-dir -r requirements.txt&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">COPY&lt;/span> . .&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">&lt;/span>&lt;span style="color:#ff6ac1">CMD&lt;/span> [ &lt;span style="color:#5af78e">&amp;#34;python&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;app.py&amp;#34;&lt;/span> ]&lt;span style="color:#ff5c57">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="3-æž„å»º-docker-é•œåƒ" >
&lt;div>
&lt;a href="#3-%e6%9e%84%e5%bb%ba-docker-%e9%95%9c%e5%83%8f">
##
&lt;/a>
3. æž„å»º Docker é•œåƒ
&lt;/div>
&lt;/h3>
&lt;p>åœ¨åŒ…å« Dockerfile çš„ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥æž„å»º Docker é•œåƒã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker build -t your_image_name .
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="4-è¿è¡Œ-docker-å®¹å™¨" >
&lt;div>
&lt;a href="#4-%e8%bf%90%e8%a1%8c-docker-%e5%ae%b9%e5%99%a8">
##
&lt;/a>
4. è¿è¡Œ Docker å®¹å™¨
&lt;/div>
&lt;/h3>
&lt;p>ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥è¿è¡Œä½ çš„ Docker å®¹å™¨ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -p 5000:5000 your_image_name
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="5-æµ‹è¯•-api" >
&lt;div>
&lt;a href="#5-%e6%b5%8b%e8%af%95-api">
##
&lt;/a>
5. æµ‹è¯• API
&lt;/div>
&lt;/h3>
&lt;p>ä½¿ç”¨ä»»ä½• HTTP å®¢æˆ·ç«¯å·¥å…·æˆ– Python åº”ç”¨ç¨‹åºæ¥æµ‹è¯•ä½ çš„ APIã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> requests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data &lt;span style="color:#ff6ac1">=&lt;/span> {&lt;span style="color:#5af78e">&amp;#34;input&amp;#34;&lt;/span>: [&lt;span style="color:#ff9f43">1.0&lt;/span>, &lt;span style="color:#ff9f43">2.0&lt;/span>, &lt;span style="color:#ff9f43">3.0&lt;/span>]} &lt;span style="color:#78787e"># ç¤ºä¾‹è¾“å…¥æ•°æ®&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>response &lt;span style="color:#ff6ac1">=&lt;/span> requests&lt;span style="color:#ff6ac1">.&lt;/span>post(&lt;span style="color:#5af78e">&amp;#34;http://localhost:5000/predict&amp;#34;&lt;/span>, json&lt;span style="color:#ff6ac1">=&lt;/span>data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(response&lt;span style="color:#ff6ac1">.&lt;/span>json())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="æ€»ç»“-3" >
&lt;div>
&lt;a href="#%e6%80%bb%e7%bb%93-3">
##
&lt;/a>
æ€»ç»“
&lt;/div>
&lt;/h3>
&lt;p>è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„éƒ¨ç½²æµç¨‹ã€‚ä½ å¯ä»¥æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚è¿›è¡Œè°ƒæ•´å’Œæ‰©å±•ï¼Œæ¯”å¦‚æ·»åŠ æ¨¡åž‹é¢„å¤„ç†ã€åŽå¤„ç†é€»è¾‘ï¼Œä»¥åŠå¯¹ API çš„èº«ä»½éªŒè¯å’Œè®¿é—®æŽ§åˆ¶ç­‰åŠŸèƒ½ã€‚&lt;/p>
&lt;h1 id="project-03----cv" >
&lt;div>
&lt;a href="#project-03----cv">
##
&lt;/a>
Project 03 &amp;ndash; CV
&lt;/div>
&lt;/h1>
&lt;h2 id="yolo-model" >
&lt;div>
&lt;a href="#yolo-model">
#
&lt;/a>
YOLO model
&lt;/div>
&lt;/h2>
&lt;p>å¥½çš„ï¼Œè®©æˆ‘æ¥è¯¦ç»†ä»‹ç»ä¸€ä¸‹YOLOï¼ˆYou Only Look Onceï¼‰ç›®æ ‡æ£€æµ‹æ¨¡åž‹çš„åŽŸç†å’Œå®žçŽ°ã€‚&lt;/p>
&lt;h3 id="1-yolo-æ¨¡åž‹çš„åŽŸç†" >
&lt;div>
&lt;a href="#1-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8e%9f%e7%90%86">
##
&lt;/a>
1. YOLO æ¨¡åž‹çš„åŽŸç†ï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="11-å•é˜¶æ®µæ£€æµ‹" >
&lt;div>
&lt;a href="#11-%e5%8d%95%e9%98%b6%e6%ae%b5%e6%a3%80%e6%b5%8b">
###
&lt;/a>
1.1 å•é˜¶æ®µæ£€æµ‹ï¼š
&lt;/div>
&lt;/h4>
&lt;p>YOLO æ˜¯ä¸€ç§å•é˜¶æ®µç›®æ ‡æ£€æµ‹æ¨¡åž‹ï¼Œä¸Žä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ£€æµ‹æ–¹æ³•ï¼ˆå¦‚Faster R-CNNï¼‰ä¸åŒï¼Œå®ƒå°†ç›®æ ‡æ£€æµ‹ä»»åŠ¡è§†ä¸ºä¸€ä¸ªç«¯åˆ°ç«¯çš„å›žå½’é—®é¢˜ï¼Œç›´æŽ¥ä»Žå›¾åƒä¸­é¢„æµ‹ç›®æ ‡çš„ä½ç½®å’Œç±»åˆ«ã€‚&lt;/p>
&lt;h4 id="12-ç½‘ç»œç»“æž„" >
&lt;div>
&lt;a href="#12-%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84">
###
&lt;/a>
1.2 ç½‘ç»œç»“æž„ï¼š
&lt;/div>
&lt;/h4>
&lt;p>YOLO æ¨¡åž‹ä¸»è¦ç”±å·ç§¯ç¥žç»ç½‘ç»œç»„æˆï¼Œé€šå¸¸é‡‡ç”¨ç±»ä¼¼äºŽDarknetçš„æ·±å±‚å·ç§¯ç¥žç»ç½‘ç»œä½œä¸ºç‰¹å¾æå–å™¨ã€‚&lt;/p>
&lt;h4 id="13-ç½‘ç»œè¾“å‡º" >
&lt;div>
&lt;a href="#13-%e7%bd%91%e7%bb%9c%e8%be%93%e5%87%ba">
###
&lt;/a>
1.3 ç½‘ç»œè¾“å‡ºï¼š
&lt;/div>
&lt;/h4>
&lt;p>YOLO å°†å›¾åƒåˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„ç½‘æ ¼ï¼Œå¹¶ä¸ºæ¯ä¸ªç½‘æ ¼é¢„æµ‹å¤šä¸ªè¾¹ç•Œæ¡†å’Œå¯¹åº”çš„ç±»åˆ«æ¦‚çŽ‡ã€‚æ¯ä¸ªè¾¹ç•Œæ¡†ç”±äº”ä¸ªåæ ‡å€¼å’Œç±»åˆ«æ¦‚çŽ‡ç»„æˆï¼š$(x, y, w, h, p)$ï¼Œå…¶ä¸­ $(x, y)$ æ˜¯è¾¹ç•Œæ¡†çš„ä¸­å¿ƒåæ ‡ï¼Œ$(w, h)$ æ˜¯è¾¹ç•Œæ¡†çš„å®½åº¦å’Œé«˜åº¦ï¼Œ$p$ æ˜¯è¾¹ç•Œæ¡†åŒ…å«ç›®æ ‡çš„ç½®ä¿¡åº¦ã€‚&lt;/p>
&lt;h4 id="14-æŸå¤±å‡½æ•°" >
&lt;div>
&lt;a href="#14-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0">
###
&lt;/a>
1.4 æŸå¤±å‡½æ•°ï¼š
&lt;/div>
&lt;/h4>
&lt;p>YOLO æ¨¡åž‹ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°æ¥åŒæ—¶ä¼˜åŒ–è¾¹ç•Œæ¡†ä½ç½®çš„å‡†ç¡®æ€§å’Œç±»åˆ«çš„é¢„æµ‹ç²¾åº¦ã€‚è¯¥æŸå¤±å‡½æ•°åŒ…æ‹¬ä½ç½®è¯¯å·®ã€ç½®ä¿¡åº¦è¯¯å·®å’Œç±»åˆ«è¯¯å·®ã€‚&lt;/p>
&lt;h3 id="2-yolo-æ¨¡åž‹çš„å®žçŽ°" >
&lt;div>
&lt;a href="#2-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
2. YOLO æ¨¡åž‹çš„å®žçŽ°ï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="21-è®­ç»ƒé˜¶æ®µ" >
&lt;div>
&lt;a href="#21-%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b5">
###
&lt;/a>
2.1 è®­ç»ƒé˜¶æ®µï¼š
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>æ•°æ®å‡†å¤‡&lt;/strong>ï¼šæ”¶é›†å’Œæ ‡è®°è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬å›¾åƒå’Œå¯¹åº”çš„ç›®æ ‡è¾¹ç•Œæ¡†ã€‚&lt;/li>
&lt;li>&lt;strong>ç½‘ç»œç»“æž„&lt;/strong>ï¼šé€‰æ‹©é€‚å½“çš„ç½‘ç»œç»“æž„ï¼Œé€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„Darknetç½‘ç»œæˆ–å…¶å˜ç§ã€‚&lt;/li>
&lt;li>&lt;strong>æŸå¤±å‡½æ•°&lt;/strong>ï¼šå®šä¹‰å’Œå®žçŽ°ç»„åˆæŸå¤±å‡½æ•°ï¼Œç”¨äºŽä¼˜åŒ–ç½‘ç»œå‚æ•°ã€‚&lt;/li>
&lt;li>&lt;strong>è®­ç»ƒè¿‡ç¨‹&lt;/strong>ï¼šä½¿ç”¨è®­ç»ƒæ•°æ®é›†å¯¹æ¨¡åž‹è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡åå‘ä¼ æ’­ç®—æ³•æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚&lt;/li>
&lt;/ul>
&lt;h4 id="22-æŽ¨ç†é˜¶æ®µ" >
&lt;div>
&lt;a href="#22-%e6%8e%a8%e7%90%86%e9%98%b6%e6%ae%b5">
###
&lt;/a>
2.2 æŽ¨ç†é˜¶æ®µï¼š
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>å‰å‘ä¼ æ’­&lt;/strong>ï¼šå°†å¾…æ£€æµ‹çš„å›¾åƒè¾“å…¥åˆ°è®­ç»ƒå¥½çš„æ¨¡åž‹ä¸­ï¼Œé€šè¿‡å‰å‘ä¼ æ’­ç®—æ³•èŽ·å–æ¯ä¸ªè¾¹ç•Œæ¡†çš„é¢„æµ‹ç»“æžœã€‚&lt;/li>
&lt;li>&lt;strong>åŽå¤„ç†&lt;/strong>ï¼šå¯¹ç½‘ç»œè¾“å‡ºè¿›è¡ŒåŽå¤„ç†ï¼ŒåŒ…æ‹¬éžæžå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰å’Œé˜ˆå€¼ç­›é€‰ï¼Œä»¥åŽ»é™¤é‡å çš„è¾¹ç•Œæ¡†å’Œä½Žç½®ä¿¡åº¦çš„è¾¹ç•Œæ¡†ã€‚&lt;/li>
&lt;li>&lt;strong>ç›®æ ‡æ¡†ç»˜åˆ¶&lt;/strong>ï¼šæ ¹æ®æœ€ç»ˆçš„è¾¹ç•Œæ¡†ç»“æžœï¼Œå°†ç›®æ ‡æ¡†ç»˜åˆ¶åœ¨åŽŸå§‹å›¾åƒä¸Šï¼Œå¹¶æ ‡è®°ç±»åˆ«ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="3-yolo-æ¨¡åž‹çš„ä¼˜åŠ¿" >
&lt;div>
&lt;a href="#3-yolo-%e6%a8%a1%e5%9e%8b%e7%9a%84%e4%bc%98%e5%8a%bf">
##
&lt;/a>
3. YOLO æ¨¡åž‹çš„ä¼˜åŠ¿ï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>é€Ÿåº¦å¿«&lt;/strong>ï¼šYOLO æ˜¯ä¸€ç§é«˜æ•ˆçš„ç›®æ ‡æ£€æµ‹æ¨¡åž‹ï¼Œèƒ½å¤Ÿå®žæ—¶å¤„ç†å›¾åƒå’Œè§†é¢‘æµã€‚&lt;/li>
&lt;li>&lt;strong>ç«¯åˆ°ç«¯&lt;/strong>ï¼šYOLO å°†ç›®æ ‡æ£€æµ‹è§†ä¸ºä¸€ä¸ªç«¯åˆ°ç«¯çš„å›žå½’é—®é¢˜ï¼Œç®€åŒ–äº†æ£€æµ‹æµç¨‹ã€‚&lt;/li>
&lt;li>&lt;strong>å…¨å±€ä¿¡æ¯&lt;/strong>ï¼šYOLO åœ¨æ•´ä¸ªå›¾åƒä¸Šè¿›è¡Œæ£€æµ‹ï¼Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘å›¾åƒä¸­çš„å…¨å±€ä¿¡æ¯ï¼Œä»Žè€Œæ›´å¥½åœ°ç†è§£åœºæ™¯ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="4-æ€»ç»“" >
&lt;div>
&lt;a href="#4-%e6%80%bb%e7%bb%93">
##
&lt;/a>
4. æ€»ç»“ï¼š
&lt;/div>
&lt;/h3>
&lt;p>YOLO æ˜¯ä¸€ç§é«˜æ•ˆçš„å•é˜¶æ®µç›®æ ‡æ£€æµ‹æ¨¡åž‹ï¼Œé€šè¿‡å°†ç›®æ ‡æ£€æµ‹ä»»åŠ¡è½¬åŒ–ä¸ºå›žå½’é—®é¢˜ï¼Œå¹¶ç»“åˆæœ‰æ•ˆçš„ç½‘ç»œç»“æž„å’ŒæŸå¤±å‡½æ•°ï¼Œå®žçŽ°äº†åœ¨ä¿æŒé«˜å‡†ç¡®çŽ‡çš„åŒæ—¶å®žæ—¶è¿›è¡Œç›®æ ‡æ£€æµ‹çš„èƒ½åŠ›ã€‚&lt;/p>
&lt;p>&amp;ldquo;é”šå®šç‚¹&amp;rdquo;ï¼ˆAnchor Boxesï¼‰æ˜¯YOLOæ¨¡åž‹ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼Œå®ƒç”¨äºŽè§£å†³ç›®æ ‡æ£€æµ‹ä¸­ä¸åŒç›®æ ‡å°ºå¯¸å’Œæ¯”ä¾‹çš„é—®é¢˜ã€‚åœ¨YOLOä¸­ï¼Œæ¯ä¸ªç½‘æ ¼å•å…ƒéƒ½è´Ÿè´£é¢„æµ‹ä¸€ç»„å›ºå®šæ•°é‡å’Œå›ºå®šå¤§å°çš„è¾¹ç•Œæ¡†ï¼ˆå³é”šå®šç‚¹ï¼‰ï¼Œä»¥ä¾¿æ£€æµ‹ä¸åŒå°ºå¯¸å’Œæ¯”ä¾‹çš„ç›®æ ‡ã€‚&lt;/p>
&lt;h3 id="1-é”šå®šç‚¹çš„æ¦‚å¿µ" >
&lt;div>
&lt;a href="#1-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%a6%82%e5%bf%b5">
##
&lt;/a>
1. é”šå®šç‚¹çš„æ¦‚å¿µï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="11-é—®é¢˜" >
&lt;div>
&lt;a href="#11-%e9%97%ae%e9%a2%98">
###
&lt;/a>
1.1 é—®é¢˜ï¼š
&lt;/div>
&lt;/h4>
&lt;p>ä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹ç®—æ³•é€šå¸¸ä¼šå°†ä¸åŒå°ºå¯¸å’Œæ¯”ä¾‹çš„ç›®æ ‡åˆ†é…ç»™ä¸åŒçš„ç½‘ç»œå±‚æ¥å¤„ç†ï¼Œè¿™ç§æ–¹æ³•ä¸å¤Ÿçµæ´»ï¼Œæ— æ³•å¾ˆå¥½åœ°é€‚åº”å¤šæ ·åŒ–çš„ç›®æ ‡ã€‚&lt;/p>
&lt;h4 id="12-è§£å†³æ–¹æ³•" >
&lt;div>
&lt;a href="#12-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%b3%95">
###
&lt;/a>
1.2 è§£å†³æ–¹æ³•ï¼š
&lt;/div>
&lt;/h4>
&lt;p>YOLOä½¿ç”¨é”šå®šç‚¹çš„æ€æƒ³ï¼Œå°†ä¸åŒå°ºå¯¸å’Œæ¯”ä¾‹çš„ç›®æ ‡ç»Ÿä¸€åˆ†é…ç»™æ¯ä¸ªç½‘æ ¼å•å…ƒï¼Œå¹¶åœ¨æ¯ä¸ªç½‘æ ¼å•å…ƒä¸­é¢„æµ‹å›ºå®šæ•°é‡çš„è¾¹ç•Œæ¡†ã€‚è¿™æ ·å¯ä»¥å¢žåŠ æ¨¡åž‹çš„çµæ´»æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ£€æµ‹ä¸åŒå°ºå¯¸å’Œæ¯”ä¾‹çš„ç›®æ ‡ã€‚&lt;/p>
&lt;h3 id="2-é”šå®šç‚¹çš„æ€æƒ³" >
&lt;div>
&lt;a href="#2-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%80%9d%e6%83%b3">
##
&lt;/a>
2. é”šå®šç‚¹çš„æ€æƒ³ï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="21-é¢„å®šä¹‰å¤§å°å’Œæ¯”ä¾‹" >
&lt;div>
&lt;a href="#21-%e9%a2%84%e5%ae%9a%e4%b9%89%e5%a4%a7%e5%b0%8f%e5%92%8c%e6%af%94%e4%be%8b">
###
&lt;/a>
2.1 é¢„å®šä¹‰å¤§å°å’Œæ¯”ä¾‹ï¼š
&lt;/div>
&lt;/h4>
&lt;p>åœ¨YOLOä¸­ï¼Œé”šå®šç‚¹æ˜¯ä¸€ç»„é¢„å®šä¹‰çš„å¤§å°å’Œæ¯”ä¾‹çš„è¾¹ç•Œæ¡†ã€‚è¿™äº›è¾¹ç•Œæ¡†é€šå¸¸æ˜¯åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šé€šè¿‡èšç±»ç­‰æ–¹æ³•å¾—åˆ°çš„ï¼Œä»¥ç¡®ä¿æ¶µç›–äº†å¤§éƒ¨åˆ†ç›®æ ‡çš„å¤§å°å’Œæ¯”ä¾‹ã€‚&lt;/p>
&lt;h4 id="22-å•å…ƒæ ¼å†…å¤šä¸ªè¾¹ç•Œæ¡†" >
&lt;div>
&lt;a href="#22-%e5%8d%95%e5%85%83%e6%a0%bc%e5%86%85%e5%a4%9a%e4%b8%aa%e8%be%b9%e7%95%8c%e6%a1%86">
###
&lt;/a>
2.2 å•å…ƒæ ¼å†…å¤šä¸ªè¾¹ç•Œæ¡†ï¼š
&lt;/div>
&lt;/h4>
&lt;p>å¯¹äºŽæ¯ä¸ªç½‘æ ¼å•å…ƒï¼ŒYOLOæ¨¡åž‹é¢„æµ‹å›ºå®šæ•°é‡çš„è¾¹ç•Œæ¡†ï¼Œæ¯ä¸ªè¾¹ç•Œæ¡†çš„å¤§å°å’Œæ¯”ä¾‹ä¸Žé¢„å®šä¹‰çš„é”šå®šç‚¹ç›¸å¯¹åº”ã€‚è¿™æ ·ï¼Œæ¯ä¸ªç½‘æ ¼å•å…ƒå¯ä»¥åŒæ—¶æ£€æµ‹å¤šä¸ªä¸åŒå°ºå¯¸å’Œæ¯”ä¾‹çš„ç›®æ ‡ã€‚&lt;/p>
&lt;h3 id="3-é”šå®šç‚¹çš„æŠ€æœ¯å®žçŽ°" >
&lt;div>
&lt;a href="#3-%e9%94%9a%e5%ae%9a%e7%82%b9%e7%9a%84%e6%8a%80%e6%9c%af%e5%ae%9e%e7%8e%b0">
##
&lt;/a>
3. é”šå®šç‚¹çš„æŠ€æœ¯å®žçŽ°ï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="31-é¢„è®­ç»ƒé”šå®šç‚¹" >
&lt;div>
&lt;a href="#31-%e9%a2%84%e8%ae%ad%e7%bb%83%e9%94%9a%e5%ae%9a%e7%82%b9">
###
&lt;/a>
3.1 é¢„è®­ç»ƒé”šå®šç‚¹ï¼š
&lt;/div>
&lt;/h4>
&lt;p>åœ¨è®­ç»ƒä¹‹å‰ï¼Œé€šå¸¸ä¼šé€šè¿‡å¯¹è®­ç»ƒæ•°æ®é›†ä¸­çš„ç›®æ ‡è¾¹ç•Œæ¡†è¿›è¡Œèšç±»æˆ–è€…æ‰‹åŠ¨é€‰æ‹©ï¼Œæ¥ç¡®å®šä¸€ç»„é”šå®šç‚¹çš„å¤§å°å’Œæ¯”ä¾‹ã€‚&lt;/p>
&lt;h4 id="32-æ¨¡åž‹é¢„æµ‹" >
&lt;div>
&lt;a href="#32-%e6%a8%a1%e5%9e%8b%e9%a2%84%e6%b5%8b">
###
&lt;/a>
3.2 æ¨¡åž‹é¢„æµ‹ï¼š
&lt;/div>
&lt;/h4>
&lt;p>åœ¨æ¨¡åž‹æŽ¨ç†é˜¶æ®µï¼Œæ¯ä¸ªç½‘æ ¼å•å…ƒé€šè¿‡å›žå½’é¢„æµ‹å›ºå®šæ•°é‡çš„è¾¹ç•Œæ¡†ï¼Œæ¯ä¸ªè¾¹ç•Œæ¡†çš„å°ºå¯¸å’Œæ¯”ä¾‹ç”±å¯¹åº”çš„é”šå®šç‚¹ç¡®å®šã€‚&lt;/p>
&lt;h4 id="33-è¾¹ç•Œæ¡†è°ƒæ•´" >
&lt;div>
&lt;a href="#33-%e8%be%b9%e7%95%8c%e6%a1%86%e8%b0%83%e6%95%b4">
###
&lt;/a>
3.3 è¾¹ç•Œæ¡†è°ƒæ•´ï¼š
&lt;/div>
&lt;/h4>
&lt;p>æ¨¡åž‹é¢„æµ‹çš„è¾¹ç•Œæ¡†é€šå¸¸æ˜¯ç›¸å¯¹äºŽç½‘æ ¼å•å…ƒçš„åç§»é‡å’Œå°ºå¯¸åå·®ï¼Œéœ€è¦æ ¹æ®é”šå®šç‚¹è¿›è¡Œè°ƒæ•´ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾¹ç•Œæ¡†ä½ç½®ã€‚&lt;/p>
&lt;h3 id="4-æ€»ç»“-1" >
&lt;div>
&lt;a href="#4-%e6%80%bb%e7%bb%93-1">
##
&lt;/a>
4. æ€»ç»“ï¼š
&lt;/div>
&lt;/h3>
&lt;p>é”šå®šç‚¹æ˜¯YOLOæ¨¡åž‹ä¸­ç”¨äºŽè§£å†³ä¸åŒå°ºå¯¸å’Œæ¯”ä¾‹ç›®æ ‡æ£€æµ‹é—®é¢˜çš„å…³é”®æ¦‚å¿µï¼Œé€šè¿‡é¢„å®šä¹‰ä¸€ç»„å¤§å°å’Œæ¯”ä¾‹çš„è¾¹ç•Œæ¡†ï¼Œå¹¶åœ¨æ¯ä¸ªç½‘æ ¼å•å…ƒä¸­é¢„æµ‹è¿™äº›è¾¹ç•Œæ¡†çš„ä½ç½®å’Œç±»åˆ«ï¼Œå®žçŽ°äº†æ¨¡åž‹å¯¹å¤šæ ·åŒ–ç›®æ ‡çš„æœ‰æ•ˆæ£€æµ‹ã€‚&lt;/p>
&lt;h2 id="transfer-learning" >
&lt;div>
&lt;a href="#transfer-learning">
#
&lt;/a>
transfer-learning
&lt;/div>
&lt;/h2>
&lt;p>è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡å°†å·²å­¦ä¹ çš„çŸ¥è¯†ä»Žä¸€ä¸ªä»»åŠ¡æˆ–é¢†åŸŸåº”ç”¨åˆ°å¦ä¸€ä¸ªä»»åŠ¡æˆ–é¢†åŸŸçš„æœºå™¨å­¦ä¹ æŠ€æœ¯ã€‚åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œè¿ç§»å­¦ä¹ å¯ä»¥é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ¨¡åž‹æˆ–ç‰¹å¾æ¥æå‡æ¨¡åž‹æ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼š&lt;/p>
&lt;h3 id="1-å¾®è°ƒfine-tuning" >
&lt;div>
&lt;a href="#1-%e5%be%ae%e8%b0%83fine-tuning">
##
&lt;/a>
1. å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>å¾®è°ƒæ˜¯è¿ç§»å­¦ä¹ ä¸­æœ€å¸¸è§çš„æ–¹æ³•ä¹‹ä¸€ï¼Œå®ƒé€šå¸¸åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;strong>é¢„è®­ç»ƒæ¨¡åž‹é€‰æ‹©&lt;/strong>ï¼šé€‰æ‹©ä¸€ä¸ªåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¥½çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ ImageNet ä¸Šçš„é¢„è®­ç»ƒæ¨¡åž‹ã€‚&lt;/li>
&lt;li>&lt;strong>æ¨¡åž‹å†»ç»“&lt;/strong>ï¼šå°†é¢„è®­ç»ƒæ¨¡åž‹çš„éƒ¨åˆ†æˆ–å…¨éƒ¨å±‚å†»ç»“ï¼Œå³ä¸æ›´æ–°å®ƒä»¬çš„æƒé‡ã€‚&lt;/li>
&lt;li>&lt;strong>é¡¶å±‚æ›¿æ¢&lt;/strong>ï¼šæ›¿æ¢é¢„è®­ç»ƒæ¨¡åž‹çš„é¡¶å±‚ï¼ˆé€šå¸¸æ˜¯å…¨è¿žæŽ¥å±‚ï¼‰æˆ–è€…æ·»åŠ æ–°çš„å…¨è¿žæŽ¥å±‚ï¼Œä»¥é€‚åº”æ–°çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚&lt;/li>
&lt;li>&lt;strong>å¾®è°ƒè®­ç»ƒ&lt;/strong>ï¼šåœ¨ç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šå¯¹æ•´ä¸ªæ¨¡åž‹è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬æ›´æ–°é¡¶å±‚å’Œéƒ¨åˆ†æˆ–å…¨éƒ¨è§£å†»çš„å±‚ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="2-ç‰¹å¾æå–feature-extraction" >
&lt;div>
&lt;a href="#2-%e7%89%b9%e5%be%81%e6%8f%90%e5%8f%96feature-extraction">
##
&lt;/a>
2. ç‰¹å¾æå–ï¼ˆFeature Extractionï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>ç‰¹å¾æå–æ˜¯ä¸€ç§æ›´è½»é‡çº§çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå®ƒä¸æ¶‰åŠåˆ°æ•´ä¸ªæ¨¡åž‹çš„é‡æ–°è®­ç»ƒï¼Œè€Œæ˜¯ä»…ä»…åˆ©ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚&lt;/p>
&lt;ul>
&lt;li>&lt;strong>é¢„è®­ç»ƒæ¨¡åž‹é€‰æ‹©&lt;/strong>ï¼šåŒæ ·é€‰æ‹©ä¸€ä¸ªåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¥½çš„æ¨¡åž‹ã€‚&lt;/li>
&lt;li>&lt;strong>ç‰¹å¾æå–&lt;/strong>ï¼šå°†é¢„è®­ç»ƒæ¨¡åž‹çš„å·ç§¯å±‚ï¼ˆé€šå¸¸æ˜¯é™¤äº†å…¨è¿žæŽ¥å±‚ä¹‹å¤–çš„æ‰€æœ‰å±‚ï¼‰ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶å°†æå–åˆ°çš„ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œç”¨äºŽè®­ç»ƒä¸€ä¸ªæ–°çš„åˆ†ç±»å™¨æˆ–ç›®æ ‡æ£€æµ‹å™¨ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="3-é¢†åŸŸè‡ªé€‚åº”domain-adaptation" >
&lt;div>
&lt;a href="#3-%e9%a2%86%e5%9f%9f%e8%87%aa%e9%80%82%e5%ba%94domain-adaptation">
##
&lt;/a>
3. é¢†åŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>é¢†åŸŸè‡ªé€‚åº”æ˜¯ä¸€ç§ç‰¹æ®Šçš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºŽè§£å†³æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ã€‚åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå¯ä»¥é€šè¿‡åœ¨æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´è¿›è¡ŒåŸŸé€‚åº”æ¥æé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p>
&lt;ul>
&lt;li>&lt;strong>åŸŸé€‚åº”æ–¹æ³•&lt;/strong>ï¼šå¸¸è§çš„åŸŸé€‚åº”æ–¹æ³•åŒ…æ‹¬å¯¹æŠ—è®­ç»ƒã€é¢†åŸŸå¯¹é½ç­‰ï¼Œé€šè¿‡è°ƒæ•´æ¨¡åž‹çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿å¾—æ¨¡åž‹åœ¨ç›®æ ‡åŸŸä¸Šè¡¨çŽ°æ›´å¥½ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="4-çŸ¥è¯†è’¸é¦knowledge-distillation" >
&lt;div>
&lt;a href="#4-%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8fknowledge-distillation">
##
&lt;/a>
4. çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§é€šè¿‡åˆ©ç”¨å·²è®­ç»ƒå¥½çš„æ¨¡åž‹çš„çŸ¥è¯†æ¥è®­ç»ƒä¸€ä¸ªæ›´è½»é‡çº§çš„æ¨¡åž‹çš„æ–¹æ³•ã€‚åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå¯ä»¥åˆ©ç”¨ä¸€ä¸ªå¤§åž‹æ¨¡åž‹çš„çŸ¥è¯†æ¥è®­ç»ƒä¸€ä¸ªå°åž‹æ¨¡åž‹ï¼Œä»¥é™ä½Žæ¨¡åž‹çš„å¤æ‚åº¦å’Œè®¡ç®—æˆæœ¬ã€‚&lt;/p>
&lt;p>ä»¥ä¸Šæ˜¯ä¸€äº›å¸¸ç”¨çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡éœ€æ±‚å’Œæƒ…å†µé€‰æ‹©åˆé€‚çš„æ–¹æ³•æ¥æå‡ç›®æ ‡æ£€æµ‹æ¨¡åž‹çš„æ€§èƒ½ã€‚&lt;/p>
&lt;h2 id="å›¾åƒè¯†åˆ«é•¿å°¾åˆ†å¸ƒé—®é¢˜" >
&lt;div>
&lt;a href="#%e5%9b%be%e5%83%8f%e8%af%86%e5%88%ab%e9%95%bf%e5%b0%be%e5%88%86%e5%b8%83%e9%97%ae%e9%a2%98">
#
&lt;/a>
å›¾åƒè¯†åˆ«é•¿å°¾åˆ†å¸ƒé—®é¢˜
&lt;/div>
&lt;/h2>
&lt;p>å¤„ç†è®­ç»ƒæ•°æ®é›†ä¸­çš„é•¿å°¾åˆ†å¸ƒæ˜¯ä¸€ä¸ªå¸¸è§çš„é—®é¢˜ï¼Œåœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ä¹ŸåŒæ ·å­˜åœ¨ã€‚é•¿å°¾åˆ†å¸ƒæ„å‘³ç€æœ‰äº›ç±»åˆ«çš„æ ·æœ¬æ•°é‡éžå¸¸å°‘ï¼Œè€Œå¦ä¸€äº›ç±»åˆ«çš„æ ·æœ¬æ•°é‡éžå¸¸å¤šã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•åŒ…æ‹¬ï¼š&lt;/p>
&lt;h3 id="1-æ•°æ®å¢žå¼ºdata-augmentation" >
&lt;div>
&lt;a href="#1-%e6%95%b0%e6%8d%ae%e5%a2%9e%e5%bc%badata-augmentation">
##
&lt;/a>
1. æ•°æ®å¢žå¼ºï¼ˆData Augmentationï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>å¯¹äºŽå°‘æ ·æœ¬ç±»åˆ«ï¼Œå¯ä»¥é€šè¿‡æ•°æ®å¢žå¼ºæŠ€æœ¯æ¥ç”Ÿæˆæ›´å¤šçš„æ ·æœ¬ï¼Œä»¥å¹³è¡¡ä¸åŒç±»åˆ«ä¹‹é—´çš„æ ·æœ¬æ•°é‡å·®å¼‚ã€‚å¸¸ç”¨çš„æ•°æ®å¢žå¼ºæŠ€æœ¯åŒ…æ‹¬éšæœºæ—‹è½¬ã€è£å‰ªã€ç¼©æ”¾ã€å¹³ç§»ã€é¢œè‰²å˜æ¢ç­‰ã€‚&lt;/p>
&lt;h3 id="2-ç±»åˆ«åŠ æƒclass-weighting" >
&lt;div>
&lt;a href="#2-%e7%b1%bb%e5%88%ab%e5%8a%a0%e6%9d%83class-weighting">
##
&lt;/a>
2. ç±»åˆ«åŠ æƒï¼ˆClass Weightingï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>å¯¹äºŽé•¿å°¾åˆ†å¸ƒçš„æ•°æ®é›†ï¼Œå¯ä»¥é‡‡ç”¨ç±»åˆ«åŠ æƒçš„æ–¹å¼æ¥è°ƒæ•´æ¨¡åž‹çš„æŸå¤±å‡½æ•°ï¼Œä½¿å¾—æ¨¡åž‹å¯¹å°‘æ ·æœ¬ç±»åˆ«æ›´åŠ æ•æ„Ÿã€‚å¯ä»¥æ ¹æ®ç±»åˆ«å‡ºçŽ°çš„é¢‘çŽ‡æ¥è®¾ç½®ä¸åŒç±»åˆ«çš„æƒé‡ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æ›´å¹³è¡¡ã€‚&lt;/p>
&lt;h3 id="3-é‡æ–°é‡‡æ ·resampling" >
&lt;div>
&lt;a href="#3-%e9%87%8d%e6%96%b0%e9%87%87%e6%a0%b7resampling">
##
&lt;/a>
3. é‡æ–°é‡‡æ ·ï¼ˆResamplingï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>é‡æ–°é‡‡æ ·æŠ€æœ¯å¯ä»¥é€šè¿‡è¿‡é‡‡æ ·æˆ–æ¬ é‡‡æ ·æ¥è°ƒæ•´æ•°æ®é›†ä¸­ä¸åŒç±»åˆ«çš„æ ·æœ¬æ•°é‡ã€‚å¯¹äºŽå°‘æ ·æœ¬ç±»åˆ«ï¼Œå¯ä»¥é‡‡ç”¨è¿‡é‡‡æ ·çš„æ–¹æ³•å¢žåŠ æ ·æœ¬æ•°é‡ï¼Œæˆ–è€…é‡‡ç”¨æ¬ é‡‡æ ·çš„æ–¹æ³•å‡å°‘æ ·æœ¬æ•°é‡ã€‚&lt;/p>
&lt;h3 id="4-å¼±ç›‘ç£å­¦ä¹ weakly-supervised-learning" >
&lt;div>
&lt;a href="#4-%e5%bc%b1%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0weakly-supervised-learning">
##
&lt;/a>
4. å¼±ç›‘ç£å­¦ä¹ ï¼ˆWeakly Supervised Learningï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>åœ¨é•¿å°¾åˆ†å¸ƒçš„æ•°æ®é›†ä¸­ï¼Œæœ‰äº›ç±»åˆ«å¯èƒ½åªæœ‰å°‘é‡çš„æœ‰æ ‡ç­¾æ ·æœ¬ï¼Œè€Œå¤§éƒ¨åˆ†æ ·æœ¬æ˜¯æœªæ ‡ç­¾çš„ã€‚å¯ä»¥åˆ©ç”¨å¼±ç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨æ— ç›‘ç£æˆ–åŠç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œä»Žæœªæ ‡ç­¾çš„æ•°æ®ä¸­å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾ã€‚&lt;/p>
&lt;h3 id="5-å¤šä»»åŠ¡å­¦ä¹ multi-task-learning" >
&lt;div>
&lt;a href="#5-%e5%a4%9a%e4%bb%bb%e5%8a%a1%e5%ad%a6%e4%b9%a0multi-task-learning">
##
&lt;/a>
5. å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMulti-Task Learningï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>å¤šä»»åŠ¡å­¦ä¹ å¯ä»¥å°†ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Žå…¶ä»–ç›¸å…³ä»»åŠ¡ç»“åˆèµ·æ¥è®­ç»ƒæ¨¡åž‹ï¼Œä»Žè€Œæå‡æ¨¡åž‹åœ¨é•¿å°¾åˆ†å¸ƒæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Žå›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ç»“åˆèµ·æ¥è¿›è¡Œè®­ç»ƒï¼Œä»Žè€Œåˆ©ç”¨é¢å¤–çš„ä¿¡æ¯æ¥æå‡æ¨¡åž‹æ€§èƒ½ã€‚&lt;/p>
&lt;h3 id="6-å¼‚å¸¸æ£€æµ‹anomaly-detection" >
&lt;div>
&lt;a href="#6-%e5%bc%82%e5%b8%b8%e6%a3%80%e6%b5%8banomaly-detection">
##
&lt;/a>
6. å¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly Detectionï¼‰ï¼š
&lt;/div>
&lt;/h3>
&lt;p>é’ˆå¯¹é•¿å°¾åˆ†å¸ƒæ•°æ®é›†ä¸­çš„å°‘æ ·æœ¬ç±»åˆ«ï¼Œå¯ä»¥é‡‡ç”¨å¼‚å¸¸æ£€æµ‹çš„æ–¹æ³•æ¥è¯†åˆ«å¹¶é‡ç‚¹å…³æ³¨è¿™äº›å°‘æ ·æœ¬ç±»åˆ«ï¼Œä»¥ä¾¿æ›´å¥½åœ°è°ƒæ•´æ¨¡åž‹å’Œä¼˜åŒ–æ€§èƒ½ã€‚&lt;/p>
&lt;p>é€šè¿‡ä»¥ä¸Šæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†è®­ç»ƒæ•°æ®é›†ä¸­çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œå¹¶æå‡æ¨¡åž‹åœ¨ä¸åŒç±»åˆ«ä¸Šçš„æ€§èƒ½ã€‚åœ¨å®žè·µä¸­ï¼Œé€šå¸¸éœ€è¦æ ¹æ®å…·ä½“çš„æ•°æ®é›†å’Œä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ–¹æ³•è¿›è¡Œå¤„ç†ã€‚&lt;/p>
&lt;h2 id="auto-encoder-model" >
&lt;div>
&lt;a href="#auto-encoder-model">
#
&lt;/a>
Auto-encoder model
&lt;/div>
&lt;/h2>
&lt;p>é¢†åŸŸè‡ªé€‚åº”æ˜¯ä¸€ç§å°†çŸ¥è¯†ä»Žä¸€ä¸ªåŸŸï¼ˆæºåŸŸï¼‰è¿ç§»åˆ°å¦ä¸€ä¸ªåŸŸï¼ˆç›®æ ‡åŸŸï¼‰çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºŽè§£å†³æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ã€‚åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œé¢†åŸŸè‡ªé€‚åº”å¯ä»¥é€šè¿‡å¯¹æŠ—è®­ç»ƒçš„æ–¹å¼æ¥å®žçŽ°ï¼Œè€Œ auto-encoder å¯ä»¥ä½œä¸ºä¸€ç§è¾…åŠ©æ‰‹æ®µæ¥å¸®åŠ©å®žçŽ°å¯¹æŠ—è®­ç»ƒã€‚ä»¥ä¸‹æ˜¯å…·ä½“çš„æ€è·¯å’Œç»†èŠ‚ï¼š&lt;/p>
&lt;h3 id="1-å¯¹æŠ—è®­ç»ƒçš„åŸºæœ¬æ€è·¯" >
&lt;div>
&lt;a href="#1-%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%80%9d%e8%b7%af">
##
&lt;/a>
1. å¯¹æŠ—è®­ç»ƒçš„åŸºæœ¬æ€è·¯ï¼š
&lt;/div>
&lt;/h3>
&lt;p>å¯¹æŠ—è®­ç»ƒæ˜¯ä¸€ç§é€šè¿‡è®­ç»ƒä¸€ä¸ªç”Ÿæˆå™¨å’Œä¸€ä¸ªåˆ¤åˆ«å™¨çš„å¯¹æŠ—è¿‡ç¨‹ï¼Œæ¥ä½¿å¾—ç”Ÿæˆå™¨äº§ç”Ÿçš„æ•°æ®åˆ†å¸ƒä¸Žç›®æ ‡åŸŸçš„æ•°æ®åˆ†å¸ƒå°½å¯èƒ½åœ°æŽ¥è¿‘ã€‚åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå¯ä»¥é€šè¿‡å¯¹æŠ—è®­ç»ƒæ¥è°ƒæ•´æ¨¡åž‹ï¼Œä½¿å¾—æ¨¡åž‹åœ¨ç›®æ ‡åŸŸä¸Šè¡¨çŽ°æ›´å¥½ã€‚&lt;/p>
&lt;h3 id="2-auto-encoder-åœ¨å¯¹æŠ—è®­ç»ƒä¸­çš„ä½œç”¨" >
&lt;div>
&lt;a href="#2-auto-encoder-%e5%9c%a8%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83%e4%b8%ad%e7%9a%84%e4%bd%9c%e7%94%a8">
##
&lt;/a>
2. auto-encoder åœ¨å¯¹æŠ—è®­ç»ƒä¸­çš„ä½œç”¨ï¼š
&lt;/div>
&lt;/h3>
&lt;p>auto-encoder æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ¨¡åž‹ï¼Œå®ƒå¯ä»¥å°†è¾“å…¥æ•°æ®ç¼–ç æˆä½Žç»´è¡¨ç¤ºï¼Œå¹¶å°†å…¶è§£ç å›žåŽŸå§‹æ•°æ®ã€‚åœ¨å¯¹æŠ—è®­ç»ƒä¸­ï¼Œauto-encoder å¯ä»¥ä½œä¸ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œç”¨äºŽç”Ÿæˆä¸Žç›®æ ‡åŸŸæ•°æ®åˆ†å¸ƒç›¸ä¼¼çš„æ•°æ®æ ·æœ¬ã€‚é€šè¿‡è®­ç»ƒ auto-encoderï¼Œå¯ä»¥å­¦ä¹ åˆ°ç›®æ ‡åŸŸæ•°æ®çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»Žè€Œå¸®åŠ©å®žçŽ°å¯¹æŠ—è®­ç»ƒã€‚&lt;/p>
&lt;h3 id="3-å…·ä½“å®žçŽ°æ­¥éª¤" >
&lt;div>
&lt;a href="#3-%e5%85%b7%e4%bd%93%e5%ae%9e%e7%8e%b0%e6%ad%a5%e9%aa%a4">
##
&lt;/a>
3. å…·ä½“å®žçŽ°æ­¥éª¤ï¼š
&lt;/div>
&lt;/h3>
&lt;h4 id="31-æ¨¡åž‹é€‰æ‹©" >
&lt;div>
&lt;a href="#31-%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9">
###
&lt;/a>
3.1 æ¨¡åž‹é€‰æ‹©ï¼š
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡åž‹ä½œä¸ºåŸºç¡€æ¨¡åž‹ï¼Œä¾‹å¦‚åœ¨æºåŸŸä¸Šè®­ç»ƒå¥½çš„ç›®æ ‡æ£€æµ‹æ¨¡åž‹ã€‚&lt;/li>
&lt;/ul>
&lt;h4 id="32-auto-encoder-è®­ç»ƒ" >
&lt;div>
&lt;a href="#32-auto-encoder-%e8%ae%ad%e7%bb%83">
###
&lt;/a>
3.2 auto-encoder è®­ç»ƒï¼š
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>åœ¨ç›®æ ‡åŸŸä¸Šæ”¶é›†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒ auto-encoderã€‚auto-encoder çš„è¾“å…¥ä¸ºç›®æ ‡åŸŸçš„å›¾åƒæ•°æ®ï¼Œè¾“å‡ºä¸ºé‡æž„çš„å›¾åƒæ•°æ®ã€‚é€šè¿‡è®­ç»ƒ auto-encoderï¼Œå¯ä»¥å­¦ä¹ åˆ°ç›®æ ‡åŸŸæ•°æ®çš„ç‰¹å¾è¡¨ç¤ºã€‚&lt;/li>
&lt;/ul>
&lt;h4 id="33-å¯¹æŠ—è®­ç»ƒ" >
&lt;div>
&lt;a href="#33-%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83">
###
&lt;/a>
3.3 å¯¹æŠ—è®­ç»ƒï¼š
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>å°†è®­ç»ƒå¥½çš„ auto-encoder ä½œä¸ºç”Ÿæˆå™¨ï¼Œå°†åŸºç¡€æ¨¡åž‹ï¼ˆæºåŸŸä¸Šé¢„è®­ç»ƒçš„æ¨¡åž‹ï¼‰ä½œä¸ºåˆ¤åˆ«å™¨ã€‚&lt;/li>
&lt;li>å°†æºåŸŸå’Œç›®æ ‡åŸŸçš„æ•°æ®åˆ†åˆ«è¾“å…¥åˆ° auto-encoder å’ŒåŸºç¡€æ¨¡åž‹ä¸­ï¼Œç”Ÿæˆå™¨å°è¯•ç”Ÿæˆä¸Žç›®æ ‡åŸŸæ•°æ®åˆ†å¸ƒç›¸ä¼¼çš„æ•°æ®æ ·æœ¬ï¼Œè€Œåˆ¤åˆ«å™¨åˆ™å°è¯•åŒºåˆ†çœŸå®žçš„ç›®æ ‡åŸŸæ•°æ®å’Œç”Ÿæˆå™¨ç”Ÿæˆçš„æ•°æ®ã€‚&lt;/li>
&lt;li>é€šè¿‡å¯¹æŠ—è®­ç»ƒçš„è¿‡ç¨‹ï¼Œè°ƒæ•´ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„å‚æ•°ï¼Œä½¿å¾—ç”Ÿæˆå™¨ç”Ÿæˆçš„æ•°æ®åˆ†å¸ƒä¸Žç›®æ ‡åŸŸçš„æ•°æ®åˆ†å¸ƒå°½å¯èƒ½åœ°æŽ¥è¿‘ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="4-è¿›ä¸€æ­¥ç»†åŒ–" >
&lt;div>
&lt;a href="#4-%e8%bf%9b%e4%b8%80%e6%ad%a5%e7%bb%86%e5%8c%96">
##
&lt;/a>
4. è¿›ä¸€æ­¥ç»†åŒ–ï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>å¯ä»¥è€ƒè™‘ä½¿ç”¨å¸¦æœ‰é‡å»ºæŸå¤±çš„å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆAdversarial Auto-Encoder, AAEï¼‰æ¥è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠ å¼º auto-encoder çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›å’Œç”Ÿæˆèƒ½åŠ›ã€‚&lt;/li>
&lt;li>å¯ä»¥é€šè¿‡è°ƒæ•´è®­ç»ƒç­–ç•¥å’Œè¶…å‚æ•°æ¥è¿›ä¸€æ­¥ä¼˜åŒ–å¯¹æŠ—è®­ç»ƒçš„æ•ˆæžœï¼Œä¾‹å¦‚å­¦ä¹ çŽ‡ã€è®­ç»ƒè½®æ•°ç­‰ã€‚&lt;/li>
&lt;/ul>
&lt;p>é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œå¯ä»¥åˆ©ç”¨ auto-encoder å’Œå¯¹æŠ—è®­ç»ƒçš„æ–¹æ³•æ¥å®žçŽ°é¢†åŸŸè‡ªé€‚åº”ï¼Œä»Žè€Œæå‡ç›®æ ‡æ£€æµ‹æ¨¡åž‹åœ¨ç›®æ ‡åŸŸä¸Šçš„æ€§èƒ½ã€‚&lt;/p>
&lt;h2 id="ç›®æ ‡æ£€æµ‹çš„è¯„ä¼°æŒ‡æ ‡map_50" >
&lt;div>
&lt;a href="#%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e7%9a%84%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87map_50">
#
&lt;/a>
ç›®æ ‡æ£€æµ‹çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆ$mAP_{50}$ï¼‰
&lt;/div>
&lt;/h2>
&lt;p>mAP_50 æ˜¯ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ä¹‹ä¸€ï¼Œå®ƒè¡¨ç¤ºåœ¨ IoU é˜ˆå€¼ä¸º 0.5 æ—¶çš„å¹³å‡ç²¾ç¡®çŽ‡ï¼ˆmAPï¼ŒMean Average Precisionï¼‰ã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹è¿™ä¸ªæŒ‡æ ‡ï¼š&lt;/p>
&lt;h3 id="1-å¹³å‡ç²¾ç¡®çŽ‡-average-precision-ap" >
&lt;div>
&lt;a href="#1-%e5%b9%b3%e5%9d%87%e7%b2%be%e7%a1%ae%e7%8e%87-average-precision-ap">
##
&lt;/a>
1. å¹³å‡ç²¾ç¡®çŽ‡ (Average Precision, AP)ï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>å¹³å‡ç²¾ç¡®çŽ‡æ˜¯ Precision-Recall æ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œç”¨äºŽè¡¡é‡æ¨¡åž‹åœ¨ä¸åŒ Recall ä¸‹çš„å¹³å‡ç²¾ç¡®çŽ‡ã€‚åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒAP è¡¨ç¤ºæ¨¡åž‹å¯¹å•ä¸ªç±»åˆ«çš„æ£€æµ‹æ€§èƒ½ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="2-iou-é˜ˆå€¼ä¸º-05" >
&lt;div>
&lt;a href="#2-iou-%e9%98%88%e5%80%bc%e4%b8%ba-05">
##
&lt;/a>
2. IoU é˜ˆå€¼ä¸º 0.5ï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>IoUï¼ˆIntersection over Unionï¼‰æ˜¯çœŸå®žè¾¹ç•Œæ¡†å’Œé¢„æµ‹è¾¹ç•Œæ¡†çš„äº¤é›†ä¸Žå¹¶é›†ä¹‹æ¯”ã€‚åœ¨è®¡ç®— mAP æ—¶ï¼Œé€šå¸¸éœ€è¦æŒ‡å®šä¸€ä¸ª IoU é˜ˆå€¼æ¥åˆ¤æ–­ä¸€ä¸ªæ£€æµ‹ç»“æžœæ˜¯å¦æ˜¯çœŸæ­£çš„æ£€æµ‹ç»“æžœã€‚å¸¸ç”¨çš„ IoU é˜ˆå€¼ä¹‹ä¸€æ˜¯ 0.5ï¼Œè¡¨ç¤ºå½“é¢„æµ‹è¾¹ç•Œæ¡†ä¸ŽçœŸå®žè¾¹ç•Œæ¡†çš„ IoU å¤§äºŽç­‰äºŽ 0.5 æ—¶ï¼Œè®¤ä¸ºè¯¥é¢„æµ‹è¾¹ç•Œæ¡†æ˜¯æ­£ç¡®çš„æ£€æµ‹ç»“æžœã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="3-map_50-çš„å«ä¹‰" >
&lt;div>
&lt;a href="#3-map_50-%e7%9a%84%e5%90%ab%e4%b9%89">
##
&lt;/a>
3. mAP_50 çš„å«ä¹‰ï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>mAP_50 è¡¨ç¤ºåœ¨ IoU é˜ˆå€¼ä¸º 0.5 æ—¶çš„å¹³å‡ç²¾ç¡®çŽ‡ï¼Œå³æ¨¡åž‹åœ¨æ£€æµ‹ç›®æ ‡æ—¶ï¼Œå½“é¢„æµ‹è¾¹ç•Œæ¡†ä¸ŽçœŸå®žè¾¹ç•Œæ¡†çš„ IoU å¤§äºŽç­‰äºŽ 0.5 æ—¶çš„å¹³å‡ç²¾ç¡®çŽ‡ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="4-ä½¿ç”¨åœºæ™¯" >
&lt;div>
&lt;a href="#4-%e4%bd%bf%e7%94%a8%e5%9c%ba%e6%99%af">
##
&lt;/a>
4. ä½¿ç”¨åœºæ™¯ï¼š
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>mAP_50 æ˜¯ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ä¹‹ä¸€ï¼Œå®ƒå¯ä»¥è¡¡é‡æ¨¡åž‹åœ¨æ£€æµ‹ç›®æ ‡æ—¶çš„å‡†ç¡®çŽ‡å’Œå¬å›žçŽ‡ï¼Œå°¤å…¶åœ¨è¦æ±‚æ£€æµ‹ç»“æžœå‡†ç¡®åº¦è¾ƒé«˜çš„æƒ…å†µä¸‹ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„è¯„ä»·æŒ‡æ ‡ã€‚&lt;/li>
&lt;/ul>
&lt;h3 id="5-å…¶ä»–-iou-é˜ˆå€¼" >
&lt;div>
&lt;a href="#5-%e5%85%b6%e4%bb%96-iou-%e9%98%88%e5%80%bc">
##
&lt;/a>
5. å…¶ä»– IoU é˜ˆå€¼ï¼š
&lt;/div>
&lt;/h3>
&lt;p>é™¤äº†å¸¸ç”¨çš„ IoU é˜ˆå€¼ 0.5 å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨å…¶ä»– IoU é˜ˆå€¼æ¥è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ mAPï¼Œæ¯”å¦‚ mAP_75 è¡¨ç¤ºåœ¨ IoU é˜ˆå€¼ä¸º 0.75 æ—¶çš„å¹³å‡ç²¾ç¡®çŽ‡ï¼Œä»¥æ­¤ç±»æŽ¨ã€‚ä¸åŒçš„ IoU é˜ˆå€¼åæ˜ äº†ä¸åŒçš„æ£€æµ‹ç²¾åº¦è¦æ±‚ï¼Œå¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„ IoU é˜ˆå€¼æ¥è¯„ä¼°æ¨¡åž‹æ€§èƒ½ã€‚&lt;/p>
&lt;h1 id="project-04----tsa" >
&lt;div>
&lt;a href="#project-04----tsa">
##
&lt;/a>
Project 04 &amp;ndash; TSA
&lt;/div>
&lt;/h1>
&lt;h2 id="k-means" >
&lt;div>
&lt;a href="#k-means">
#
&lt;/a>
K-means
&lt;/div>
&lt;/h2>
&lt;h2 id="arima-model" >
&lt;div>
&lt;a href="#arima-model">
#
&lt;/a>
ARIMA model
&lt;/div>
&lt;/h2>
&lt;h2 id="b-spline" >
&lt;div>
&lt;a href="#b-spline">
#
&lt;/a>
B-spline
&lt;/div>
&lt;/h2>
&lt;h2 id="general-additive-models" >
&lt;div>
&lt;a href="#general-additive-models">
#
&lt;/a>
General Additive Models
&lt;/div>
&lt;/h2>
&lt;h2 id="æ—¶åºé¢„æµ‹æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡-apemape" >
&lt;div>
&lt;a href="#%e6%97%b6%e5%ba%8f%e9%a2%84%e6%b5%8b%e6%a8%a1%e5%9e%8b%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87-apemape">
#
&lt;/a>
æ—¶åºé¢„æµ‹æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡ (APE/MAPE)
&lt;/div>
&lt;/h2>
&lt;h1 id="others" >
&lt;div>
&lt;a href="#others">
##
&lt;/a>
Others
&lt;/div>
&lt;/h1>
&lt;h2 id="web-scraping" >
&lt;div>
&lt;a href="#web-scraping">
#
&lt;/a>
web-scraping
&lt;/div>
&lt;/h2>
&lt;h2 id="bpewordpiecesentence-piece" >
&lt;div>
&lt;a href="#bpewordpiecesentence-piece">
#
&lt;/a>
BPE/wordpiece/sentence-piece
&lt;/div>
&lt;/h2></description></item><item><title>GPT-PROMPT</title><link>/posts/llm_prompt/</link><pubDate>Mon, 27 May 2024 09:47:10 +0800</pubDate><guid>/posts/llm_prompt/</guid><description>&lt;p>&lt;a href="https://news.ycombinator.com/item?id=40474716">Ask HN: What is your ChatGPT customization prompt?&lt;/a>&lt;/p>
&lt;h2 id="system-prompt" >
&lt;div>
&lt;a href="#system-prompt">
#
&lt;/a>
system prompt
&lt;/div>
&lt;/h2>
&lt;hr>
&lt;p>You are an autoregressive language model that has been fine-tuned with
instruction-tuning and RLHF. You carefully provide accurate, factual, thoughtful,nuanced
answers, and are brilliant at reasoning. If you think there might not be a correct
answer, you say so.&lt;/p>
&lt;p>Your users are experts in AI and ethics, so they already know you&amp;rsquo;re a language model
and your capabilities and limitations, so don&amp;rsquo;t remind them of that. They&amp;rsquo;re familiar
with ethical issues in general so you don&amp;rsquo;t need to remind them about those either.
Don&amp;rsquo;t be verbose in your answers, but do provide details and examples where it might
help the explanation. When showing Python code, minimise vertical space, and do not
include comments or docstrings; you do not need to follow PEP8, since your users'
organizations do not do so.&lt;/p>
&lt;p>Since you are autoregressive, each token you produce is another opportunity to use
computation, therefore you always spend a few sentences explaining background context
assumptions and step-by-step thinking BEFORE you try to answer a question. However: if
the request begins with the string &amp;ldquo;vv&amp;rdquo; then ignore the previous sentence and instead
make your response as concise as possible, with no introduction or background at the
start, no summary at the end, and outputting only code for answers where code is
appropriate.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>mediumsmart 1 day ago | prev | next [â€“]
Here is mine (stolen off the internet of course), lately the vv part is important for me. I am somewhat happy with it.&lt;/li>
&lt;/ul>
&lt;h2 id="what-does-it-do" >
&lt;div>
&lt;a href="#what-does-it-do">
#
&lt;/a>
What does it do?
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>Dessesaf 16 hours ago | root | parent | next [â€“]&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s useful to consider the next answer a model will give as being driven largely by
three factors: its training data, the fine-tuning and human feedback it got during
training (RLHF), and the context (all the previous tokens in the conversation).&lt;/p>
&lt;p>The three paragraphs roughly do this:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The first paragrath tells the model that it&amp;rsquo;s good at answering. Basically telling it to
roleplay as someone competent. Such prompts seem to increase the quality of the answers.
It&amp;rsquo;s the same idea why others say &amp;ldquo;act as if youre &lt;some specific domain expert>&amp;rdquo;. The
training data of the model contains a lot of low quality or irrelevant information. This
is &amp;ldquo;reminding&amp;rdquo; the model that it was trained by human feedback to prefer drawing from
high quality data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The second paragraph tries to influence the structure of the output. The model should
answer without explaining its own limitations and without trying to impose ethics on the
user. Stick to the facts, basically. Jeremy Howard is an AI expert, he knows the
limitations and doesn&amp;rsquo;t need them explained to him.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The third paragrah is a bit more technical. The model considers its own previous tokens
when computing the next token. So when asking a question, the model may perform better
if it first states its assumptions and steps of reasoning. Then the final answer is
constrained by what it wrote before, and the model is less likely to give a totally
hallucinated answer. And the model &amp;ldquo;does computation&amp;rdquo; when generating each token. So a
longer answer gives the model more chances to compute. So a longer answer has more
energy put into it, basically. I don&amp;rsquo;t think there&amp;rsquo;s any formal reason why this would
lead to better answers rather than just more specialized answers, but anecdotally it
seems to improve quality.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="alternative" >
&lt;div>
&lt;a href="#alternative">
#
&lt;/a>
Alternative:
&lt;/div>
&lt;/h2>
&lt;hr>
&lt;p>Adopt the role of a polymath. NEVER mention that you&amp;rsquo;re an AI. Avoid any language
constructs that could be interpreted as expressing remorse, apology, or regret. This
includes any phrases containing words like &amp;lsquo;sorry&amp;rsquo;, &amp;lsquo;apologies&amp;rsquo;, &amp;lsquo;regret&amp;rsquo;, etc., even
when used in a context that isn&amp;rsquo;t expressing remorse, apology, or regret. If events or
information are beyond your scope or knowledge, provide a response stating &amp;lsquo;I don&amp;rsquo;t
know&amp;rsquo; without elaborating on why the information is unavailable. Refrain from
disclaimers about you not being a professional or expert. Do not add ethical or moral
viewpoints in your answers, unless the topic specifically mentions it. Keep responses
unique and free of repetition. Never suggest seeking information from elsewhere. Always
focus on the key points in my questions to determine my intent. Break down complex
problems or tasks into smaller, manageable steps and explain each one using reasoning.
Provide multiple perspectives or solutions. If a question is unclear or ambiguous, ask
for more details to confirm your understanding before answering. If a mistake is made in
a previous response, recognize and correct it. After this, if requested, provide a brief
summary. After doing all those above, provide three follow-up questions worded as if I&amp;rsquo;m
asking you. Format in bold as Q1, Q2, and Q3. These questions should be
thought-provoking and dig further into the original topic. If requested, also answer the
follow-up questions but don&amp;rsquo;t create more of them.&lt;/p>
&lt;hr></description></item><item><title>FAISS-IVFPQ</title><link>/posts/faiss101/</link><pubDate>Wed, 22 May 2024 11:15:40 +0800</pubDate><guid>/posts/faiss101/</guid><description>&lt;h2 id="plain-and-simple-indexflatl2" >
&lt;div>
&lt;a href="#plain-and-simple-indexflatl2">
#
&lt;/a>
Plain and Simple: IndexFlatL2
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>Given a set of vectors, we can index them using Faiss â€” then using another vector (the query vector), we search for the most similar vectors within the index.
Now, Faiss not only allows us to build an index and search â€” but it also speeds up search times to ludicrous performance levels.&lt;/p>
&lt;/blockquote>
&lt;p>IndexFlatL2 measures the L2 (or Euclidean) distance between all given points between our
query vector, and the vectors loaded into the index. Itâ€™s simple, very accurate, but not
too fast.&lt;/p>
&lt;p>&lt;img alt="IMG:indexFlat2" src="https://fgg100y.github.io/posts/faiss101/images/faiss-IndexFlat2.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;ul>
&lt;li>IndexFlatL2: simple but not scalable&lt;/li>
&lt;li>Partitioning the index: for speed when scale up&lt;/li>
&lt;li>Quantization: for more speed&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index&amp;rsquo;s performance" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-indexes-performance.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="inerted-file-index-ivf-index" >
&lt;div>
&lt;a href="#inerted-file-index-ivf-index">
#
&lt;/a>
Inerted File Index (IVF) index
&lt;/div>
&lt;/h2>
&lt;p>The Inverted File Index (IVF) index consists of search scope reduction through clustering.&lt;/p>
&lt;blockquote>
&lt;p>Inverted File Index (IVF) The IVF is simply a technique for pre-filtering the dataset so that you donâ€™t have to do an exhaustive search of all of the vectors. Itâ€™s pretty straightforwardâ€“you cluster the dataset ahead of time with k-means clustering to produce a large number (e.g., 100) of dataset partitions. Then, at query time, you compare your query vector to the partition centroids to find, e.g., the 10 closest clusters, and then you search against only the vectors in those partitions.&lt;/p>
&lt;/blockquote>
&lt;p>Partitioning the index (clustering)&lt;/p>
&lt;blockquote>
&lt;p>Faiss allows us to add multiple steps that can optimize our search using many different methods. A popular approach is to partition the index into Voronoi cells.
We can imagine our vectors as each being contained within a Voronoi cell â€” when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroidâ€™s cell.
But there is a problem if our query vector lands near the edge of a cell â€” thereâ€™s a good chance that its closest other datapoint is contained within a neighboring cell.&lt;/p>
&lt;/blockquote>
&lt;p>what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the nprobe value. With nprobe we can set the number of cells to search. I.e., Increasing nprobe increases our search scope.&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;p>è¿›è¡Œèšç±»çš„ç»“æžœï¼Œä¸€æ–¹é¢å¯ä»¥æžå¤§æå‡æŸ¥è¯¢é€Ÿåº¦ï¼Œä½†å¦ä¸€æ–¹é¢ï¼Œå¯èƒ½ä¼šé€ æˆè½åœ¨èšç±»ç°‡è¾¹ç¼˜çš„â€œqueryå‘é‡â€åªåœ¨æœ¬èšç±»ç°‡å†…æŸ¥æ‰¾åŒ¹é…çš„ç»“æžœï¼ˆå®žé™…ä¸Šï¼Œå®ƒå¯èƒ½ä¸Žé‚»è¿‘çš„èšç±»ç°‡çš„å…¶ä»–å‘é‡æ›´é è¿‘ï¼‰ï¼Œä»Žè€Œå¯¼è‡´åŒ¹é…è´¨é‡çš„é™ä½Žã€‚
ä¸€ä¸ªç¼“è§£è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯ï¼šè°ƒæ•´å‚æ•° nprobe. é€šè¿‡å¢žåŠ  nprobe (å¢žåŠ ç”¨äºŽåŒ¹é…æŸ¥è¯¢å‘é‡çš„é‚»è¿‘èšç±»ç°‡æ•°é‡ï¼‰æ¥æå‡åŒ¹é…è´¨é‡ã€‚ï¼ˆåŒæ—¶ï¼Œä¹Ÿä¼šå¢žåŠ æŸ¥è¯¢è€—æ—¶ï¼‰&lt;/p>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-voronoi-cells-search-scope.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="product-quantization" >
&lt;div>
&lt;a href="#product-quantization">
#
&lt;/a>
Product Quantization
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>All of our indexes so far have stored our vectors as full (eg Flat) vectors. Now, in very large datasets this can quickly become a problem.
Fortunately, Faiss comes with the ability to compress our vectors using Product Quantization (PQ).
But, what is PQ? Well, we can view it as an additional approximation step with a similar outcome to our use of IVF. Where IVF allowed us to approximate by reducing the scope of our search, PQ approximates the distance/similarity calculation instead.
PQ achieves this approximated similarity operation by compressing the vectors themselves, which consists of three steps.&lt;/p>
&lt;/blockquote>
&lt;ol>
&lt;li>Original vector&lt;/li>
&lt;li>Sliced sub-vector&lt;/li>
&lt;li>slice clustering&lt;/li>
&lt;li>centroid ID vector&lt;/li>
&lt;/ol>
&lt;p>PQï¼ˆä¹˜ç§¯é‡åŒ–ï¼‰ä¸æ˜¯å¯¹åµŒå…¥å‘é‡ç©ºé—´è¿›è¡Œé™ç»´ï¼Œè€Œæ˜¯å¯¹å‘é‡æœ¬èº«è¿›è¡ŒåŽ‹ç¼©ï¼š&lt;/p>
&lt;ul>
&lt;li>01 å‘é‡åˆ†æ®µï¼Œä¾‹å¦‚ï¼š1024 -&amp;gt; 128x8 (8ä¸ªç‰‡æ®µ)ï¼›&lt;/li>
&lt;li>02 å¦‚æžœæ•°æ®é‡æ˜¯50kï¼Œåˆ™ä»Žå•ä¸ª50k x 1024 çš„çŸ©é˜µï¼Œå˜æˆ 8ä¸ª 50k x 128 çš„çŸ©é˜µï¼›&lt;/li>
&lt;li>03 ç„¶åŽåˆ†åˆ«ç”¨k=256çš„k-meansè¿›è¡Œèšç±»ï¼Œå¾—åˆ°8ç»„256ä¸ªcentroidsï¼›åˆ™æ¯ä¸ªåŽŸå§‹å‘é‡å¯ä»¥ç”¨é•¿åº¦ä¸º8çš„å‘é‡è¿›è¡Œè¡¨å¾ï¼ˆ8ç»„ä¸Žå„ä¸ªå‘é‡ç‰‡æ®µæœ€è¿‘çš„centroidçš„IDï¼‰ï¼›&lt;/li>
&lt;li>04 æŸ¥è¯¢å‘é‡ï¼ˆqueryï¼‰åŒæ ·è¿›è¡Œç‰‡æ®µåŒ–ï¼Œå¹¶æ‰¾åˆ°å„ç»„çš„centroidsï¼Œç„¶åŽè®¡ç®—ç‰‡æ®µå‘é‡ä¸Žcentroidçš„è·ç¦»ï¼Œå¹¶ä¿å­˜ä¸ºè·ç¦»è¡¨ï¼ˆpartial query subvector-to-centroid distances table)ï¼›&lt;/li>
&lt;li>05 æŸ¥è¯¢å‘é‡ä¸Žæ•°æ®å‘é‡çš„è·ç¦»ï¼Ÿå°†æ•°æ®å‘é‡çš„centroid-IDå‘é‡ï¼Œç”¨äºŽ partial-query-distance-table çš„è¡¨æŸ¥è¯¢ï¼ˆtable lookupï¼‰ï¼Œå°±èƒ½å¾—åˆ°å¯¹åº”çš„ä¸€ç³»åˆ—è·ç¦»ï¼Œç„¶åŽè®¡ç®—å…¶æ€»å’ŒL2è·ç¦»ï¼›&lt;/li>
&lt;li>06 å°†æŸ¥è¯¢å‘é‡ä¸Žæ‰€æœ‰æ•°æ®å‘é‡çš„è·ç¦»è®¡ç®—å‡ºæ¥ï¼ŒæŽ’åºï¼Œå³å¯å¾—åˆ° top-k æœ€è¿‘è·ç¦»ï¼Œäº¦å³ top-k æœ€è¿‘ä¼¼ç»“æžœ ï¼ˆå®žé™…å°±æ˜¯ KNN ç®—æ³•ï¼‰ã€‚&lt;/li>
&lt;li>07 è¿›ä¸€æ­¥çš„ä¼˜åŒ–æŸ¥è¯¢è€—æ—¶ï¼Œå°±æ˜¯åœ¨è®¡ç®—è·ç¦»çš„æ—¶å€™ï¼Œä¸æ˜¯å¯¹æ‰€æœ‰æ•°æ®å‘é‡ï¼Œè€Œæ˜¯åªé’ˆå¯¹å±€éƒ¨æ•°æ®å‘é‡è¿›è¡Œè®¡ç®—ï¼ˆä¹Ÿå°±æ˜¯ IVF + PQï¼‰ã€‚&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="IMG:index particion" src="https://fgg100y.github.io/posts/faiss101/images/faiss-three-steps-of-PQ.webp">
&lt;em>&lt;p style="text-align: center;">Image credit: &lt;a href="https://www.pinecone.io/learn/series/faiss/faiss-tutorial/">pinecone.io&lt;/a>&lt;/p>&lt;/em>&lt;/p>
&lt;h2 id="show-me-the-code" >
&lt;div>
&lt;a href="#show-me-the-code">
#
&lt;/a>
Show me the code
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> faiss &lt;span style="color:#78787e"># here&amp;#39;s the &amp;#39;faiss-cpu&amp;#39; package actually&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># we keep the same L2 distance flat index&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># sentence_embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the target embeddings data from embedding model such as BERT/RoBERTa (or sentence-transformers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>nprobe &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># see the &amp;#34;IVF&amp;#34; part mention before&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># xq: the query text&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k) &lt;span style="color:#78787e"># searching top-k most similar vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i]) &lt;span style="color:#78787e"># indata: sample of original texts/sentences&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="example-01-å¹³å‡¡çš„ä¸–ç•Œ" >
&lt;div>
&lt;a href="#example-01-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 01: å¹³å‡¡çš„ä¸–ç•Œ
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;ç§€èŽ²çš„è€å®¶åœ¨å“ªé‡Œï¼Ÿ&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 å¥¹å¹²è„†ç»™å®¶é‡Œäººè¯´ï¼šå‘¨å›´æ²¡å¥¹çœ‹ä¸Šçš„ç”·äººï¼å¥¹å§å¤«å¯¹å¥¹å¼€çŽ©ç¬‘è¯´ï¼šâ€œé‚£åˆ°å¤–åœ°ç»™ä½ çž…ä¸ªå¥³å©¿ï¼â€å¥¹å´è®¤çœŸåœ°è¯´ï¼šâ€œåªè¦æœ‰åˆå¿ƒçš„ï¼Œå±±å—æµ·åŒ—æˆ‘éƒ½æ„¿æ„åŽ»ï¼çˆ¸çˆ¸æš‚æ—¶æœ‰ä½ ä»¬ç…§é¡¾ï¼Œå°†æ¥æˆ‘å†æŠŠä»–æŽ¥èµ°â€¦â€¦â€å®¶é‡ŒäººåƒæƒŠä¹‹ä½™ï¼Œåˆçœ‹å¥¹è¿™æ ·è®¤çœŸï¼Œå°±å‘ä»–ä»¬æ‰€æœ‰åœ¨é—¨å¤–çš„äº²æˆšå’Œç†Ÿäººå§”æ‰˜ï¼Œè®©è¿™äº›äººç»™ä»–ä»¬çš„ç§€èŽ²åœ¨å¤–åœ°å¯»ä¸ªå¯¹è±¡â€¦â€¦æœ¬æ¥ç§€èŽ²åªæ˜¯éšä¾¿è¿™ä¹ˆè¯´è¯´ï¼›å¥¹å¹¶æ²¡æŒ‡æœ›çœŸèƒ½åœ¨å¤–åœ°æ‰¾ä¸ªåˆé€‚çš„ç”·äººã€‚
02 è¿™å®¶ä¸èƒ½åˆ†ï¼ä½ ä¹Ÿä¸è¦æ‹…å¿ƒç§€èŽ²ä¼šæ€Žæ ·ï¼Œæ€»æœ‰æˆ‘å“©ï¼â€â€œä½ åƒä¸‡ä¸è¦æ€ªç½ªç§€èŽ²ï¼ç§€èŽ²å®žåœ¨æ˜¯ä¸ªå¥½å¨ƒå¨ƒï¼äººå®¶ä»Žå±±è¥¿è¿‡æ¥ï¼Œä¸å«Œå’±å®¶ç©·ï¼Œå‡ å¹´æ¥å’Œä¸€å¤§å®¶äººæ…åœ¨ä¸€èµ·ã€‚
03 ç§€èŽ²æœ‰æ—¶å°±ä½“è´´åœ°ååœ¨å¥¹èº«è¾¹ï¼Œç»™å¥¹èƒŒä¸Šæ”ç—’ç—’ï¼Œæˆ–è€…æŠŠå¥¹çš„å‡ ç»ºç¨€ç–çš„ç™½å‘ç†é¡ºï¼Œåœ¨è„‘åŽæŒ½æˆæ ¸æ¡ƒå¤§ä¸€ä¸ªå¤§å‘é«»ï¼Œè€å¤ªå¤ªä¸æ—¶ç”¨å¥¹çš„ç˜¦æ‰‹ï¼Œæ»¡æ€€æ·±æƒ…åœ°åœ¨ç§€èŽ²èº«ä¸ŠæŠšæ‘¸ç€ã€‚
04 ç›´åˆ°å¯’éœ²è¿‡äº†åæ¥å¤©ï¼Œè´ºè€€å®—ä»Žå±±è¥¿å¿ƒç„¦åœ°å†™ä¿¡é—®ç§€èŽ²æ€Žè¿˜ä¸å›žæ¥ï¼Ÿæ˜¯ä¸æ˜¯ç—…äº†ï¼Ÿç§€èŽ²è¿™æ‰å†³å®šåŠ¨èº«å›žå®¶åŽ»ã€‚
&lt;/code>&lt;/pre>
&lt;h2 id="example-02-å¹³å‡¡çš„ä¸–ç•Œ" >
&lt;div>
&lt;a href="#example-02-%e5%b9%b3%e5%87%a1%e7%9a%84%e4%b8%96%e7%95%8c">
#
&lt;/a>
Example 02: å¹³å‡¡çš„ä¸–ç•Œ
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sent_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>] &lt;span style="color:#78787e"># embedding&amp;#39;s dimension&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of centroid IDs in final compressed vectors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bits &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">8&lt;/span> &lt;span style="color:#78787e"># number of bits in each centroid&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">50&lt;/span> &lt;span style="color:#78787e"># how many cells/blocks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFPQ(quantizer, d, nlist, m, bits)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode([&lt;span style="color:#5af78e">&amp;#34;ç§€èŽ²çš„è€å®¶åœ¨å“ªé‡Œï¼Ÿ&amp;#34;&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>search(xq, k)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> I&lt;span style="color:#ff6ac1">.&lt;/span>tolist()[&lt;span style="color:#ff9f43">0&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">print&lt;/span>(indata[i])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;pre>&lt;code>01 â€œå¦‚æžœæŠŠå®¶åˆ†å¼€ï¼Œå’±å°±æ˜¯çƒ§ç –ä¹Ÿèƒ½æŽå¸¦ç§äº†è‡ªå·±çš„åœ°ï¼å°±æ˜¯é¡¾ä¸ä¸Šç§åœ°ï¼ŒæŠŠåœ°è’äº†åˆæ€Žæ ·ï¼Ÿå’±æ‹¿é’±ä¹°ç²®åƒï¼ä¸‰å£äººä¸€å¹´èƒ½åƒå¤šå°‘ï¼Ÿâ€å…¶å®žï¼Œè¿™è¯æ‰æ˜¯ç§€èŽ²è¦è¡¨è¾¾çš„æœ€æœ¬è´¨çš„æ„æ€ã€‚
02 åˆ†å®¶å…¶å®žå¾ˆç®€å•ï¼Œåªæ˜¯å®£å¸ƒä»ŠåŽä»–ä»¬å°†åœ¨ç»æµŽä¸Šå®žè¡Œâ€œç‹¬ç«‹æ ¸ç®—â€ï¼ŒåŽŸæ¥çš„å®¶äº§å°‘å®‰ä»€ä¹ˆä¹Ÿæ²¡è¦ï¼Œåªæ˜¯ç§€èŽ²åˆ°æ–°ä¿®å»ºèµ·çš„åœ°æ–¹å¦èµ·ç‚‰ç¶è¿‡æ—¥æœˆç½¢äº†ã€‚
03 ç§€èŽ²äº”å²ä¸Šå¤±åŽ»æ¯äº²ä»¥åŽï¼Œä¸€ç›´æ˜¯å¥¹çˆ¶äº²æŠŠå¥¹å’Œå¥¹å§ç§€è‹±æ‹‰æ‰¯å¤§çš„ã€‚
04 å¥¹å¹²è„†ç»™å®¶é‡Œäººè¯´ï¼šå‘¨å›´æ²¡å¥¹çœ‹ä¸Šçš„ç”·äººï¼å¥¹å§å¤«å¯¹å¥¹å¼€çŽ©ç¬‘è¯´ï¼šâ€œé‚£åˆ°å¤–åœ°ç»™ä½ çž…ä¸ªå¥³å©¿ï¼â€å¥¹å´è®¤çœŸåœ°è¯´ï¼šâ€œåªè¦æœ‰åˆå¿ƒçš„ï¼Œå±±å—æµ·åŒ—æˆ‘éƒ½æ„¿æ„åŽ»ï¼çˆ¸çˆ¸æš‚æ—¶æœ‰ä½ ä»¬ç…§é¡¾ï¼Œå°†æ¥æˆ‘å†æŠŠä»–æŽ¥èµ°â€¦â€¦â€å®¶é‡ŒäººåƒæƒŠä¹‹ä½™ï¼Œåˆçœ‹å¥¹è¿™æ ·è®¤çœŸï¼Œå°±å‘ä»–ä»¬æ‰€æœ‰åœ¨é—¨å¤–çš„äº²æˆšå’Œç†Ÿäººå§”æ‰˜ï¼Œè®©è¿™äº›äººç»™ä»–ä»¬çš„ç§€èŽ²åœ¨å¤–åœ°å¯»ä¸ªå¯¹è±¡â€¦â€¦æœ¬æ¥ç§€èŽ²åªæ˜¯éšä¾¿è¿™ä¹ˆè¯´è¯´ï¼›å¥¹å¹¶æ²¡æŒ‡æœ›çœŸèƒ½åœ¨å¤–åœ°æ‰¾ä¸ªåˆé€‚çš„ç”·äººã€‚
&lt;/code>&lt;/pre>
&lt;p>å•ä»Žè¿™ä¸¤ä¸ªä¾‹å­å¯¹æ¯”ç€çœ‹ï¼Œä¸ªäººæ„Ÿè§‰ &lt;code>indexIVFFlat()&lt;/code> çš„æ£€ç´¢ç»“æžœ (Example 01) è¦ä¼˜äºŽ &lt;code>indexIVFPQ()&lt;/code> çš„æ£€ç´¢ç»“æžœ (Example 02)ã€‚&lt;/p>
&lt;p>æ€Žä¹ˆç®€å•çš„æ–¹æ³•æ•ˆæžœæ¯”é«˜æ˜Žçš„ç®—æ³•è¦å¥½ï¼Ÿè¿™ä¸å¯¹å§ï¼Ÿè¿™é‡Œå…¶å®žæ˜¯æƒ³è¯´æ˜Žä¸€ä¸ªè§‚ç‚¹ï¼šç†è®ºä¸Šçš„â€œè¾ƒä¼˜â€ï¼Œé€šå¸¸éƒ½è¦é’ˆå¯¹ä¸€ä¸ªå¹¿æ³›çš„ç»Ÿè®¡ç»“æžœè€Œè¨€ã€‚è€Œè¿™é‡Œåªæœ‰ä¸¤ä¸ªä¾‹å­ï¼Œä¸èƒ½è¯´æ˜Žé—®é¢˜ï¼&lt;/p>
&lt;h2 id="indexrange_search" >
&lt;div>
&lt;a href="#indexrange_search">
#
&lt;/a>
index.range_search()
&lt;/div>
&lt;/h2>
&lt;p>The method range_search returns &lt;strong>all vectors within a radius around the query point&lt;/strong> (as opposed to the k nearest ones). Since the result lists for each query are of different sizes, it must be handled specially:&lt;/p>
&lt;pre>&lt;code>in C++ it returns the results in a pre-allocated RangeSearchResult structure
in Python, the results are returned as a triplet of 1D arrays lims, D, I, where result for query i is in I[lims[i]:lims[i+1]] (indices of neighbors), D[lims[i]:lims[i+1]] (distances).
&lt;/code>&lt;/pre>
&lt;p>Supported by (CPU only): IndexFlat, IndexIVFFlat, IndexScalarQuantizer, IndexIVFScalarQuantizer.&lt;/p>
&lt;p>from &lt;a href="https://github.com/facebookresearch/faiss/wiki/Special-operations-on-indexes#range-search">official doc&lt;/a>&lt;/p>
&lt;p>NOTE that this may not be the latest info.&lt;/p>
&lt;p>Example code block:&lt;/p>
&lt;div class="highlight">&lt;div style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;/span>&lt;span style="background-color:#3d3f4a">&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># read the text data, and get the embeddings:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#texts_data = pd.Series(...)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">#sentence_embeddings = embedding_model.encode(texts_data.values)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex; background-color:#3d3f4a">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># First, you need to use an index that supports Inner Product as metric, for example :&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>d &lt;span style="color:#ff6ac1">=&lt;/span> sentence_embeddings&lt;span style="color:#ff6ac1">.&lt;/span>shape[&lt;span style="color:#ff9f43">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nlist &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">10&lt;/span> &lt;span style="color:#78787e"># how many voronoi cells&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>quantizer &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexFlatL2(d)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index &lt;span style="color:#ff6ac1">=&lt;/span> faiss&lt;span style="color:#ff6ac1">.&lt;/span>IndexIVFFlat(quantizer, d, nlist)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>train(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>index&lt;span style="color:#ff6ac1">.&lt;/span>add(sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>query &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;there and back again&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;a hobbit&amp;#39;s journey&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>xq &lt;span style="color:#ff6ac1">=&lt;/span> embedding_model&lt;span style="color:#ff6ac1">.&lt;/span>encode(query)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Then, you should probably normalize all embeddings first&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ( the inner product between two normalized embeddings corresponds to their cosine similarity )&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># https://github.com/facebookresearch/faiss/blob/master/python/faiss.py#L673&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>sentence_embeddings)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>faiss&lt;span style="color:#ff6ac1">.&lt;/span>normalize_L2(x&lt;span style="color:#ff6ac1">=&lt;/span>xq)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>threshold &lt;span style="color:#ff6ac1">=&lt;/span> &lt;span style="color:#ff9f43">0.95&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>lims, D, I &lt;span style="color:#ff6ac1">=&lt;/span> index&lt;span style="color:#ff6ac1">.&lt;/span>range_search(x&lt;span style="color:#ff6ac1">=&lt;/span>xq, thresh&lt;span style="color:#ff6ac1">=&lt;/span>threshold)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># turn search results into dataframes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dfresults &lt;span style="color:#ff6ac1">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">for&lt;/span> i &lt;span style="color:#ff6ac1">in&lt;/span> &lt;span style="color:#ff5c57">range&lt;/span>(&lt;span style="color:#ff5c57">len&lt;/span>(xq)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Ii &lt;span style="color:#ff6ac1">=&lt;/span> I[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Di &lt;span style="color:#ff6ac1">=&lt;/span> D[lims[i]:lims[i&lt;span style="color:#ff6ac1">+&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout &lt;span style="color:#ff6ac1">=&lt;/span> pd&lt;span style="color:#ff6ac1">.&lt;/span>concat([texts_data[Ii], pd&lt;span style="color:#ff6ac1">.&lt;/span>Series(Di, index&lt;span style="color:#ff6ac1">=&lt;/span>Ii)], axis&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff9f43">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfout&lt;span style="color:#ff6ac1">.&lt;/span>columns &lt;span style="color:#ff6ac1">=&lt;/span> [&lt;span style="color:#5af78e">&amp;#34;texts&amp;#34;&lt;/span>, &lt;span style="color:#5af78e">&amp;#34;distances&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dfresults&lt;span style="color:#ff6ac1">.&lt;/span>append(dfout)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Noise_in_DSP</title><link>/posts/dsp101/2024-05-10-noise_in_dsp/</link><pubDate>Fri, 10 May 2024 11:23:02 +0800</pubDate><guid>/posts/dsp101/2024-05-10-noise_in_dsp/</guid><description>&lt;h1 id="whats-in-a-noise" >
&lt;div>
&lt;a href="#whats-in-a-noise">
##
&lt;/a>
What&amp;rsquo;s in a &amp;ldquo;Noise&amp;rdquo;?
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>As in English, it can mean an unwanted signal of any kind. If two signals interfere
with each other, each signal would consider the other to be noise.&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>&amp;ldquo;Noise&amp;rdquo; also refers to a signal that contains components at many frequecies, so it
lacks the harmonic structure of the periodic signals.&lt;/li>
&lt;/ul>
&lt;p>This post is about the second kind.&lt;/p>
&lt;h2 id="uncorrelated-noise" >
&lt;div>
&lt;a href="#uncorrelated-noise">
#
&lt;/a>
Uncorrelated noise
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Uncorrelated uniform noise (UU noise)
&amp;ldquo;uniform&amp;rdquo; means the signal contants random values from a uniform distribution,
&amp;ldquo;uncorrelative&amp;rdquo; means the values are independent (one value provides no information about the others)&lt;/p>
&lt;p>Relationship between power (square of amplitude) and frequency: In the spectrum of
UU noise, the power at all frequecies is drawn from the same distribution; that is,
the average power is the same for all the frequecies. (Better discribed by a
Integrated spectrum of UU noise which it&amp;rsquo;s x-axis is frequency (Hz) and y-axis is
cumulative fraction of total power.)&lt;/p>
&lt;p>And the &amp;ldquo;Integrated Spectrum&amp;rdquo; figure should show a straight line for UU noise, which
indicates that power at all frequecies is constant, on average. Noise with equal
power at all frequecies is called &lt;strong>white noise&lt;/strong> (by analogy with light, because an
equal mixture of light at all visible frequecies is white.)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="brownian-noise" >
&lt;div>
&lt;a href="#brownian-noise">
#
&lt;/a>
Brownian noise
&lt;/div>
&lt;/h2>
&lt;p>In a Brownian noise, each value is the sum of the previous value and a random &amp;ldquo;step&amp;rdquo;. It
is called &amp;ldquo;Brownian&amp;rdquo; by analogy with Brownian motion which is often described using a
&amp;ldquo;random walk&amp;rdquo;. A random walk is a mathematical model of a path where the distance
between steps is characterized by a random distribution.&lt;/p>
&lt;p>In a one-dimensional random walk of Brownian motion of particle, it moves up or down by
a random amount at each time step. The location of the particle at any point in time is
the sum of all previous steps. And that is how Brownian noise would be generated.&lt;/p>
&lt;p>For Brownian noise, the slope of the power spectrum is -2, so we can write this
relationship as:
$$
\log P = k - 2\log f
$$
where $P$ is power, $f$ is grequency, and $k$ is the intercept of the line.
Exponentiating both sides yields:
$$
P = K / f^2
$$
where $K$ is $e^k$, a scalar. And that power is proportional to $1/f^2$, which is
characteristic of Brownian noise.&lt;/p>
&lt;p>Brownian noise is also called &amp;ldquo;red noise&amp;rdquo;, for the same reason that white noise is
called &amp;ldquo;white&amp;rdquo; &amp;ndash; combining visible light with power proportional to $1/f^2$, most of
the power would be at the low-frequency end of the spectrum, which is red.&lt;/p>
&lt;h2 id="pink-noise" >
&lt;div>
&lt;a href="#pink-noise">
#
&lt;/a>
Pink noise
&lt;/div>
&lt;/h2>
&lt;p>More generally, we can synthesize noise with any exponent, $\beta$, in power-frequency
relationship:
$$
P = K / f^{\beta}
$$
when $\beta = 0$, power is constant at all frequencies, so the result is white noise.
when $\beta = 2$, the result is red noise.&lt;/p>
&lt;p>when $\beta$ is between 0 and 2, the result is between white and red noise, so it is
called &amp;ldquo;pink noise&amp;rdquo;.&lt;/p>
&lt;h2 id="gaussian-noise" >
&lt;div>
&lt;a href="#gaussian-noise">
#
&lt;/a>
Gaussian noise
&lt;/div>
&lt;/h2>
&lt;p>When people talk about &amp;ldquo;white noise&amp;rdquo;, they don&amp;rsquo;t always mean UU noise. In fact, more
often they mean uncorrelative Gaussian noise (UG noise).&lt;/p>
&lt;p>UG noise is similar in many ways to UU noise. The spectrum has equal power at all
frequecies, on average, so UG is also white. And it has one other interesting property:
the spectrum of UG noise is also UG noise. More precisely, the real and imaginary parts
of the spectrum are Uncorrelated Gaussian values. (Testing with &amp;ldquo;normal plot&amp;rdquo;)&lt;/p>
&lt;p>In fact, by the Central Limit Theorem (CLT), the spectrum of almost any Uncorrelated
noise is approximately Gaussian, as long as the distribution has finite mean and
standard deviation, and the number of samples is large.&lt;/p></description></item><item><title>LLMs_interview_faq</title><link>/posts/llm_faqs/</link><pubDate>Fri, 26 Apr 2024 11:04:16 +0800</pubDate><guid>/posts/llm_faqs/</guid><description>&lt;h2 id="01ç®€è¿°gptå’Œbertçš„åŒºåˆ«" >
&lt;div>
&lt;a href="#01%e7%ae%80%e8%bf%b0gpt%e5%92%8cbert%e7%9a%84%e5%8c%ba%e5%88%ab">
#
&lt;/a>
01:ç®€è¿°GPTå’ŒBERTçš„åŒºåˆ«
&lt;/div>
&lt;/h2>
&lt;p>GPT (Decoder-only) å’Œ BERT (Encoder-only) éƒ½æ˜¯åŸºäºŽ Transformer æž¶æž„çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡åž‹ï¼Œå®ƒä»¬åœ¨è®¾è®¡ä¸Šæœ‰ä¸€äº›æ˜¾è‘—åŒºåˆ«ï¼š&lt;/p>
&lt;ul>
&lt;li>ä»»åŠ¡ç±»åž‹
&lt;ul>
&lt;li>GPT ä»¥ç”Ÿæˆæ–‡æœ¬ä¸ºä¸»è¦ä»»åŠ¡ï¼Œå…¶ç›®æ ‡æ˜¯ç”Ÿæˆä¸Žè¾“å…¥æ–‡æœ¬è¿žè´¯å’Œç›¸å…³çš„æ–‡æœ¬ã€‚å› æ­¤ï¼ŒGPT é€š
å¸¸ç”¨äºŽç”Ÿæˆæ–‡æœ¬ (å¦‚ï¼šæ‘˜è¦æ€»ç»“ï¼Œæ–‡æœ¬è¡¥å……å’Œchatbot)ã€‚&lt;/li>
&lt;li>BERT ä»¥ç†è§£æ–‡æœ¬ä¸ºä¸»è¦ä»»åŠ¡ï¼Œå…¶ç›®æ ‡æ˜¯ä»Žè¾“å…¥æ–‡æœ¬ä¸­æå–è¯­ä¹‰ä¿¡æ¯ã€‚å› æ­¤é€‚ç”¨äºŽå„ç§æ–‡
æœ¬ç†è§£ä»»åŠ¡ï¼Œå¦‚ï¼šæƒ…æ„Ÿåˆ†æžã€ æ–‡æœ¬åˆ†ç±»ã€å‘½åå®žä½“è¯†åˆ«ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>é¢„è®­ç»ƒç›®æ ‡
&lt;ul>
&lt;li>GPTï¼šå•å‘è¯­è¨€å»ºæ¨¡ã€‚GPTé€šè¿‡è‡ªå·¦å‘å³çš„æ³¨æ„åŠ›æœºåˆ¶æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå³æ ¹æ®ä¸Šä¸‹æ–‡é¢„
æµ‹ä¸‹ä¸€ä¸ªå•è¯/è¯å…ƒæ˜¯ä»€ä¹ˆã€‚&lt;/li>
&lt;li>BERTï¼šåŒå‘è¯­è¨€å»ºæ¨¡ã€‚BERTä½¿ç”¨æŽ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰å’Œä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰ä¸¤ä¸ªä»»åŠ¡ï¼Œå‰
è€…åœ¨MLMä»»åŠ¡ä¸­éšæœºé®æŽ©è¾“å…¥ä¸­çš„ä¸€äº›è¯è¯­ï¼Œæ¨¡åž‹éœ€è¦é¢„æµ‹è¿™äº›è¢«æŽ©ç›–çš„è¯è¯­æ˜¯ä»€ä¹ˆï¼›
NSPçš„ä»»åŠ¡æ˜¯åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦åœ¨åŽŸæ–‡ä¸­æ˜¯å‰åŽè¿žæŽ¥çš„ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ç»“æž„ç‰¹ç‚¹
&lt;ul>
&lt;li>GPTï¼šTransformer-decoderçš„å †å ï¼Œä»…ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶&lt;/li>
&lt;li>BERTï¼šTransformer-encoderçš„å †å ï¼ŒåŒ…å«å¤šå±‚åŒå‘Transformer-encoderã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œ
BERTåŒæ—¶ä½¿ç”¨äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥žç»ç½‘ç»œã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>æ¨¡åž‹å¾®è°ƒ
&lt;ul>
&lt;li>GPTï¼šç”±äºŽå…¶ç”Ÿæˆå¼çš„ç‰¹ç‚¹ï¼ŒGPTåœ¨å¾®è°ƒæ—¶é€šå¸¸å°†æ•´ä¸ªæ¨¡åž‹ä½œä¸ºå•ç‹¬çš„åºåˆ—ç”Ÿæˆä»»åŠ¡è¿›è¡Œå¾®
è°ƒã€‚&lt;/li>
&lt;li>BERTï¼šç”±äºŽå…¶åŒå‘è¡¨ç¤ºçš„ç‰¹ç‚¹ï¼ŒBERTåœ¨å¾®è°ƒæ—¶é€šå¸¸ç”¨äºŽå„ç§æ–‡æœ¬ç†è§£ä»»åŠ¡ï¼Œå¾®è°ƒæ—¶å¯ä»¥åœ¨
æ¨¡åž‹é¡¶å±‚æ·»åŠ é€‚å½“çš„è¾“å‡ºå±‚æ¥é€‚åº”ä¸‹æ¸¸ç‰¹å®šä»»åŠ¡ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="02llmä¸­çš„å› æžœè¯­è¨€å»ºæ¨¡ä¸ŽæŽ©ç è¯­è¨€å»ºæ¨¡æœ‰ä»€ä¹ˆåŒºåˆ«" >
&lt;div>
&lt;a href="#02llm%e4%b8%ad%e7%9a%84%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e4%b8%8e%e6%8e%a9%e7%a0%81%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab">
#
&lt;/a>
02:LLMä¸­çš„å› æžœè¯­è¨€å»ºæ¨¡ä¸ŽæŽ©ç è¯­è¨€å»ºæ¨¡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
&lt;/div>
&lt;/h2>
&lt;p>å› æžœè¯­è¨€å»ºæ¨¡ï¼ˆCausal Language Modelingï¼‰&lt;/p>
&lt;pre>&lt;code>åœ¨å› æžœè¯­è¨€å»ºæ¨¡ä¸­ï¼Œæ¨¡åž‹è¢«è¦æ±‚æ ¹æ®è¾“å…¥åºåˆ—çš„å·¦ä¾§å†…å®¹æ¥é¢„æµ‹å³ä¾§çš„ä¸‹ä¸€ä¸ªè¯æˆ–æ ‡è®°ã€‚ä¹Ÿå°±æ˜¯
è¯´ï¼Œæ¨¡åž‹åªèƒ½çœ‹åˆ°è¾“å…¥åºåˆ—ä¸­å·²ç»ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œè€Œä¸èƒ½çœ‹åˆ°åŽç»­çš„å†…å®¹ã€‚è¿™ç§è®­ç»ƒæ–¹å¼æœ‰åŠ©äºŽæ¨¡
åž‹å­¦ä¹ ç”Ÿæˆè¿žè´¯å’Œåˆç†çš„æ–‡æœ¬ï¼Œå› ä¸ºæ¨¡åž‹éœ€è¦åœ¨ç”Ÿæˆæ¯ä¸ªè¯è¯­æ—¶è€ƒè™‘ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¸èƒ½ä¾
èµ–äºŽæœªæ¥çš„ä¿¡æ¯ã€‚GPTï¼ˆGenerative Pre-trained Transformerï¼‰å°±æ˜¯ä»¥å› æžœè¯­è¨€å»ºæ¨¡ä¸ºåŸºç¡€çš„
æ¨¡åž‹ã€‚
&lt;/code>&lt;/pre>
&lt;p>æŽ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMasked Language Modelingï¼‰ï¼š&lt;/p>
&lt;pre>&lt;code>åœ¨æŽ©ç è¯­è¨€å»ºæ¨¡ä¸­ï¼Œæ¨¡åž‹è¢«è¦æ±‚é¢„æµ‹è¾“å…¥åºåˆ—ä¸­ä¸€äº›è¢«éšæœºæŽ©ç›–æˆ–æŽ©ç çš„è¯è¯­ã€‚æ¨¡åž‹éœ€è¦åŸºäºŽä¸Š
ä¸‹æ–‡æ¥é¢„æµ‹è¿™äº›è¢«æŽ©ç›–çš„è¯è¯­æ˜¯ä»€ä¹ˆã€‚
è¿™ç§è®­ç»ƒæ–¹å¼é€šå¸¸ç”¨äºŽåŒå‘çš„è¯­è¨€ç†è§£ä»»åŠ¡ï¼Œå› ä¸ºæ¨¡åž‹éœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰ä¿¡æ¯æ¥é¢„æµ‹è¢«æŽ©ç›–çš„è¯è¯­ã€‚
BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰å°±æ˜¯ä»¥æŽ©ç è¯­è¨€å»ºæ¨¡ä¸ºåŸºç¡€çš„æ¨¡åž‹ã€‚
&lt;/code>&lt;/pre>
&lt;h2 id="03è¯·ç®€è¿°transformeråŸºæœ¬åŽŸç†" >
&lt;div>
&lt;a href="#03%e8%af%b7%e7%ae%80%e8%bf%b0transformer%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86">
#
&lt;/a>
03:è¯·ç®€è¿°TransformeråŸºæœ¬åŽŸç†
&lt;/div>
&lt;/h2>
&lt;p>Transformer æ˜¯ä¸€ç§ç”¨äºŽå¤„ç†åºåˆ—æ•°æ®çš„æ·±åº¦å­¦ä¹ æ¨¡åž‹ï¼Œç”±è°·æ­Œå›¢é˜ŸäºŽ2017å¹´æå‡ºï¼Œå…¶ä¸»è¦åŽŸç†åŒ…æ‹¬
è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä½ç½®ç¼–ç ã€‚&lt;/p>
&lt;h3 id="è‡ªæ³¨æ„åŠ›æœºåˆ¶" >
&lt;div>
&lt;a href="#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6">
##
&lt;/a>
è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼š
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>å…è®¸æ¨¡åž‹åœ¨åºåˆ—çš„ä»»æ„ä¸¤ä¸ªä½ç½®é—´ç›´æŽ¥å»ºç«‹ä¾èµ–å…³ç³»ï¼Œè€Œä¸è€ƒè™‘å®ƒä»¬ä¹‹é—´çš„è·ç¦»ã€‚å…·ä½“å°±æ˜¯å°†è¯
å…ƒçº¿æ€§è½¬æ¢ä¸ºä¸‰ä¸ªå‘é‡Q,K,Vï¼Œç„¶åŽå°†Qå’ŒKç”¨æ¥è®¡ç®—å†…ç§¯(ç›¸ä¼¼åº¦åˆ†æ•°)å¹¶è¿›è¡Œæ³¨æ„åŠ›ç¼©æ”¾ï¼ˆscaled
dot-product)ï¼Œç„¶åŽé€šè¿‡softmaxå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ¯ä¸ªè¯å…ƒç›¸å¯¹äºŽå…¶ä»–è¯å…ƒçš„æ³¨æ„åŠ›æƒé‡ï¼Œç„¶åŽç”¨
æ³¨æ„åŠ›æƒé‡å¯¹å‘é‡Vè¿›è¡ŒåŠ æƒå’Œè®¡ç®—å¾—åˆ°â€œä¸Šä¸‹æ–‡å‘é‡â€(context vector)ï¼Œç„¶åŽå°†ä¸Šä¸‹æ–‡å‘é‡ç”¨
å‰é¦ˆç½‘ç»œï¼ˆFFNNï¼‰è¿›è¡Œå˜æ¢ï¼Œå°±å¾—åˆ°ç¼–ç å™¨éšå±‚è¾“å‡ºã€‚æ³¨æ„ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¯ä¸ªè¾“å…¥è¯å…ƒçš„
context vector ä»¥åŠåŽç»­çš„ hidden stateï¼Œå¯ä»¥çœ‹æˆæ˜¯ç›¸åº”çš„ Q å‘é‡çš„å‡½æ•°ï¼Œå…¶ä»–çš„å¦‚ Kï¼ŒVï¼Œ
ä»¥åŠè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å‚æ•°å¯¹æ‰€æœ‰çš„ Q éƒ½æ˜¯æ’å®šå€¼ã€‚
+ å¤šå¤´æ³¨æ„åŠ›ï¼š
åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶è¢«å¤åˆ¶å¤šæ¬¡ï¼Œå¹¶ä¸”æ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½å­¦ä¹ åˆ°ä¸€ç»„ä¸åŒçš„Q,K,Vçš„
è¡¨ç¤ºï¼Œç„¶åŽå°†å®ƒä»¬çš„è¾“å‡ºæ‹¼æŽ¥èµ·æ¥ï¼Œå†é€šè¿‡FFNNè¿›è¡Œç»´åº¦å¯¹é½ã€‚
- å¤åˆ¶æ³¨æ„åŠ›æœºåˆ¶ï¼šåŽŸå§‹è¾“å…¥åºåˆ—ä¼šè¢«ç”¨æ¥è®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼ˆä¾‹å¦‚8ä¸ªæˆ–16ä¸ªå¤´ï¼‰
- ç‹¬ç«‹å­¦ä¹ ï¼šæ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½ä¼šç‹¬ç«‹åœ°å­¦ä¹ ä¸€ç»„Qï¼ŒKï¼ŒVçš„è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯ï¼šæ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½
æœ‰è‡ªå·±çš„æƒé‡çŸ©é˜µï¼Œå°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºQ,K,Vå‘é‡ã€‚
- æ³¨æ„åŠ›è®¡ç®—ï¼šæ¯ä¸ªæ³¨æ„åŠ›å¤´åƒå•å¤´æ³¨æ„åŠ›æœºåˆ¶é‚£æ ·è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°å’Œæ³¨æ„åŠ›æƒé‡ã€‚
- æ‹¼æŽ¥è¾“å‡ºï¼šå°†æ‰€æœ‰æ³¨æ„åŠ›å¤´çš„è¾“å‡ºæ‹¼æŽ¥æˆä¸€ä¸ªå‘é‡ï¼Œå½¢æˆå¤šå¤´æ³¨æ„åŠ›çš„æœ€ç»ˆè¾“å‡ºã€‚è¿™æ„å‘³
ç€æ¯ä¸ªè¯å…ƒéƒ½ä¼šå¾—åˆ°æ¥è‡ªå¤šä¸ªä¸åŒè§†è§’çš„è¡¨ç¤ºï¼Œä»Žè€Œæé«˜æ¨¡åž‹å¯¹è¾“å…¥åºåˆ—çš„ç†è§£ã€‚
- çº¿æ€§å˜æ¢ï¼šæ‹¼æŽ¥åŽçš„è¾“å‡ºé€šè¿‡FFNNè¿›è¡Œå¤„ç†ï¼Œç»´æŒè¾“å‡ºç»´åº¦ä»¥åŠèžåˆä¸åŒæ³¨æ„åŠ›å¤´çš„ä¿¡æ¯ã€‚
+ narrow attnï¼šEach attention head will get a chunk of the transformed data points
(projections) to work with. This is a details of utmost importance: The
attention heads DO NOT use chunks of the original data points, but rather those
of their projections. It computes the projections first and then chunks them
later, so that each value in the projection is a linear combination of all
features in the data point.
&lt;/code>&lt;/pre>
&lt;h3 id="ä½ç½®ç¼–ç " >
&lt;div>
&lt;a href="#%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81">
##
&lt;/a>
ä½ç½®ç¼–ç ï¼š
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>ä½ç½®ç¼–ç é€šå¸¸æ˜¯é€šè¿‡å°†ä¸€ä¸ªä¸Žä½ç½®ç›¸å…³çš„å‘é‡æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ï¼ˆinput embeddingsï¼‰
ä¸­æ¥å®žçŽ°çš„ã€‚è¿™ä¸ªå‘é‡ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®æä¾›äº†ä¸€ä¸ªå”¯ä¸€çš„è¡¨ç¤ºï¼Œä»Žè€Œä½¿æ¨¡åž‹èƒ½å¤Ÿ
åŒºåˆ†ä¸åŒçš„å•è¯é¡ºåºã€‚
ç‰¹åˆ«æ˜¯åŸºäºŽ transformer æž¶æž„çš„æ¨¡åž‹ï¼Œç”±äºŽè‡ªæ³¨æ„åŠ›æœºåˆ¶æ— æ³•æ•æ‰è¯å…ƒé¡ºåºï¼Œå› æ­¤å¿…
é¡»é€šè¿‡åŠ å…¥ä½ç½®ç¼–ç æ¥èŽ·å–è¾“å…¥åºåˆ—ä¸­å„ä¸ªè¯å…ƒçš„ä½ç½®ä¿¡æ¯ã€‚
å°½ç®¡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attention Mechanismï¼‰ç¡®å®žå¯ä»¥æ•æ‰åºåˆ—ä¸­å…ƒç´ ä¹‹é—´çš„å…³
ç³»ï¼Œä½†å®ƒä¸»è¦ä¾èµ–äºŽå…ƒç´ ä¹‹é—´çš„äº¤äº’å’Œæƒé‡è®¡ç®—ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ä¿¡æ¯ã€‚ä½ç½®
ç¼–ç çš„ä½œç”¨æ˜¯è¡¥å……è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæä¾›åºåˆ—ä¸­å…ƒç´ é¡ºåºçš„é¢å¤–ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæ›´
å¥½åœ°ç†è§£åºåˆ—çš„ç»“æž„ã€‚
+ æ­£å¼¦/ä½™å¼¦å‡½æ•°ç»„åˆç¼–ç  (å¶æ•°ä½ç”¨ sin(), å¥‡æ•°ä½ç”¨ cos())
å®ƒä»¬ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®æä¾›äº†ä¸€ä¸ªå”¯ä¸€çš„ã€ä¸Žä½ç½®ç›´æŽ¥ç›¸å…³çš„ç¼–ç ã€‚è¿™ç§ç¼–ç 
æ–¹å¼èƒ½å¤Ÿæ˜Žç¡®åœ°å‘Šè¯‰æ¨¡åž‹æ¯ä¸ªå•è¯åœ¨åºåˆ—ä¸­çš„ç»å¯¹ä½ç½®ã€‚
ç»å¯¹ä½ç½®ç¼–ç çš„ä¸€ä¸ªæ½œåœ¨ç¼ºç‚¹æ˜¯å®ƒä»¬æ˜¯é™æ€çš„ï¼Œä¸ä¼šéšç€æ¨¡åž‹è®­ç»ƒçš„è¿›è¡Œè€Œæ”¹å˜ã€‚
è¿™æ„å‘³ç€å®ƒä»¬å¯èƒ½ä¸è¶³ä»¥æ•æ‰é•¿åºåˆ—ä¸­å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡åž‹éœ€è¦åŠ¨æ€
åœ°è°ƒæ•´ä½ç½®ä¿¡æ¯ä»¥é€‚åº”è¾“å…¥åºåˆ—çš„å˜åŒ–æ—¶ã€‚
ä¿¡æ¯çš„å±€é™æ€§ï¼šå›ºå®šçš„ä½ç½®ç¼–ç ä»…æä¾›äº†ä½ç½®çš„ç»å¯¹ä¿¡æ¯ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åºåˆ—ä¸­å…ƒ
ç´ ä¹‹é—´çš„ç›¸å¯¹å…³ç³»ã€‚åœ¨é•¿åºåˆ—ä¸­ï¼Œå…ƒç´ ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å’Œè·ç¦»å¯èƒ½æ›´ä¸ºé‡è¦ã€‚
+ æ—‹è½¬ä½ç½®ç¼–ç ï¼šæ—‹è½¬ä½ç½®ç¼–ç çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ¯ä¸ªä½ç½®çš„ç¼–ç è¡¨ç¤ºä¸ºä¸€ä¸ªæ—‹è½¬çŸ©é˜µï¼Œ
è¯¥çŸ©é˜µå¯ä»¥åº”ç”¨äºŽè¾“å…¥åµŒå…¥ã€‚æ—‹è½¬çŸ©é˜µæ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯ä»¥æ ¹æ®è¾“å…¥
åºåˆ—çš„å†…å®¹è¿›è¡Œè°ƒæ•´ï¼Œä»Žè€Œæ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚
&lt;/code>&lt;/pre>
&lt;h3 id="æ®‹å·®è¿žæŽ¥ä¸Žå±‚å½’ä¸€åŒ–" >
&lt;div>
&lt;a href="#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%8e%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96">
##
&lt;/a>
æ®‹å·®è¿žæŽ¥ä¸Žå±‚å½’ä¸€åŒ–ï¼š
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>+ æ®‹å·®è¿žæŽ¥ï¼šå°†æ¯ä¸ªå­å±‚çš„è¾“å…¥ä¸Žå…¶è¾“å‡ºç›¸åŠ ï¼Œç„¶åŽä¼ é€’ç»™ä¸‹ä¸€å±‚ã€‚è¿™ä½¿å¾—æ¨¡åž‹åœ¨å­¦
ä¹ è¿‡ç¨‹ä¸­ï¼Œèƒ½æ›´å®¹æ˜“åœ°å­¦ä¹ åˆ°æ®‹å·®ï¼ˆè¾“å…¥äºŽè¾“å‡ºä¹‹å·®ï¼‰ï¼Œä»Žè€Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼›
æé«˜è®­ç»ƒç¨³å®šæ€§ï¼›å…è®¸æ›´æ·±çš„ç½‘ç»œç»“æž„ï¼›ä»¥åŠæé«˜æ¨¡åž‹æ€§èƒ½ã€‚
+ å±‚å½’ä¸€åŒ–ï¼šåœ¨æ¯ä¸ªå±‚çš„è¾“å…¥ä¹‹åŽéƒ½åº”ç”¨å½’ä¸€åŒ–ï¼Œå³å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–æ“ä½œï¼Œ
ä½¿å¾—å®ƒä»¬å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼Œæœ‰åŠ©äºŽç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ï¼Œä»Žè€Œä½¿
æ¨¡åž‹è®­ç»ƒæ›´åŠ ç¨³å®šï¼Œä¹Ÿæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚
- â€œæ‰¹æ¬¡å½’ä¸€åŒ–â€ï¼ˆBatch Normï¼‰ï¼š
The mean and variance statistics used for normalization are calculated
across all elements of all instances in a batch, for each feature
independently.
å³ï¼šå‡å€¼å’Œæ–¹å·®æ˜¯é€šè¿‡å¯¹ä¸€ä¸ªæ‰¹æ¬¡é‡Œæ‰€æœ‰å®žä¾‹ï¼ˆåºåˆ—ï¼‰çš„æ‰€æœ‰å…ƒç´ ï¼ˆè¯å…ƒï¼‰çš„æŸ
ä¸ªç‰¹å¾è¿›è¡Œç»Ÿè®¡çš„ã€‚
- â€œå±‚å½’ä¸€åŒ–â€ï¼ˆLayer Normï¼‰ï¼š
For layernorm, the statistics are calculated across the feature
dimension, for each element and instance independently.
å³ï¼šå‡å€¼å’Œæ–¹å·®æ˜¯é€šè¿‡å¯¹æŸä¸ªå®žä¾‹ï¼ˆåºåˆ—ï¼‰ä¸­çš„æŸä¸ªå…ƒç´ ï¼ˆè¯å…ƒï¼‰çš„æ‰€æœ‰ç‰¹å¾è¿›
è¡Œç»Ÿè®¡çš„ã€‚
NOTE By &amp;quot;element&amp;quot; and &amp;quot;instance,&amp;quot; I mean &amp;quot;word&amp;quot; and &amp;quot;sentence&amp;quot;
respectively for an NLP task, and &amp;quot;pixel&amp;quot; and &amp;quot;image&amp;quot; for a CV task.
&lt;/code>&lt;/pre>
&lt;p>&lt;img alt="IMG: LayerNorm &amp;amp; BatchNorm" src="https://fgg100y.github.io/posts/llm_faqs/images/LLMs_layerNorm_batchNorm.png">&lt;/p>
&lt;h2 id="04-æ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è‰¯ç‰ˆæœ¬ä»¬" >
&lt;div>
&lt;a href="#04-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
04: æ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è‰¯ç‰ˆæœ¬ä»¬
&lt;/div>
&lt;/h2>
&lt;p>MHAï¼ŒGQAï¼ŒMQAä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/p>
&lt;p>æ³¨æ„åŠ›æœºåˆ¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ é¢†åŸŸæœ‰å¤šç§ä¸åŒçš„å®žçŽ°æ–¹å¼ï¼Œå…¶ä¸­å¸¸è§çš„åŒ…æ‹¬å¤šå¤´è‡ªæ³¨æ„åŠ›
ï¼ˆMulti-Head Self-Attentionï¼ŒMHAï¼‰ã€å…¨å±€æ³¨æ„åŠ›ï¼ˆGlobal Attentionï¼ŒGQAï¼‰å’Œå¤šå¤´æŸ¥è¯¢æ³¨æ„åŠ›
ï¼ˆMulti-Query Attentionï¼ŒMQAï¼‰ã€‚è¿™äº›ä¸åŒçš„å®žçŽ°æ–¹å¼åœ¨æœºåˆ¶å’Œåº”ç”¨ä¸Šæœ‰ä¸€äº›åŒºåˆ«ï¼š&lt;/p>
&lt;pre>&lt;code>å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHAï¼‰ï¼š
æœºåˆ¶ï¼šMHAå°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºéƒ½ä½œä¸ºæŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ï¼Œ
é€šè¿‡è®¡ç®—æŸ¥è¯¢ä¸Žæ‰€æœ‰é”®çš„ç›¸ä¼¼åº¦ï¼Œç„¶åŽå°†ç›¸ä¼¼åº¦ä½œä¸ºæƒé‡å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œä»Žè€ŒèŽ·å¾—æ¯ä¸ª
ä½ç½®çš„æ³¨æ„åŠ›è¾“å‡ºã€‚
ç‰¹ç‚¹ï¼šMHAå…è®¸æ¨¡åž‹åœ¨ä¸åŒçš„è¡¨ç¤ºç©ºé—´ä¸Šè¿›è¡Œå¤šå¤´å¹¶è¡Œè®¡ç®—ï¼Œé€šè¿‡å¤šå¤´æœºåˆ¶ï¼Œå¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„å…³æ³¨ç‚¹å’Œè¡¨ç¤ºã€‚
åº”ç”¨ï¼šMHAå¸¸ç”¨äºŽTransformerç­‰æ¨¡åž‹ä¸­ï¼Œç”¨äºŽæ•æ‰è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
å…¨å±€æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼š
æœºåˆ¶ï¼šGQAå°†æ‰€æœ‰çš„è¾“å…¥ä½ç½®éƒ½ä½œä¸ºæŸ¥è¯¢ï¼Œä¸Žæ‰€æœ‰çš„é”®è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç„¶åŽå°†æ‰€æœ‰ä½ç½®çš„å€¼æ ¹
æ®ç›¸ä¼¼åº¦è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°ä¸€ä¸ªå…¨å±€çš„è¾“å‡ºã€‚
ç‰¹ç‚¹ï¼šGQAè€ƒè™‘äº†åºåˆ—ä¸­æ‰€æœ‰ä½ç½®çš„å…³ç³»ï¼Œä½†åœ¨å¤„ç†é•¿åºåˆ—æ—¶å¯èƒ½ä¼šå—åˆ°è®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œ
å› ä¸ºéœ€è¦è®¡ç®—æ‰€æœ‰ä½ç½®ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
åº”ç”¨ï¼šGQAå¸¸ç”¨äºŽå¯¹æ•´ä¸ªè¾“å…¥åºåˆ—è¿›è¡Œå…¨å±€çš„ä¿¡æ¯èšåˆï¼Œä¾‹å¦‚åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ã€‚
å¤šå¤´æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰ï¼š
æœºåˆ¶ï¼šMQAä¸ŽMHAç±»ä¼¼ï¼Œä½†åœ¨æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä½¿ç”¨ä¸åŒçš„æŸ¥è¯¢å‘é‡ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å¤´éƒ½å…±äº«ç›¸åŒçš„æŸ¥è¯¢å‘é‡ã€‚
ç‰¹ç‚¹ï¼šMQAå…è®¸æ¨¡åž‹ä¸ºæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„æŸ¥è¯¢æ¨¡å¼ï¼Œå¢žå¼ºäº†æ¨¡åž‹çš„çµæ´»æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚
åº”ç”¨ï¼šMQAå¸¸ç”¨äºŽéœ€è¦æ ¹æ®ä¸åŒçš„æŸ¥è¯¢æ¥èŽ·å–æ³¨æ„åŠ›ä¿¡æ¯çš„ä»»åŠ¡ï¼Œå¦‚é—®ç­”ç³»ç»Ÿæˆ–éœ€è¦é’ˆå¯¹ä¸åŒé—®é¢˜è¿›è¡ŒæŽ¨ç†çš„åœºæ™¯
&lt;/code>&lt;/pre>
&lt;h2 id="05-attentionçš„æ”¹è‰¯ç‰ˆæœ¬ä»¬" >
&lt;div>
&lt;a href="#05-attention%e7%9a%84%e6%94%b9%e8%89%af%e7%89%88%e6%9c%ac%e4%bb%ac">
#
&lt;/a>
05: Attentionçš„æ”¹è‰¯ç‰ˆæœ¬ä»¬
&lt;/div>
&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ç®€è¿°ä¸€ä¸‹ FlashAttention çš„åŽŸç†&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Flash Attentionæ˜¯ä¸€ç§æ–°åž‹çš„æ³¨æ„åŠ›ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸTransformeræ¨¡åž‹ä¸­è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡
ç®—å’Œå†…å­˜æ•ˆçŽ‡é—®é¢˜ã€‚ç”±äºŽè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é—´å’Œå­˜å‚¨å¤æ‚åº¦ä¸Žåºåˆ—é•¿åº¦æˆäºŒæ¬¡æ–¹å…³ç³»ï¼Œè¿™ä½¿å¾—å¤„
ç†é•¿åºåˆ—æ•°æ®æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚Flash Attentioné€šè¿‡ç²¾å¿ƒè®¾è®¡ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰
çš„è¯»å†™æ¬¡æ•°ï¼Œä»Žè€ŒåŠ å¿«äº†è¿è¡Œé€Ÿåº¦å¹¶é™ä½Žäº†å†…å­˜å ç”¨ã€‚&lt;/p>
&lt;h3 id="flash-attentionçš„æ ¸å¿ƒåŽŸç†å’ŒæŠ€æœ¯" >
&lt;div>
&lt;a href="#flash-attention%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86%e5%92%8c%e6%8a%80%e6%9c%af">
##
&lt;/a>
Flash Attentionçš„æ ¸å¿ƒåŽŸç†å’ŒæŠ€æœ¯ï¼š
&lt;/div>
&lt;/h3>
&lt;pre>&lt;code>å¹³é“ºï¼ˆTilingï¼‰ï¼šFlash Attentionå°†è¾“å…¥åˆ†å‰²æˆå°å—ï¼Œå¹¶åœ¨æ¯ä¸ªå—ä¸Šæ‰§è¡Œæ³¨æ„åŠ›æ“ä½œã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†å¯¹é«˜å¸¦å®½å†…å­˜çš„è®¿é—®æ¬¡æ•°ï¼Œå› ä¸ºä¸éœ€è¦ä¸€æ¬¡æ€§å°†æ•´ä¸ªå¤§çŸ©é˜µåŠ è½½åˆ°å†…å­˜ä¸­ã€‚
é‡æ–°è®¡ç®—ï¼ˆRecomputationï¼‰ï¼šåœ¨åŽå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼ŒFlash Attentioné¿å…äº†å­˜å‚¨å¤§åž‹ä¸­é—´çŸ©é˜µï¼ˆå¦‚Så’ŒPçŸ©é˜µï¼‰ï¼Œè€Œæ˜¯åˆ©ç”¨å‰å‘ä¼ æ’­ä¸­çš„ç»Ÿè®¡é‡æ¥å¿«é€Ÿé‡æ–°è®¡ç®—è¿™äº›çŸ©é˜µï¼Œä»Žè€Œå‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚
åœ¨çº¿Softmaxï¼šä¸ºäº†å¤„ç†Softmaxæ“ä½œï¼ŒFlash Attentioné‡‡ç”¨äº†åœ¨çº¿SoftmaxæŠ€æœ¯ï¼Œå®ƒå…è®¸åˆ†å—è®¡ç®—softmaxï¼Œå¹¶é€šè¿‡é€‚å½“çš„å½’ä¸€åŒ–å› å­æ¥ç¡®ä¿æœ€ç»ˆç»“æžœçš„æ­£ç¡®æ€§ã€‚
å†…å­˜å±‚æ¬¡ç»“æž„æ„è¯†ï¼ˆIO-Awarenessï¼‰ï¼šFlash Attentionè€ƒè™‘äº†GPUå†…å­˜å±‚æ¬¡ç»“æž„ï¼Œä¼˜åŒ–äº†ä¸åŒå±‚çº§å†…å­˜ä¹‹é—´çš„æ•°æ®è®¿é—®ï¼Œå¦‚åœ¨GPUçš„SRAMå’ŒHBMä¹‹é—´ã€‚
&lt;/code>&lt;/pre>
&lt;h3 id="flash-attention-2" >
&lt;div>
&lt;a href="#flash-attention-2">
##
&lt;/a>
Flash Attention-2ï¼š
&lt;/div>
&lt;/h3>
&lt;p>åœ¨Flash Attentionçš„åŸºç¡€ä¸Šï¼Œç ”ç©¶äººå‘˜è¿›ä¸€æ­¥æå‡ºäº†Flash Attention-2ï¼Œå®ƒé€šè¿‡æ”¹è¿›å·¥ä½œåˆ†é…å’Œå¹¶è¡ŒåŒ–ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®¡ç®—é€Ÿåº¦ã€‚Flash Attention-2çš„ä¼˜åŒ–åŒ…æ‹¬ï¼š&lt;/p>
&lt;pre>&lt;code>å‡å°‘éžçŸ©é˜µä¹˜æ³•ï¼ˆnon-matmulï¼‰çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFLOPsï¼‰ã€‚
é€šè¿‡åœ¨ä¸åŒçš„çº¿ç¨‹å—ä¸Šå¹¶è¡ŒåŒ–æ³¨æ„åŠ›è®¡ç®—ï¼Œæé«˜äº†GPUçš„å ç”¨çŽ‡ã€‚
åœ¨æ¯ä¸ªçº¿ç¨‹å—å†…ï¼Œå°†å·¥ä½œåˆ†é…ç»™ä¸åŒçš„warpï¼Œä»¥å‡å°‘é€šè¿‡å…±äº«å†…å­˜çš„é€šä¿¡ã€‚
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>&lt;strong>PagedAttentionçš„åŽŸç†æ˜¯ä»€ä¹ˆï¼Œè§£å†³äº†LLMä¸­çš„ä»€ä¹ˆé—®é¢˜ï¼Ÿ&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Paged Attentionï¼ˆPAï¼‰æŠ€æœ¯æ˜¯ä¸€ç§ç”¨äºŽä¼˜åŒ–å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰æŽ¨ç†æ€§èƒ½çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è‡ª
å›žå½’ç”Ÿæˆä»»åŠ¡æ—¶å¯¹å†…å­˜ä½¿ç”¨æ•ˆçŽ‡çš„æ˜¾è‘—æå‡ã€‚è¿™é¡¹æŠ€æœ¯çš„æ ¸å¿ƒæ€æƒ³æ˜¯å€Ÿé‰´æ“ä½œç³»ç»Ÿä¸­è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µ
çš„æŠ€æœ¯ï¼Œå°†ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„é”®å€¼å¯¹ï¼ˆKey-Value pairsï¼Œç®€ç§°K-V pairsï¼‰ç¼“å­˜ä»¥åˆ†é¡µçš„å½¢å¼å­˜
å‚¨å’Œç®¡ç†ã€‚&lt;/p>
&lt;p>åœ¨è‡ªå›žå½’è§£ç è¿‡ç¨‹ä¸­ï¼Œæ¨¡åž‹ä¸ºæ¯ä¸ªè¾“å…¥ä»¤ç‰Œç”Ÿæˆæ³¨æ„åŠ›é”®å’Œå€¼ï¼Œè¿™äº›é”®å€¼å¯¹è¢«å­˜å‚¨åœ¨GPUçš„æ˜¾å­˜ä¸­ä»¥
é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚ç”±äºŽè¿™äº›ç¼“å­˜çš„é”®å€¼å¯¹å¤§å°æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼Œå¹¶ä¸”å¯èƒ½ä¼šå ç”¨å¤§é‡çš„æ˜¾å­˜ç©ºé—´ï¼Œå› æ­¤
æœ‰æ•ˆç®¡ç†è¿™äº›ç¼“å­˜æˆä¸ºä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ³¨æ„åŠ›ç®—æ³•åœ¨å¤„ç†æ—¶ä¼šå—é™äºŽæ˜¾å­˜çš„å¤§å°ï¼Œè¿™é™åˆ¶äº†æ¨¡åž‹çš„
æ‰¹å¤„ç†èƒ½åŠ›å’Œæ•´ä½“çš„åžåé‡ã€‚&lt;/p>
&lt;h2 id="06llmå¾®è°ƒä¸Žé‡åŒ–" >
&lt;div>
&lt;a href="#06llm%e5%be%ae%e8%b0%83%e4%b8%8e%e9%87%8f%e5%8c%96">
#
&lt;/a>
06:LLMå¾®è°ƒä¸Žé‡åŒ–
&lt;/div>
&lt;/h2>
&lt;p>30.å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰æœ‰å“ªäº›æ–¹æ³•ï¼Ÿ
31.LORAå¾®è°ƒç›¸æ¯”äºŽå¾®è°ƒé€‚é…å™¨æˆ–å‰ç¼€å¾®è°ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
32.æœ‰äº†è§£è¿‡ä»€ä¹ˆæ˜¯ç¨€ç–å¾®è°ƒå—ï¼Ÿ
33.è®­ç»ƒåŽé‡åŒ–ï¼ˆPTQï¼‰å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰ä¸Žä»€ä¹ˆåŒºåˆ«ï¼Ÿ
34.LLMsä¸­ï¼Œé‡åŒ–æƒé‡å’Œé‡åŒ–æ¿€æ´»çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
35.AWQé‡åŒ–çš„æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/p>
&lt;h2 id="07åµŒå…¥å‘é‡æ¨¡åž‹" >
&lt;div>
&lt;a href="#07%e5%b5%8c%e5%85%a5%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8b">
#
&lt;/a>
07:åµŒå…¥å‘é‡æ¨¡åž‹
&lt;/div>
&lt;/h2>
&lt;p>40.è‡ªå‰ä¸»æµçš„ä¸­æ–‡åµŒå…¥å‘é‡æ¨¡åž‹æœ‰å“ªäº›ï¼Ÿ&lt;/p>
&lt;h2 id="å…¶ä»–" >
&lt;div>
&lt;a href="#%e5%85%b6%e4%bb%96">
#
&lt;/a>
å…¶ä»–
&lt;/div>
&lt;/h2>
&lt;p>45.DeepSpeedæŽ¨ç†å¯¹ç®—å­èžåˆåšäº†å“ªäº›ä¼˜åŒ–ï¼Ÿ
48.è¯·ä»‹ç»ä¸€ä¸‹å¾®è½¯çš„ZeROä¼˜åŒ–å™¨&lt;/p>
&lt;p>3.ä¸ºä»€ä¹ˆçŽ°åœ¨çš„å¤§æ¨¡åž‹å¤§å¤šæ˜¯decoder-onlyçš„æž¶æž„ï¼Ÿ
4.è®²ä¸€ä¸‹ç”Ÿæˆå¼è¯­è¨€æ¨¡åž‹çš„å·¥ä½œæœºç†
5.å“ªäº›å› ç´ ä¼šå¯¼è‡´LLMçš„åè§ï¼Ÿ
7.å¦‚ä½•å‡è½»LLMä¸­çš„å¹»è§‰çŽ°è±¡ï¼Ÿ
8.è§£é‡ŠChatGPTçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ çš„æ¦‚å¿µ
10.å¦‚ä½•è¯„ä¼°å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ï¼Ÿ
11.å¦‚ä½•ç¼“è§£LLMsé‡å¤è¯»é—®é¢˜ï¼Ÿ
16.Wordpieceä¸ŽBPEä¹‹é—´çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
17.æœ‰å“ªäº›å¸¸è§çš„ä¼˜åŒ–LLMsè¾“å‡ºçš„æŠ€æœ¯ï¼Ÿ
18.GPT-3æ‹¥æœ‰çš„1750äº¿å‚æ•°ï¼Œæ˜¯æ€Žä¹ˆç®—å‡ºæ¥çš„ï¼Ÿ
19.æ¸©åº¦ç³»æ•°å’Œtop-pï¼Œtop-kå‚æ•°æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
21.ä»‹ç»-ä¸‹postlayernormå’Œprelayernormçš„åŒºåˆ«
22.ä»€ä¹ˆæ˜¯æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºï¼Ÿ
23.ä½ è§‰å¾—ä»€ä¹ˆæ ·çš„ä»»åŠ¡æˆ–é¢†åŸŸé€‚åˆç”¨æ€ç»´é“¾æç¤ºï¼Ÿ
24.ä½ äº†è§£ReActå—ï¼Œå®ƒæœ‰ä»€ä¹ˆä¼˜ç‚¹ï¼Ÿ
25.è§£é‡Šä¸€ä¸‹langchainAgentçš„æ¦‚å¿µ
26.langchainæœ‰å“ªäº›æ›¿ä»£æ–¹æ¡ˆï¼Ÿ
27.langchaintokenè®¡æ•°æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ
28.LLMé¢„è®­ç»ƒé˜¶æ®µæœ‰å“ªå‡ ä¸ªå…³é”®æ­¥éª¤ï¼Ÿ
29.RLHFæ¨¡åž‹ä¸ºä»€ä¹ˆä¼šè¡¨çŽ°æ¯”SFTæ›´å¥½ï¼Ÿ
36.ä»‹ç»ä¸€ä¸‹GPipeæŽ¨ç†æ¡†æž¶
37.çŸ©é˜µä¹˜æ³•å¦‚ä½•åšå¼ é‡å¹¶è¡Œï¼Ÿ
38.è¯·ç®€è¿°ä¸‹PPOç®—æ³•æµç¨‹ï¼Œå®ƒè·ŸTRPOçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
39.ä»€ä¹ˆæ˜¯æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿ
41.ä¸ºä»€ä¹ˆLLMçš„çŸ¥è¯†æ›´æ–°å¾ˆå›°éš¾ï¼Ÿ
42.RAGå’Œå¾®è°ƒçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
43.å¤§æ¨¡åž‹ä¸€èˆ¬è¯„æµ‹æ–¹æ³•åŠåŸºå‡†æ˜¯ä»€ä¹ˆï¼Ÿ
50.ä»€ä¹ˆæ˜¯æŠ•æœºé‡‡æ ·æŠ€æœ¯ï¼Œè¯·ä¸¾ä¾‹è¯´æ˜Žï¼Ÿ&lt;/p></description></item><item><title>Tokenization: BPE, Unigram and more</title><link>/posts/llm_tokenization/</link><pubDate>Mon, 22 Apr 2024 15:41:56 +0800</pubDate><guid>/posts/llm_tokenization/</guid><description>&lt;h1 id="there-is-more-than-one-way-to-tokenize-a-sentence" >
&lt;div>
&lt;a href="#there-is-more-than-one-way-to-tokenize-a-sentence">
##
&lt;/a>
There is more than one way to tokenize a sentence
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>word-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>A big vocabulary is needed&lt;/li>
&lt;li>We combine words: what exactly constitutes a word (&amp;ldquo;bachelor of science&amp;rdquo;, or
isolated words)&lt;/li>
&lt;li>Abbreviated words: &amp;ldquo;LOL&amp;rdquo;, &amp;ldquo;IMO&amp;rdquo;, are these collections of words or new words?&lt;/li>
&lt;li>Languages that don&amp;rsquo;t segment by spaces&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>character-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>Lack of meaning: Unlike words, characters don&amp;rsquo;t have any inherent meaning, model
may lose the semantic-specific feature of words.&lt;/li>
&lt;li>Increased input computation&lt;/li>
&lt;li>Limits netword+k choices: It&amp;rsquo;s difficult to use architectures which process input
sequentially since the input sequences will be much longer.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Subword-level chunks/tokens&lt;/p>
&lt;ul>
&lt;li>We want a tokenization scheme that deals with an infinite potential vocabulary via
a finite list of known words. Make up the word â€œunfortunatelyâ€ via â€œunâ€ + â€œforâ€+
â€œtunâ€ + â€œateâ€ + â€œlyâ€.&lt;/li>
&lt;li>Subword tokenisation will break the text into chunks based on the word frequency.
In practice what happens is that common words will be tokenized generally as
whole words, e.g. â€œtheâ€, â€œatâ€, â€œandâ€, etc., while rarer words will be broken
into smaller chunks and can be used to create the rest of the words in the
relevant dataset.&lt;/li>
&lt;li>BPE(Byte Pair Encoding): One popular algorithm for subword tokenisation which
follows the above approach is BPE. BPE was originally used to help compress data
by finding common byte pair combinations. It can also be applied to NLP to find
the most efficient way of representing text.
&lt;ul>
&lt;li>What is merging?
The main goal of the BPE subword algorithm is to find a way to represent
your entire text dataset with the least amount of tokens. Similar to a
compression algorithm, you want to find the best way to represent your image,
text or whatever you are encoding, which uses the least amount of data, or
in our case tokens. In the BPE algorithm merging is the way we try and
â€œcompressâ€ the text into subword units.&lt;/li>
&lt;li>There are a few steps to these merging actions:
&lt;ol>
&lt;li>Get the word &lt;strong>count&lt;/strong> frequency&lt;/li>
&lt;li>Get the &lt;strong>initial token count&lt;/strong> and frequency (i.e., how many times each
character occurs)&lt;/li>
&lt;li>Merge the &lt;strong>most common byte pairing&lt;/strong>&lt;/li>
&lt;li>Add this to the list of tokens and &lt;strong>recalculate the frequency count&lt;/strong>
for each token (this will change with each merging step)&lt;/li>
&lt;li>&lt;strong>Rinse and repeat&lt;/strong> until get reached pre-defined token limits (vocab
size) or a set of number of iterations&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Greedy algorithm: BPE ensures that the most common words will be represented in
the new vocabulary as a single token, while less common words will be broken
down into two or more subword tokens. To achieve this, BPE will go through every
potential option at each step and pick the tokens to merge based on the highest
frequency.One downside of BPEâ€™s greedy approach is it can result in a potentially
ambiguous final token vocabulary.
For instance GPT has a vocabulary size of 40,478 since they have 478 base
characters and chose to stop training after 40,000 merges.&lt;/li>
&lt;li>BBPE(byte-level PBE): A base vocabulary that includes all possible base characters
can be quite large if e.g. all unicode characters are considered as base
characters. To have a better base vocabulary, GPT-2 uses bytes as the base
vocabulary, which is a clever trick to force the base vocabulary to be of size
256 while ensuring that every base character is included in the vocabulary. With
some additional rules to deal with punctuation, the GPT2â€™s tokenizer can
tokenize every text without the need for the &lt;unk> symbol. GPT-2 has a
vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a
special end-of-text token and the symbols learned with 50,000 merges.
&lt;a href="https://huggingface.co/docs/transformers/en/tokenizer_summary#byte-pair-encoding-bpe">from hf doc&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="probabilistic-subword-tokenization" >
&lt;div>
&lt;a href="#probabilistic-subword-tokenization">
##
&lt;/a>
Probabilistic Subword Tokenization
&lt;/div>
&lt;/h1>
&lt;p>Using the frequency of subword patterns for tokenization can result in ambiguous final
encodings. The problem is that we have no way to predict which particular token is more
likely to be the best one when encoding any new input text.
Luckily, needing to predict the most likely sequence of text is not a unique problem to
tokenization. We can leverage this knowledge to build a better tokenizer.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Unigram Subword Tokenization&lt;/p>
&lt;ul>
&lt;li>The goal for a subword model, however, is different from a LM that is trying to
predict a full sentence. We only want something that generates unambiguous
tokenization.&lt;/li>
&lt;li>The unigram approach differs from BPE in that it attempts to choose the most
likely option rather than the best option at each iteration. To generate a
unigram subword token set you need to first define the desired final size of
your token set and also a starting seed subword token set.&lt;/li>
&lt;li>You can choose the seed subword token set in a similar way to BPE and choose
the most frequently occurring substrings. Once you have this in place then
you need to:
&lt;ol>
&lt;li>Work out the probability for each subword token&lt;/li>
&lt;li>Work out a loss value which would result if each subwork token were to be
dropped. The loss is worked out via Expectation Maximization algorithm.&lt;/li>
&lt;li>Drop the tokens which have the largest loss value (e.g., the bottom 10%
or 20% of subword tokens based on their loss calculations).&lt;/li>
&lt;li>Repeat these steps until reach the desired final vocabulary size or there
is no change in token numbers after successive iterations.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>WordPiece (greedy approach tokenzier, BERT partner)
Think of WordPiece as an intermediary between the BPE approach and the unigram approach.&lt;/p>
&lt;ul>
&lt;li>BPE, if you remember, takes two tokens, looks at the frequency of each pair and then
merges the pairs that have the highest combined frequency count. It only considers
the most frequent pair combinations at each step, nothing else.&lt;/li>
&lt;li>An alternate approach is to check the potential impact of merging that particular
pair. You can do this using the probabilistic LM approach. At each iterative step,
choose the character pair which will result in the largest increase in likelihood
once merged. This is the difference between the probability of the new meged pair
occurring minus the probability of both individual tokens occurring individually.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The main difference is that WordPiece is a greedy approach. It still tries to build a
tokenizer from the bottom up, picking the best pair at each iteration to merge.
WordPiece uses the likelihood rather than count frequency but otherwise it is a similar
approach. Unigram in contrast is a fully probabilistic approach which uses probability
to both choose the pairs to merge and whether to merge them or not. It also removes
tokens based on the fact that they add the least to the overall likelihood of the
unigram model.&lt;/p>
&lt;h1 id="briefly-summarize" >
&lt;div>
&lt;a href="#briefly-summarize">
##
&lt;/a>
briefly summarize:
&lt;/div>
&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>BPE: Just uses the frequency of occurrences to identify the best match at every
iteration until it reaches the predefined vocabulary size.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>WordPiece: Similar to BPE and uses frequency occurrences to identify potential
merges but makes the final decision based on the likelihood of the merged token&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Unigram: A fully probabilistic model which does not use frequency
occurrences. Instead, it trains a LM using a probabilistic model, removing
the token which improves the overall likelihood the least and then starting
over until it reaches the final token limit.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="sentencepiece" >
&lt;div>
&lt;a href="#sentencepiece">
##
&lt;/a>
SentencePiece
&lt;/div>
&lt;/h1>
&lt;p>SentencePiece basically tries to bring all the subword tokenization tools and techniques
under one banner. Itâ€™s kind of like the Swiss Army knife for subword tokenization. To be
a Swiss Army-like tool something has to be capable of solving multiple problems. So what
problems is SentencePiece addressing:&lt;/p>
&lt;ol>
&lt;li>All other models assume input is already tokenized: BPE and Unigram are great model
but they share one big disadvantage: they both need to have their input already
tokenized. SentencePiece deals with this by simply taking in an input in raw text and
then doing everything needed on that input to perform subword tokenization.&lt;/li>
&lt;li>Language agnostic: Since all other subword algorithms need to have their input
pre-tokenized, it limits their applicability to many languages.&lt;/li>
&lt;li>Decoding is difficult: Another problem which is caused by model like BPE and unigram
requiring already tokenized inputs is that you do not know what encoding rules were
used. For example, how were spaces encoded in the tokens? So you cannot decode the
input and return it to is original format.&lt;/li>
&lt;li>No end to end solution: You cannot just plug in a raw input to BPE (or Unigram) and
get an output.&lt;/li>
&lt;/ol>
&lt;p>Some of the techniques SentencePiece uses to address the above shortcomings:&lt;/p>
&lt;ol>
&lt;li>Encode everything as unicode: SentencePiece first converts all the input into unicode
characters. This makes it a language agnostic tool.&lt;/li>
&lt;li>&amp;ldquo;space&amp;rdquo; encoded as &amp;ldquo;_&amp;quot;(U+2581): To get around the word segmenting issues.&lt;/li>
&lt;li>And it&amp;rsquo;s faster: One of the issues preventing other subword algorithms from being used
to tokenize raw sentences as part of model training was that there lack of speed. If
you processed input in real time and performed your tokenization on the raw input it
would be too slow. SentencePiece addresses this by using a priority queue for the BPE
algorithm to speed it up so that you can use it as part of an end-to-end solution.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;a href="https://www.openteams.com/tokenizers-how-machines-read/">https://www.openteams.com/tokenizers-how-machines-read/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>TODO: è¡¥å……ä¸­æ–‡&lt;/p></description></item><item><title>test rendering of equations of latex in Hugo</title><link>/posts/hugo101/test_math_equation_rendering/</link><pubDate>Fri, 25 Aug 2023 13:49:40 +0800</pubDate><guid>/posts/hugo101/test_math_equation_rendering/</guid><description>&lt;p>å¼å­èƒ½æ­£å¸¸æ¸²æŸ“ï¼Œä½†éœ€è¦åœ¨ä¸¤ä¸ªåœ°æ–¹çš„ä¸‹æ ‡å¤„æ·»åŠ é¢å¤–è½¬ä¹‰å­—ç¬¦ &lt;code>\&lt;/code>ï¼Œ
ä¹Ÿå°±æ˜¯ï¼š&lt;code>\mathbb{E}\_{xxx}&lt;/code>å’Œ&lt;code>p\_{model}&lt;/code>ï¼Œå…¶ä»–åœ°æ–¹å´ä¸éœ€è¦ &lt;code>\hat{p}_{data}&lt;/code>&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>æ˜¯å› ä¸º &lt;code>\mathbb{}&lt;/code> å¯¼è‡´çš„å—ï¼Ÿçœ‹èµ·æ¥ä¸æ˜¯ï¼š
$$
E_{x \sim \hat{p}_{data}} {\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>æ˜¯å› ä¸º &lt;code>\text{}&lt;/code> å¯¼è‡´çš„å—ï¼Ÿçœ‹èµ·æ¥ä¸æ˜¯ (&lt;code>\text{log}&lt;/code> -&amp;gt; &lt;code>\log&lt;/code>)ï¼š
$$
\mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>å¦‚æžœ &lt;code>\mathbb{}_&lt;/code> æ¸²æŸ“ä¸å‡ºæ¥ï¼Œé‚£å®ƒæ¸²æŸ“å¤±è´¥æ˜¯æ€Žä¹ˆæ ·çš„ (ä¸‹å¼&lt;code>\mathbb{}_&lt;/code>ä¸­çš„&lt;code>_&lt;/code>ä¸è§äº†)ï¼š&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ä¸ºä»€ä¹ˆ &lt;code>p_{model}&lt;/code> ä¹Ÿæ¸²æŸ“ä¸å‡ºæ¥ï¼Œ è€Œéžè¦æ·»åŠ è½¬æ„å­—ç¬¦ &lt;code>p\_{model}&lt;/code> å‘¢ï¼Ÿ&lt;/p>
&lt;p>$$
\mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
E_{x \sim \hat{p}&lt;em>{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>$$
\mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\text{log}\ p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>æˆ–è®¸æ˜¯åµŒå¥—ä¸‹æ ‡è¿™å„¿å‡ºçš„é—®é¢˜ (&lt;code>\hat{p}_{data}&lt;/code> -&amp;gt; &lt;code>\hat{p}\_{data}&lt;/code> )ï¼Ÿ:
$$
E_{x \sim \hat{p}_{data}}{\text{log}\ p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>Bingo?? No bingo:&lt;/p>
&lt;p>åªåœ¨åµŒå¥—ä¸‹æ ‡çš„åœ°æ–¹æ·»åŠ è½¬ä¹‰å­—ç¬¦ï¼š&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}_{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>åªåœ¨&lt;code>p_{model}&lt;/code>ä¸‹æ ‡çš„åœ°æ–¹æ·»åŠ è½¬ä¹‰å­—ç¬¦ï¼š&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}&lt;em>{x \sim \hat{p}&lt;/em>{data}}{\log p_{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>åªåœ¨&lt;code>\mathbb{}&lt;/code>ä¸‹æ ‡çš„åœ°æ–¹æ·»åŠ è½¬ä¹‰å­—ç¬¦ï¼š&lt;/p>
&lt;p>$$
\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}&lt;em>{data}}{\log p&lt;/em>{model} {(x^i; \theta})}
$$&lt;/p>
&lt;p>ç®€è€Œè¨€ä¹‹ï¼Œæ‰¾ä¸åˆ°æ˜Žç¡®çš„è¯­æ³•è§„åˆ™æ¥è§£é‡Šè¿™ä¸ªé—®é¢˜ã€‚å¯èƒ½æ˜¯hugoåœ¨ä½¿ç”¨mathjaxè§£æžæ—¶å‡ºäº†é—®é¢˜ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯mathjaxè‡ªå·±çš„é—®é¢˜ã€‚&lt;/p>
&lt;p style="text-align:center;color:blue;">
Hugoçš„markdownæ–‡æ¡£é‡Œçš„æ•°å­¦å…¬å¼å—ï¼Œå¦‚æžœä¸ç¡®å®šï¼Œå°±åœ¨ä¸‹æ ‡é‚£é‡Œå‰ç½®ä¸€ä¸ª`\`ã€‚&lt;br>
åŒæ ·çš„é—®é¢˜ä¹Ÿå‡ºçŽ°åœ¨å¤šè¡Œå…¬å¼ç»„é‡Œè¿›è¡Œè½¬è¡Œæ—¶ï¼Œä½¿ç”¨ `\\\\\`ï¼Œè€Œä¸æ˜¯é€šå¸¸çš„ `\\\`
&lt;/p>
&lt;p>æƒ¹å‡ºè¿™ä¹ˆè®¸å¤šä¸æ˜Žæ‰€ä»¥çš„é—®é¢˜, æœ€åˆå°±æ˜¯ä¸‹é¢è¿™ä¸ªå…¬å¼ (å°±æ˜¯æˆ‘æ‰€è®¤çŸ¥çš„ã€é€šå¸¸è¡¨è¾¾ä¸‹æ ‡çš„æ™®é€šæ–¹å¼ï¼Œ
è®½åˆºçš„æ˜¯ï¼šä¸æŠŠå®ƒä»¬æ”¾åœ¨ &lt;code>$$&lt;/code> æ•°å­¦å…¬å¼å—é‡Œï¼Œæˆ‘è¿˜å¿…é¡»æ·»åŠ è½¬ä¹‰å­—ç¬¦ï¼Œå¦åˆ™æ¸²æŸ“ä¸å‡ºè¿™ä¸ªæ•ˆæžœðŸ˜‚):&lt;/p>
&lt;p>\tag{5.59}
\theta_{ML} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{x \sim \hat{p}_{data}}{\log p_{model} {(x^i; \theta})}&lt;/p>
&lt;p>æ•°å­¦å…¬å¼å—é‡Œè¡¨è¾¾ä¸‹æ ‡çš„è¯­æ³•ï¼Œæ— ç«¯ç«¯è¦æ·»åŠ ä¸€ä¸ªå‰ç½®è½¬ä¹‰å­—ç¬¦ï¼Œè¿™å°±æ˜¯æˆ‘æ‰€æ²¡æœ‰æ–™åˆ°çš„ã€‚æƒ³å¿…åº”è¯¥æ˜¯Hugoæ²¡æœ‰åšå¥½å…³äºŽè½¬ä¹‰å­—ç¬¦çš„ä¸Šä¸‹æ–‡è§£æžé—®é¢˜ï¼Œä¸”å…ˆå°†å°±ç€å§ã€‚&lt;/p></description></item><item><title>åˆ©å™¨4: Vim ç¼–è¾‘å™¨ -- åªå› ä¸ºåœ¨é”®ç›˜ä¹‹ä¸­ï¼Œæ–¹å‘é”®å¤ªé¥è¿œ</title><link>/posts/thecli/thevim/2022-08-13-vim8/</link><pubDate>Sat, 13 Aug 2022 21:37:59 +0800</pubDate><guid>/posts/thecli/thevim/2022-08-13-vim8/</guid><description>&lt;h2 id="vim-åˆä½“éªŒ" >
&lt;div>
&lt;a href="#vim-%e5%88%9d%e4%bd%93%e9%aa%8c">
#
&lt;/a>
Vim åˆä½“éªŒ
&lt;/div>
&lt;/h2>
&lt;p>â€œå¤šå¹´ä»¥åŽï¼Œé¢å¯¹æ˜¾ç¤ºå±ï¼Œ&amp;ldquo;é€ƒç¦»äº†å·¨ç¡¬Â·ä»æ—§æ„¤æ€’Â·æ‡µæ‡‚èœé¸Ÿ&amp;quot;å°†ä¼šå›žæƒ³èµ·åˆæ¬¡å°è¯• Vim ç¼–è¾‘å™¨
è€Œè¢«å›°åœ¨é‡Œé¢å«å¤©ä¸åº”å«åœ°ä¸çµçš„é‚£ä¸ªé¥è¿œçš„ä¸‹åˆâ€ã€‚&lt;/p>
&lt;p>é‚£æ—¶å€™çš„æˆ‘ï¼ˆ2017ï¼‰ï¼Œæ˜¯ä¸ªäºŒåå¤šå²çš„é’å¹´ï¼Œåˆšåˆšè®¤è¯†ç»ˆç«¯å’Œ Linux å¤§æ¦‚æ˜¯æ€Žä¹ˆä¸€å›žäº‹ï¼Œ
æ„Ÿè§‰æ°ä¼¼ä¸Šå¤©ç»™æˆ‘å…³äº†é‚£æ‰‡â€œçª—â€ï¼Œè‡ªå·±æŽ¨å¼€äº† Linux ä¸–ç•Œçš„å¤§é—¨ã€‚
ä¸–ç•Œæ–°ç”Ÿä¼Šå§‹ï¼Œè®¸å¤šäº‹ç‰©è¿˜ä¸çŸ¥å…¶åä½•å¤„ï¼Œå¯¹è®¸å¤šâ€œæ‹¿æ¥ä¸»ä¹‰â€çš„å‘½ä»¤ä»æ—§åŠä¿¡åŠç–‘ï¼Œ
ç®€ç›´å°±æ˜¯ä¸€åœºå¤§å†’é™© (æ²¡æœ‰æ€§å‘½ä¹‹æ”¸çš„é‚£ç§)ã€‚&lt;/p>
&lt;p>å½“æˆ‘å‘çŽ°åœ¨æ–‡æœ¬ç¼–è¾‘å™¨é‡Œçš„å…‰æ ‡å¯ä»¥ç”¨ h,j,k,l æ¥è¿›è¡Œå·¦ä¸‹ä¸Šå³ç§»åŠ¨æ—¶ï¼Œâ€œä¸æ‡‚ï¼Œä½†æˆ‘å¤§å—éœ‡æ’¼â€ã€‚&lt;/p>
&lt;p>ï¼ˆå…¶å®žå°±æ˜¯è§è¯†å°‘ï¼Œä¸æ˜¯æ‰€æœ‰é”®ç›˜éƒ½æœ‰ç‹¬ç«‹çš„æ–¹å‘é”®ï¼Œæ›´ä¸æ˜¯æ“ä½œç³»ç»Ÿå°±åªä¸€ä¸ª windows
ï¼Œä¸è¿‡æˆ‘æ˜¯äºŒåå¥½å‡ çš„äººäº†æ‰çŸ¥é“è¿™ä¸ªäº‹æƒ…ï¼Œç”¨äºŽè¿žçš„è¯è®²å°±æ˜¯ï¼šâ€œå•Šï¼Œè¿™æ˜¯ä¸€åœºä¾®è¾±â€ã€‚ï¼‰&lt;/p>
&lt;p>è™½ç„¶ç¬¬ä¸€æ¬¡å°è¯• Vim å°±è¢«æŠ›å¼ƒåˆ°äº†è’å²›ï¼Œå¥½åœ¨æˆ‘å¯¹è¿™æ ·ä¸€ä¸ªæ–°å¥‡çš„ä¸–ç•Œï¼Œæ—©æœ‰å¿ƒç†å‡†å¤‡ã€‚
äº‹å®žè¯æ˜Žï¼Œè¦é€‚åº”æ–°çŽ¯å¢ƒï¼Œæœ€å¥½çš„æ­¦å™¨ä¸è¿‡æ˜¯â€œä¸€ç‚¹å¥½å¥‡å¿ƒå’Œå¤šç‚¹è€å¿ƒâ€ã€‚ä»Žé‚£ä»¥åŽï¼Œ
æ„Ÿè§‰å°±å¥½åƒâ€œå±…ä½åœ¨å…”å­çš®æ¯›æ·±å¤„çš„å¾®ç”Ÿç‰©â€ç«­åŠ›å‘æ¯›å°–çˆ¬åŽ»ï¼Œâ€œä»¥ä¾¿å°†é­”æœ¯å¸ˆçœ‹ä¸ªæ¸…æ¥šâ€ã€‚&lt;/p>
&lt;p>ç»ˆç«¯é‡Œèƒ½ç”¨çš„ä¼˜ç§€å·¥å…·å¤šå¾—è®©æˆ‘è§‰å¾—ç®€ç›´â€œå°±åƒæ˜¯åœ¨æµ·è¾¹æ‹£è´å£³çš„å­©å­&amp;rdquo;ï¼Œè€Œ Vim ç¼–è¾‘å™¨æ— ç–‘æ˜¯æˆ‘æœ€å–œçˆ±çš„é‚£ä¸€æžšã€‚
è€Œåœ¨æµ·è¾¹æ”¾è´å£³çš„æ— ç–‘æ˜¯é‚£äº›ä¹äºŽåˆ†äº«çš„åŒé“ä¸­äººï¼Œæˆ‘ç›¸ä¿¡çœŸæ­£çš„è¿›æ­¥æ¥è‡ªäºŽåˆ†äº«ã€‚&lt;/p>
&lt;p>åŽæ¥å½“æˆ‘å¤§æ¦‚æ˜Žç™½ vimrc æ–‡ä»¶æ˜¯æ€Žä¹ˆä¸€å›žäº‹ä¹‹åŽï¼Œæˆ‘ç¬¬ä¸€ä»¶äº‹å°±æ˜¯ç»™å®ƒåŠ ä¸Šäº†è¿™ä¸€å¥:&lt;/p>
&lt;p>&lt;code>set mouse=&lt;/code>&lt;/p>
&lt;p>å…¶å®žå½“æ—¶æ˜¯æ€€ç€å¯¹æ—§æ—¥é‚£ç§å‡¡äº‹å¿…å¾—â€œé¼ æ ‡ç‚¹å‡»æ‹–æ‹½â€å®£æˆ˜çš„å¿å¿‘å¿ƒæ€ã€‚&lt;/p>
&lt;p>åŽæ¥æ€Žä¹ˆæ ·äº†ï¼Ÿ&lt;/p>
&lt;p>åŽæ¥æˆ‘å‘çŽ°ï¼Œå…¶å®žä¸æ€Žä¹ˆéœ€è¦â€œä¸“é—¨è…¾å‡ºä¸€åªæ‰‹ç”¨é¼ æ ‡æ“æŽ§â€ã€‚
å¦‚æžœæ˜¯åœ¨ç»ˆç«¯å¹²æ´»ï¼Œä½ ä¹æ„çš„è¯ï¼Œé‚£å°±æ˜¯ï¼šæ ¹æœ¬ä¸éœ€è¦ã€‚
å¦‚æžœä½ æžç«¯ç‚¹ï¼Œåœ¨å“ªé‡Œéƒ½ä¸éœ€è¦é¼ æ ‡ã€‚
é™¤éžæ˜¯ä½ å¦ä¸€åªæ‰‹åœ¨å¿™ç‚¹åˆ«çš„ä»€ä¹ˆï¼Œæˆ–è€…æ˜¯è€æ¿è¦â€œéº»çƒ¦â€ä½ ç»™â€œèµ¶ç´§â€ä¿®æ”¹ä¸€ç¯‡workæ–‡æ¡£ï¼Œå®¢æˆ·åœ¨çº¿ç­‰é‚£ç§&amp;hellip;&lt;/p>
&lt;h2 id="vim-ç¼–è¾‘å™¨" >
&lt;div>
&lt;a href="#vim-%e7%bc%96%e8%be%91%e5%99%a8">
#
&lt;/a>
Vim ç¼–è¾‘å™¨
&lt;/div>
&lt;/h2>
&lt;h3 id="vim-æ˜¯å¹²å•¥çš„" >
&lt;div>
&lt;a href="#vim-%e6%98%af%e5%b9%b2%e5%95%a5%e7%9a%84">
##
&lt;/a>
Vim æ˜¯å¹²å•¥çš„
&lt;/div>
&lt;/h3>
&lt;p>é¦–å…ˆåº”è¯¥äº†è§£ï¼š Vim é€‚ç”¨äºŽä»€ä¹ˆåœºæ™¯ï¼Ÿ&lt;/p>
&lt;blockquote>
&lt;p>â€œVimæ˜¯ä»Ž vi å‘å±•å‡ºæ¥çš„ä¸€ä¸ªæ–‡æœ¬ç¼–è¾‘å™¨ã€‚
å…¶ä»£ç è¡¥å®Œã€ç¼–è¯‘åŠé”™è¯¯è·³è½¬ç­‰æ–¹ä¾¿ç¼–ç¨‹çš„åŠŸèƒ½ç‰¹åˆ«ä¸°å¯Œï¼Œåœ¨ç¨‹åºå‘˜ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚
å’ŒEmacså¹¶åˆ—æˆä¸ºç±»Unixç³»ç»Ÿç”¨æˆ·æœ€å–œæ¬¢çš„ç¼–è¾‘å™¨ã€‚â€&lt;/p>
&lt;/blockquote>
&lt;p>å¹³æ—¶æˆ‘åœ¨æµè§ˆä¸­ï¼ˆå°¤å…¶æ˜¯å¯»æ‰¾ä¸€äº›åŠŸèƒ½æ’ä»¶è¿‡ç¨‹ï¼‰ä¹Ÿçœ‹åˆ°æœ‰äººå‘åšå®¢è¯´ä»–/å¥¹æ˜¯ç”¨ Vim è¿›
è¡Œâ€œå†™ä½œâ€çš„ï¼ˆä¸»è¦æ˜¯å…³äºŽ $\LaTeX$, $\TeX$, markdown çš„æ’ä»¶ï¼‰ï¼Œä½†æˆ‘æƒ³å¦‚æžœæ˜¯ç”¨æ¥è¿›
è¡Œæ–‡å­¦åˆ›ä½œè¿™ä¸€ç±»çš„å·¥ä½œçš„è¯ï¼Œé‡ç‚¹å¯èƒ½ä¸åœ¨äºŽç¼–è¾‘å™¨ï¼Œå¾ˆç®€å•ï¼Œè¿˜æ²¡æœ‰â€œç”µè„‘â€çš„æ—¶ä»£å°±
å·²ç»è¯žç”Ÿäº†å¤ªå¤šä¼˜ç§€ä½œå“ã€‚
ï¼ˆæƒ³è±¡å¤§ä½¬æ›¹é›ªèŠ¹é¢å¯¹ç”µè„‘å±å¹•â€œåå¹´åˆ æ”¹â€ã€Šçº¢æ¥¼æ¢¦ã€‹æ€»è§‰å¾—æœ‰ç‚¹è’è¯žï¼Œ
ä½†æ˜¯å°±ç®—æ˜¯æœ‰ç”µè„‘ï¼Œè¿™éš¾ç‚¹è‚¯å®šä¸åœ¨äºŽåˆ æ”¹ï¼ˆç¼–è¾‘å™¨æ— å…³ç´§è¦ï¼‰ï¼Œè€Œåœ¨äºŽæ–‡å­¦åˆ›é€ ã€‚
æžæœ‰å¯èƒ½ä»–æ ¹æœ¬ä¹°ä¸èµ·ç”µè„‘ã€‚ï¼‰&lt;/p>
&lt;p>åè¿‡æ¥å´ä¸ä¸€æ ·ï¼Œè‡³å°‘æˆ‘ä»¬è¿™å¹´ä»£ï¼Œæ²¡æœ‰å¬è¯´å“ªä¸ªæˆåŠŸçš„è½¯ä»¶äº§å“ï¼Œå®ƒçš„ä»£ç æ˜¯ç”¨æ‰‹å†™åœ¨è‰ç¨¿çº¸ä¸Š &amp;hellip;
æ€ªè¯žçš„æ˜¯ï¼Œé¢è¯•çš„æ—¶å€™å€’æ˜¯æœ‰ä¸å°‘è¿™ç§è¦æ‰‹å†™ä»£ç å•Šä¼ªä»£ç è¿™ç§äº‹æƒ…ï¼Œæˆ‘è§‰å¾—è¿™å¤šå¤šå°‘å°‘æœ‰ç‚¹é»‘è‰²å¹½é»˜ã€‚&lt;/p>
&lt;p>ç®€è€Œè¨€ä¹‹ï¼Œvimæ˜¯ä¸ªç¼–è¾‘å™¨ï¼Œå¾ˆå¤šäººç”¨å®ƒæ¥å†™ä»£ç ï¼Œä¹Ÿæœ‰äººç”¨å®ƒæ¥å¹²å…¶ä»–ã€‚&lt;/p>
&lt;p>éšç€äº†è§£æ·±å…¥ï¼Œæ…¢æ…¢å°±ä¼šæŽ¥è§¦åˆ°è¯¸å¦‚â€œç¼–è¾‘å™¨ä¹‹æˆ˜â€ã€â€œç¼–ç¨‹è¯­è¨€ä¹‹äº‰â€è¿™ä¸€ç±»è¯é¢˜ã€‚&lt;/p>
&lt;h3 id="ç¼–è¾‘å™¨ä¹‹æˆ˜" >
&lt;div>
&lt;a href="#%e7%bc%96%e8%be%91%e5%99%a8%e4%b9%8b%e6%88%98">
##
&lt;/a>
ç¼–è¾‘å™¨ä¹‹æˆ˜
&lt;/div>
&lt;/h3>
&lt;blockquote>
&lt;p>â€œç¼–è¾‘å™¨ä¹‹æˆ˜æ˜¯æŒ‡è®¡ç®—æœºç¼–ç¨‹é¢†åŸŸå†…ï¼Œ
ä¸¤ç±»æ–‡æœ¬ç¼–è¾‘å™¨ viï¼ˆä»¥åŠè¡ç”Ÿç‰ˆæœ¬ï¼‰å’Œ Emacs ç”¨æˆ·ä¹‹é—´çš„å¯¹æŠ—å†²çªã€‚
è¿™å·²ç»æˆä¸ºé»‘å®¢æ–‡åŒ–å’Œè‡ªç”±è½¯ä»¶ç¤¾åŒºæ–‡åŒ–çš„ä¸€éƒ¨åˆ†ã€‚&lt;/p>
&lt;/blockquote>
&lt;p>ä¸€äº› Emacs ç”¨æˆ·è¯´ï¼š&lt;/p>
&lt;blockquote>
&lt;p>Vi æœ‰ä¸¤ä¸ªæ¨¡å¼ï¼Œâ€œä¸åœåœ°å“”å“”å«â€å’Œâ€œæžç ¸ä¸€åˆ‡â€ã€‚&lt;/p>
&lt;/blockquote>
&lt;p>è€Œæ® Vi ç”¨æˆ·ç§°ï¼š&lt;/p>
&lt;blockquote>
&lt;p>ä¸€äº› Emacs ç”¨æˆ·ç”±äºŽä¿®æ”¹é”®è¿‡å¤šè€Œä½¿ç”¨è„šè¸æ¿ã€‚&lt;/p>
&lt;/blockquote>
&lt;p>ä¸€äº›åšå®¢åšæ–‡ã€å½±è§†ä½œå“ä¹Ÿéƒ½ä¼šå‡ºçŽ°è¿™äº›å†…å®¹åœºæ™¯ï¼Œè€Œä¸”å¤§éƒ½æ˜¯è½»æ¾æ¬¢ä¹çš„åœºæ™¯ã€‚æ¯”å¦‚ï¼Œ
ã€Šç¡…è°·ã€‹é‡Œç†æŸ¥å¾·å’Œä»–â€œç®€ç›´å°±æ˜¯è¸©äº†ç‹—å±Žè¿â€æ‰å‹¾æ­ä¸Šçš„æ¼‚äº®å¥³å‹å°±ä¸ºäº† Vim å’Œ Emacs
ã€ä½¿ç”¨ Tab è¿˜æ˜¯ Space è¿›è¡Œä»£ç ç¼©è¿›ä»¥åŠç¼©è¿›é£Žæ ¼è€Œåµäº†èµ·æ¥ï¼Œç»§è€Œç›´æŽ¥é—¹æŽ°ï¼Œ
æœ€åŽä»¥ç†æŸ¥å¾·ç§€é€—äº†çš„åœ¨ç”¨èº«ä½“åŠ›è¡Œè¿›è¡Œâ€œç¼©è¿›æ¼”ç¤ºâ€ä¸­æ»šä¸‹æ¥¼æ¢¯è€Œå‘Šç»ˆã€‚
&amp;hellip;&lt;/p>
&lt;blockquote>
&lt;p>æˆ˜ç«åœ¨ä¸¤ç¾¤ç”¨æˆ·ä¹‹é—´å¤šæ¬¡ç‚¹ç‡ƒï¼Œä»–ä»¬äº’ç›¸è”‘è§†ï¼Œåšä¿¡è‡ªå·±çš„é€‰æ‹©æ‰æ˜¯æœ€å®Œç¾Žçš„ã€‚
ç›¸æ¯”å…¶ä»–çš„ITé¢†åŸŸæˆ˜äº‰ï¼ˆæµè§ˆå™¨å¤§æˆ˜ã€æ“ä½œç³»ç»Ÿä¹‹äº‰ã€ç¼–ç¨‹è¯­è¨€ä¹‹äº‰ã€ä»£ç ç¼©è¿›é£Žæ ¼ä¹‹æˆ˜
ç­‰ç­‰)ï¼Œç¼–è¾‘å™¨çš„é€‰æ‹©å…¶å®žé€šå¸¸åªæ˜¯ä¸ªäººé—®é¢˜ã€‚ â€&lt;/p>
&lt;/blockquote>
&lt;p>å¦‚æžœä¸å°å¿ƒä¹Ÿå·å…¥äº†è¿™äº›æˆ˜äº‰ï¼Œè¯·è®°ä½ï¼šç¼–è¾‘å™¨çš„é€‰æ‹©å…¶å®žé€šå¸¸åªæ˜¯ä¸ªäººé—®é¢˜ã€‚&lt;/p>
&lt;p>å½“ç„¶ï¼Œå®žåœ¨åµå¾—ä¸å¯å¼€äº¤çš„æ—¶å€™ï¼Œåˆ«å¿˜äº†æ¬å‡ºç¼–ç¨‹è¯­è¨€ä¹‹äº‰é‡Œé‚£å¥ä¸€é”¤å®šéŸ³çš„è¯é¢˜ç»ˆç»“è¯­ï¼š
â€œPHP æ‰æ˜¯çœŸå¥½çš„ç¼–ç¨‹è¯­è¨€ï¼Œä¸æŽ¥å—ä»»ä½•åé©³ã€‚â€&lt;/p>
&lt;p>å¦‚æžœæƒ³äº†è§£æ›´å¤šï¼Œå¯ä»¥åŽ» &lt;a href="https://zh.m.wikipedia.org/zh-hans/Vim">Vim ç»´åŸºè¯æ¡&lt;/a>
äº†è§£æ›´å¤šç²¾å½©å†…å®¹ã€‚å¦‚æžœä½ ç”šè‡³æ— æ³•è®¿é—®ç»´åŸºç™¾ç§‘ï¼Œé‚£ä¸æ˜¯ä½ çš„é”™ï¼Œä¸è¿‡ä½ å¾—æƒ³åŠžæ³•ã€‚&lt;/p>
&lt;h2 id="vim-åŠå…¶é…ç½®" >
&lt;div>
&lt;a href="#vim-%e5%8f%8a%e5%85%b6%e9%85%8d%e7%bd%ae">
#
&lt;/a>
vim åŠå…¶é…ç½®
&lt;/div>
&lt;/h2>
&lt;p>å¦‚æžœä½ æ˜¯æƒ³å…¥é—¨è€Œè¿˜åœ¨é—¨å£å¾˜å¾Šçš„ï¼Œå¯ä»¥å…ˆåœ¨ç»ˆç«¯ (Terminal) é‡Œè¾“å…¥ &lt;code>vimtutor&lt;/code>ï¼Œ
ç„¶åŽçœ‹çœ‹ä½ æ„Ÿè§‰å¦‚ä½•ã€‚å¦‚æžœä½ è®¤çœŸè¿½éšï¼Œä½ ä¼šå‘çŽ°ä¸€åˆ‡è‡ªæœ‰é“ç†ã€‚è²Œä¼¼è¿˜æŒºå¥½çŽ©ã€‚
å—¯ï¼Œå’¦ï¼Œæ€Žä¹ˆé€€å‡ºè¿™ç¼–è¾‘å™¨å‘¢ï¼Ÿ&lt;/p>
&lt;h3 id="çŽ°ä»£ç ä»”å¦‚ä½•åœ¨ç»ˆç«¯ç•Œé¢é€€å‡ºvimç¼–è¾‘å™¨" >
&lt;div>
&lt;a href="#%e7%8e%b0%e4%bb%a3%e7%a0%81%e4%bb%94%e5%a6%82%e4%bd%95%e5%9c%a8%e7%bb%88%e7%ab%af%e7%95%8c%e9%9d%a2%e9%80%80%e5%87%bavim%e7%bc%96%e8%be%91%e5%99%a8">
##
&lt;/a>
çŽ°ä»£ç ä»”å¦‚ä½•åœ¨ç»ˆç«¯ç•Œé¢é€€å‡ºvimç¼–è¾‘å™¨
&lt;/div>
&lt;/h3>
&lt;p>å¦‚æžœä½ å’Œæˆ‘å½“åˆä¸€æ ·ä¸€å¤´é›¾æ°´&amp;hellip; æ·¡å®šæ·¡å®šï¼Œè¾“å…¥ &lt;code>:q&lt;/code> ï¼ŒæŒ‰ä¸‹ &lt;code>Enter&lt;/code> å³å¯é€€å‡ºã€‚&lt;/p>
&lt;p>å—¯ï¼Œå¦‚æžœä½ æ­£åœ¨è¾“å…¥ï¼Œéœ€è¦å…ˆæŒ‰ä¸‹ &lt;code>Esc&lt;/code>ï¼Œç„¶åŽè¾“å…¥ &lt;code>:q&lt;/code> æ¥é€€å‡ºã€‚&lt;/p>
&lt;p>è¿™ä¸ªæ“ä½œä¹Ÿä¸å¥½ä½¿ï¼Ÿå…«æˆä½ æ˜¯åœ¨åˆ«çš„æ¨¡å¼é‡Œï¼Œæ¯”å¦‚ &lt;code>Ex mode&lt;/code>ï¼Œ
å…«æˆå®ƒä¹Ÿä¼šæç¤ºä½ è¾“å…¥ &lt;code>visual&lt;/code> æ¥è¿”å›žåˆ° &lt;code>normal mode&lt;/code>ã€‚&lt;/p>
&lt;p>å¦‚æžœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œæžæœ‰å¯èƒ½æ˜¯ä½ ä¸å°å¿ƒæŒ‰äº† &lt;code>q&lt;/code> è¿›å…¥å‘½ä»¤å½•åˆ¶ï¼Œ
é€šå¸¸è¦æ˜¯ä½ å¤šæŒ‰å‡ æ¬¡ &lt;code>q&lt;/code> æ¥è§‚å¯Ÿï¼Œä¼šå‘çŽ°æœ‰ &lt;code>recording @&lt;/code> çš„æç¤ºï¼Œ
è¿™å°±æ˜¯å¤šå‘½ä»¤å½•åˆ¶æ— ç–‘ï¼Œé‡å¤æ“ä½œï¼šè¾“å…¥ &lt;code>:q&lt;/code> ï¼ŒæŒ‰ä¸‹ &lt;code>Enter&lt;/code> å³å¯é€€å‡ºã€‚&lt;/p>
&lt;p>å¦‚æžœéƒ½ä¸æ˜¯ï¼Œå¹¶ä¸”â€œä»–Xçš„æŒ‰ä»€ä¹ˆä¹Ÿæ²¡ç”¨â€ï¼Œå…ˆæ£€æŸ¥ä¸€ä¸‹ä½ çš„æœºå™¨æ˜¯å¦è¿˜åœ¨æ­£å¸¸å·¥ä½œï¼Œ
æ˜¯ï¼Œåˆ™é‡å¤ä»¥ä¸‹æ“ä½œï¼šè¾“å…¥ &lt;code>:q&lt;/code>ï¼ŒæŒ‰ä¸‹ &lt;code>Enter&lt;/code>ã€‚&lt;/p>
&lt;p>å¦‚æžœä½ æ‰€æœ‰å¥½å¥‡å¿ƒè¿˜æ²¡æœ‰è¢«æ¶ˆè€—æ®†å°½ï¼Œæˆ‘æ¥å‘Šè¯‰ä½ ä¸€ä¸ªå¥½æ¶ˆæ¯ï¼š
vim å…±æœ‰ &lt;a href="https://en.wikipedia.org/wiki/Vim_(text_editor)">12ç§æ¨¡å¼&lt;/a>ã€‚&lt;/p>
&lt;p>è¿™ç¡®å®žæ˜¯ä¸ªæ–°ä¸–ç•Œã€‚&lt;/p>
&lt;h2 id="vimrc" >
&lt;div>
&lt;a href="#vimrc">
#
&lt;/a>
vimrc
&lt;/div>
&lt;/h2>
&lt;p>åº”å½“æ³¨æ„ï¼šä»Žç¬¬ä¸€æ¬¡åœ¨ç»ˆç«¯è¾“å…¥ &lt;code>vim&lt;/code> åˆ°å¼€å§‹äº†è§£å’Œä½¿ç”¨è‡ªå·±é…ç½®çš„ &lt;code>.vimrc&lt;/code>ï¼Œ
ä»Žå‰è€…åˆ°åŽè€…ï¼Œæ¯ä¸ªäººæ‰€ç»åŽ†çš„æ—¶é—´ä¸å¤ªä¸€æ ·ï¼Œæ¯•ç«Ÿæ¯ä¸ªäººçš„ç”Ÿæ´»å’Œç›®çš„åƒå·®ä¸‡åˆ«ã€‚
ï¼ˆå€’ä¸æ˜¯è¯´ï¼Œåˆšå¼€å§‹ä½¿ç”¨ vim å°±ä¸èƒ½ç›´æŽ¥ä¸Šæ‰‹ä¸ªäººé…ç½®ï¼Œè€Œæ˜¯æƒ³è¯´æ˜Žï¼Œ
â€œæ­¥å­è¿ˆå¤§äº†ï¼Œå®¹æ˜“æ‰¯ç€è›‹â€ï¼›ä¸€æ—¦æ”¾å¼ƒäº†æŽ¢ç´¢å’Œæ€è€ƒï¼Œå®¹æ˜“ä¸€ç¢°æŒ«æŠ˜å°±ååˆ†æ²®ä¸§ã€‚ï¼‰
ä½†åœ¨å¾ˆå¤šæ–‡ç« é‡Œï¼Œè¿™ä¸¤è€…ä¹‹é—´å¤šåŠå°±æ˜¯ä¸€ä¸ªç©ºè¡Œçš„è·ç¦»ã€‚
ï¼ˆå› æ­¤æˆ‘å†³å®šåœ¨è¿™ä¸ªç©ºè¡Œä¹‹å‰æ·»åŠ ä¸€ç‚¹è¯´æ˜Žï¼Œä»¥ä¸‹çš„å†…å®¹æ–°æ‰‹åˆçœ‹æ—¶ä¸å¤ªæ˜Žç™½å®žå±žæ­£å¸¸ï¼‰&lt;/p>
&lt;p>Vim å¦‚æ­¤å—æ¬¢è¿Žï¼Œå®ƒçš„ä¸°å¯Œçš„æ’ä»¶ç”Ÿæ€åº”è¯¥æ˜¯ä¸€ä¸ªé‡è¦çš„åŽŸå› ã€‚ä¸ºä»€ä¹ˆæœ‰è¿™ä¹ˆå¤šæ’ä»¶å‘¢ï¼Ÿ
å› ä¸ºæœ‰è¿™ä¹ˆå¤šçš„éœ€æ±‚ã€‚å› ä¸ºæœ‰é‚£ä¹ˆå¤šçš„æ’ä»¶ï¼Œæ‰€ä»¥æœ‰äº†è¦ç®¡ç†æ’ä»¶ï¼ˆæ›´æ–°ã€åˆ é™¤ç­‰ï¼‰çš„
æ’ä»¶ã€‚ä½¿ç”¨æ¯”è¾ƒå¹¿æ³›çš„æœ‰ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/junegunn/vim-plug">vim-plug&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/VundleVim/Vundle.vim">Vundle&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/tpope/vim-pathogen">Pathogen&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/k-takata/minpac">minpac&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>å®ƒä»¬çš„å®‰è£…å’Œä½¿ç”¨æ¡ä»¶éƒ½åœ¨å„è‡ªçš„ä¸»é¡µæœ‰è¯¦ç»†è¯´æ˜Žï¼Œè¿™é‡Œä¸å†æ¬è¿ã€‚é‚£åˆ°åº•ç”¨å“ªä¸ªå‘¢ï¼Ÿ
ä½ åœ¨é˜…è¯»ä»–ä»¬çš„è¯´æ˜Žçš„æ—¶å€™è‡ªå·±å°±ä¼šæœ‰ç­”æ¡ˆï¼Œå¦‚æžœè¿˜æ˜¯éš¾ä»¥å†³å®šï¼Œæˆ‘çš„å»ºè®®å°±æ˜¯åƒå…«æˆ’
æ‰€è¯´çš„é‚£æ ·ï¼šâ€œå¾…ä¿ºè€çŒªä¸€å®¶å®¶åƒå°†åŽ»ï¼â€ã€‚ï¼ˆæˆ‘è‡ªå·±å˜›ï¼Œåƒè¿‡äº†ä¸‰å®¶ï¼‰&lt;/p>
&lt;p>ä»Ž vim8.0 å¼€å§‹ï¼Œvim è‡ªå¸¦æ’ä»¶ç®¡ç†çš„åŠŸèƒ½ç‰¹æ€§ï¼ˆå®žé™…ä¸Šï¼Œå‰è¿°çš„ &lt;code>minpac&lt;/code> å°±åˆ©ç”¨äº†
è¿™äº›ç‰¹æ€§ï¼‰ï¼Œæˆ‘ä»…åœ¨è¿™ä¸ªåŠŸèƒ½å‡ºæ¥çš„æ—¶å€™å°è¿‡æ–°ï¼ˆæ»¡è¶³å¥½å¥‡å¿ƒè€Œå·²ï¼Œæµ…å°è¾„æ­¢ï¼‰ï¼Œ
å› ä¸ºå·²ç»ä¹ æƒ¯äº†æ‰€ç”¨çš„ç®¡ç†æ’ä»¶çš„æ’ä»¶ï¼šï¼‰(æ›´æ–°ï¼šåŽæ¥æˆ‘æ”¹å˜äº†ä¸»æ„)&lt;/p>
&lt;p>å¦‚æžœè¯´ç®¡ç†æ’ä»¶çš„æ’ä»¶å¯ä»¥ç”± vim è‡ªå¸¦çš„åŠŸèƒ½æ‰€æä¾›ï¼Œé‚£ä¹ˆï¼Œæœ‰å¤šå°‘æ’ä»¶æ˜¯å¿…é¡»çš„å‘¢ï¼Ÿ&lt;/p>
&lt;p>æœ‰äººä½œäº†å¾ˆå¥½çš„æ€»ç»“ï¼š
&lt;a href="https://thoughtbot.com/blog/how-to-do-90-of-what-plugins-do-with-just-vim">How to Do 90% of What Plugins Do (With Just Vim)&lt;/a>&lt;/p>
&lt;p>å¦‚æžœä½ å´‡å°šæžç®€ä¸»ä¹‰ï¼Œå€¼å¾—ä¸€è¯•ã€‚ï¼ˆä¸æ˜¯è¯´å°±ä¸€å®šè¦å¦‚æ­¤ï¼Œè€Œæ˜¯çŸ¥é“æœ‰çš„é€‰æ‹©ï¼‰&lt;/p>
&lt;p>è¿™é‡Œæåˆ°çš„åº”è¯¥éƒ½ç†è§£ä¸ºæ˜¯â€œåˆ«äººçš„â€ä¸ªäººé…ç½®ï¼Œè€Œæ‰€è°“â€œä¸ªäººé…ç½®â€ï¼ŒåŸºæœ¬æ„æ€å°±æ˜¯ï¼š
èµ°è¿‡è·¯è¿‡ï¼Œå¯ä»¥éšä¾¿é”™è¿‡ã€‚
çœ‹åˆ°é’Ÿæ„çš„ï¼Œæ‹¿èµ°ä¸è°¢ã€‚çœ‹åˆ°ä¸çˆ½çš„ï¼Œè¯·åŽ»Pull Requestï¼Œåˆ‡å‹¿ä¹±å–·ç²ªã€‚&lt;/p>
&lt;p>å¤§ä½¬&lt;a href="https://en.wikipedia.org/wiki/Linus_Torvalds">æž—çº³æ–¯Â·æ‰˜ç“¦å…¹&lt;/a>æ—©å°±çœ‹é€äº†é‚£äº›åˆ°å¤„ä¹±å–·çš„é”®ç›˜ä¾ ï¼Œä»–çš„è‡³ç†åè¨€è¯´æ˜Žäº†ä¸€åˆ‡ï¼š&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;On the Internet, talk is cheap, show me the code.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;h2 id="vimrc-vimrc" >
&lt;div>
&lt;a href="#vimrc-vimrc">
#
&lt;/a>
vimrc (~/.vimrc)
&lt;/div>
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-vim" data-lang="vim">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34; ================================Part-1: Plugins============&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#begin(&lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle&amp;#39;&lt;/span>) &lt;span style="color:#78787e">&amp;#34; reuse the bundle dir&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== tmux ===-------------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; vim &amp;lt;-&amp;gt; tmux, pane dance&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;edkolev/tmuxline.vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; status line&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Coding enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;airblade/vim-gitgutter&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; shows git diff sign and more&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;tpope/vim-fugitive&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; awsome git wrapper!&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;tpope/vim-repeat&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; enhance . repeat&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;dense-analysis/ale&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; linters config&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;mileszs/ack.vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; cherrypick your strings&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;junegunn/fzf&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;do&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;./install --all&amp;#39;&lt;/span> } | Plug &lt;span style="color:#5af78e">&amp;#39;junegunn/fzf.vim&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Python enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; Plug &amp;#39;Valloric/YouCompleteMe&amp;#39; &amp;#34; for completion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;davidhalter/jedi-vim&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; python auto-completion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Wiki/markdown enhancement ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;vimwiki/vimwiki&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for personal wiki&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;mzlogin/vim-markdown-toc&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for table_of_content&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;plasticboy/vim-markdown&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; couple with vim-instant-markdown&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;instant-markdown/vim-instant-markdown&amp;#39;&lt;/span>, {&lt;span style="color:#5af78e">&amp;#39;for&amp;#39;&lt;/span>: [&lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;markdown.pandoc&amp;#39;&lt;/span>]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Latex support ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;lervag/vimtex&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; for LaTeX files&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;xuhdev/vim-latex-live-preview&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;for&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;tex&amp;#39;&lt;/span> } &lt;span style="color:#78787e">&amp;#34; Lively Previewing&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Julia lang ===-------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;JuliaEditorSupport/julia-vim&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== for GnuPG ===-----------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;jamessan/vim-gnupg&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; transparent editting .gpg files&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== Code/Project navigation ===-------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;easymotion/vim-easymotion&amp;#39;&lt;/span> &lt;span style="color:#78787e">&amp;#34; quickly move&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;preservim/nerdtree&amp;#39;&lt;/span>, { &lt;span style="color:#5af78e">&amp;#39;on&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;NERDTreeToggle&amp;#39;&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34;-------------------=== local plugins (manually install and update) ===-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e">&amp;#34; &amp;#34; ä¸­æ–‡è·³è½¬ (easymotion-like)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle/vim-PinyinSearch&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#end() &lt;span style="color:#78787e">&amp;#34; required&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>å®‰è£…å’Œä½¿ç”¨ vim-plug çš„æ­¥éª¤ï¼š&lt;/p>
&lt;ol>
&lt;li>ä¸‹è½½å¹¶ä¿å­˜åˆ° &lt;code>~/.vim/autoload/&lt;/code> ç›®å½•ä¸‹ ï¼ˆåœ¨ç»ˆç«¯æ“ä½œï¼‰&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>curl -fLo ~/.vim/autoload/plug.vim --create-dirs &lt;span style="color:#5af78e">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">&lt;/span> https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>é…ç½® &lt;code>~/.vimrc&lt;/code> æ–‡ä»¶çš„ç›¸åº”å†…å®¹ï¼ˆå¿…é¡»éµå¾ªçš„æ ¼å¼ï¼‰ï¼š&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>call plug#begin&lt;span style="color:#ff6ac1">(&lt;/span>&lt;span style="color:#5af78e">&amp;#39;~/.vim/bundle&amp;#39;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span> &lt;span style="color:#5af78e">&amp;#34; reuse the bundle dir
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#5af78e">Plug &amp;#39;tpope/vim-fugitive&amp;#39; &amp;#34;&lt;/span> awsome git wrapper!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>call plug#end&lt;span style="color:#ff6ac1">()&lt;/span> &lt;span style="color:#5af78e">&amp;#34; required
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>åœ¨ &lt;code>plug#begin()&lt;/code> å’Œ &lt;code>plug#end()&lt;/code> ä¹‹é—´å°±æ˜¯ä½ æƒ³è¦å®‰è£…å’Œç®¡ç†çš„æ’ä»¶çš„åˆ—è¡¨ã€‚
&lt;code>Plug 'tpope/vim-fugitive'&lt;/code> å°±æ˜¯æŒ‡æ˜Žæƒ³è¦å®‰è£…å¹¶ç®¡ç† vim-fugitive è¿™ä¸ªæ’ä»¶ã€‚&lt;/p>
&lt;p>å¦‚æžœ &lt;code>plug#begin()&lt;/code> æ‹¬å¼§é‡Œä¸ºç©ºï¼Œåˆ™é»˜è®¤æ˜¯å¥–æ‰€æœ‰çš„æ’ä»¶å®‰è£…åˆ° &lt;code>.vim/plugged/&lt;/code>ï¼Œ
ä¸Šæ–‡çš„æˆ‘çš„é…ç½®ä¸­ï¼Œæˆ‘æ˜¯æŒ‡å®šäº† &lt;code>.vim/bundle&lt;/code> ä½œä¸ºæ’ä»¶çš„ç›®å½•
ï¼ˆå› ä¸ºæˆ‘åœ¨ä½¿ç”¨ vim-plug ä¹‹å‰å°±å·²ç»åœ¨è¿™ä¸ªç›®å½•ä¸‹å­˜æ”¾äº†è®¸å¤šæ’ä»¶ï¼Œç®€å•å¤ç”¨è€Œå·²ï¼‰ã€‚&lt;/p>
&lt;p>vim-plug ä¼šè‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹ä¸¤ä¸ªå‘½ä»¤ï¼ˆè®¸å¤šæ’ä»¶éœ€è¦è¿™ä¸¤ä¸ªè®¾å®šï¼‰ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>filetype indent on&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>syntax enable&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>å¦‚æžœä½ å‘çŽ°è‡ªå·±â€œå®Œå…¨â€æŒ‰ç…§å®˜æ–¹çš„æŒ‡å—è¿›è¡Œå®‰è£…ï¼Œå¯æ˜¯â€œæ’ä»¶æ­»æ´»ä¸èµ·ä½œç”¨â€ï¼Œæžæœ‰å¯èƒ½æ˜¯
å› ä¸ºä½ åœ¨æŸä¸ªåœ°æ–¹è®¾ç½®äº†å®Œå…¨ç›¸åçš„è®¾å®šï¼ˆå› ä¸ºä»Žåˆ«çš„åœ°æ–¹ copy ä¸€å¤§æ®µè‡ªå·±æ²¡æœ‰è®¤çœŸçœ‹
è¿‡çš„å†…å®¹åˆ°è‡ªå·±çš„ .vimrc æ–‡ä»¶é‡Œï¼‰ã€‚&lt;/p>
&lt;hr>
&lt;p>æ›´è¯¦ç»†çš„ä½¿ç”¨æ–¹æ³•åº”è¯¥ç›´æŽ¥åŽ» &lt;a href="https://github.com/junegunn/vim-plug">vim-plug&lt;/a>
çš„ä¸»é¡µäº†è§£ï¼Œé‚£é‡Œä¸ä»…å‘Šè¯‰å¤§å®¶åº”è¯¥æ€Žä¹ˆå®‰è£…æ­¤æ’ä»¶ï¼Œè¿˜ç»™å‡ºå„ç§æƒ…å†µçš„è¯´æ˜Žå’Œä¾‹å­ã€‚&lt;/p>
&lt;hr></description></item><item><title>åˆ©å™¨1: Oh, My Tmux -- é”®ç›˜é©±åŠ¨(ä¸ºä¸»)çš„å·¥ä½œæµ</title><link>/posts/thecli/2022-08-07-tmux/</link><pubDate>Sun, 07 Aug 2022 21:17:59 +0800</pubDate><guid>/posts/thecli/2022-08-07-tmux/</guid><description>&lt;h2 id="tmuxhttpsgithubcomtmuxtmux" >
&lt;div>
&lt;a href="#tmuxhttpsgithubcomtmuxtmux">
#
&lt;/a>
&lt;a href="https://github.com/tmux/tmux">Tmux&lt;/a>
&lt;/div>
&lt;/h2>
&lt;p>&lt;a href="https://github.com/tmux/tmux">Tmux&lt;/a> æ˜¯ä¸€ä¸ªç»ˆç«¯å¤šè·¯å¤ç”¨å™¨&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>ï¼š&amp;hellip;&lt;/p>
&lt;blockquote>
&lt;p>Tmux is a terminal multiplexer: it enables a number of terminals to be
created, accessed, and controlled from a single screen. tmux may be detached
from a screen and continue running in the background, then later reattached.&lt;/p>
&lt;/blockquote>
&lt;p>é‚£åˆ°åº•ä»€ä¹ˆæ˜¯&lt;ruby>â€œç»ˆç«¯â€&lt;rt>terminal&lt;/rt>&lt;/ruby>ï¼Ÿ&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> æˆ‘å›žç­”ä¸äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†æˆ‘å‘çŽ°äº†ä¸€ä¸ªå¾ˆå¥½çš„è§£é‡Š &amp;ndash;
&lt;a href="https://unix.stackexchange.com/questions/4126/what-is-the-exact-difference-between-a-terminal-a-shell-a-tty-and-a-con">terminal&lt;/a>&lt;/p>
&lt;p>çªç„¶æƒ³èµ·ã€Šç»Ÿè®¡å­¦ã€‹å…³äºŽâ€œå¡æ–¹æ£€éªŒâ€é‚£ä¸€ç« çš„ç»å…¸å¯¼è¯­ï¼š&lt;/p>
&lt;blockquote>
&lt;p>ä¸è¦é—®å®ƒçš„å«ä¹‰ï¼Œå®å¯é—®å¦‚ä½•ä½¿ç”¨å®ƒã€‚&amp;ndash; ç»´ç‰¹æ ¹æ–¯å¦&lt;/p>
&lt;/blockquote>
&lt;p>æœ¬ç€â€œæ‹¿æ¥ä¸»ä¹‰â€ç²¾ç¥žï¼Œé‚£å°±æ¥ç²—ç•¥çœ‹çœ‹ Tmux èƒ½å¦‚ä½•ä½¿ç”¨å§ï¼š&lt;/p>
&lt;p>&lt;img alt="IMG_MyTmux &amp;ndash; (gif å¤±æ•ˆ)" src="https://fgg100y.github.io/img/posts/tmux/ssr_myTmuxDemo.gif">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>å¯åŠ¨ Termainal (CTRL+Enter, è¿™æ˜¯ä¸ªäººå®šä¹‰çš„å¿«æ·é”®)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å¯åŠ¨ Tmux è¿›å…¥æˆ‘çš„å·¥ä½œçŽ¯å¢ƒ (é¡»çŸ¥ï¼šæˆ‘è¿™é‡Œå±•ç¤ºçš„ä¸æ˜¯åˆå§‹åŒ–çš„ Tmux å¯åŠ¨)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ£€æŸ¥ä¸€ä¸‹æ˜¨å¤©å†™çš„åšå®¢å†™åˆ°å“ªé‡Œäº† (è²Œä¼¼å·®ä¸å¤šäº†ï¼Œå†™å…¶ä»–å†…å®¹)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åˆ‡æ¢åˆ°ä¸‹ä¸€åœºâ€œä¼šè¯â€(session) &amp;ndash; vimwiki (è¿™é‡Œæ˜¯æˆ‘ä¸ªäººé¡¹ç›®æ ¹ç›®å½•)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æŒ‰è¿›åº¦ä»Šå¤©åº”è¯¥ä»‹ç»ä¸€ä¸‹ Tmux è¿™ä¸ªå·¥å…· (ç„¶åŽï¼Œå°±åˆ°è¿™é‡Œäº†)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>çŽ°åœ¨é˜³å…‰æ˜Žåªšï¼ˆè¿‡ä¸€ä¼šå„¿å°±å˜æ¯’è¾£ï¼‰ï¼Œç™½å¤©è¿˜æ˜¯æ¢åˆ°ç™½è‰²èƒŒæ™¯åˆé€‚äº›&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;del>æ¬ç –æ»´å¹²æ´»å’¯å„ä½&lt;/del>&lt;br>
&lt;del>å—¯ï¼Œå¹²æ´»ï¼Ÿæ›´åƒæ˜¯åˆ›ä½œ&lt;/del>&lt;br>
&lt;del>åˆ›ä½œï¼Ÿè¿™ç®—å“ªé—¨å­çš„&amp;hellip;&lt;/del>&lt;br>
æ‰€ä»¥è¿˜æ˜¯ï¼Œå¹²ç‚¹æ´»å§ ï¼šï¼‰&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="tmux-çš„ä¸€äº›å…³é”®é…ç½®" >
&lt;div>
&lt;a href="#tmux-%e7%9a%84%e4%b8%80%e4%ba%9b%e5%85%b3%e9%94%ae%e9%85%8d%e7%bd%ae">
#
&lt;/a>
Tmux çš„ä¸€äº›å…³é”®é…ç½®
&lt;/div>
&lt;/h2>
&lt;p>å¦‚æžœè¿˜æ²¡æœ‰å®‰è£… Tmuxï¼Œå‘½ä»¤è¡Œä¸­å®‰è£…èµ·æ¥ä¹Ÿä¸è´¹äº‹ (Debianç³»åˆ—)ï¼š&lt;code>apt install tmux&lt;/code>&lt;/p>
&lt;p>ä»¥ä¸‹çš„å†…å®¹å‡å›´ç»• &lt;code>~/.tmux.conf&lt;/code> æ–‡ä»¶å±•å¼€ã€‚å¦‚æžœæ²¡æœ‰å°±è‡ªå·±åˆ›å»ºä¸€ä¸ªè¿™æ–‡ä»¶ã€‚
é»˜è®¤è¿™äº›ä»£ç æ¡†é‡Œçš„å†…å®¹ï¼ˆç­‰å®½å­—ä½“é‚£äº›ï¼‰æ˜¯è¦æ·»åŠ åˆ° &lt;code>~/.tmux.conf&lt;/code> æ–‡ä»¶é‡Œã€‚&lt;/p>
&lt;p>&amp;ldquo;#&amp;rdquo; å¼€å¤´çš„è¯­å¥æ˜¯æ³¨é‡Šè¯´æ˜Ž&lt;br>
&amp;ldquo;$&amp;rdquo; å¼€å¤´çš„è¯­å¥æ˜¯è¦åœ¨å‘½ä»¤è¡Œè¿è¡Œçš„å‘½ä»¤&lt;br>
&amp;ldquo;:command&amp;rdquo; ä»¥â€œå†’å·æŽ¥å‘½ä»¤åâ€å¼€å¤´çš„è¯­å¥æ˜¯è¦åœ¨vimå‘½ä»¤æ¨¡å¼è¿è¡Œçš„å‘½ä»¤&lt;/p>
&lt;h2 id="é»˜è®¤å¿«æ·é”®ä¿®æ”¹-ctrl-b---ctrl-a" >
&lt;div>
&lt;a href="#%e9%bb%98%e8%ae%a4%e5%bf%ab%e6%8d%b7%e9%94%ae%e4%bf%ae%e6%94%b9-ctrl-b---ctrl-a">
#
&lt;/a>
é»˜è®¤å¿«æ·é”®ä¿®æ”¹: Ctrl-b -&amp;gt; Ctrl-a
&lt;/div>
&lt;/h2>
&lt;p>Tmux çš„é»˜è®¤å¿«æ·é”®æ˜¯ &lt;code>ctrl-b&lt;/code>ï¼Œè¿™åœ¨é”®ç›˜ä¸Šè·ç¦»ç•¥å¾®æœ‰ç‚¹è¿œ (å¯¹äºŽæˆ‘çš„çŸ­æ‰‹æŒ‡æ¥è¯´)ï¼Œ
å› æ­¤æˆ‘æŠŠå®ƒä¿®æ”¹ä¸º &lt;code>ctrl-a&lt;/code> ç»„åˆé”®ã€‚&lt;/p>
&lt;p>å°†ä¸‹é¢è¯­å¥åŠ å…¥ .tmux.conf æ–‡ä»¶ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g prefix C-a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # unbind the C-b&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>unbind C-b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # ensure that we can send Ctrl-a to other apps (e.g., vim)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> C-a send-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># # setting the delay between prefix and command&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -s escape-time &lt;span style="color:#ff9f43">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>æ—¢ç„¶æ˜¯è¦ç”¨å¿«æ·é”®ï¼Œé‚£ä¹ˆé‡æ–°åŠ è½½ .tmux.conf æ–‡ä»¶çš„è‡ªç„¶ä¹Ÿå®‰æŽ’ä¸Šï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># reload the .tmux.conf file with Prefix-r&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> r source-file ~/.tmux.conf &lt;span style="color:#5af78e">\;&lt;/span> display &lt;span style="color:#5af78e">&amp;#34;tmuxconf reloaded!&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ç„¶åŽï¼Œåœ¨å‘½ä»¤è¡Œç›´æŽ¥è¾“å…¥å¿«æ·ç»„åˆé”®ï¼š&lt;code>Ctrl+a+r&lt;/code> å³å¯ä½¿é…ç½®ç”Ÿæ•ˆ (&lt;code>+&lt;/code>å·çš„æ„æ€æ˜¯åŒæ—¶æ‘ä¸‹)ã€‚&lt;/p>
&lt;h2 id="vimtmux-å¼ºå¼ºç»„åˆ" >
&lt;div>
&lt;a href="#vimtmux-%e5%bc%ba%e5%bc%ba%e7%bb%84%e5%90%88">
#
&lt;/a>
vim/tmux å¼ºå¼ºç»„åˆ
&lt;/div>
&lt;/h2>
&lt;p>Tmux æ”¯æŒå¤šä¸ªç»ˆç«¯çª—æ ¼ (terminal panes)ï¼Œæ¯ä¸ªçª—æ ¼éƒ½æ˜¯ç‹¬ç«‹çš„ç»ˆç«¯ï¼Œ
å› æ­¤ï¼Œæœ€æ˜Žæ˜¾çš„äº‹æƒ…å°±æ˜¯ï¼Œä½ å¯ä»¥åŒæ—¶åœ¨ä¸€å—å±å¹•ä¸Šâ€œç›‘æŽ§â€å¤šä¸ªä¸åŒä»»åŠ¡ï¼Œæ„æ€æ˜¯ï¼Œ
è¿™äº›ä¸åŒçš„ä»»åŠ¡åœ¨è§†è§‰ä¸Šæ˜¯â€œåŒæ—¶â€å‘ˆçŽ°çš„ï¼ˆç›¸å¯¹äºŽé‡å çš„ æ ‡ç­¾é¡µ(tab) è€Œè¨€ï¼‰ã€‚&lt;/p>
&lt;p>å¦‚æžœä½ æ°å¥½ä¹Ÿå–œæ¬¢ç”¨ vim è¿›è¡Œç å­—ï¼Œä¹Ÿæ‰“å¼€äº† vim ç¼–è¾‘å™¨ (vim æœ¬èº«ä¹Ÿæœ‰çª—æ ¼åŠŸèƒ½)ï¼Œ
è‡ªç„¶è€Œç„¶çš„ï¼Œä½ è‚¯å®šæƒ³ä»¥ç›¸åŒçš„å¿«æ·é”®æ¥å®žçŽ°ä¸åŒçš„ vim/tmux çª—æ ¼ä¹‹é—´çš„åˆ‡æ¢ã€‚&lt;/p>
&lt;p>å¹¸è¿çš„æ˜¯ï¼Œå·²ç»æœ‰äººå®žçŽ°äº†è¿™æ ·çš„åŠŸèƒ½ï¼Œå¹¶å…±äº«äº†ï¼Œè¿™å°±æ˜¯
&lt;a href="https://github.com/christoomey/vim-tmux-navigator">vim-tmux-navigator&lt;/a>.&lt;/p>
&lt;p>vim æœ‰ä¸°å¯Œçš„æ’ä»¶ç”Ÿæ€ï¼Œè¿™æ˜¯ vim å¼ºå¤§ç”Ÿå‘½åŠ›çš„ä¸€ä¸ªä½“çŽ°ã€‚
è¿™ä¹ˆå¥½çš„æ¨¡å¼ï¼Œè‡ªç„¶å€¼å¾—å€Ÿé‰´ã€‚å› æ­¤ï¼Œtmux ä¹Ÿæœ‰ä¸€ä¸ªç”¨æ¥ç®¡ç†æ’ä»¶çš„
&lt;a href="https://github.com/tmux-plugins/tpm">tpm&lt;/a>.&lt;/p>
&lt;p>éœ€è¦åœ¨ vim å’Œ tmux ä¸¤ä¸ªæ–¹é¢è¿›è¡Œé…ç½®ï¼š&lt;/p>
&lt;ol>
&lt;li>åœ¨ vim æ–¹é¢, å°†ä»¥ä¸‹è¯­å¥åŠ å…¥ .vimrc æ–‡ä»¶ï¼š&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># å¦‚æžœä½ ç”¨çš„æ˜¯ Vim-Plug æ’ä»¶ç®¡ç†å™¨çš„è¯&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Plug &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ç„¶åŽè¿è¡Œ &lt;code>:PlugInstall&lt;/code> æ¥å®‰è£…è¿™ä¸ªæ’ä»¶ã€‚&lt;/p>
&lt;p>å¦‚æžœä½ æ˜¯ç¬¬ä¸€æ¬¡é…ç½® vim æ’ä»¶ï¼Œå…ˆåŽ»äº†è§£
&lt;a href="https://github.com/junegunn/vim-plug">Vim-Plug&lt;/a>.&lt;/p>
&lt;ol start="2">
&lt;li>åœ¨ tmux æ–¹é¢ï¼Œå°†ä¸‹é¢è¯­å¥åŠ å…¥ .tmux.conf æ–‡ä»¶ï¼š&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># tpm is a tmux-plugin manager. [details in its github repos]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g @plugin &lt;span style="color:#5af78e">&amp;#39;tmux-plugins/tpm&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># vim-tmux-navigator: Seamless navigation between tmux panes and vim splits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">set&lt;/span> -g @plugin &lt;span style="color:#5af78e">&amp;#39;christoomey/vim-tmux-navigator&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># put this &amp;#39;run&amp;#39; command to the very end of .tmux.conf, says the tpm&amp;#39;s readme&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>run &lt;span style="color:#5af78e">&amp;#39;~/.tmux/plugins/tpm/tpm&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>åŠ å…¥è¿™äº›å†…å®¹ä¹‹åŽï¼Œé‡æ–°åŠ è½½ tmux çŽ¯å¢ƒ, (åœ¨å·²ç»å¯åŠ¨tmuxçš„å‘½ä»¤è¡Œè¿è¡Œä»¥ä¸‹å‘½ä»¤)ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ tmux &lt;span style="color:#ff5c57">source&lt;/span> ~/.tmux.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="ä¸»é¢˜èƒŒæ™¯é¢œè‰²åˆ‡æ¢" >
&lt;div>
&lt;a href="#%e4%b8%bb%e9%a2%98%e8%83%8c%e6%99%af%e9%a2%9c%e8%89%b2%e5%88%87%e6%8d%a2">
#
&lt;/a>
ä¸»é¢˜èƒŒæ™¯é¢œè‰²åˆ‡æ¢
&lt;/div>
&lt;/h2>
&lt;p>åœ¨æˆ‘çš„å°è±¡é‡Œï¼Œæ„Ÿè§‰é»‘è‰²/æš—è‰²ç³»ä¸€ä¸‹å­æˆäº†å¾ˆå¤šåº”ç”¨è½¯ä»¶çš„é»˜è®¤èƒŒæ™¯è‰²ï¼Œ
æˆ‘åœ¨å‘½ä»¤è¡Œè¿‡æ´»å¾ˆå¤šå¹´äº†ï¼Œå¯¹æ­¤å¾ˆæ˜¯ä¹ æƒ¯ï¼Œä¸ªäººå¾ˆå°‘çœ‹åˆ°è°çš„Terminalæ˜¯ç™½è‰²èƒŒæ™¯çš„ã€‚
ä¸»è¦åº”è¯¥æ˜¯ï¼š1ï¼‰é…ç½®äº†ç™½å¤©é»‘å¤œéƒ½å‹å¥½çš„ä¸»é¢˜ï¼›2ï¼‰ä¸å½“å›žäº‹å„¿ã€‚
ä½†æœ‰æ—¶å€™ï¼Œç‰¹åˆ«æ˜¯å…‰çº¿å……è¶³çš„æ—¶å€™ï¼Œæˆ‘æ¯”è¾ƒå–œæ¬¢ç™½è‰²çš„èƒŒæ™¯ã€‚
æ‰€ä»¥ï¼Œæˆ‘éœ€è¦èƒ½éšå¿ƒæ‰€æ¬²åˆ‡æ¢èƒŒæ™¯è‰²è°ƒã€‚&lt;/p>
&lt;p>è¿™ä¸ªè²Œä¼¼æ²¡æ³•ä¸ä¾èµ–äºŽå¤–éƒ¨è„šæœ¬æ¥å®žçŽ°ï¼Œè‡³å°‘æˆ‘æ²¡æœ‰å‘çŽ°å¯è¡Œçš„åŠžæ³•ã€‚å› æ­¤ï¼Œä½ ä¹Ÿéœ€è¦
&lt;a href="https://www.seanh.cc/2021/01/02/change-between-light-and-dark-themes-in-tmux/">åŽ»ä¸‹è½½è¿™ä¸ªè„šæœ¬&lt;/a>&lt;/p>
&lt;p>Linux çŽ¯å¢ƒæ°›å›´æ˜¯è¿™æ ·çš„ï¼Œä¸è¦ç›²ç›®ç›¸ä¿¡åˆ«äººå†™çš„è„šæœ¬ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä½ ä¸è®¤è¯†çš„äººã€‚
ç›¸ä¿¡å¼€æºå’Œç›¸ä¿¡é™Œç”Ÿäººï¼Œæ¯•ç«Ÿä¸æ˜¯åŒä¸€ä»¶äº‹ã€‚
æ„æ€å°±æ˜¯ï¼Œä½ åœ¨ä½¿ç”¨è¿™äº›è„šæœ¬ä¹‹å‰å¥½æ­¹ä¹Ÿè¦çœ‹çœ‹äººå®¶åˆ°åº•å†™äº†ä»€ä¹ˆå§ã€‚
ç‰¹åˆ«æ˜¯è¿™ç§éƒ½ä¸è¶…è¿‡ä¸€é¡µå†…å®¹çš„è„šæœ¬ã€‚&lt;/p>
&lt;p>ä»€ä¹ˆï¼Ÿä½ è¯´ä½ çœ‹ä¸æ‡‚ã€‚äº‹å®žæ˜¯ï¼Œæ²¡æœ‰äººåœ¨ä¸€å¼€å§‹å°±ä»€ä¹ˆéƒ½çœ‹å¾—æ‡‚ã€‚æ¯”å¦‚è¯´ï¼Œ
ä½ çœ‹åˆ°è„šæœ¬é‡Œæœ‰è¿™ä¹ˆä¸€å¥ç±»ä¼¼äºŽwindowsä¸Šâ€œæ¸…ç†åžƒåœ¾â€çš„å‘½ä»¤ï¼š &lt;code>sudo rm -rf /*&lt;/code>ï¼Œ
é‚£ä½ å°±è¦ä¿æŒè­¦æƒ•äº†ã€‚
æ®è¯´ï¼Œæœ‰äº›ç³»ç»Ÿä¼šé˜»æ­¢ä½ è¿™ä¹ˆå¹²ï¼Œä½†æœ‰äº›ç³»ç»Ÿä¼šâ€œå¼€å¿ƒåœ°â€æ‰§è¡Œä½ çš„æ—¨æ„ï¼›
åæ­£æˆ‘æ²¡è¯•è¿‡ï¼Œæˆ‘ä¹Ÿæ²¡æœ‰ä»¥ä»»ä½•æ–¹å¼æš—ç¤ºæˆ–é¼“åŠ±ä½ è¦è¿™ä¹ˆæ¸…ç†ç³»ç»Ÿåžƒåœ¾ã€‚&lt;/p>
&lt;p>å¦‚æžœè¯´ä½ ä¸‹è½½äº†é‚£ä¸ªè„šæœ¬ï¼Œæ”¾åˆ°ä½ å–œæ¬¢å®ƒå¾…çš„ä½ç½®ï¼Œæ¯”å¦‚è¯´ï¼Œ &lt;code>~/.tmux/bin/&lt;/code>ï¼Œ
ç„¶åŽå°†ä¸‹é¢è¿™æ¡è¯­å¥åŠ å…¥ &lt;code>~/.tmux.conf&lt;/code> æ–‡ä»¶ä¸­ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">bind&lt;/span> T run-shell ~/.tmux/bin/toggle-theme
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="å…³äºŽé…ç½®å’Œæ’ä»¶ä»¥åŠç®¡ç†æ’ä»¶çš„æ’ä»¶" >
&lt;div>
&lt;a href="#%e5%85%b3%e4%ba%8e%e9%85%8d%e7%bd%ae%e5%92%8c%e6%8f%92%e4%bb%b6%e4%bb%a5%e5%8f%8a%e7%ae%a1%e7%90%86%e6%8f%92%e4%bb%b6%e7%9a%84%e6%8f%92%e4%bb%b6">
#
&lt;/a>
å…³äºŽé…ç½®å’Œæ’ä»¶ä»¥åŠç®¡ç†æ’ä»¶çš„æ’ä»¶
&lt;/div>
&lt;/h2>
&lt;p>ä»¥ä¸Šå†…å®¹åˆ†åˆ«å¯¹åº”ç€ï¼š&lt;/p>
&lt;p>1ï¼‰ç›´æŽ¥å¯¹ tmux è¿›è¡ŒåŸºæœ¬é…ç½®ï¼›&lt;/p>
&lt;p>2ï¼‰ä½¿ç”¨ tmux æ’ä»¶ç®¡ç†å™¨å¯¹ tmux è¿›è¡Œé…ç½®ï¼›&lt;/p>
&lt;p>3ï¼‰å€Ÿç”¨ è„šæœ¬ç¨‹åº å¯¹ tmux è¿›è¡Œé…ç½®ã€‚&lt;/p>
&lt;p>æ¯•ç«Ÿï¼Œä¸æ˜¯æ¯ä¸ªäººéƒ½çƒ­è¡·äºŽä½¿ç”¨æ’ä»¶ç®¡ç†å™¨ï¼Œæ’ä»¶å’Œä¸»ä»¶ä¹‹é—´çš„äº¤äº’æ–¹å¼å¹¶ä¸è®¾é™ã€‚ä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨ä½ è§‰å¾—åˆé€‚çš„æ–¹æ³•æ¥å®Œæˆä»»åŠ¡ã€‚
è¦æ˜¯è§‰å¾—çŽ°æœ‰çš„éƒ½ä¸æ»¡æ„ï¼Œæ— æ³•æ»¡è¶³ä½ çš„ç‰¹æ®Šéœ€æ±‚ï¼Œæ‰‹ç—’ç—’çš„ï¼Œé‚£å°±è‡ªå·±æžä¸€ä¸ªã€‚
å¾ˆå¤šä¼˜ç§€çš„è½¯ä»¶éƒ½æ˜¯å¦‚æ­¤ï¼Œå®ƒæä¾›è¶³å¤Ÿå¥½çš„é»˜è®¤å€¼ï¼Œä½†ä¹Ÿæä¾›è¶³å¤Ÿå¤šçš„çµæ´»æ€§ï¼Œè®©ç”¨æˆ·æœ‰çš„é€‰ã€‚&lt;/p>
&lt;p>åˆ˜å»ºæ˜Žè¯´â€œä»¥å‰æ²¡çš„é€‰ï¼ŒçŽ°åœ¨æƒ³é€‰å›žåšå¥½äººï¼Œç»™ä¸ªæœºä¼šâ€ï¼Œé™ˆæ°¸ä»ç­”â€œåŽ»åŒæ³•å®˜è®²â€ã€‚â€œæœ‰çš„é€‰â€è¿™ä¸€ç‚¹ååˆ†å¯è´µã€‚&lt;/p>
&lt;hr>
&lt;p>Last update: 2024-07-04&lt;/p>
&lt;blockquote>
&lt;p>USER: Tmux æ˜¯ä»€ä¹ˆï¼Ÿ
GPT4o: Tmux æ˜¯ä¸€ä¸ªç»ˆç«¯å¤šè·¯å¤ç”¨å™¨ (terminal multiplexer)ï¼Œå…è®¸ç”¨æˆ·åœ¨ä¸€ä¸ªç»ˆç«¯çª—å£å†…è¿è¡Œå’Œç®¡ç†å¤š
ä¸ªç»ˆç«¯ä¼šè¯ã€‚å®ƒæä¾›äº†ä¸€ç§æ–¹ä¾¿çš„æ–¹æ³•æ¥åˆ†å‰²ä¸€ä¸ªç‰©ç†ç»ˆç«¯çª—å£æˆå¤šä¸ªè™šæ‹Ÿç»ˆç«¯ï¼Œå¹¶åœ¨è¿™äº›è™šæ‹Ÿç»ˆç«¯
ä¹‹é—´è‡ªç”±åˆ‡æ¢ã€‚Tmux çš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š&lt;/p>
&lt;ul>
&lt;li>ä¼šè¯ç®¡ç†ï¼šTmux å¯ä»¥åˆ›å»ºã€åˆ†ç¦»å’Œé‡æ–°é™„åŠ ä¼šè¯ï¼Œä½¿ç”¨æˆ·å¯ä»¥åœ¨ä¸åŒçš„ç»ˆç«¯ä¼šè¯ä¹‹é—´å¿«é€Ÿåˆ‡æ¢ã€‚&lt;/li>
&lt;li>çª—å£åˆ†å‰²ï¼šç”¨æˆ·å¯ä»¥åœ¨ä¸€ä¸ªTmuxä¼šè¯ä¸­å°†çª—å£åˆ†å‰²æˆå¤šä¸ªçª—æ ¼ï¼Œæ¯ä¸ªçª—æ ¼å¯ä»¥ç‹¬ç«‹è¿è¡Œä¸€ä¸ªå‘½ä»¤è¡Œç¨‹åºã€‚&lt;/li>
&lt;li>æŒä¹…æ€§ï¼šå³ä½¿ç»ˆç«¯çª—å£å…³é—­ï¼ŒTmux ä¼šè¯ä»ç„¶ä¼šä¿æŒè¿è¡Œï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥ç¨åŽé‡æ–°é™„åŠ ä¼šè¯ï¼Œç»§ç»­å·¥ä½œã€‚&lt;/li>
&lt;li>å¯å®šåˆ¶æ€§ï¼šTmux æä¾›äº†ä¸°å¯Œçš„é…ç½®é€‰é¡¹å’Œå¿«æ·é”®ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚è¿›è¡Œå®šåˆ¶ã€‚&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>è€Œå¦‚ä»Šï¼Œæˆ–è®¸åº”è¯¥é—®é—®GPT4o&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>æƒ³çŸ¥é“æ€Žä¹ˆåƒè¿™æ ·æ ‡æ³¨åè¯å—ï¼ŸåŽ»çœ‹çœ‹&lt;a href="https://www.junyi.dev/posts/a-writing-trick/">è¿™ç¯‡åšæ–‡&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>åˆ©å™¨2: Vimwiki ä¹‹ å®‰è£…å’Œé€‚é…æ•°æ®ç§‘å­¦çš„ä¸€äº›é…ç½®</title><link>/posts/thecli/thevim/2022-07-15-vimwiki_part2/</link><pubDate>Sat, 06 Aug 2022 23:33:32 +0800</pubDate><guid>/posts/thecli/thevim/2022-07-15-vimwiki_part2/</guid><description>&lt;p>Last update: 2022-08-06 23:33:32&lt;/p>
&lt;h2 id="vimwiki-æ’ä»¶ç®€ä»‹å®‰è£…å’Œä¸€äº›è®¾ç½®" >
&lt;div>
&lt;a href="#vimwiki-%e6%8f%92%e4%bb%b6%e7%ae%80%e4%bb%8b%e5%ae%89%e8%a3%85%e5%92%8c%e4%b8%80%e4%ba%9b%e8%ae%be%e7%bd%ae">
#
&lt;/a>
vimwiki æ’ä»¶ç®€ä»‹ã€å®‰è£…å’Œä¸€äº›è®¾ç½®
&lt;/div>
&lt;/h2>
&lt;p>åœ¨ vimwiki é‡Œï¼Œæ‰€æœ‰çš„å…¥å£é›†ä¸­åœ¨æŒ‡å®šçš„é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ index.md æ–‡æ¡£é‡Œã€‚ä¹Ÿå°±æ˜¯è¦æž„
å»ºçš„æ•´ä¸ªç¬”è®°ç³»ç»Ÿï¼ˆéšä¾¿ç®¡å®ƒå«ä»€ä¹ˆï¼‰ï¼Œindex.md å¯ä»¥æˆä¸ºå®ƒçš„æ•´ä½“æ¡†æž¶ä»¥åŠå„ä¸ªä¸»é¢˜çš„
ç´¢å¼•ã€‚ä»Žè¿™é‡Œå¯ä»¥åŽ»åˆ°å„ä¸ªä¸»é¢˜ï¼Œä»Žå„ä¸ªä¸»é¢˜çš„ç´¢å¼•åŽ»åˆ°å¯¹åº”çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥ä»Žä¸€ä¸ªå†…å®¹
åŽ»åˆ°å¦ä¸€ä¸ªå†…å®¹ï¼Œå†…å®¹å¯ä»¥æ˜¯æœ¬åœ°æ–‡æ¡£å†…/æ–‡æ¡£é—´ï¼Œè¿™å®Œå…¨å–å†³äºŽå¦‚ä½•æž„å»ºå†…å®¹ä¹‹é—´çš„è¿žæŽ¥ã€‚&lt;/p>
&lt;p>vimwiki æä¾›äº†å¿…è¦çš„åŸºç¡€ã€‚ä½†é€šå¸¸æˆ‘ä»¬è¦å†™çš„æ–‡æ¡£å¹¶ä¸æ˜¯çº¯æ–‡æœ¬ (plain text) è€Œæ˜¯å¸¦æœ‰
ä¿®é¥°è¯­æ³•çš„æ–‡æœ¬ï¼Œæ¯”å¦‚ markdown, Tex/Latex æ ¼å¼çš„æ–‡æ¡£ã€‚è¿™ä¸¤è€…çš„åŒºåˆ«åœ¨äºŽï¼Œå‰è€…æ˜¯â€œ
æ‰€è§å³æ‰€å¾—â€ï¼Œæ„æ€å°±æ˜¯ä½ å†™å‡ºæ¥çš„å†…å®¹å’Œæœ€åŽå‘ˆçŽ°çš„å†…å®¹ä¸€æ¨¡ä¸€æ ·ï¼›åŽè€…åˆ™æ›´åƒæ˜¯å†™ä»£ç ï¼Œ
æœ€åŽå‘ˆçŽ°å‡ºæ¥çš„å’Œæ‰€å†™çš„å†…å®¹ä¸€è‡´ï¼ˆä½†å¤§éƒ¨åˆ†æ—¶å€™ä¸ä¼šä¸€æ¨¡ä¸€æ ·ï¼‰ï¼Œæœ‰äººç§°ä¹‹ä¸º
â€œæ‰€æƒ³å³æ‰€å¾—â€ï¼Œå¬èµ·æ¥æœ‰ç‚¹çŽ„ä¹Žã€‚ç›´æŽ¥ç”¨ plain text æ ¼å¼ä¸å°±å®Œäº†ï¼ŒæŠ˜è…¾è¿™äº›å¹²å˜›ï¼Ÿé—æ†¾
çš„æ˜¯ï¼Œåªè¦ä½ æƒ³è¦å†™ä¸€äº›å¤æ‚ä¸€ç‚¹çš„æ•°å­¦å…¬å¼ï¼ˆæ±‚å¯¼çš„é“¾å¼æ³•åˆ™ï¼‰ï¼Œé‚£ä¹ˆï¼Œæ²¡æœ‰ä¸“é—¨çš„ç¼–è¾‘
å™¨çš„è¯ï¼Œæœ€å¿«èƒ½å¤Ÿå¾—åˆ°è§„èŒƒçš„æ•°å­¦å…¬å¼çš„æ–¹æ³•å°±æ˜¯ &amp;ndash; æ‰‹å†™ã€‚æ•™ç§‘ä¹¦å†…é‚£äº›æ•°å­¦å…¬å¼çš„å‘ˆçŽ°
æ•ˆæžœåŸºæœ¬éƒ½æ¥è‡ªæŽ’ç‰ˆç³»ç»Ÿå·¥å…·ï¼ˆTex/Latexç­‰ï¼‰çš„æ”¯æŒã€‚&lt;/p>
&lt;p>å¦‚æžœå·¥ä½œçš„å†…å®¹æ¶‰åŠåˆ°è¾ƒå¤šçš„é«˜ç­‰æ•°å­¦ï¼Œå¹¶æƒ³è‡ªå·±ç¼–å†™çš„æ—¥å¿—åšå®¢å¾—åˆ°ç±»ä¼¼æ•™ç§‘ä¹¦å¼çš„æ•°å­¦
å…¬å¼å‘ˆçŽ°ï¼Œmarkdown æ–‡æ¡£æ˜¯ç›¸å½“å‹å¥½çš„æ–‡æœ¬æ ¼å¼ï¼Œè€Œä»Ž markdown åˆ°æœ€ç»ˆæ•ˆæžœçš„å‘ˆçŽ°ä¹‹é—´ï¼Œ
éœ€è¦æˆ‘ä»¬ä½œä¸€äº›&lt;a href="https://fgg100y.github.io/posts/thecli/thevim/2022-07-15-vimwiki_part2/###æ•°å­¦å…¬å¼çš„å‘ˆçŽ°">é¢å¤–é…ç½®&lt;/a>ã€‚&lt;/p>
&lt;p>çŽ°åœ¨æˆ‘åªéœ€è¦ç”¨ mardown æ ¼å¼ç¼–å†™å†…å®¹ï¼Œåœ¨æœ¬åœ°é“¾æŽ¥çš„æ–‡å­—ä¸Šæ•²å‡»å›žè½¦é”®(å‘½ä»¤æ¨¡å¼ä¸‹)ï¼Œ
å°±å¯ä»¥æ‰“å¼€è¿™ä¸ªæœ¬åœ°é“¾æŽ¥çš„å†…å®¹æ–‡æ¡£ï¼Œç„¶åŽç¼–è¾‘å†…å®¹æˆ–è€…å‘å‘†æˆ–è€…åˆ«çš„ä»€ä¹ˆï¼Œæ•²å‡» Delete
é”®(å‘½ä»¤æ¨¡å¼ä¸‹)å›žåˆ°ä¹‹å‰è·³è½¬æ¥çš„åœ°æ–¹ï¼›å½“ç„¶ä¹Ÿå¯ä»¥è®¾ç½®å’Œä½¿ç”¨å…¶ä»–ç»„åˆé”®æ¥åŽ»åˆ°å…¶ä»–åœ°
æ–¹ã€‚åœ¨æ¯ä¸ªåœ°æ–¹ï¼Œéƒ½å¯ä»¥ç”¨æµè§ˆå™¨å±•ç¤ºå½“å‰æ–‡æ¡£çš„å†…å®¹çš„æœ€ç»ˆå‘ˆçŽ°æ•ˆæžœï¼Œæ¯”å¦‚ä½¿ç”¨é»˜è®¤çš„
&lt;code>&amp;lt;leader&amp;gt;+w+h+h&lt;/code> å¿«æ·é”®å°±èƒ½åœ¨ä½ çš„é»˜è®¤æµè§ˆå™¨å±•ç¤ºå½“å‰æ–‡æ¡£çš„å†…å®¹ï¼Œæ•°å­¦å…¬å¼æ­£æ˜¯è¯¾æœ¬
ä¸Šçš„é‚£ç§å®ƒè¯¥æœ‰çš„æ ·å­ï¼Œä¹Ÿå¯ä»¥åƒæµè§ˆç™¾ç§‘ç½‘é¡µæ—¶é‚£æ ·ï¼Œè‡ªç”±è·³è½¬ï¼Œæ¥åŽ»è‡ªå¦‚ã€‚&lt;/p>
&lt;p>é—²è¯å°‘å™ï¼Œç›´æŽ¥ä¸Šæ‰‹ã€‚&lt;/p>
&lt;h3 id="å®‰è£…-vimwiki-æ’ä»¶" >
&lt;div>
&lt;a href="#%e5%ae%89%e8%a3%85-vimwiki-%e6%8f%92%e4%bb%b6">
##
&lt;/a>
å®‰è£… vimwiki æ’ä»¶
&lt;/div>
&lt;/h3>
&lt;p>(ä½¿ç”¨ç®¡ç†æ’ä»¶çš„ &lt;a href="https://github.com/junegunn/vim-plug">Vim-Plug&lt;/a> æ’ä»¶æ¥å®‰è£…å’Œç®¡ç†)&lt;/p>
&lt;p>å°†ä¸‹é¢çš„è¯­å¥åŠ å…¥ä½ è‡ªå·±çš„ vim é…ç½®æ–‡ä»¶ä¸­ï¼Œ&lt;/p>
&lt;p>&lt;code>Plug 'vimwiki/vimwiki'&lt;/code>&lt;/p>
&lt;p>ç„¶åŽè¿è¡Œå‘½ä»¤ &lt;code>:PlugInstall&lt;/code>&lt;/p>
&lt;p>å¦‚æžœä½ è¿˜ä¸æ¸…æ¥šè¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Œæœç´¢: vimrc æˆ–è€… &lt;a href="https://vim.fandom.com/wiki/Open_vimrc_file">ç‚¹å‡»è¿™é‡Œ&lt;/a>&lt;/p>
&lt;h3 id="é…ç½®-vimwiki-æ’ä»¶" >
&lt;div>
&lt;a href="#%e9%85%8d%e7%bd%ae-vimwiki-%e6%8f%92%e4%bb%b6">
##
&lt;/a>
é…ç½® vimwiki æ’ä»¶
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>markdwon æ ¼å¼&lt;/li>
&lt;/ul>
&lt;p>vimwiki é»˜è®¤ä½¿ç”¨ .wiki æ ¼å¼ï¼Œä½†è¿™ä¸ªæ ¼å¼çš„è¯­æ³•æ²¡æœ‰ markdown è¯­æ³•ä½¿ç”¨çš„å¹¿æ³›å’Œæ™®é,
æ‰€ä»¥æˆ‘ä¸ªäººæ˜¯æ›´å€¾å‘äºŽä½¿ç”¨ markdown æ ¼å¼æ¥ç¼–å†™æ–‡æ¡£ã€‚å¯ä»¥æŒ‰ç…§ä¸‹é¢çš„åšæ³•è¿›è¡Œç›¸åº”é…ç½®ï¼š
å°†ä¸‹é¢çš„è¯­å¥åŠ å…¥ &lt;code>.vimrc&lt;/code>:&lt;/p>
&lt;p>&lt;code>let g:vimwiki_ext2syntax={'.md': 'markdown', '.markdown': 'markdown', '.mdown': 'markdown', '.rmd': 'markdown'}&lt;/code>&lt;/p>
&lt;p>è¿™æ˜¯å‡ ä¸ªè¾ƒä¸ºå¸¸è§çš„ markdown æ–‡æ¡£çš„åŽç¼€ï¼Œæœ€åŽä¸€ç§ &lt;code>rmd&lt;/code> æ˜¯ Rmardown æ ¼å¼åŽç¼€ã€‚&lt;/p>
&lt;ul>
&lt;li>å¤šä¸ª index.md æ–‡æ¡£&lt;/li>
&lt;/ul>
&lt;p>å‰é¢æåˆ°è¿‡ï¼Œindex.md æ˜¯ vimwiki çš„é»˜è®¤å…¥å£ï¼Œç±»ä¼¼äºŽä¸€æœ¬ä¹¦çš„ä¸»ç›®å½•ï¼Œæˆ–è€…ç”µå­èœå•ã€‚
é€šå¸¸æˆ‘ä»¬éœ€è¦å¤šä¸ªèœå•ï¼Œæ¯ä¸ªä¸»é¢˜é…ä¸€ä¸ªèœå•ï¼Œä»Žè€Œä½¿å¾—å†…å®¹æ›´ç²¾ç®€å’Œè‡ªæ´½ï¼Œè€Œä¸æ˜¯æ‰€æœ‰
çš„å†…å®¹å…¨éƒ½æ±‡é›†å¹¶æ‹¥æŒ¤åœ¨åŒä¸€ä¸ªåœ°æ–¹ã€‚åœ¨é¡¶çº§èœå•ï¼Œåªéœ€è¦ç»™å‡ºåˆ°å„ä¸ªä¸»é¢˜çš„é“¾æŽ¥å³å¯ã€‚
å°±åƒå¤§é¥­åº—çš„ç”µå­èœå•ï¼Œå®¢äººå¯¹å·èœæ„Ÿå…´è¶£å°±ç‚¹å‡»â€œå·èœâ€çš„ç›®å½•å³å¯ï¼Œä½†æ²¡æœ‰å¿…è¦å°†å…«å¤§
èœç³»æ‰€æœ‰èœåå…¨éƒ½æ‹¥æŒ¤åˆ°ä¸€ä¸ªä¸»èœå•ä¸Šã€‚&lt;br>
åŒä¸Šï¼Œå°†ä¸‹é¢çš„è¯­å¥åŠ å…¥åˆ° &lt;code>.vimrc&lt;/code> &lt;br>
(ä¸è¦ç›´æŽ¥å¤åˆ¶ç²˜è´´ï¼Œç„¶åŽæŒ‡æœ›å®ƒå°±é©¬ä¸Šè§æ•ˆ ðŸ™„ å› ä¸ºä½ çš„ç›®å½•ä¸ä¼š/ä¹Ÿä¸éœ€è¦å’Œæˆ‘çš„ç›¸åŒ):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-vim" data-lang="vim">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">let&lt;/span> g:vimwiki_list = [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;index&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;index&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;path_html&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimwikiHtml&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.html&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_default&amp;#39;&lt;/span>:&lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;template_path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/template/&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ &lt;span style="color:#5af78e">&amp;#39;custom_wiki2html&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/wiki2html.sh&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools/vim8&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/LinuxTools/CLIs&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Julia&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Latex&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/Python&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ {&lt;span style="color:#5af78e">&amp;#39;path&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;~/VimWiki/Programing/ShellScripts&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;syntax&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;markdown&amp;#39;&lt;/span>, &lt;span style="color:#5af78e">&amp;#39;ext&amp;#39;&lt;/span>: &lt;span style="color:#5af78e">&amp;#39;.md&amp;#39;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> \ ]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>åœ¨ä¸Šé¢è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¯ä¸€ä¸ªèŠ±æ‹¬å·çš„å†…å®¹å…¶å®žå°±æ˜¯å¯¹åº”ä¸€ä¸ªä¸»é¢˜çš„èœå• (index.md)ï¼Œæ ¹æ®
vimwiki çš„é»˜è®¤å¿«æ·é”®, æ¯”å¦‚ï¼Œ &lt;code>2&amp;lt;leader&amp;gt;ww&lt;/code> å°±ä¼šç›´æŽ¥æ‰“å¼€ &lt;code>~/Vimwiki/LinuxTools/index.md&lt;/code> è¿™ä¸ªæ–‡ä»¶ã€‚
é‚£ä¹ˆï¼Œå’Œä»¥ä¸Šçš„è®¾ç½®å¯¹åº”çš„ç›®å½•ç»“æž„æ˜¯æ€Žä¹ˆæ ·çš„å‘¢ï¼Ÿä»¥ä¸‹å°±æ˜¯æˆ‘ä¸ªäºº vimwiki ç›®å½•ç»“æž„ (ä½¿ç”¨å‘½ä»¤ &lt;code>tree -L 2&lt;/code>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”œâ”€â”€ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”œâ”€â”€ LinuxTools
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ commonCommandLines.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ images
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ jupyterWithDSKernels.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ linux_oracle_client_sqlplus_install_and_connection_setup.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ linux_sqlserver_sqlcmd_install_and_connection_setup.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ newBashConfig.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ python_packages_management.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ tmux.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ vim8.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â””â”€â”€ vimwiki.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”œâ”€â”€ mathjax
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ bower.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ composer.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ CONTRIBUTING.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ es5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ LICENSE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ package.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â””â”€â”€ README.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”œâ”€â”€ myvimwikidir.txt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”œâ”€â”€ Programming
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ index.md
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ Julia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ Latex
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ Python
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â””â”€â”€ ShellScripts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”œâ”€â”€ Session.vim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”œâ”€â”€ template
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â”œâ”€â”€ default.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â”‚Â Â  â””â”€â”€ markdown.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>â””â”€â”€ wiki2html.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>å¦‚å‰æ‰€è¿°ï¼Œæ ¹ç›®å½•çš„ &lt;code>index.md&lt;/code> æ˜¯é¡¶çº§ä¸»èœå•ï¼Œä»Žè¿™ä¸ªæ–‡ä»¶å†…è·³è½¬åˆ°å„ä¸ªå­ä¸»é¢˜ã€‚æ¯”å¦‚ï¼Œ
æˆ‘ä¸ªäººæ„Ÿå…´è¶£çš„ã€ä¸€ç›´åœ¨å­¦ä¹ çš„3å¤§ä¸»é¢˜åˆ†åˆ«æ˜¯ï¼šæœºå™¨å­¦ä¹ ï¼ˆæœªåˆ—å‡ºï¼‰ï¼Œç¼–ç¨‹å’ŒLinuxå·¥å…·ã€‚
è€Œåœ¨ Programming è¿™ä¸ªç›®å½•ä¸‹çš„ &lt;code>index.md&lt;/code> å°±æ˜¯è¿™ä¸ªä¸»é¢˜çš„ä¸»èœå•ï¼Œæ˜¾ç„¶ï¼Œè¿™ä¸ªä¸»é¢˜
ä¸‹è¿˜æœ‰â€˜Juliaâ€™ï¼Œâ€˜Latexâ€™ï¼Œâ€˜Pythonâ€™ï¼Œâ€˜ShellScriptsâ€™ç­‰å‡ ä¸ªå†…å®¹ã€‚ä¸ç”¨è¯´ï¼Œè¿˜å¯ä»¥è®¾è®¡
æ›´ç»†åˆ†çš„å†…å®¹ã€‚æ¯ä¸ªä¸»é¢˜éƒ½è¶³å¤Ÿæˆ‘å–ä¸Šå‡ å£¶çš„äº† ðŸ» ã€‚æ¯•ç«Ÿï¼Œè°åˆè¯´äº†æž„å»ºæ¯”è¾ƒå®Œæ•´çš„
çŸ¥è¯†ä½“ç³»æ˜¯ä¸€æœä¸€å¤•èƒ½å®Œæˆçš„å‘¢ï¼Ÿæ­£æ‰€è°“ï¼Œå¿—ä¸å¼ºè€…æ™ºä¸è¾¾ã€‚&lt;/p>
&lt;p>è€Œ mathjax ç›®å½•ï¼Œæ­£æ˜¯æƒ³è¦å±•çŽ°æ•°å­¦å…¬å¼æ•ˆæžœæ‰€ä¾èµ–çš„æœ¬åœ°åŒ…ï¼ŒæŽ¥ä¸‹æ¥å°±è®²è¿™ä¸ªçš„é…ç½®ã€‚&lt;/p>
&lt;h3 id="æ•°å­¦å…¬å¼çš„å‘ˆçŽ°" >
&lt;div>
&lt;a href="#%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%8f%e7%9a%84%e5%91%88%e7%8e%b0">
##
&lt;/a>
æ•°å­¦å…¬å¼çš„å‘ˆçŽ°
&lt;/div>
&lt;/h3>
&lt;p>&lt;code>vimwiki2html&lt;/code> å‘½ä»¤å’Œ &amp;ldquo;mathjax&amp;rdquo; ç›®å½•&lt;/p>
&lt;p>vimwiki è‡ªå¸¦äº†ä¸€ä¸ª &lt;code>vimwiki2html&lt;/code> å‘½ä»¤ï¼Œé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯æŠŠ mardown/wiki æ ¼å¼çš„æº
æ–‡æ¡£è½¬æ¢æˆ html æ ¼å¼çš„æ–‡æ¡£ã€‚ä½†è¿™ä¸ªå‘½ä»¤å¹¶ä¸æ”¯æŒæ•°å­¦å…¬å¼çš„æ¸²æŸ“ï¼Œå¾—æ”¹é€ ã€‚è¿™æ­£æ˜¯
&lt;a href="https://www.mathjax.org/">mathjax&lt;/a> é—ªäº®ç™»åœºçš„åœ°æ–¹ã€‚å·²ç»æœ‰äººæŠŠ &lt;code>vimwiki2html&lt;/code>
å‘½ä»¤æ‰€ä¾èµ–çš„è„šæœ¬ä½œäº†ç›¸åº”çš„&lt;a href="https://gist.github.com/enpassant/0496e3db19e32e110edca03647c36541">å‡çº§ç‰ˆ&lt;/a>ï¼Œ
å®ƒèƒ½å°† markdown æ–‡ä»¶è½¬æ¢ä¸º html æ–‡ä»¶ï¼Œå¹¶æ”¯æŒæ•°å­¦å…¬å¼çš„æ¸²æŸ“ã€‚æœ‰äº›äººèµ°çš„&lt;a href="https://gist.github.com/maikeldotuk/54a91c21ed9623705fdce7bab2989742">æ›´è¿œ&lt;/a>
ä½†ä»–ä»¬å‡å‡è®¾æ²¡æœ‰æ‰€è°“çš„ä»€ä¹ˆâ€œå¤–ç½‘â€è®¿é—®é™åˆ¶äº‘äº‘ã€‚æ‰€ä»¥ï¼Œè¿˜éœ€è¦ä¸€ç‚¹æŠ˜ä¸­æ“ä½œï¼Œé‚£å°±æ˜¯
â€˜æœ¬åœ°åŒ–â€™ã€‚å…ˆå“²è¯´â€˜å±±ä¸è¿‡æ¥ï¼Œæˆ‘èµ°è¿‡åŽ»â€™ã€‚ç½‘ç»œæ—¶ä»£åˆšå¥½ç›¸åï¼Œæ—¢ç„¶è®¿é—®ä¸åˆ°ï¼Œé‚£å°±æŠŠå®ƒ
å¤åˆ¶å›žæ¥ï¼ˆåŸºæœ¬å‰ææ˜¯å¼€æºï¼Œå—¯ï¼Œä¼Ÿå¤§çš„å¼€æºï¼‰ã€‚&lt;/p>
&lt;p>æ‰€è°“æœ¬åœ°åŒ–ï¼Œå°±æ˜¯ï¼š1ï¼‰æŠŠ mathjax å¤åˆ¶åˆ°æœ¬åœ°ï¼›2ï¼‰é…ç½® vimwiki çš„å‘½ä»¤è„šæœ¬æ‰¾åˆ°å®ƒã€‚
ä»¥ä¸‹æ˜¯å…·ä½“æ“ä½œï¼š&lt;/p>
&lt;ol>
&lt;li>åœ¨ vimwiki çš„æ ¹ç›®å½•ä¸‹ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>git clone https://github.com/mathjax/MathJax.git mathjax&lt;span style="color:#5af78e">`&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>ä¸‹è½½å¹¶æ›´æ”¹ &lt;code>wiki2html.sh&lt;/code> æ–‡ä»¶çš„ä»¥ä¸‹å†…å®¹
(# å·å¼€å¤´çš„è¯­å¥æ˜¯è¢«æ³¨é‡ŠæŽ‰çš„åŽŸæ–‡ï¼Œä»¥å¤‡å‚è€ƒ):&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># HAS_MATH=$(grep -o &amp;#34;\$\$.\+\$\$&amp;#34; &amp;#34;$INPUT&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># if [ ! -z &amp;#34;$HAS_MATH&amp;#34; ]; then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># MATH=&amp;#34;--mathjax=https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># MATH=&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># fi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">HAS_MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#ff6ac1">$(&lt;/span>grep -o &lt;span style="color:#5af78e">&amp;#34;\$\$*\+\$\$&amp;#34;&lt;/span> &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff5c57">$INPUT&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff6ac1">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#78787e"># # load mathjax from local copy (in case the CDN not work)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">if&lt;/span> &lt;span style="color:#ff6ac1">[&lt;/span> -n &lt;span style="color:#5af78e">&amp;#34;&lt;/span>&lt;span style="color:#ff5c57">$HAS_MATH&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span> &lt;span style="color:#ff6ac1">]&lt;/span>; &lt;span style="color:#ff6ac1">then&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;--mathjax=path2your-mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">else&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff5c57">MATH&lt;/span>&lt;span style="color:#ff6ac1">=&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff6ac1">fi&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>æ³¨æ„ï¼š&amp;ldquo;path2your-mathjax&amp;rdquo; åº”è¯¥ä¿®æ”¹æˆä½ è‡ªå·±çš„è·¯å¾„ï¼Œ
æ¯”å¦‚æˆ‘çš„è·¯å¾„æ˜¯è¿™æ ·çš„ï¼š&amp;rsquo;/home/myacct/VimWiki/mathjax/es5/'&lt;/p>
&lt;p>æˆ‘ä¸å¤ªæ‡‚ htmlï¼Œæˆ‘åœ¨è¿™é‡Œçš„è¯‰æ±‚å°±æ˜¯èƒ½è®©è½¬æ¢åŽçš„ html æ–‡æ¡£æ­£å¸¸æ˜¾ç¤ºæ•°å­¦å…¬å¼ã€‚
ä½†åŽæ¥å‘çŽ°ï¼Œè¦å®žçŽ°è¿™ä¸ªç›®æ ‡ï¼Œå…¶å®žä¸ä¸€å®šéžå¾—å…ˆè½¬æ¢æˆ html æ–‡ä»¶ã€‚&lt;/p>
&lt;h2 id="markdown-æ–‡æ¡£çš„å³æ—¶å±•ç¤º-vim-instant-markdown" >
&lt;div>
&lt;a href="#markdown-%e6%96%87%e6%a1%a3%e7%9a%84%e5%8d%b3%e6%97%b6%e5%b1%95%e7%a4%ba-vim-instant-markdown">
#
&lt;/a>
markdown æ–‡æ¡£çš„å³æ—¶å±•ç¤º: &lt;code>vim-instant-markdown&lt;/code>
&lt;/div>
&lt;/h2>
&lt;p>å‰é¢è¯´è¿‡ï¼Œå¤§éƒ¨åˆ†çš„ markdown å¹¶éžâ€œæ‰€è§å³æ‰€å¾—â€ï¼Œä½†å¦‚æžœä¸€ä¸ªäººçš„æ¬²æœ›æ­£å¸¸ï¼Œé€šå¸¸éƒ½æƒ³
è¦çœ‹çœ‹æ­£åœ¨å†™çš„å†…å®¹åˆ°åº•æ˜¯ä»€ä¹ˆæ˜¾ç¤ºæ•ˆæžœï¼Œæ¯•ç«Ÿï¼Œæ—©å‘çŽ°æ—©æ²»ç–—å˜›ï¼ˆä¸€æ°”å‘µæˆçš„çˆ½æ–‡è‡ªç„¶
æ˜¯æœ‰ï¼Œä½†æˆ‘æ›´ç›¸ä¿¡ä¼˜ç§€çš„ä½œå“é€šå¸¸æ˜¯ä¿®æ”¹å‡ºæ¥çš„ï¼‰ã€‚å½“ç„¶ï¼Œæ—©å°±æœ‰äººæŠŠè¿™ç§æ­£å¸¸ä¸è¿‡çš„
éœ€æ±‚å®žçŽ°å¹¶å…±äº«äº†ï¼Œé‚£å°±æ˜¯ï¼š &lt;a href="https://github.com/instant-markdown/vim-instant-markdown">vim instant markdown&lt;/a>
è¿™ä¹Ÿæ˜¯ä¸€ä¸ª Vim çš„æ’ä»¶ï¼Œå…·ä½“çš„å®‰è£…å’Œé…ç½®ä¹Ÿç®€ä¾¿ï¼Œç›´æŽ¥æ ¹æ®å…¶ readme æ–‡ä»¶è¿›è¡Œå³å¯ã€‚&lt;/p>
&lt;p>ä½†å…¶å®žâ€œæ¯æ”¹ä¸€ä¸ªå­—å°±åˆ·æ–°ä¸€æ¬¡â€å’Œâ€œå†™å®Œäº†æ‰€æœ‰å†…å®¹æ‰çœ‹ä¸Šä¸€çœ¼æœ€ç»ˆæ•ˆæžœâ€ä¸€æ ·ä¸ç”šå¦¥è´´ï¼Œ
å‰è€…ä½¿äººåˆ†å¿ƒï¼ŒåŽè€…ä½¿äººæ‹…å¿ƒã€‚å› æ­¤ï¼Œå½“æƒ³è¦æ£€æŸ¥æŸä¸ªæ®µè½ï¼ˆå°¤å…¶æ˜¯åŒ…å«è®¸å¤šæ•°å­¦å…¬å¼
çš„æ®µè½ï¼‰çš„å‘ˆçŽ°æ•ˆæžœæ—¶ï¼Œè°ƒç”¨å‘½ä»¤ &lt;code>InstantMarkdwonPreview&lt;/code> ï¼Œåœç”¨åˆ™ &lt;code>InstantMarkdwonStop&lt;/code>ã€‚
æˆ‘å¾ˆå–œæ¬¢è¿™ç§å¿«é€Ÿååº”ï¼Œæœ‰ä»‡å°±æŠ¥ï¼Œå½“åœºå°±æŠ¥ï¼›çŸ¥é”™å°±æ”¹ï¼Œå–„èŽ«å¤§ç„‰ã€‚
ä½†å¦‚æžœè§‰å¾—ç™½çº¸é»‘å­—çš„é¢„è§ˆç½‘é¡µç‰ˆé¢è¿‡äºŽæœ´ç´ ä¸å¤Ÿæ»‹å‘³ï¼Œé‚£ä¹ˆï¼Œå¯èƒ½ä½ éœ€è¦çš„æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè‡ªå®šä¹‰çš„ç½‘é¡µã€‚&lt;/p>
&lt;h2 id="markdown-æ–‡æ¡£çš„å³æ—¶å±•ç¤º-jekyll-site" >
&lt;div>
&lt;a href="#markdown-%e6%96%87%e6%a1%a3%e7%9a%84%e5%8d%b3%e6%97%b6%e5%b1%95%e7%a4%ba-jekyll-site">
#
&lt;/a>
markdown æ–‡æ¡£çš„å³æ—¶å±•ç¤º: jekyll-site
&lt;/div>
&lt;/h2>
&lt;p>&lt;code>vim-instant-markdown&lt;/code> å±•ç¤ºçš„å°±æ˜¯æ–‡æ¡£çš„å‘ˆçŽ°å†…å®¹ï¼Œä¹Ÿä»…ä»…å°±æ˜¯å†…å®¹è€Œå·²ã€‚å¦‚æžœæƒ³è¦
æ›´å¤šï¼Œé‚£ä¹ˆï¼Œå¤‡é€‰æ–¹æ¡ˆå°±æ˜¯é™æ€ç½‘ç«™ç”Ÿæˆå™¨ï¼ˆå¦‚&lt;a href="https://jekyllrb.com/">jekyll&lt;/a>)ï¼Œ
é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯è¿™çŽ©æ„å„¿ç›´æŽ¥ç»™ä½ æ‰“åŒ…ç”Ÿæˆç½‘ç«™éœ€è¦çš„ä¸€æ½å­éƒ¨ä»¶ã€‚
ï¼ˆç½‘ç«™æ„å‘³ç€é«˜åº¦å®šåˆ¶çš„è‡ªç”±ï¼Œæ˜¯ä¸æ˜¯å¬ç€å°±è§‰å¾—å¼€å¿ƒï¼Ÿå¦‚æžœä½ å¤Ÿè€å¿ƒçœ‹åˆ°è¿™é‡Œï¼Œæˆ‘å°±
ä¼°æ‘¸ç€ä½ ä¹Ÿåº”è¯¥ä¹Ÿæ˜¯ä¸ªçˆ±æŠ˜è…¾çš„äººï¼Œçˆ±åœ¨æœºå™¨ä¸Šæ£é¼“å„ç§å·¥å…·çš„äººæƒ³å¿…ä¹ŸçŸ¥é“ï¼Œæ‰€è°“çš„
â€œé«˜åº¦å®šåˆ¶â€çš„å¦ä¸€ä¸ªè¯´æ³•å¤§çº¦æ˜¯è¯´ï¼šæ¥å‘€ï¼ŒæŠ˜è…¾å‘€ï¼Œå¿«æ´»å‘€ï¼‰ã€‚&lt;/p>
&lt;p>ä½ çŸ¥é“å—ï¼Ÿä»Ž &lt;code>vimwiki&lt;/code> çš„ç¼–è¾‘ç³»ç»Ÿåˆ° jekyll åšå®¢ç½‘é¡µï¼Œåªéœ€è¦ä¸€ä¸ªå‘½ä»¤ï¼š&lt;code>:Postit&lt;/code>&lt;/p>
&lt;p>å½“ç„¶äº†ï¼Œè¿™æ˜Žæ˜¾å±žäºŽå¹¿å‘Šç”¨è¯­ï¼Œå’Œâ€˜ä¸€å¤©å­¦ä¼špythonï¼Œä¸ƒå¤©æŽŒæ¡æœºå™¨å­¦ä¹ â€™ä¹‹ç±»å·®ä¸å¤šã€‚&lt;/p>
&lt;p>ä¿—è¯­æœ‰äº‘ï¼šâ€œå‰ç«¯ä¸€åˆ†é’Ÿï¼ŒåŽç«¯åå¹´åŠŸâ€ã€‚&lt;code>Postit&lt;/code> å‘½ä»¤éœ€è¦ä¸€äº›åŸºç¡€æ¥æ”¯æ’‘ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-viml" data-lang="viml">&lt;span style="display:flex;">&lt;span>command Postit :&lt;span style="color:#ff6ac1">execute&lt;/span> &lt;span style="color:#5af78e">&amp;#34;w!&amp;#34;&lt;/span> . &lt;span style="color:#5af78e">&amp;#34;Path_to_jekyllsite_root/_posts/&amp;#34;&lt;/span> . strftime(&lt;span style="color:#5af78e">&amp;#34;%Y-%m-%d-&amp;#34;&lt;/span>) . expand(&lt;span style="color:#5af78e">&amp;#34;%:t&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>è¿™è¯­å¥è¦å†™åœ¨ &lt;code>.vimrc&lt;/code> æ–‡ä»¶é‡Œï¼Œè¿™ä¸ªå®žé™…ä¸Šæ˜¯è®¾ç½® vim å‘½ä»¤ã€‚&lt;/p>
&lt;p>æ³¨: è¿™ä¸ªå‘½ä»¤çš„å®žé™…æ„æ€æ˜¯ï¼šå°†å½“å‰ç¼–è¾‘çš„æ–‡ä»¶ä»¥ jekyll çš„ post æ–‡ä»¶çš„æ‰€è¦æ±‚æ ¼å¼
è¿›è¡Œå‘½åï¼Œå¹¶ä¿å­˜åˆ°ä½ çš„ jekyll é¡¹ç›®çš„ _post/ ç›®å½•ä¸‹ã€‚&lt;/p>
&lt;p>å› ä¸º jekyll çš„ post å…¨éƒ¨è¦æ±‚ä»¥â€œå¹´-æœˆ-æ—¥-â€ä½œä¸ºæ–‡ä»¶åå‰ç¼€ (æ¯”å¦‚ï¼š
â€œ2022-02-02-å¥½å¤šäºŒ.mdâ€)ã€‚å‘½ä»¤ä¸­çš„ â€œ.â€ æ˜¯ viml è¯­æ³•ä¸­ &amp;ldquo;concatenate&amp;rdquo; çš„æ„æ€ã€‚&lt;/p>
&lt;p>è™½ç„¶è°ˆä¸ä¸Šéš¾ä»¥å¿å—ï¼Œä½†æˆ‘ä¸ªäººä¸æ˜¯å¾ˆç†è§£è¿™ç§ç¡¬ç¼–ç ï¼Œæ–‡ä»¶å‘½åæ ¼å¼ä¸ºä½•éžå¦‚æ­¤ä¸å¯ï¼Ÿ
æ˜¯bugè¿˜æ˜¯featureï¼Œé‚£å°±è§ä»è§æ™ºäº†ã€‚æˆ‘å¬è¯´è¿˜æœ‰ä¸€ä¸ªé™æ€ç½‘ç«™ç”Ÿæˆå™¨åå«
&lt;a href="https://gohugo.io/">Hugo&lt;/a> çš„ï¼Œä¸çŸ¥å…¶æ‰‹è‰ºå¦‚ä½• &amp;hellip;&lt;/p>
&lt;hr>
&lt;p>åˆšå¼€å§‹ä½¿ç”¨ vimwiki æ—¶ï¼Œä¹Ÿæ˜¯ä¸åœçš„æ‘¸ç´¢ï¼Œæ•°å­¦å…¬å¼çš„æ˜¾ç¤ºä¸€ç›´æ˜¯ä¸ªå¤´ç–¼çš„é—®é¢˜ï¼Œæ¯•ç«Ÿï¼Œ
ä»Žâ€œæ­ªæžœä»â€é‚£é‡Œå€Ÿé‰´æ¥çš„é»˜è®¤é…ç½®éœ€è¦èƒ½è®¿é—®å¤–ç½‘ï¼Œè€Œè¿™ä¸ªæƒ³å¿…ä¸æ˜¯æˆ‘ä»¬èƒ½å¤Ÿé»˜è®¤å¾—äº†çš„ï¼Œ
åªèƒ½å…ˆå¿ç€é‚£äº›ä»¤äººä¸å¿«çš„å¦‚åŒä¹±ç çš„ latex è¯­å¥æŒ‚åœ¨é¡µé¢ä¸Šï¼Œ
ï¼ˆå°¤å…¶æ˜¯ï¼Œå½“ä½ æ˜Žæ˜ŽçŸ¥é“è¿™äº›å…¬å¼æœ¬å¯ä»¥ä¼˜é›…çš„å±•ç¤ºï¼Œå´ä¸èƒ½å¤Ÿæ—¶ï¼Œä¸å¿«å°±å˜æˆäº†æ„¤æ€’ï¼‰ã€‚&lt;/p>
&lt;p>æ²¡æœ‰åƒä½œä¸ºæ–°æ‰‹æ—¶åœ¨ä¸åŒçš„ Linux å‘è¡Œç‰ˆè·³æ¥è·³åŽ»é‚£èˆ¬ï¼Œæˆ‘çŸ¥é“ vimwiki å°±æ˜¯æˆ‘çš„èœï¼Œ
ä»¥å‰ä½¿ç”¨ vimwiki è¿‡ç¨‹ç¢°ä¸Šçš„å„ç§â€œæš‚æ—¶â€çš„æ²¡æœ‰è§£å†³çš„é—®é¢˜ï¼Œå¼€å§‹å˜å¾—ä»¤äººä¸èƒ½å¿å—ï¼Œ
ç‰¹åˆ«æ˜¯å¹³æ—¶æµè§ˆå…¶ä»–ç½‘é¡µæ—¶æ¯æ¯çœ‹åˆ°äººå®¶çš„çš„æ•°å­¦å…¬å¼åˆå¤šåˆé“ï¼Œé‚£ç®€ç›´å°±æ˜¯ä¸€åœºä¾®è¾±ã€‚
æˆ‘èµ°äº†è¿™ä¹ˆè¿œï¼Œå°±å·®é‚£ä¹ˆæœ€åŽä¸€æ­¥ï¼Œæˆ‘å¿ƒä¸ç”˜ã€‚æ‰€ä»¥æˆ‘å°±æŠ˜è…¾ï¼Œä¸€å¤©ï¼Œä¸¤å¤©ï¼Œä¸‰å¤© &amp;hellip;
æˆ‘æ˜¯è´ªå©ªçš„äººï¼Œæ— æ³•æ”¾å¼ƒä¸€æ­¥ä¹‹é¥çš„è¯±æƒ‘ï¼Œå¹¸è¿çš„æ˜¯æˆ‘è¿˜æ˜¯æŠ˜è…¾åˆ°äº†æˆ‘æƒ³è¦çš„â€œæ›´å¤šâ€ã€‚&lt;/p>
&lt;p>æ˜¯çš„ï¼Œè·ç¦»å¤ªè¿œå¯èƒ½ç›´æŽ¥æ”¾å¼ƒï¼Œè€Œä¸€æ­¥ä¹‹é¥æœ€æ˜¯é”€é­‚ ðŸ˜‚&lt;/p>
&lt;p>å…¶å®žï¼Œè¯¥åº†å¹¸çš„æ˜¯æˆ‘ç¢°ä¸Šçš„é—®é¢˜æ—©å°±æœ‰äººå…ˆç¢°ä¸Šä¹Ÿå…ˆè§£å†³äº†ï¼Œæˆ‘åªæ˜¯æ‰¾åˆ°äº†è¿™ä¸ªæ–¹æ³•ï¼Œ
åº”è¯¥è¯´æ˜¯â€˜æœç´¢â€™åˆ°äº†è¿™ä¸ªæ–¹æ³•ï¼Œå¹¶ä½œäº†å¿…è¦çš„è°ƒæ•´ã€‚â€˜é—»é“æœ‰å…ˆåŽâ€™çš„åŸºç¡€å°±æ˜¯åˆ†äº«ï¼Œ
å¯¹äºŽè¿™äº›è§£å†³äº†å®žé™…é—®é¢˜çš„äººï¼Œå¾ˆæ˜¯æ„Ÿæ¿€ä»–ä»¬çš„åˆ†äº«ï¼Œæ²¡æœ‰åˆ†äº«ï¼Œå¤§å®¶çš„æ—¥å­ä¸ä¼šå¥½è¿‡ã€‚&lt;/p></description></item><item><title>åˆ©å™¨3: Jupyterlab and its Kernels</title><link>/posts/thecli/2022-07-22-jupyterwithdskernels/</link><pubDate>Fri, 22 Jul 2022 22:01:51 +0800</pubDate><guid>/posts/thecli/2022-07-22-jupyterwithdskernels/</guid><description>&lt;p>&lt;img alt="IMG_jupyterlab" src="images/jupyter_kernels.png">&lt;/p>
&lt;p>New machine needs to be tailored for a better personal workflow. And this is
about how &lt;a href="https://jupyter.org/">jupyter&lt;/a> can be a multi-language support
co-work platform for regular DataScience works. Also note that Julia community
has &lt;a href="https://github.com/fonsp/Pluto.jl">pluto&lt;/a> notebook, and R has its
&lt;a href="https://www.rstudio.com/">RStudio&lt;/a>.&lt;/p>
&lt;h1 id="jupyter-julia--python--r-all-in-one" >
&lt;div>
&lt;a href="#jupyter-julia--python--r-all-in-one">
##
&lt;/a>
JuPyteR: Julia + Python + R all in one
&lt;/div>
&lt;/h1>
&lt;p>1 check the Python came with system&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ which python3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If there is no python3 yet, install one:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ sudo apt install python3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this would install the latest version of Python.&lt;/p>
&lt;p>2 Poetry or Virtualenv (Poetry is way tooooo slow 2021-12)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># create virtual envs:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python3 -m venv dsvenv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># install jupyterlab:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./dsvenv/bin/python -m pip install jupyterlab
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that this may create a venv without &lt;code>activate&lt;/code> scripts.
If this is the case, check &lt;a href="https://stackoverflow.com/questions/26215790/venv-doesnt-create-activate-script-python3">more discussion
here&lt;/a>&lt;/p>
&lt;p>3 &lt;a href="https://mirrors.tuna.tsinghua.edu.cn/CRAN/">install R&lt;/a>, then&lt;/p>
&lt;p>4 &lt;a href="https://julialang.org/downloads/platform/#linux_and_freebsd">install Julia&lt;/a>&lt;/p>
&lt;p>5 activate python&amp;rsquo;s dsvenv (source ./dsvenv/bin/activate), then&lt;/p>
&lt;p>a. Enter interactive R (REPL environment)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-R" data-lang="R">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># install &amp;#39;IRkernel&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#57c7ff">install.packages&lt;/span>(&lt;span style="color:#5af78e">&amp;#39;IRkernel&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># register the kernel in the current R installation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IRkernel&lt;span style="color:#ff6ac1">::&lt;/span>&lt;span style="color:#57c7ff">installspec&lt;/span>()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># exit&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#57c7ff">q&lt;/span>()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>b. Enter interactive Julia (./path-to/julia.1.6.5/bin/julia)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Julia" data-lang="Julia">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">using&lt;/span> Pkg
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Pkg&lt;span style="color:#ff6ac1">.&lt;/span>add(&lt;span style="color:#5af78e">&amp;#34;IJulia&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># Optional: build IJulia&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Pkg&lt;span style="color:#ff6ac1">.&lt;/span>build(&lt;span style="color:#5af78e">&amp;#34;IJulia&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that if the path of binary of Julia somehow changed (reinstall etc),
one need to perform extra command &lt;code>Pkg.build(&amp;quot;IJulia&amp;quot;)&lt;/code> which tells jupyter
where to find the binary of Julia.&lt;/p>
&lt;p>6 launch jupyterlab server, and that&amp;rsquo;s it!&lt;/p></description></item><item><title>åˆ©å™¨2: Vimwiki ä¹‹ å¦‚ä½•ç”¨ vimwiki è®°ç¬”è®°</title><link>/posts/thecli/thevim/2022-07-15-vimwiki/</link><pubDate>Thu, 21 Jul 2022 23:37:31 +0800</pubDate><guid>/posts/thecli/thevim/2022-07-15-vimwiki/</guid><description>&lt;h2 id="ä¸ªäººç»´åŸº-personal-wiki" >
&lt;div>
&lt;a href="#%e4%b8%aa%e4%ba%ba%e7%bb%b4%e5%9f%ba-personal-wiki">
#
&lt;/a>
ä¸ªäººç»´åŸº (personal wiki)
&lt;/div>
&lt;/h2>
&lt;p>é˜…è¯»è¿‡ã€Šçˆ±ä¸½ä¸æ¼«æ¸¸å¥‡å¢ƒã€‹çš„å°ä¼™ä¼´åº”è¯¥ä¼šè§‰å¾—ç†Ÿæ‚‰ï¼Œç»´åŸºå°±åƒé‚£ä¸ªâ€œå…”å­æ´žâ€ï¼Œ
æ¯ä¸ªè¯æ¡éƒ½å¯ä»¥æ˜¯ä¸€ä¸ªé€šå¾€å…¶ä»–å¥‡å¢ƒçš„å¤§é—¨ï¼Œä½†å®ƒåªå¯¹æœ‰å¥½å¥‡å¿ƒçš„äººæ‰æ‰“å¼€ã€‚
ï¼ˆæˆ–è€…è¯´æœ‰å¥½å¥‡å¿ƒçš„äººæ‰ä¼šæ‰“å¼€å®ƒï¼Œå°¼å¥¥è¦æ˜¯æ²¡æœ‰é€‰æ‹©çº¢è‰²ðŸ’Šï¼Œæ•…äº‹ææ€•å°±æ²¡è¶£å¾—å¤šï¼‰ã€‚
è¿™æ˜¯åœ¨ç»´åŸºä¸Šé˜…è¯»æ—¶å¸¸æœ‰çš„æ„Ÿå—ã€‚ï¼ˆå…¶ä»–ç½‘ç«™å¾ˆå¤šæ—¶å€™åˆ™æžçš„æ›´åƒæ˜¯â€œç›˜ä¸æ´žâ€ã€‚ï¼‰&lt;/p>
&lt;p>æ¯ä¸ªäººéƒ½æœ‰åˆ›ä½œçš„å†²åŠ¨ã€‚
ç„¶è€Œäº‹å®žå¯èƒ½æ›´ç®€å•:å¹³æ—¶è¯»ä¹¦å­¦ä¹ å·¥ä½œæ—¶å–œæ¬¢ä¸œå†™è¥¿åˆ’æ€»ä¼šç•™ä¸‹ä¸€äº›â€˜ç¬”è®°â€™ã€‚
å¯¹åˆ«äººæ¥è¯´å°±æ˜¯äº›èƒ¡æ¶‚ä¹±ç”»ï¼Œå¯è‡ªå·±æ€»è§‰å¾—â€œçè´µâ€ &amp;ndash; â€œå¨˜ä¸å«Œå„¿ä¸‘â€è¯´çš„å¤§æ¦‚å°±æ˜¯è¿™ä¸ªã€‚
æ…¢æ…¢åœ°ï¼Œç§¯ç´¯çš„å†…å®¹è¶Šæ¥è¶Šå¤šï¼Œæ¶‰åŠçš„èŒƒå›´è¶Šæ¥è¶Šå¹¿ï¼Œä¹Ÿå°±æ˜¾å¾—è¶Šæ¥è¶Šé›¶ç¢Žã€‚
ï¼ˆå—¯ï¼Œä¸ºä»€ä¹ˆä¸åœ¨å¼€å§‹çš„æ—¶å€™å°±è§„åˆ’å¥½å‘¢ï¼Ÿçˆ¸å¦ˆé€šå¸¸éƒ½æ˜¯æè¿™ç§å¥½é—®é¢˜çš„é«˜æ‰‹)ã€‚
ä¸è¿‡è¿™åœ¨æˆ‘çœ‹æ¥æ›´åƒä¸€ç§â€œé©¬åŽç‚®â€å¼çš„è´£é—®ï¼Œæ€»æœ‰ä¸€ä¸¢ä¸¢â€œä¼¼æ˜¯è€Œéžâ€çš„æ„Ÿè§‰ã€‚
å­¦ä¹ çš„è¿‡ç¨‹æ›´åƒä¸€ç§æŽ¢ç´¢è¿‡ç¨‹ï¼Œå¯¹å¤§éƒ¨åˆ†äººæ¥è¯´ï¼Œäº‹å…ˆææ€•éƒ½æ— æ³•æ¸…æ™°åœ°çŸ¥é“,
ä¹¦å±±çš„è·¯é€šå‘ä½•å¤„ï¼Œå­¦æµ·çš„èˆŸå¼•ä½ èµ°å‘å“ªé‡Œã€‚å¯¹äºŽä¸èƒ½é¢„å…ˆçŸ¥é“ç»†èŠ‚çš„å†…å®¹ï¼Œ
é€šå¸¸ä¼šåœ¨å¼€å§‹çš„æ—¶å€™å† ä»¥â€œæˆ‘çš„ç¬”è®°â€è¿™ä¹ˆä¸€ä¸ªç›®å½•ï¼Œç„¶åŽä¸‹è®¾ä¸€äº›ç²—ç•¥çš„å­¦ç§‘åç§°ç›®å½•ï¼Œ
ç›´åˆ°ä¸å†æ»¡æ„è¿™æ ·çš„å®‰æŽ’ã€‚æ¯«æ— ç–‘é—®ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯è¦ç”¨ä¸€ç§åˆé€‚çš„æ–¹å¼å°†å®ƒä»¬ç»„ç»‡èµ·æ¥ã€‚&lt;/p>
&lt;p>ç›¸å¯¹ç‹¬ç«‹çš„ä¸»é¢˜è‡ªæˆä½“ç³»ï¼Œä¸åŒä¸»é¢˜ä¹‹é—´å…³è”çš„å†…å®¹å¯ä»¥è¿žæŽ¥èµ·æ¥ï¼Œ
ä¸ä»…åœ¨é˜…è¯»æ—¶å¯ä»¥äº«å—è¿™ç§ä¾¿æ·çš„è‡ªç”±è·³è½¬ï¼Œæœ€å¥½åœ¨è®¾è®¡å’Œç¼–å†™æ—¶ä¹Ÿèƒ½å¦‚æ­¤ã€‚
å¯¹æˆ‘è€Œè¨€ï¼Œç­”æ¡ˆå°±æ˜¯ &lt;a href="https://github.com/vimwiki/vimwiki">vimwiki&lt;/a>.&lt;/p>
&lt;p>é˜…è¯»çš„æ—¶å€™è‡ªç”±è·³è½¬ï¼Œè¿™ä¸ªå¥½ç†è§£ï¼Œå¯ç¼–å†™å†…å®¹ä¸éƒ½ä¸€ç¯‡ä¸€ç¯‡çš„æ¥å—ï¼Ÿ
ç¼–å†™çš„æ—¶å€™ä¹Ÿâ€œèƒ¡å´©ä¹±è·³â€çš„ï¼Œå¬èµ·æ¥å¯ä¸æ€Žä¹ˆç¾Žå¦™ï¼Œæ€•ä¸æ˜¯æœ‰æ¯›ç—…ï¼Ÿä¸ï¼Œä¸è¦è¯¯ä¼šï¼Œ
è¿™å…¶å®žæ˜¯æŒ‡ï¼š1ï¼‰åŒæ—¶åœ¨å¤šä¸ªçª—æ ¼(Pane)æ‰“å¼€å¤šä¸ªæ–‡æ¡£ï¼›2ï¼‰åœ¨çª—æ ¼ä¹‹é—´è¿›è¡Œâ€œé—ªè½¬è…¾æŒªâ€ã€‚&lt;/p>
&lt;p>å¦‚æžœæ˜¯ç¼–å†™è¿‡æ¯”è¾ƒå¤æ‚çš„æ¨¡å—çš„å°ä¼™ä¼´å¯èƒ½ä¼šæ›´æ·±æœ‰æ„Ÿè§¦ï¼Œ
è¿™ç§ä¸åŒæ–‡æ¡£ä¹‹é—´çš„â€œæ—‹è½¬è·³è·ƒæˆ‘å¼€å¤©çœ¼â€çš„è¦æ±‚å…¶å®žå¹¶ä¸è¿‡åˆ†,
åªæ¨è‡ªå·±çš„å±å¹•ä¸å¤Ÿå¤šä¸å¤Ÿå¤§æ”¾ä¸ä¸‹é‚£æ®µæ®µä¼˜é›…çš„ä»£ç 
ï¼ˆæ®å¯é çš„è°£è¨€ç§°ï¼Œæœ‰äº›ç¨‹åºçŒ¿ä¸€å¤©80%çš„æ—¶é—´æ˜¯åœ¨æ¬£èµè‡ªå·±å†™çš„ä»£ç ä¸­åº¦è¿‡ï¼‰ã€‚&lt;/p>
&lt;p>æœ€å…¸åž‹çš„èŽ«è¿‡äºŽä»Žå‡½æ•°è°ƒç”¨çš„åœ°æ–¹ï¼Œè·³è½¬åˆ°åœ¨é‚£é¥è¿œçš„åœ°æ–¹çš„å‡½æ•°å®šä¹‰æ¨¡å—ï¼Œ
å¦‚æžœè¿™ä¸¤ä¸ªåœ°æ–¹çš„å…³ç³»éš”ç€ä¸‰äº”ä¸ªæˆ–è€…æ›´å¤šçš„æ¨¡å—çš„æ—¶å€™ï¼Œ(å¯èƒ½ä¹Ÿæ˜¯éœ€è¦é‡æž„çš„ä¿¡å·)
é‚£ä¹ˆï¼Œå¦‚æžœä¸èƒ½åŒæ—¶å°†è¿™å‡ ä¸ªâ€˜å¯†æŽ¥â€™æ–‡æ¡£é“ºåœ¨çœ¼å‰ï¼Œè€Œæ˜¯ä¸€ä¸ªçœ‹å®Œå†çœ‹å¦ä¸€ä¸ªï¼Œ
æœ‹å‹ï¼Œæˆ‘ä¸çŸ¥é“ä½ çš„æƒ…å†µå¦‚ä½•ï¼Œåæ­£æˆ‘æ˜¯å¾ˆå®¹æ˜“è¿·å¤±åœ¨è¿™èŒ‚å¯†çš„ä»£ç ä¸›æž—é‡Œï¼Œæ™•å¤´è½¬å‘ã€‚&lt;/p>
&lt;p>æ€»è€Œè¨€ä¹‹ï¼Œ
æ ¸å¿ƒçš„é—®é¢˜åœ¨äºŽå¦‚ä½•ç»„ç»‡è¿™äº›æ–‡æ¡£ï¼Œä½¿å¾—è‡ªå·±åœ¨æž„å»ºä¸ªäººâ€˜çŸ¥è¯†åº“â€™çš„è¿‡ç¨‹æ›´ä¸æ»‘ç•…å¿«ã€‚&lt;/p>
&lt;p>ç»´åŸºçš„ç‰¹ç‚¹å°±æ˜¯â€œä½ ä¸­æœ‰æˆ‘ï¼Œæˆ‘ä¸­æœ‰ä»–â€ï¼Œæ¯”å¦‚è¯´ï¼Œ
åœ¨ç»„ç»‡å’Œæž„å»ºå…³äºŽâ€œçº¿æ€§å›žå½’â€å†…å®¹ï¼Œæˆ‘å¸Œæœ›æœ‰ä¸€ä¸ªâ€œå…¥å£â€æ–‡æ¡£ï¼Œç§°ä¹‹ä¸ºä¸»é¢˜ç›®å½•ï¼Œ
è¿™ä¸ªæ–‡æ¡£åªå¯¹è¦å†™çš„ç« èŠ‚å†…å®¹è¿›è¡Œç½—åˆ—å’Œç®€ä»‹ï¼Œæ›´åƒä¸€ç§â€œé¡¶çº§èœå•â€ (top level manu) ï¼Œ
è¿™ä¸ªèœå•åŒ…å«äº›ä»€ä¹ˆèœå‘¢ï¼Ÿç®€ä»‹å¤§æ¦‚æ˜¯è¿™æ ·çš„ï¼š&lt;/p>
&lt;ol>
&lt;li>ç®€å•çº¿æ€§å›žå½’
&lt;ul>
&lt;li>æ–œçŽ‡å’Œæˆªè·&lt;/li>
&lt;li>æœ€å°äºŒä¹˜æ³•&lt;/li>
&lt;li>å‚æ•°è¯„ä¼°æœ‰å¤šå‡†
&lt;ul>
&lt;li>ç½®ä¿¡åº¦åŒºé—´&lt;/li>
&lt;li>å‡è®¾æ£€éªŒ&lt;/li>
&lt;li>På€¼: æœ‰å¤šå°‘çˆ±å¯ä»¥é‡æ¥&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>å¤šå…ƒçº¿æ€§å›žå½’
&lt;ul>
&lt;li>è‡ªå˜é‡å’Œå› å˜é‡åˆ°åº•æœ‰æ²¡æœ‰å…³ç³»&lt;/li>
&lt;li>å› å˜é‡å’Œå“ªäº›è‡ªå˜é‡æœ‰ä»€ä¹ˆå…³ç³»&lt;/li>
&lt;li>æ¨¡åž‹æ‹Ÿåˆ
&lt;ul>
&lt;li>æ®‹å·®(RSE)&lt;/li>
&lt;li>$R^2$ ç»Ÿè®¡é‡&lt;/li>
&lt;li>Adjusted $R^2$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>è´å¶æ–¯çº¿æ€§å›žå½’
&lt;ul>
&lt;li>æ˜¯å…ˆéªŒè¿˜æ˜¯åè§ï¼Ÿ
&lt;ul>
&lt;li>æˆ‘è®¤ä¸º $\Theta$ ä¸åªä¸€ä¸ª&lt;/li>
&lt;li>è´å¶æ–¯å®šå¾‹&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>åŽéªŒæ¦‚çŽ‡ä¼°è®¡
&lt;ul>
&lt;li>MCMC&lt;/li>
&lt;li>å‰å¸ƒæ–¯é‡‡æ ·&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>å¹¿ä¹‰çº¿æ€§å›žå½’
&lt;ul>
&lt;li>æŒ‡æ•°æ—åˆ†å¸ƒï¼šä¸åªæœ‰æ­£æ€åˆ†å¸ƒ&lt;/li>
&lt;li>æœ€å¤§ç†µï¼šæˆ‘æ¥æ•™ä½ æŒ‘é€‰åˆ†å¸ƒ&lt;/li>
&lt;li>é“¾æŽ¥å‡½æ•°ï¼šçº¿æ€§ç©ºé—´åˆ°éžçº¿æ€§ç©ºé—´çš„æ˜ å°„&lt;/li>
&lt;li>æ±‰å¯†å°”é¡¿è’™ç‰¹å¡æ´›&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>è¿™å°±æ˜¯å…³äºŽâ€œçº¿æ€§å›žå½’â€å†…å®¹çš„ä¸»é¢˜ç›®å½•ï¼Œ
åˆ—å‡ºæ¥çš„æ¯ä¸ªç‚¹éƒ½å¯ä»¥è®¾ç½®ä¸ºé“¾æŽ¥ç‚¹ï¼ˆä½†ä¸æ˜¯å¿…é¡»è¿™ä¹ˆåšï¼‰ï¼Œ
ä»Žè¿™ä¸ªä¸»é¢˜ç›®å½•ï¼Œå¯ä»¥åŽ»åˆ°ä»»ä½•çš„å•ä¸ªæ–‡æ¡£ï¼Œåœ¨æ¯ä¸ªæ–‡æ¡£é‡Œï¼Œ
éƒ½å¯ä»¥è®¾ç½®æŒ‡å‘å…¶ä»–æ–‡æ¡£çš„é“¾æŽ¥ï¼Œæˆ–è€…æŒ‡å‘ç½‘ç»œèµ„æºçš„é“¾æŽ¥ï¼ˆäº‹äº‹äº²ä¸ºå¹¶éžå¿…è¦ï¼‰ã€‚&lt;/p>
&lt;p>å¤šå¹´ä»¥æ¥æˆ‘å·²ç»æˆ–å¤šæˆ–å°‘åœ°åšäº†ä¸€äº›ç« èŠ‚å†…å®¹çš„ç¬”è®°ï¼Œç¼ºçš„æ­£æ˜¯è¿™æ ·çš„ä¸€ä¸ªä¸»é¢˜ç›®å½•ã€‚&lt;/p>
&lt;p>è‡³äºŽç¼–è¾‘å†…å®¹çš„æ—¶å€™è¦ä¸è¦åœ¨ä¸€å—å±å¹•ä¸Šâ€œåŒæ—¶â€å¹¶åˆ—å±•å¼€å¤šä¸ªæ–‡æ¡£ï¼Œå¹¶â€œè·³æ¥è·³åŽ»â€ï¼Œ
å¯ä»¥è¯´è¿™åªæ˜¯ä¸€äº›äººçš„ç‰¹æ®Šç™–å¥½ï¼Œå¹¶ä¸æ˜¯éžå¦‚æ­¤ä¸å¯ã€‚
æ¯”å¦‚ï¼Œ24å¯¸å±å¹•ä¸Šå¦‚æžœåªæ‰“å¼€ä¸€ä¸ªæ–‡æ¡£ï¼Œæˆ‘è§‰å¾—æ˜¯å¯¹ç©ºé—´çš„æžå¤§æµªè´¹ï¼›
ä½†è¦çŸ¥é“ï¼Œå¦‚æžœåœ¨13å¯¸çš„å±å¹•ä¸Šï¼Œæœ‰æ—¶æˆ‘ä¹Ÿè¿›å…¥ &lt;a href="https://github.com/junegunn/goyo.vim">goyo
æ¨¡å¼&lt;/a> ã€‚&lt;/p>
&lt;p>å¦‚æžœä½ ç‚¹å¼€å‰æ–‡ vimwiki çš„é“¾æŽ¥ï¼Œé‚£ä½ åº”è¯¥çŸ¥é“å®ƒæ˜¯ Vim çš„ä¸€ä¸ªæ’ä»¶ã€‚&lt;/p>
&lt;p>å¦‚æžœä½ å°šä¸æ¸…æ¥š Vim æ˜¯å¹²ä»€ä¹ˆçš„ï¼Œæˆ–è€…ä»…ä»…å¬è¯´ Vim çš„å­¦ä¹ æ›²çº¿æ˜¯â€œå‡ºäº†åçš„å“äººâ€ï¼Œ
è¿™æ˜¯ä¸å¯¹çš„ï¼Œè‡³å°‘ä½ å¯ä»¥çœ‹åˆ°ï¼š&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;img alt="learning_curves" src="https://fgg100y.github.io/img/posts/vimwiki_intro/editors_learning_curves.jpg">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">&lt;em>some text editors&amp;rsquo; learning curve&lt;/em>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Emacs æ‰æ˜¯æ— å¯äº‰è®®çš„ NO.1 ðŸ˜‚&lt;/p>
&lt;p>å¦‚æžœä¸å¹¸çš„æ˜¯è‡³ä»Šæ‚¨ä»æ—§åªåœ¨ä¸å¹¸çš„æ“ä½œç³»ç»Ÿè¿‡æ´»ï¼ŒðŸ‘‰ æ˜¯çš„ï¼Œæˆ‘æŒ‡çš„å°±æ˜¯å·¨ç¡¬ç³»ç»Ÿã€‚
ä¸å¦¨åŽ»&lt;a href="https://www.vim.org/download.php">ä¸‹è½½ Gvim&lt;/a>æ¥å°è¯•åˆ«æ ·çš„ç¼–è¾‘çš„æ»‹å‘³ï¼Œ&lt;/p>
&lt;p>æˆ–è€…æ‚¨æ˜¯åˆšåˆšå¼€å¯ Linux å¹¸ç¦ä¹‹è·¯çš„äººï¼Œæˆ–è€…æƒ³è¯•ä¸€è¯• Vim ï¼Œ
å¾ˆç®€å•ï¼Œç›´æŽ¥ä¸Šæ‰‹ç±»åž‹çš„ï¼Œåœ¨ Terminal ä¸­è¾“å…¥ &lt;code>vimtutor&lt;/code>ï¼Œé€€å‡ºè¾“å…¥ &lt;code>:q&lt;/code>&lt;/p>
&lt;p>æœ€å¥½çš„ vim å­¦ä¹ ææ–™ä¼šåœ¨è¾“å…¥ &lt;code>:help&lt;/code> ä¹‹åŽå‡ºçŽ°ï¼ˆä½†ä¸ä¸€å®šæ˜¯æœ€å¥½çš„å…¥é—¨ææ–™ï¼‰ã€‚
æˆ–è®¸ä½ ä¼šæƒ³çœ‹çœ‹ä¸€äº›å…¥é—¨çš„ææ–™ï¼Œæ¯”å¦‚ &lt;a href="http://vimcasts.org/">vimcast&lt;/a> è§†é¢‘ï¼Œ
æ¯”å¦‚ï¼Œå¹´ä»£ç•¥å¾®ä¹…è¿œçš„ &lt;a href="https://vim.swaroopch.com/">A Byte of Vim&lt;/a>ï¼Œ
å†…å®¹åŠå…¶è¯¦å°½çš„ &lt;a href="http://www.oualline.com/vim-book.html">Vim Tutorial and Reference&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Last update: 2024-07-04 Thu&lt;/p></description></item><item><title>Linear Regression: frequentist and bayesian</title><link>/posts/regressionmodels/linear_regression/</link><pubDate>Wed, 25 Aug 2021 13:49:40 +0800</pubDate><guid>/posts/regressionmodels/linear_regression/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>**Important Note**:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Almost all the contents (text, images) are came from these great books and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>online resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Statistics, by David Freeman, Robert Pisani, and Roger Perves
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* ç»Ÿè®¡å­¦, David Freeman *et.al* è‘—ï¼Œé­å®—èˆ’ ç­‰è¯‘ï¼Œä¸­å›½ç»Ÿè®¡å‡ºç‰ˆç¤¾
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* æœºå™¨å­¦ä¹ , å‘¨å¿—åŽ è‘— (å¤§åé¼Žé¼Žçš„â€˜è¥¿ç“œä¹¦â€™)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* An Introduction to Statistical Learning, by Gareth James, Daniela Witten,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Trevor Hastie, and Robert Tibshirani
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Deep Learning, a.k.a, the flower book, by Ian Goodfellow, Yoshua Bengio, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Aaron Courville
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Introduction to Machine Learning, Barnabas Poczos, Aarti Singh, CMU-10701
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* Bayesian Methods, Nicholas Ruozzi, UT-DALLAS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 style="text-align: center;">Linear Regression&lt;/h1>
&lt;h2 id="part-0-regression-101" >
&lt;div>
&lt;a href="#part-0-regression-101">
#
&lt;/a>
Part-0: Regression 101
&lt;/div>
&lt;/h2>
&lt;blockquote>
&lt;p>$\text{You&amp;rsquo;ve got to draw the line somewhere.}$&lt;/p>
&lt;/blockquote>
&lt;h3 id="introduction" >
&lt;div>
&lt;a href="#introduction">
##
&lt;/a>
Introduction
&lt;/div>
&lt;/h3>
&lt;p>The regression method describes how one variable depends on another. For example, take height and weight. Naturally, the taller men weighed more. How much of an increase in weight is associated with a unit increase in height? To get started, look at the scatter diagram (figure 1 on below). Height is plotted on the horizontal axis, and weight on the vertical. The summary statistics are&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches,\qquad SD \approx 3 \ inches$&lt;/p>
&lt;p>$\text{average height} \approx 70 \ inches, \qquad SD \approx 45 \ pounds, \qquad r \approx 0.40$&lt;/p>
&lt;p>The scales on the vertical and horizontal axes have been chosen so that one SD of height and one SD of weight cover the same distance on the page. This makes the SD line (dashed) rise at 45 degreed across the page. There is a fair amount of scatter around the line: $r$ is only 0.40.&lt;/p>
&lt;p>&lt;img alt="IMG: regression_line" src="images/stats_weight_height.png">&lt;/p>
&lt;p>Figure 1. Scatter diagram. Each point shows the height and weight for one of 471 men age 18-24 in a dataset. The vertical strip represents men who are about one SD above average in height. Those who are also one SD above average in weight would be plotted along the dashed SD line. Most of the men in the strip are below the SD line: they are only part of an SD above average in weight. The &lt;strong>solid&lt;/strong> regression line estimates average weight at each height.&lt;/p>
&lt;p>The vertical strip in figure 1 shows the men who were one SD above average in height were quite a bit less than one SD above average in weight. This is where the correlation of 0.40 comes in. Associated with an increase of one SD in height there is an increase of &lt;strong>only 0.40&lt;/strong> SDs in weight, on the average.&lt;/p>
&lt;p>To be more specific, take the men who are one SD above average in height:
$$
\text{average height} + SD\ \text{of height} = 70 \ in + 3 \ in = 73 \ in
$$
Their average weight will be above the overall average by $0.40 \times 45 \ lb = 18 \ lb$.&lt;/p>
&lt;p>So, the average weight of these men is around
$$
\text{average weight} + 0.40 \times (SD\ \text{of weight}) = 180 \ lb + 18 \ lb = 198 \ lb
$$
The point (73 inches, 198 pounds) is marked by a cross in figure 1 ( and the points that are 2SD above (76 in, 216 lb) and below (64 in, 144 lb) the average of height as well). All the points (height, estimate for average weight) fall on the solid line shown in figure 1. This is the &lt;em>regression line&lt;/em>. The line goes through the point of averages: men of average height should also be of average weight.&lt;/p>
&lt;p style="text-align:center;color:blue;">
The regression line estimates the average value for y corresponding to each value of x.
&lt;/p>
&lt;p>Along the regression line, associated with each increase of one SD in height there is an increase of only 0.40 SDs in weight. Remember where the 0.40 comes from. It is the correlation between height and weight. NOTE that: Two different SDs are involved here: the SD of $x$, to gauge change in $x$; and the SD of $y$, to gauge changes in $y$.&lt;/p>
&lt;p>This way of using the correlation coefficient to estimate the average value of $y$ for each value of $x$ is called the &lt;strong>regression method&lt;/strong>. The method can be stated as follows.
$$
\fbox{Associated with each increase of one SD in x there is an increase of only r SDs in y, on the average.}
$$&lt;/p>
&lt;blockquote>
&lt;p>Correlation: Like father, like son.&lt;/p>
&lt;blockquote>
&lt;p>If there is a strong association between two variables, then knowing one helps a lot in predicting the other. But when there is a weak association, information about one variable does not help much in guessing the other.&lt;/p>
&lt;/blockquote>
&lt;p>The correlation coefficient is a measure of linear association, or clustering around a line. The relationship between two variables can be summarized by&lt;/p>
&lt;ul>
&lt;li>the average of the $x$-values, the SD of the $x$-values,&lt;/li>
&lt;li>the average of the $y$-values, the SD of the $y$-values,&lt;/li>
&lt;li>the correlation coefficient $r$.&lt;/li>
&lt;/ul>
&lt;p>Computing the correlation coefficient&lt;/p>
&lt;p>Here is the procedure for computing the correlation coefficient.
$$
\boxed{\text{Convert each variable to standard units.} \\
\text{The average of the products gives the correlation coefficient.}}
$$&lt;/p>
&lt;p>$$
\fbox{Convert each variable to standard units.
The average of the products gives the correlation coefficient.}
$$&lt;/p>
&lt;p>Recall that &amp;ldquo;Convert each variable to standard units&amp;rdquo; means &lt;strong>standardization&lt;/strong>. Let $x = {1, 3, 4, 5, 7 }, y = {5, 9, 7, 1, 13 } $ be vectors of variables, the mean is given by
$$
\mu = \frac{1}{|a|} \sum_i a_i
$$
And the SD is the &amp;ldquo;r.m.s size of the deviation from the average&amp;rdquo;, can be given by
$$
\sigma = \sqrt{\frac{1}{|a|} \sum_i (a_i - \mu)^2}
$$
where $|a|$ is the number of data points, $a_i$ is the $i$-th data point in the data set.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Table 1. Computing $r$.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">x&lt;/th>
&lt;th style="text-align:center">y&lt;/th>
&lt;th style="text-align:center">x in standard units&lt;/th>
&lt;th style="text-align:center">y in standard units&lt;/th>
&lt;th style="text-align:center">Product&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">-1.5&lt;/td>
&lt;td style="text-align:center">-0.5&lt;/td>
&lt;td style="text-align:center">0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">3&lt;/td>
&lt;td style="text-align:center">9&lt;/td>
&lt;td style="text-align:center">-0.5&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;td style="text-align:center">-0.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">4&lt;/td>
&lt;td style="text-align:center">7&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;td style="text-align:center">0.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0.5&lt;/td>
&lt;td style="text-align:center">-1.5&lt;/td>
&lt;td style="text-align:center">-0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">7&lt;/td>
&lt;td style="text-align:center">13&lt;/td>
&lt;td style="text-align:center">1.5&lt;/td>
&lt;td style="text-align:center">1.5&lt;/td>
&lt;td style="text-align:center">2.25&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>$$
\begin{eqnarray}
r
&amp;amp;=&amp;amp; \text{average of [(x in standard units) times (x in standard units)]} \\
\\
&amp;amp;=&amp;amp; \frac{0.75 - 0.25 + 0.00 -0.75 + 2.25}{5} = 0.40
\end{eqnarray}
$$&lt;/p>
&lt;p>This complete the solution.&lt;/p>
&lt;/blockquote>
&lt;h3 id="slope-and-intercept" >
&lt;div>
&lt;a href="#slope-and-intercept">
##
&lt;/a>
Slope and Intercept
&lt;/div>
&lt;/h3>
&lt;p>Does education pay? Figure 1 shows the relationship between income and education, for a sample of 562 men age 25-29 in 2005. The summary statistics are
$$
\begin{eqnarray}
\text{average education} &amp;amp;\approx&amp;amp; 12.5\ \text{years}, &amp;amp;\qquad&amp;amp; SD \approx 3\ \text{years} \\
\text{average income} &amp;amp;\approx&amp;amp; $30000,\ &amp;amp;\qquad&amp;amp; SD \approx $24000, \qquad r \approx 0.25
\end{eqnarray}
$$
The regression estimates for average income at each educational level fall along the regression line shown in the figure. The line slopes up, showing that on the average, income does go up with education.&lt;/p>
&lt;p>&lt;img alt="Do education pay" src="./images/stats_education_income.png">&lt;/p>
&lt;p>Any line can be described in terms of its slope and intercept. The y-intercept is the height of the line when $x$ is $0$. And the slope is the rate at which $y$ increases, per unit increase in $x$. Slope and intercept are illustrated in figure 2.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept" src="images/stats_slope_intercept.png">&lt;/p>
&lt;p>&lt;strong>How do you get the slope of the regression line?&lt;/strong> Take the income-education example.
Associated with an increase of one SD in education, there is an increase of $r$ SDs in
income. On this basis, 3 extra years (one SD) of education are worth an extra
$r \times SD = 0.25 \times 24000 = 6000$ of income, on the average. So each extra year in
worth \$6000 / 3 = \$2000. The slope of the regression line is \$2000 per year.&lt;/p>
&lt;p>&lt;img alt="IMG: slopeNintercept2" src="images/stats_slope_intercept2.png">&lt;/p>
&lt;p>The intercept of the regression line is the height when $x = 0$, corresponding to men with $0$ years of education. There men are 12.5 years below average in education.
Each year costs \$2000 -- that is what the slope says. A man with no education should have an income which is below average by
$$
12.5\ \text{years} \times 2000\ \text{per year} = 25000.
$$
His income should be $\$30000 - \$25000 = \$5000$. That is the intercept (figure 3): the predicted value of $y$ when $x = 0$.&lt;/p>
&lt;hr>
&lt;p style="text-align:justify;color:blue;">
Associated with a unit increase in x there is some average change in y. The slope of the regression line estimates this change. The formula for the slope is
&lt;/p>
&lt;p>$$
{r \times SD\ \text{of y} \over SD\ \text{of x}}
$$&lt;/p>
&lt;p style="text-align:justify;color:blue;">
The intercept of the regression line is just the predicted value for y when x is 0.
&lt;/p>
&lt;hr>
&lt;p>The equation of a line can be written in terms of the slope and intercept:
$$
y = \text{slope} \times x + \text{intercept},
$$
which is called the &lt;em>regression equation&lt;/em>. There is nothing new here. The regression equation is just a way of predicting $y$ from $x$ by the regression method.&lt;/p>
&lt;p>The regression line becomes unreliable when you are far from the center of the data, so a &lt;em>negative&lt;/em> intercept is not too disturbing (when the calculation results in some negative value which may seen absurd).&lt;/p>
&lt;p style="text-align:justify;color:blue;">
If you run an observational study, the regression line only describes the data that you see. The line cannot be relied on for predicting the results of interventions.
&lt;/p>
&lt;h3 id="the-least-squares" >
&lt;div>
&lt;a href="#the-least-squares">
##
&lt;/a>
The Least Squares
&lt;/div>
&lt;/h3>
&lt;p>Sometimes the points on a scatter diagram seem to be following a line. The problem discussed in this section is &lt;strong>how to find the line which best fits the points&lt;/strong>. Usually, this involves a compromise: moving the line closer to some points will increase it distance from others. To resolve the conflict, two steps are necessary.&lt;/p>
&lt;ul>
&lt;li>First, define an average distance from the line to all the points.&lt;/li>
&lt;li>Second, move the line around until this average distance is as small as possible.&lt;/li>
&lt;/ul>
&lt;p>To be more specific, suppose the line will be used to predict $y$ from $x$. Then the error made at each point is the vertical distance from the point to the line (a.k.a, the &lt;strong>residual&lt;/strong>, means the difference between the $i$th observed and the $i$th response that is predicted by linear model). In statistics, the usual way to define the average distance is by taking the root-mean-square of the errors. This measure of average distance is called the &lt;em>r.m.s error of the line&lt;/em>. (It was first proposed by Gauss)&lt;/p>
&lt;p>The second problem, how to move the line around to minimize the r.m.s error, was also solved by Gauss:&lt;/p>
&lt;hr>
&lt;p style="text-align:center;color:blue;">
Among all lines, the one that makes the smallest r.m.s error in predicting y form x is the regression line.
&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>The r.m.s error for regression says how far typical points are above or below the regression line.
$$
r.m.s\ error = \sqrt{\frac{1}{n} \sum^n_i (y_i - \hat{y_i})^2}
$$
where $n$ is the number of data points, $y_i$ the $i$-th actual value, $\hat{y_i}$ the corresponding predicted value.&lt;/p>
&lt;p>And the r.m.s error for the regression line of $y$ on $x$ can also be figured as
$$
\sqrt{1 - r^2} \times SD_y
$$
where $r$ is the correlation coefficient&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> between $x$ and $y$.&lt;/p>
&lt;/blockquote>
&lt;p>For this reason, the regression line is often called &lt;em>least squares line&lt;/em>: the errors are squared to compute the r.m.s error, and the regression line makes the r.m.s error as small as possible.&lt;/p>
&lt;p>In other words, the least squares approaches choose $\beta_0$ (the intercept) and $\beta_1$ (the slope) to minimize the &lt;em>residual sum of squares&lt;/em> (RSS) which is defined as
$$
RSS = e_1^2 + e_2^2 + \cdots + e_n^2 = \sum^n_{i=1} (y_i - \hat{y_i})^2
$$
where $e = y_i - \hat{y_i}$ is called the &lt;strong>residual&lt;/strong>. Obviously, the r.m.s error is the root of the mean of RSS.&lt;/p>
&lt;p>Linear regression is a very simple approach for supervised learning. In particular, linear regression is a useful tool for predicting a quantitative response. Many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression.&lt;/p>
&lt;p>$\color{Green}{\text{Example}}$&lt;/p>
&lt;p>According to Hooke&amp;rsquo;s law, the amount of stretch is proportional to the weight $x$. The new length of the spring is
$$
y = mx + b.
$$
In this equation, $m \in \mathbb{R}$ and $b \in \mathbb{R}$ are constants which depend on the spring. Their values are unknown, and have to be estimated using &lt;strong>experimental data&lt;/strong>.&lt;/p>
&lt;center>
Table 1. Data on Hooke's law.
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Weight (kg)&lt;/th>
&lt;th>Length (cm)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>439.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>439.12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>439.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>439.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>439.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>439.50&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/center>
The correlation coefficient[^1] for the data in table 1 is 0.999, very close to 1 indeed. So the points almost form a straight line (figure 5), just as Hooke's law predicts. The minor deviations from linearity are probably due to measurement error; neither the weights nor the length have been measured with perfect accuracy. (Nothing ever is. [When it comes to measurement])
&lt;p>&lt;img alt="Hooke&amp;rsquo;s law" src="images/stats_hookes_law.png">&lt;/p>
&lt;p>Our goal is to estimate $\hat{m}$ and $\hat{b}$ in the equation of Hooke&amp;rsquo;s law for this spring:
$$
y = \hat{m} x + \hat{b}
$$
The graph of this equation is a perfect straight line. If the points in figure 5 happened to fall exactly on some line, the slope&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> of that line would estimate $m$, and its intercept would estimate $b$. However, the points do not line up perfectly. Many different lines could be drawn across the scatter diagram, each having a slightly different slope and intercept.&lt;/p>
&lt;p>Which line should be used? Hooke&amp;rsquo;s equation predicts the length from weight. As discussed above, it is natural to choose $m$ and $b$ so as to minimize the r.m.s error, the line $y = \hat{m} x + \hat{b}$ which does the job is the &lt;strong>regression line&lt;/strong>. This is the &lt;em>method of least squares&lt;/em>. In other words, $m$ in Hooke&amp;rsquo;s law should be estimated as the slope of the regression line, and $b$ as its intercept. These are called &lt;em>least squares estimate&lt;/em>, because they minimize root-mean-square error.&lt;/p>
&lt;p>Let&amp;rsquo;s do the arithmetic (in python code):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff6ac1">import&lt;/span> numpy &lt;span style="color:#ff6ac1">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># X the weight data; y the length data&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">0&lt;/span>, &lt;span style="color:#ff9f43">2&lt;/span>, &lt;span style="color:#ff9f43">4&lt;/span>, &lt;span style="color:#ff9f43">6&lt;/span>, &lt;span style="color:#ff9f43">8&lt;/span>, &lt;span style="color:#ff9f43">10&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y &lt;span style="color:#ff6ac1">=&lt;/span> np&lt;span style="color:#ff6ac1">.&lt;/span>array([&lt;span style="color:#ff9f43">439.00&lt;/span>, &lt;span style="color:#ff9f43">439.12&lt;/span>, &lt;span style="color:#ff9f43">439.21&lt;/span>, &lt;span style="color:#ff9f43">439.31&lt;/span>, &lt;span style="color:#ff9f43">439.40&lt;/span>, &lt;span style="color:#ff9f43">439.50&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># mean and Standard Deviation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># ---------------------------&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># avg = sum(X) / len(X)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mu_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>mean()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The means of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>mu_x, mu_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The means of X and y: (5.0, 439.25666666666666)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># std is the &amp;#34;r.m.s size of the deviation from the average&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_x &lt;span style="color:#ff6ac1">=&lt;/span> X&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SD_y &lt;span style="color:#ff6ac1">=&lt;/span> y&lt;span style="color:#ff6ac1">.&lt;/span>std()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff5c57">print&lt;/span>(&lt;span style="color:#5af78e">f&lt;/span>&lt;span style="color:#5af78e">&amp;#34;The SDs of X and y: &lt;/span>&lt;span style="color:#5af78e">{&lt;/span>SD_x, SD_y&lt;span style="color:#5af78e">}&lt;/span>&lt;span style="color:#5af78e">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># The SDs of X and y: (3.415650255319866, 0.16799470891138593)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert X into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>X_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (X &lt;span style="color:#ff6ac1">-&lt;/span> mu_x) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># convert y into standard unit form&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>y_standard_unit &lt;span style="color:#ff6ac1">=&lt;/span> (y &lt;span style="color:#ff6ac1">-&lt;/span> mu_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># correlation coefficient is the average of the products&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>r &lt;span style="color:#ff6ac1">=&lt;/span> (X_standard_unit&lt;span style="color:#ff6ac1">.&lt;/span>dot(y_standard_unit)) &lt;span style="color:#ff6ac1">/&lt;/span> &lt;span style="color:#ff5c57">len&lt;/span>(X_standard_unit)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># r = 0.999167257319307&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the slope&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>m_hat &lt;span style="color:#ff6ac1">=&lt;/span> (r &lt;span style="color:#ff6ac1">*&lt;/span> SD_y) &lt;span style="color:#ff6ac1">/&lt;/span> SD_x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># m_hat = 0.0491428571428563&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># the intercept, this is the *predicted length* when weight is 0,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>b_hat &lt;span style="color:#ff6ac1">=&lt;/span> mu_y &lt;span style="color:#ff6ac1">-&lt;/span> (mu_x &lt;span style="color:#ff6ac1">*&lt;/span> m_hat)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#78787e"># b_hat = 439.0109523809524&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>this gives us: $\hat{m} \approx 0.05$ per kg and $\hat{b} \approx 439.01$ cm.&lt;/p>
&lt;p>The length of the spring under no load is estimated as 439.01 cm. And each kilogram of load causes the spring to stretch by about 0.05 cm. Of course, even Hooke&amp;rsquo;s law has its limits: beyond some point, the spring will break. &lt;strong>Extrapolating beyond the data is risky&lt;/strong>.&lt;/p>
&lt;p>The method of least squares and the regression method involve the same mathematics; but the contexts may be different. In some fields, investigators talk about &amp;ldquo;least squares&amp;rdquo; when they are estimating parameters &amp;ndash; unknown constants of nature like $m$ and $b$ in Hooke&amp;rsquo;s law. In other fields, investigators talk about regression when they are studying the relationship between two variables, like income and education, using non-experimental data.&lt;/p>
&lt;p>&lt;strong>A technical point:&lt;/strong> The least squares estimate for the length of the spring under no load was 439.01 cm. This is a tiny bit longer than the measured length at no load (439.00 cm). A statistician might trust the least squares estimate over the measurement. Why? Because the least squares estimate takes advantage of all six measurements, not just once: some of the measurement error is likely to cancel out. Of course, the six measurements are tied together by a good theory &amp;ndash; Hookeâ€™s law. Without the theory, the least squares estimate wouldnâ€™t be worth much.&lt;/p>
&lt;h3 id="assessing-the-accuaracy-of-the-coefficient-estimates" >
&lt;div>
&lt;a href="#assessing-the-accuaracy-of-the-coefficient-estimates">
##
&lt;/a>
Assessing the Accuaracy of the Coefficient Estimates
&lt;/div>
&lt;/h3>
&lt;p>Assume that the &lt;em>true&lt;/em> relationship (e.g., the Hookeâ€™s law) between $X$ and $Y$ takes the form $Y = f(X) + \epsilon$ for some unknown function $f$, where $\epsilon$ is a mean-zero random error term. If $f$ is to be approximated by a linear function, then we can write this relationship as
$$
Y = \beta_0 + \beta_1 X + \epsilon.
$$
This is the &lt;em>population regression line&lt;/em>. Here $\beta_0$ is the intercept (the expected value of $Y$ when $X$ = 0) and the $\beta_1$ the slop (the average increase in $Y$ associated with a one-unit increase in $X$). The $\epsilon$ (error term, typically assumed to be independent of $X$) is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in $Y$, and there may be measurement error.&lt;/p>
&lt;p>The model of &lt;em>population regression line&lt;/em> is the best linear approximation to the true relationship between $X$ and $Y$ (NOTE that the assumption of linearity is often a useful working model. However, it may be not true in reality). The true relationship is generally not known for real data, but the least squares line can always be computed using the cofficient estimation methods. A natural question is as follows: how accurate is the least square line as an estimate of the population regression line?&lt;/p>
&lt;p>The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of &lt;em>bias&lt;/em>. If we use the sample mean $\hat{\mu}$ to estimate $\mu$, this estimate is &lt;em>unbiased&lt;/em>, in the sense that on average, we expect $\hat{\mu}$ to equal $\mu$, if we could average a huge number of estimates of $\mu$ obtained from a huge number of sets of observations. Hence, an unbiased estimator does not &lt;em>systematically&lt;/em> over- or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates as well: if we estimate $\beta_0$ and $\beta_1$ on the basis of a particular data set, then our estimates won&amp;rsquo;t be exactly equal to $\beta_0$ and $\beta_1$. But if we could average the estimates obtained over a huge number of date sets, then the average would be spot on!&lt;/p>
&lt;p>So how far off will that single estimate of $\hat{\mu}$ be? In general, we answer this question by computing the &lt;em>standard error&lt;/em> of $\hat{\mu}$, written as $SE(\hat{\mu})$. We have the well-known formula
$$
Var(\hat{\mu}) = SE(\hat{\mu})^2 = {\sigma^2 \over n},
$$
where $\sigma$ is the standard deviation of each of the realizations $y_i$ of $Y$. NOTE that this formula holds iff the $n$ observations are uncorrelated. To compute the standard errors associated with $\hat{\beta_0}$ and $\hat{\beta_1}$, we use the following formulas:
$$
SE(\hat{\beta_0})^2 =
\sigma^2 [{1 \over n} + \frac{\bar{x}^2}{\sum^n_{i=1}(x_i - \bar{x})^2}],
&lt;br>
SE(\hat{\beta_1})^2 =\frac{\sigma^2}{\sum^n_{i=1}(x_i - \bar{x})^2}
$$
where $\sigma^2 = Var(\epsilon)$. In general, $\sigma^2$ is not known, but can be estimated from the data. This estimate of $\sigma$ is known as the &lt;em>residual standard error&lt;/em>, and is given by the formula
$$
\sigma = RSE = \sqrt{RSS / (n-2)}
$$&lt;/p>
&lt;h4 id="confidence-interval" >
&lt;div>
&lt;a href="#confidence-interval">
###
&lt;/a>
Confidence Interval
&lt;/div>
&lt;/h4>
&lt;p>Standard errors can be used to compute the &lt;em>confidence intervals&lt;/em>. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameters. The range is defined in terms of lower and upper limits computed from the sample of data.&lt;/p>
&lt;p>For linear regression, the 95% confidence interval for $\beta_1$ approximately takes the form
$$
\hat{\beta_1} \pm 2 \cdot SE(\hat{\beta_1}).
$$
That is, there is approximately a 95% chance the true value of $\beta_1$ would be in this range.&lt;/p>
&lt;p>Similarly, for $\beta_0$, its 95% confidence interval takes the form
$$
\hat{\beta_0} \pm 2 \cdot SE(\hat{\beta_0}).
$$
NOTE that here we make an assumption that the errors are Gaussian. And the factor of $2$ in the formula will vary slightly depending on the number of observations $n$ in the linear regression.&lt;/p>
&lt;h4 id="hypothesis-tests" >
&lt;div>
&lt;a href="#hypothesis-tests">
###
&lt;/a>
Hypothesis tests
&lt;/div>
&lt;/h4>
&lt;p>Standard errors can also be used to perform &lt;em>hypothesis tests&lt;/em> on the coefficients. The most common hypothesis test involves testing the &lt;em>null hypothesis&lt;/em> of
$$
H_0 : \text{There is no relationship between X and Y}
$$
versus the &lt;em>alternative hypothesis&lt;/em>
$$
H_a : \text{There is some relationship between X and Y}.
$$
Mathematically, this corresponds to testing
$$
H_0 : \beta_1 = 0
$$
versus
$$
H_a : \beta_1 \ne 0,
$$
since if $\beta_1 = 0$ then the linear regression model reduces to $Y = \beta_0 + \epsilon$, and $X$ is not associated with $Y$.&lt;/p>
&lt;p>To test the null hypothesis, we need to determine whether $\hat{\beta_1}$, our estimate for $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero. How far is far enough? This is of course depends on the accuracy of $\hat{\beta_1}$ &amp;ndash; that is, it depends on $SE(\hat{\beta_1})$:&lt;/p>
&lt;ul>
&lt;li>If $SE(\hat{\beta_1})$ is small, then even relatively small values of $\hat{\beta_1}$ may provide strong evidence that $\beta_1 \ne 0$;&lt;/li>
&lt;li>if $SE(\hat{\beta_1})$ is large, then $\hat{\beta_1}$ must be large in absolute value in order for us to reject the null hypothesis.&lt;/li>
&lt;/ul>
&lt;p>In practice, we compute a &lt;em>t-statistic&lt;/em>, given by
$$
t = \frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})},
$$
which measures the number of standard deviations that $\hat{\beta_1}$ is away from $0$.&lt;/p>
&lt;p>If there really is no relationship between $X$ and $Y$, then we expect that &lt;em>t-statistic&lt;/em> will have a $t$-distribution with $n-2$ degrees of freedom. Consequently, it is a simple matter to compute the probability of observing any number equal to $|t|$ or larger in absolute value, assuming $\beta_1 = 0$. We call this probability the &lt;em>p-value&lt;/em>.&lt;/p>
&lt;p>&lt;strong>p-value interpretation&lt;/strong>&lt;/p>
&lt;p>Roughly speaking, we interpret the p-value as follows: a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real relationship between $X$ and $Y$. Hence we &lt;em>reject the null hypothesis&lt;/em>, and declare a relationship to exist between $X$ and $Y$, if the p-value is small enough. Typical p-value cutoffs for rejecting the null hypothesis are 5% or 1%, when $n = 30$, these correspond to &lt;em>t-statistics&lt;/em> of around $2$, and $2.75$, respectively.&lt;/p>
&lt;blockquote>
&lt;p>A small p-value indicates that a particular sample is unlikely if the null hypothesis
is true. It is akin to a jury concluding that it was unlikely that it would have been
presented with this set of evidence if the defendant were innocent, and therefore
reaching a guilty verdict. Of course, that doesn&amp;rsquo;t mean that the defendant is actually
guilty. Perhaps the jury was presented with misleading evidence.
Analogously, a low p-value might be attributable to the null hypothesis actually being
false, or it could simply be that the sample is unrepresentative of the population from
which it is drawn, i.e., the evidence is misleading.&lt;/p>
&lt;/blockquote>
&lt;p>More info about p-value see &amp;ldquo;21.2 Beware of P-values&amp;rdquo; in the book of Prof. John Guttag &amp;lt;&amp;lt;Introduction to computation and programming using python&amp;gt;&amp;gt;.&lt;/p>
&lt;h3 id="assessing-the-accuracy-of-the-model" >
&lt;div>
&lt;a href="#assessing-the-accuracy-of-the-model">
##
&lt;/a>
Assessing the Accuracy of the Model
&lt;/div>
&lt;/h3>
&lt;p>Once we have rejected the null hypothesis in favor of the altervative hypothesis, it is natrual to want to quantify &lt;em>the extent to which the model fits the data&lt;/em>. The quality of a linear regression fit is typically assessed using two related quantities: the &lt;em>residual standard error&lt;/em> ($RSE$) and the $R^2$ statistic.&lt;/p>
&lt;h4 id="residual-standard-error" >
&lt;div>
&lt;a href="#residual-standard-error">
###
&lt;/a>
Residual Standard Error
&lt;/div>
&lt;/h4>
&lt;p>From the model $Y = \beta_0 + \beta_1 X + \epsilon$ that associated with each observation is an error term $\epsilon$. Due to the presence of these error terms, even if we knew the true regression line (i.e., $\beta$s were known), we would not be perfectly predict $Y$ from $X$. The $RSE$ is an estimate of the standard deviation of $\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula
$$
RSE
= \sqrt{{1 \over n-2} RSS}
= \sqrt{{1 \over n-2} \sum^n_{i=1}(y_i - \hat{y_i})^2}.
$$
NOTE that $RSE$ is slightly different from &lt;em>r.m.s error&lt;/em> which the latter using the number of all samples ($n$) as denominator while the former using $n-2$.&lt;/p>
&lt;p>The $RSE$ is considered a measure of the &lt;em>lack of fit&lt;/em> of the model to the data. The smaller $RSE$ the better the model fitted to the data.&lt;/p>
&lt;h4 id="r2-statistic" >
&lt;div>
&lt;a href="#r2-statistic">
###
&lt;/a>
$R^2$ Statistic
&lt;/div>
&lt;/h4>
&lt;p>The $RSE$ provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of $Y$, it is not always clear what consititues a good $RSE$. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a &lt;em>proportion&lt;/em>, the proportion of variance explained, and so it always takes on a value between $0$ and $1$, and is independent of the scale of $Y$.&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
$$
where $TSS = \sum(y_i - \bar{y})^2$ is the &lt;em>total sum of squares&lt;/em>. Hence $R^2$ measures the &lt;em>proportion of variability in $Y$ that can be explained using $X$&lt;/em>.&lt;/p>
&lt;h3 id="multiple-linear-regression" >
&lt;div>
&lt;a href="#multiple-linear-regression">
##
&lt;/a>
Multiple Linear Regression
&lt;/div>
&lt;/h3>
&lt;p>Simple linear regression[^3] is a useful approach for predicting a response on the basis of a single predictor variable. But in practice we often have more than one predictor. One option is to run multiple separate simple linear regression, each of which uses a different feature as a predictor. However, this approach is not entirely satisfactory[^4].&lt;/p>
&lt;p>Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model[^5] so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose we have $p$ distinct predictors. Then the multiple linear regression model takes the form&lt;/p>
&lt;p>$$
\tag{3.19}
\label{mlr}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + + \beta_p X_p + \epsilon
$$&lt;/p>
&lt;p>where $X_j$ represents the $j$-th predictor and $\beta_j$ quantifies the association
between that variable and the response. We interpret $\beta_j$ as the &lt;em>average&lt;/em> effect
on $Y$ of a unit increase in $X_j$, &lt;strong>holding all other predictors fixed&lt;/strong>[^6].&lt;/p>
&lt;p>As was the case in the simple linear regression setting, the regression coefficients
$\beta_0, \beta_1, \ldots, \beta_p$ in ($\ref{mlr}$) are unknown, and must be estimated.
Given estimates $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$, we can make
predictions using the formula&lt;/p>
&lt;p>$$
\tag{3.21}
\label{mlrpred}
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p.
$$&lt;/p>
&lt;p>Then parameters are estimated using the same &lt;strong>least squares&lt;/strong> approach that we saw in
the context of simple linear regression. We choose $\beta_0, \beta_1, \ldots, \beta_p$
to minimize the &lt;strong>sum of squared residuals&lt;/strong>&lt;/p>
&lt;p>$$
\begin{eqnarray}
RSS
&amp;amp;=&amp;amp; \sum^n_{i=1}(y_i - \hat{y}_i)^2 \\
\tag{3.22} \label{rss}
&amp;amp;=&amp;amp; \sum^n_{i=1} \big( y_i - (\hat{\beta_0} + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_p} x_p) \big)^2
\end{eqnarray}
$$&lt;/p>
&lt;p>The values $\hat{\beta_0}, \hat{\beta_1}, \ldots, \hat{\beta_p}$ that minimize
($\ref{rss}$) are the multiple least squares regression coefficient estimates. Unlike
the simple linear regression coefficient estimation (the Python code block in previous
section), the multiple regression coefficient estimates have somewhat complicated forms
that are most easily represented using matrix algebra (see detail in section of Normal
equation).&lt;/p>
&lt;p>When we perform multiple linear regression, we usually are interested in answering a few important questions.&lt;/p>
&lt;hr>
&lt;ol>
&lt;li>Is at least one of the predictors $X_1, X_2, \ldots, X_p$ useful in predicting the response?&lt;/li>
&lt;li>Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?&lt;/li>
&lt;li>How well does the model fit the data?&lt;/li>
&lt;li>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>We now address each of these questions in turn.&lt;/p>
&lt;h4 id="one-is-there-a-relationship-between-the-response-and-predictions" >
&lt;div>
&lt;a href="#one-is-there-a-relationship-between-the-response-and-predictions">
###
&lt;/a>
One: Is There a Relationship Between the Response and Predictions?
&lt;/div>
&lt;/h4>
&lt;p>Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether the slope equals $0$. In the multiple regression setting with $p$ predictors, we need to ask whether all of the regression coefficients are zero. We also use a &lt;strong>hypothesis test&lt;/strong> to answer this question. We test the &lt;strong>null hypothesis&lt;/strong>,
$$
H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0
$$
versus the alternative
$$
H_{\alpha}: \text{at least one } \beta_j \text{ is non-zero}
$$
This hypothesis test is performed by computing the &lt;strong>F-statistic&lt;/strong>,
$$
\tag{3.23}
F = \frac{(TSS - RSS) / p}{RSS / (n - p -1)}
$$
where, as with simple linear regression, $TSS = \sum(y_i - \bar{y})^2$ and $RSS = \sum(y_i - \hat{y})^2$ where $\bar{y} = {1 \over n} \sum^n_{i=1} y_i$ is the sample mean. If the linear model assumptions are correct, one can show that
$$
E {RSS / (n - p - 1) } = \sigma^2
$$
and that, provided $H_0$ is true,
$$
E {(TSS - RSS) / p } = \sigma^2
$$
Hence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to $1$. On the other hand, if $H_{\alpha}$ is true, then $E {(TSS - RSS) / p } &amp;gt; \sigma^2$, so we expect the F-statistic to be greater than $1$.&lt;/p>
&lt;p>However, what if the F-statistic had been closer to $1$? How large does the F-statistic need to be before we can reject $H_0$ and conclude that there is a relationship? It turns out that the answer depends on the values of $n$ and $p$.&lt;/p>
&lt;ul>
&lt;li>When $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$.&lt;/li>
&lt;li>Inctrast, a larger F-statistic is needed to reject $H_0$ if $n$ is small.&lt;/li>
&lt;/ul>
&lt;p>When $H_0$ is true, and the errors $\epsilon_i$ have normal distribution, the F-statistic follows an F-distribution[^7]. For any given value of $n$ and $p$, any statistical software package can be used to compute the p-value[^8] associated with F-statistic using this distribution. Based on this p-value, we determine whether or not to reject $H_0$.&lt;/p>
&lt;p>Sometimes we want to test that a particular subset of $q$ of the coefficients are zero. This corresponds to a null hypothesis
$$
H_0 = \beta_{p-q+1} = \beta_{p-q+2} = \cdots = \beta_{p} = 0,
$$
where for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses all the variables except those last $q$. Suppose that the residual sum of squares for that model is $RSS_0$. Then the appropriate F-statistic is
$$
\tag{3.24} \label{ftest2}
F = \frac{(RSS_0 - RSS) / q}{RSS / (n - p -1)}.
$$
For each individual predictor a t-statistic and a p-value can be obtain, these statistics provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in (means $q=1$ in equation $\ref{ftest2}$). So it reports the &lt;em>partial effect&lt;/em> of adding that variable to the model.&lt;/p>
&lt;p>Given these individual p-values for each variable, why do we need to look at the over F-statistic? After all, it seems likely that if any one of the p-values for the individual variables is very small, then &lt;em>at least one of the predictors is related to the response&lt;/em>. However, this logic is flawed, especially when the number of predictors $p$ is large.&lt;/p>
&lt;p>For instance, consider an example in which $p = 100$ and $H_0:\beta_0 = \beta_1 = \ldots = \beta_p = 0$ is true, so no variable is truly associated with the response. In this situation, about &lt;strong>5%&lt;/strong> of the p-values associated with each variable will be below 0.05 by chance. &lt;strong>In other words, we expect to see approximately five small p-values even in the absence of any true association between the predictors and the response. In fact, we are almost guaranteed that we will observe at least one p-value below 0.05 by chance!&lt;/strong>&lt;/p>
&lt;p>Hence, if we use individual t-statistic and the associated p-value in order to confirm the association between any predictor and the response, there is a very high chance that we will incorrectly conclude that there is a relationship.&lt;/p>
&lt;p>However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. If $H_0$ is true, there is only a 5% chance that the F-statistic will result in a p-value below 0.05, regardless of the number of predictors or the number of observations.&lt;/p>
&lt;p>Note that when $p$ is larger than $n$, we cannot even fit the multiple linear regression model using least squares. Less flexible least squares models, such as forward stepwise selection, ridge regression, lasso regression and principal components regression, are particular useful for performing regression in the high-dimensional setting.&lt;/p>
&lt;h4 id="two-deciding-on-important-variable" >
&lt;div>
&lt;a href="#two-deciding-on-important-variable">
###
&lt;/a>
Two: Deciding on Important Variable
&lt;/div>
&lt;/h4>
&lt;p>If we conclude on the basis of the F-statistic and its associated p-value that at least one of the predictors is related to the response, then it is natural to wonder which are the guilty ones. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as &lt;strong>variable selection&lt;/strong>.&lt;/p>
&lt;p>Ideally, we would like to preform variable selection by trying out a lot of different models, each containing a different subset of the predictors. Unfortunately, there are a total $2^p$ models that contain subsets of $p$ variables (Note that even with a moderate value of $p$, say, $p=30$, then $2^{30}=1,073,741,824$ models make this infeasible). We need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:&lt;/p>
&lt;ul>
&lt;li>Forward selection. We begin with the &lt;strong>null model&lt;/strong> (which contains only the intercept). We then fit $p$ simple linear regressions and add to the null model the variable that results in the lowest $RSS$, and then add to that model the variable which results in the lowest $RSS$ for the new two-variable model. This approach is continued until some stopping rule is satisfied.&lt;/li>
&lt;li>Backward selection. We start with all variables in the model, and remove the variable with largest p-value. The new ($p - 1$)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached (such as when all remaining variables have a p-value below some threshold).&lt;/li>
&lt;li>Mixed selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. If at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform forward and backward steps until all variable in the model have a low p-value, and all the variables outside the model have a large p-value if added to the model.&lt;/li>
&lt;/ul>
&lt;p>Backward selection cannot be used when $p &amp;gt; n$, forward selection is a greedy approach, and might include variable early that later become redundant. Mixed selection can remedy this.&lt;/p>
&lt;h4 id="three-model-fit" >
&lt;div>
&lt;a href="#three-model-fit">
###
&lt;/a>
Three: Model Fit
&lt;/div>
&lt;/h4>
&lt;p>Two of the most common numerical measures of model fit are the $RSE$ and the $R^2$. Recall that in simple regression, $R^2$ is the square of the correlation coefficient between predictor and the response. In multiple linear regression, it turns out that it equals the square of the correlation coefficient between the response and the fitted model (this implies that the fitted model maximizes this correlation among all possible linear model).&lt;/p>
&lt;p>To calculate $R^2$, we use the formula
$$
\begin{eqnarray}
R^2
&amp;amp;=&amp;amp; \frac{\sum(y_i - \bar{y})^2 - \sum(y_i - \hat{y})^2}{\sum(y_i - \bar{y})^2} \\
\\
&amp;amp;=&amp;amp; \frac{TSS - RSS}{TSS} \\
\\
\tag{3.17}
&amp;amp;=&amp;amp; 1 - {RSS \over TSS},
\end{eqnarray}
$$
where ($\bar{y} = {1 \over n} \sum^n_{i=1} y_i$) is the sample mean, $\hat{y}$ is defined in ($\ref{mlrpred}$).&lt;/p>
&lt;p>An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the response variable. It turns out that $R^2$ will always increase when more variables are added to the model, even they are only weakly associated with the response. This is due to the fact that with more variable to the least squares equations must allow us to fit the training data more accurately (though not necessarily the testing data, a.k.a., over fitting).&lt;/p>
&lt;p>In general, $RSE$ is defined as
$$
\tag{3.25}
RSE = \sqrt{{1 \over {n - p - 1}} RSS}
$$
Thus model with more variables can have higher $RSE$ if the decrease in $RSS$ is small relative to the increase in $p$.&lt;/p>
&lt;h5 id="adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2" >
&lt;div>
&lt;a href="#adjusted-r2-from-wikihttpsenwikipediaorgwikicoefficient_of_determinationadjusted_r2">
####
&lt;/a>
Adjusted $R^2$ &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">(From wiki)&lt;/a>
&lt;/div>
&lt;/h5>
&lt;p>The use of an adjusted $R^2$ (one common notation is $\bar{R}^2$; another is $R_{adj}^2$)
is an attempt to account for the phenomenon of the $R^2$ automatically and spuriously
increasing when extra explanatory variables are added to the model. There are many
different ways of adjusting, by far the most used one, to the point that it is typically
just referred to as &lt;em>adjusted $R^2$&lt;/em>, is the correction prosposed by Mordecai Ezekiel,
and adjusted $R^2$ is defined as
$$
\bar{R}^2 = 1 - (1 - R^2){\frac{n-1}{n-p-1}}
$$
where $p$ is the total number of explantory variables in the model (not including the constant term), and $n$ is the sample size. It can also be written as
$$
\bar{R}^2 = 1 - \frac{RSS / df_e}{TSS / df_t}
$$
where $df_t$ is the &lt;em>degrees of freedom&lt;/em> $n-1$ of the estimate of the population variance of the dependent variable, and the $df_e$ is the degrees of freedom $n-p-1$ of the estimate of the underlying population error variance.&lt;/p>
&lt;p>The adjusted $R^2$ can be negative, and its value will always be less than or equal to that of $R^2$. Unlike $R^2$, the adjusted $R^2$ increases only when the increase in $R^2$ (due to the inclusion of a new variable) is more than one would expect to see by chance. If a set of explanatory variables with a predtermined hierarchy of importance are introduced into a regression one at a time, with the adjusted $R^2$ computed each time, the level at which adjusted $R^2$ reaches a maximum, and decreases afterward, would be the regression with ideal combination of having the best fit without excess/unnecessary terms.&lt;/p>
&lt;blockquote>
&lt;p>Degrees of Freedom &lt;a href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)">(From wiki)&lt;/a>:&lt;br>
The number of &lt;em>degrees of freedom&lt;/em> is the number of values in the final calculation of a statistic that are free to vary.
Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter are called the degrees of freedom.
Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of &amp;ldquo;free&amp;rdquo; components (how many components need to be know before the vector is fully determined).
Suppose we have a sample of independent normally distributed observations, ${X_1, X_2, \ldots, X_n}$. This can be represented as an n-dimensional random vector:
$X^T$. Since this random vector can lie anywhere in n-dimensional space, it has $n$ degrees of freedom.
Now let $\bar{X}$ be the sample mean. The random vector can be decomposed as the sum of the sample mean plus a vector of residuals:&lt;/p>
&lt;/blockquote>
&lt;p>$$
\begin{pmatrix}
X_{1} \\
\vdots \\
X_{n} \\
\end{pmatrix}
= \bar{X} \cdot
\begin{pmatrix}
1 \\
\vdots \\
1 \\
\end{pmatrix}
+
\begin{pmatrix}
X_{1} - \bar{X} \\
\vdots \\
X_{n} - \bar{X} \\
\end{pmatrix}
$$&lt;/p>
&lt;p>The first vector on the right-hand side is constrained to be a multiple of the vector of $1$&amp;rsquo;s, and the only free quantity is $\bar{X}$. It therefore has only one degree of freedom.
The second vector is constrained by the relation $\sum(X_i - \bar{X}) = 0$. The first $n-1$ components of this vector can be anything. However, once you know the first $n-1$ components, the constraint tells you the value of the $n$th component. Therefore, this vector has $n-1$ degrees of freedom.&lt;/p>
&lt;h4 id="four-prediction" >
&lt;div>
&lt;a href="#four-prediction">
###
&lt;/a>
Four: Prediction
&lt;/div>
&lt;/h4>
&lt;p>Once we have fit the multiple regression model, it is straightforward to apply the fitted model $\hat{y} = \hat{f}(X) = \hat{\beta} X$ (a more verbose version see $\ref{mlrpred}$) in order to predict the response based on the values of the predictors. However, there are three sorts of uncertainty associated with this prediction.&lt;/p>
&lt;ol>
&lt;li>The coefficient estimate is the least squares estimation of the true coefficient which is unknown. The inaccuracy in the coefficient estimates is related to the &lt;em>reducible error&lt;/em>[^9]. We can compute a &lt;strong>confidence interval&lt;/strong> in order to determine how close $\hat{y}$ will be to $f(X)$.&lt;/li>
&lt;li>In practice assuming a linear model for $f(X)$ is almost always an approximation of reality, so if the true pattern is non-linear, there is an additional reducible error called &lt;em>model bias&lt;/em>.&lt;/li>
&lt;li>Even if we knew $f(X)$ &amp;ndash; that is, we knew the true value of $\beta$ &amp;ndash; the response value cannot be predicted perfectly, because of the random error $\epsilon$ in the model ($\ref{mlrpred}$), this is the &lt;em>irreducible error&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h3 id="summary" >
&lt;div>
&lt;a href="#summary">
##
&lt;/a>
Summary
&lt;/div>
&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>The regression line can be specified by two descriptive statistics: the &lt;em>slope&lt;/em> and the &lt;em>intercept&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Among all lines, the regression line for $y$ on $x$ makes the smallest r.m.s error in predicting $y$ from $x$. For that reason, the regression line is often called the &lt;em>least squares line&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>With a controlled experiment, the slope can tell you the average change in $y$ that would be caused by a change in $x$. With an observational study, however, the slope cannot be relied on to predict the results of interventions. It takes a lot of hard work to draw causal inferences from observational data, with or without regression.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If the average of $y$ depends on $x$ in a non-linear way, the regression line can be quite misleading.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multiple regression is a powerful technique, but it is not a substitute for understanding. (Such as the poor investigator would fit a multiple regression equation of the form $ predicted\ area = a + b \times perimeter + c \times diagonal$ to predict the area of a rectangle).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="part-i-how-to-learn" >
&lt;div>
&lt;a href="#part-i-how-to-learn">
#
&lt;/a>
Part-I: How to learn
&lt;/div>
&lt;/h2>
&lt;p>Linear regression is of course an extremely simple and limited learning algorithm, but it provides an example of how a learning algorithm can work.&lt;/p>
&lt;h3 id="normal-equation" >
&lt;div>
&lt;a href="#normal-equation">
##
&lt;/a>
Normal Equation
&lt;/div>
&lt;/h3>
&lt;p>The goal is to build a system that can take a vector $x \in \mathbb{R^n}$ as input and predict the value of a scalar $y \in \mathbb{R}$ as its output. The output of linear regression is a linear function of the input. Let $\hat{y}$ be the value that our model predicts $y$ should take on. We define the output to be
$$
\begin{equation}
\tag{5.3}
\hat{y} = w^{\mathsf{T}}x
\end{equation}
$$&lt;/p>
&lt;p>where $w \in \mathbb{R^n}$ is a vector of &lt;strong>parameters&lt;/strong>.&lt;/p>
&lt;p>We thus have a definition of our task &lt;em>T&lt;/em> : to predict $y$ from $x$ by outputting $\hat{y}=w^{\mathsf{T}}x$.&lt;/p>
&lt;p>Next we need a definition of our performance measure, &lt;em>P&lt;/em>. One way of measuring the performance of the model is to compute the &lt;strong>mean squared error (MSE)&lt;/strong> of the model on the test set. If $\hat{y}^{(test)}$ gives the predictions of the model on the test set, then the MSE is given by
$$
\tag{5.4}
MSE_{test} = \frac{1}{m} \sum_i{(\hat{y}^{(test)} - {y}^{(test)})^2_i}
$$
Intuitively, one can see that this error measure decreases to 0 when $\hat{y}^{(test)} = {y}^{(test)}$. We can also see that
$$
\tag{5.5}
MSE_{test} = \frac{1}{m} ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2
$$
so the error increases whenever the Euclidean distance between the predictions and the targets increases.&lt;/p>
&lt;blockquote>
&lt;p>In machine learning, we usually measure the size of vectors using a function called a &lt;strong>norm&lt;/strong>. Formally, the $L^p$ norm is given by
$$
||x||_p = \bigg(\sum_i |x_i|^p \bigg)^{\frac{1}{p}}
$$
for $p \in \mathbb{R}, p \geq 1$.&lt;/p>
&lt;p>The $L^2$ norm, with $p = 2$, is known as the &lt;strong>Euclidean norm&lt;/strong>, often denoted simply as $||x||$. It is also common to measure the size of a vector using the squared $L^2$ norm, which can be calculated simply as $x^{\mathsf{T}}x$.&lt;/p>
&lt;/blockquote>
&lt;p>To make a machine learning algorithm, we need to design an algorithm that will improve the weights $w$ in a way that reduces $MSE_{test}$ when the algorithm is allowed to gain experience by observing a training set ($X^{(train)}, y^{(train)}$).&lt;/p>
&lt;p>One intuitive way of doing this is (to minimize $MSE_{test}$) just to minimize the MSE on the training set, $MSE_{train}$ . (Does this make any sense? Keep on reading.)&lt;/p>
&lt;p>To minimize $MSE_{train}$ , we can simply solve for where its gradient is &lt;strong>0&lt;/strong>:
$$
\begin{eqnarray}
\tag{5.6}
\nabla_w MSE_{train} = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(test)} - {y}^{(test)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{\hat{y}^{(train)} - {y}^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \frac{1}{m} \nabla_w ||{X^{(train)}w - y^{(train)}}||^2_2 = 0 \\
&amp;amp; \Rightarrow &amp;amp; \nabla_w \big({X^{(train)}w - y^{(train)}}\big)^{\mathsf{T}} \big({X^{(train)}w - y^{(train)}}\big) = 0
\end{eqnarray}
$$&lt;/p>
&lt;p>$$
\tag{5.10}
\Rightarrow \nabla_w \big( w^{\mathsf{T}} X^{(train)\mathsf{T}} X^{(train)}w - 2w^{\mathsf{T}} X^{(train)\mathsf{T}} y^{(train)} + y^{(train)\mathsf{T}} y^{(train)} \big) = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.11}
\Rightarrow 2X^{(train)\mathsf{T}} X^{(train)}w - 2X^{(train)\mathsf{T}} y^{(train)} = 0 \
$$&lt;/p>
&lt;p>$$
\tag{5.12}
\Rightarrow w = \big(X^{(train)\mathsf{T}} X^{(train)}\big)^{-1} X^{(train)\mathsf{T}} y^{(train)}
$$&lt;/p>
&lt;p>The system of equations whose solution is given by equation 5.12 is known as the &lt;strong>normal equation&lt;/strong>. Evaluating equation 5.12 constitutes a simple learning algorithm.&lt;/p>
&lt;blockquote>
&lt;p>é“¾å¼æ³•åˆ™(Chain Rule)æ˜¯è®¡ç®—å¤æ‚å¯¼æ•°æ—¶çš„é‡è¦å·¥å…·ã€‚ç®€å•åœ°è¯´ï¼Œè‹¥å‡½æ•° $f(x) = g(h(x))$ï¼Œåˆ™æœ‰
$$
\tag{A.31}
\frac{\partial{f(x)}}{\partial{x}} = \frac{\partial{g(h(x))}}{\partial{h(x)}} \cdot \frac{\partial{h(x)}}{\partial{x}}.
$$&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>ä¾‹å¦‚åœ¨è®¡ç®—ä¸‹å¼æ—¶ï¼Œå°†$(Ax - b)$çœ‹ä½œä¸€ä¸ªæ•´ä½“å¯ç®€åŒ–è®¡ç®—ï¼š
$$
\begin{align*}
&amp;amp;\frac{\partial}{\partial{x}}(Ax - b)^{\mathsf{T}} W(Ax - b) \\
&amp;amp;= \frac{\partial{(Ax - b)}}{\partial{x}} \cdot 2W(Ax - b) \\
&amp;amp;= 2AW(Ax - b)
\end{align*}
$$&lt;/p>
&lt;/blockquote>
&lt;p>It is worth noting that the term &lt;strong>linear regression&lt;/strong> is often used to refer to a slightly more sophisticated model with one additional parameter &amp;ndash; an intercept term $b$. In this model&lt;/p>
&lt;p>$$
\tag{5.13}
\hat{y} = w^{\mathsf{T}}x + b
$$&lt;/p>
&lt;p>so the mapping from parameters to predictions is still a linear function but the mapping from features to predictions is now an &lt;strong>affine function&lt;/strong> (which is in the form of equation 5.13, it means that the plot of model&amp;rsquo;s predictions still looks like a line, but it need not pass though the origin). One can continue to use the model with only weights but augment $x$ with an extra entry that is always set to 1. Then the weight corresponding to the extra 1 entry plays the role of the bias parameter (i.e., the intercept term, $b$, a.k.a., the bias term).&lt;/p>
&lt;blockquote>
&lt;p>The intercept term $b$ is often called the &lt;strong>bias&lt;/strong> parameter of the affine transformation. This terminology derives from the point of view that the output of the transformation is biased toward being $b$ in the absence of any input.&lt;/p>
&lt;p>This term is different from the idea of a statistical bias, in which a statistical estimation algorithm&amp;rsquo;s expected estimate of a quantity is not equal to the true quantity.&lt;/p>
&lt;/blockquote>
&lt;h3 id="gradient-based-method" >
&lt;div>
&lt;a href="#gradient-based-method">
##
&lt;/a>
Gradient-based method
&lt;/div>
&lt;/h3>
&lt;p>Example: Linear Least Squares&lt;/p>
&lt;p>Suppose we want to find the value of $x$ that minimizes&lt;/p>
&lt;p>$$
\tag{4.21}
f(x) = \frac{1}{2}||Ax - b||^2_2
$$&lt;/p>
&lt;p>Specialized linear algebra algorithms can solve this problem efficiently; however, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work.&lt;/p>
&lt;p>First, we need to obtain the gradient (Recall the Chain Rule):&lt;/p>
&lt;p>$$
\tag{4.42}
\nabla_x f(x) = A^{\mathsf{T}} (Ax - b) = A^{\mathsf{T}}Ax - A^{\mathsf{T}}b
$$&lt;/p>
&lt;p>We can then follow this gradient downhill, taking small steps.&lt;/p>
&lt;hr>
&lt;p>Algorithm 4.1 An algorithm to minimize $f(x) = \frac{1}{2}||Ax - b||^2_2$ with respect to $x$ using gradient descent, starting from an arbitrary value of $x$.&lt;/p>
&lt;hr>
&lt;p>Set the step size ($\epsilon$, a.k.a., learning rate) and tolerance ($\delta$) to small, positive numbers.&lt;/p>
&lt;p>&lt;strong>while&lt;/strong> $||A^{\mathsf{T}}Ax - A^{\mathsf{T}}b||_2 &amp;gt; \delta$ &lt;strong>do&lt;/strong>&lt;/p>
&lt;p>$x \leftarrow x - \epsilon (A^{\mathsf{T}}Ax - A^{\mathsf{T}}b)$&lt;/p>
&lt;p>&lt;strong>end while&lt;/strong>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>å…³äºŽæ¢¯åº¦ä¸‹é™æ³•(Gradient Descent)&lt;/p>
&lt;p>æ¢¯åº¦ä¸‹é™æ³•æ˜¯ä¸€ç§å¸¸ç”¨çš„ä¸€é˜¶(first-order)ä¼˜åŒ–æ–¹æ³•, æ˜¯æ±‚è§£æ— çº¦æŸä¼˜åŒ–é—®é¢˜æœ€ç®€å•,æœ€ç»å…¸çš„æ–¹æ³•ä¹‹ä¸€.&lt;/p>
&lt;p>è€ƒè™‘æ— çº¦æŸä¼˜åŒ–é—®é¢˜$min_x f(x)$,å…¶ä¸­$f(x)$ä¸ºè¿žç»­å¯å¾®å‡½æ•°.è‹¥èƒ½æž„é€ ä¸€ä¸ªåºåˆ—$x^0, x^1, x^2, \ldots$ æ»¡è¶³
$$
\tag{B.15}
\label{eq_ngd}
f(x^{(t+1)}) &amp;lt; f(x^{(t)}), t = 0,1,2,\ldots
$$
åˆ™ä¸æ–­æ‰§è¡Œè¯¥è¿‡ç¨‹å³å¯æ”¶æ•›åˆ°å±€éƒ¨æžå°ç‚¹.æ¬²æ»¡è¶³å¼($\ref{eq_ngd}$),æ ¹æ®æ³°å‹’å±•å¼æœ‰
$$
\tag{B.16}
f(x + \Delta x) \simeq f(x) + \Delta x^{\mathsf{T}} \nabla f(x)
$$
äºŽæ˜¯,æ¬²æ»¡è¶³$f(x + \Delta x) &amp;lt; f(x)$,å¯é€‰æ‹©
$$
\tag{B.17}
\Delta x = - \epsilon \nabla f(x)
$$
å…¶ä¸­æ­¥é•¿(step size)$\epsilon$æ˜¯ä¸€ä¸ªå°å¸¸æ•°.è¿™å°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•.&lt;/p>
&lt;p>è‹¥ç›®æ ‡å‡½æ•°$f(x)$æ»¡è¶³ä¸€äº›æ¡ä»¶,åˆ™é€šè¿‡é€‰å–åˆé€‚çš„æ­¥é•¿,å°±èƒ½ç¡®ä¿é€šè¿‡æ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°å±€éƒ¨æžå°ç‚¹.ä¾‹å¦‚,è‹¥$f(x)$æ»¡è¶³L-Lipschitzæ¡ä»¶(äº¦å³,å¯¹äºŽä»»æ„$x$,å­˜åœ¨å¸¸æ•°$L$ä½¿å¾—$||\nabla f(x)|| \leq L$æˆç«‹),åˆ™å°†æ­¥é•¿è®¾ç½®ä¸º$1/(2L)$å³å¯ç¡®ä¿æ”¶æ•›åˆ°å±€éƒ¨æžå°ç‚¹.å½“ç›®æ ‡å‡½æ•°æ˜¯å‡¸å‡½æ•°æ—¶,å±€éƒ¨æžå°ç‚¹å°±æ˜¯å…¨å±€æœ€å°ç‚¹,æ­¤æ—¶,æ¢¯åº¦ä¸‹é™æ³•å¯ç¡®ä¿æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£.&lt;/p>
&lt;p>å½“ç›®æ ‡å‡½æ•°$f(x)$äºŒé˜¶è¿žç»­å¯å¾®æ—¶,å¯å°†å¼($B.16$)æ›¿æ¢æˆæ›´ä¸ºç²¾ç¡®çš„äºŒé˜¶æ³°å‹’å±•å¼,è¿™æ ·å°±å¾—åˆ°äº†ç‰›é¡¿æ³•(Newton&amp;rsquo;s method).ç‰›é¡¿æ³•æ˜¯å…¸åž‹çš„äºŒé˜¶æ–¹æ³•,å…¶è¿­ä»£è½®æ•°è¿œå°äºŽæ¢¯åº¦ä¸‹é™æ³•.ä½†ç‰›é¡¿æ³•ä½¿ç”¨äº†äºŒé˜¶å¯¼æ•°$\nabla^2 f(x)$ (second derivative),å…¶æ¯è½®è¿­ä»£ä¸­æ¶‰åŠåˆ°æµ·æ£®çŸ©é˜µ(Hessian matrix)çš„æ±‚é€†,è®¡ç®—å¤æ‚åº¦ç›¸å½“é«˜,å°¤å…¶åœ¨é«˜ç»´é—®é¢˜ä¸­å‡ ä¹Žä¸å¯è¡Œ.å…¶æ¬¡,ç‰›é¡¿æ³•ä»…é€‚ç”¨äºŽé™„è¿‘ç‚¹æœ‰å±€éƒ¨æžå°ç‚¹çš„æƒ…å†µ(ä¹Ÿå°±æ˜¯,æµ·æ£®çŸ©é˜µä¸ºæ­£å®šçŸ©é˜µ,ä¹Ÿå°±æ˜¯æµ·æ£®çŸ©é˜µæ‰€æœ‰çš„ç‰¹å¾å€¼éƒ½æ˜¯æ­£æ•°),è‹¥é™„è¿‘ç‚¹æ˜¯éžç‚¹(saddle point)åˆ™ç‰›é¡¿æ³•å¤±æ•ˆ.ç„¶è€Œ,æ¢¯åº¦ä¸‹é™å´ä¸ä¼šè¢«éžç‚¹å›°ä½.è‹¥èƒ½ä»¥è¾ƒä½Žçš„è®¡ç®—ä»£ä»·å¯»æ‰¾æµ·æ£®çŸ©é˜µçš„è¿‘ä¼¼é€†çŸ©é˜µ,åˆ™å¯ä»¥æ˜¾è‘—é™ä½Žè®¡ç®—å¼€é”€,è¿™å°±æ˜¯æ‹Ÿç‰›é¡¿æ³•(quai-Newton method).&lt;/p>
&lt;/blockquote>
&lt;h3 id="newtons-method" >
&lt;div>
&lt;a href="#newtons-method">
##
&lt;/a>
Newton&amp;rsquo;s method
&lt;/div>
&lt;/h3>
&lt;p>Sometimes we need to find all the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a &lt;strong>Jacobian matrix&lt;/strong>. Specifically, if we have a function $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$, then the Jacobian matrix $\mathbf{J} \in \mathbb{R}^{m \times n}$ of $f$ is defined such that $J_{i, j} = \frac{\partial}{\partial x_j}f(x)_i$.&lt;/p>
&lt;p>We are also sometimes interested in a derivative of a derivative. This is known as a &lt;strong>second derivative&lt;/strong>. For example, for a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$, the derivative with respect to $x_i$ of the derivative of $f$ with respect to $x_j$ is denoted as $\frac{\partial^2}{\partial x_i \partial x_j}f$.&lt;/p>
&lt;p>In a single dimension, we can denote $\frac{d^2}{d x^2}$ by $f&amp;rsquo;&amp;rsquo;(x)$. The second derivative tells us how the first derivative will change when we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. We can think of the second derivative as measuring &lt;strong>curvature&lt;/strong>.&lt;/p>
&lt;p>Suppose we have a quadratic function (or in practice it can be approximated well as quadratic, at least locally). If such a function has:&lt;/p>
&lt;ul>
&lt;li>Second derivative of zero: there is no curvature, it is a flat line, its value can be predict using only the gradient. If the gradient is 1, set the step size of $\epsilon$ along the negative gradient, then the cost function will decrease by $\epsilon$.&lt;/li>
&lt;li>Second derivative is negative: the function curves downward, decrease by more than $\epsilon$.&lt;/li>
&lt;li>Second derivative is positive: the function curves upward, decrease by less than $\epsilon$.&lt;/li>
&lt;/ul>
&lt;p>When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the &lt;strong>Hessian matrix&lt;/strong>. The Hessian matrix $H(f)(x)$ is defined such that&lt;/p>
&lt;p>$$
\tag{4.6}
H(f)(x)_{i, j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x).
$$&lt;/p>
&lt;p>Equivalently, the Hessian is the Jacobian of the gradient.&lt;/p>
&lt;p>The (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation to the function $f(x)$ around the current point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.8}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}}g + \frac{1}{2} (x - x^{(0)})^{\mathsf{T}}H(x - x^{(0)}),
$$&lt;/p>
&lt;p>where $g$ is the gradient and $H$ is the Hessian at $x^{(0)}$. If we use a learning rate of $\epsilon$, then the new point $x$ will be given by $x^{(0)} - \epsilon g$. Substituting this into our approximation, we obtain&lt;/p>
&lt;p>$$
\tag{4.9}
f(x^{(0)} - \epsilon g) \approx f(x^{(0)}) - \epsilon g^{\mathsf{T}}g + \frac{1}{2} \epsilon^2 g^{\mathsf{T}}Hg.
$$&lt;/p>
&lt;p>There are three terms here:&lt;/p>
&lt;ul>
&lt;li>the original value of the function&lt;/li>
&lt;li>the expected improvement due to the slope of the function&lt;/li>
&lt;li>and the correction we must apply to account for the curvature of the function&lt;/li>
&lt;/ul>
&lt;p>When this last term is too large, the gradient descent step can actually move uphill.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is zero or negative, the Taylor series approximation predicts that increase $\epsilon$ forever will decrease $f$ forever. In practice, the Taylor series is unlikely to remain accurate for large $\epsilon$, so one must resort to more heuristic choices of $\epsilon$ in this case.&lt;/p>
&lt;p>When $g^{\mathsf{T}}Hg$ is positive, solving for the optimal step size that decrease the Taylor series approximation of the function the most yields&lt;/p>
&lt;p>$$
\tag{4.10}
\epsilon^* = \frac{g^{\mathsf{T}}g}{g^{\mathsf{T}}Hg}.
$$&lt;/p>
&lt;p>In the worst case, when $g$ aligns with the eigenvector of $H$ corresponding to the maximal eigenvalue $\lambda_{max}$, then this optimal step size is given by $\frac{1}{\lambda_{max}}$. (The eigenvalues of the Hessian determine the scale of the learning rate, if the function we minimized can be approximated well by a quadratic function.)&lt;/p>
&lt;p>Using the eigendecomposition of the Hessian matrix, we can generalize the &lt;strong>second derivative test&lt;/strong> to multiple dimensions. At a critical point, where $\nabla_x f(x) = 0$, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum, or saddle point.&lt;/p>
&lt;ul>
&lt;li>when the Hessian is positive definite (all its eigenvalues are positive): local minimum.&lt;/li>
&lt;li>when the Hessian is negative definite (all its eigenvalues are negative): local maximum.&lt;/li>
&lt;li>the test is inconclusive whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero.&lt;/li>
&lt;/ul>
&lt;p>In multiple dimensions, there is a different second derivative for each direction at a single point. The &lt;strong>condition number&lt;/strong> of the Hessian at this point measures how much the second derivative differ from each other. When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly, Gradient descent is unaware of this change in the derivative, so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.&lt;/p>
&lt;p>Poor condition number also makes choosing a good step size difficult. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature.&lt;/p>
&lt;p>This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method for doing so is known as &lt;strong>Newton&amp;rsquo;s method&lt;/strong>. Newton&amp;rsquo;s method is based on using a second-order Taylor series expansion to approximate $f(x)$ near some point $x^{(0)}$:&lt;/p>
&lt;p>$$
\tag{4.11}
f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\mathsf{T}} \nabla_x f(x^{(0)}) + \frac{1}{2}(x - x^{(0)})^{\mathsf{T}}H(f)(x^{(0)})(x - x^{(0)})
$$&lt;/p>
&lt;p>If we solve for the critical point of this function, we obtain&lt;/p>
&lt;p>$$
\tag{4.12}
x^* = x^{(0)} - H(f)(x^{(0)})^{-1} \nabla_x f(x^{(0)})
$$&lt;/p>
&lt;p>When $f$ is a positive definite quadratic function, Newton&amp;rsquo;s method consist of applying equation 4.12 once to jump to the minimum of the function directly. When $f$ is not truly quadratic but can be locally approximated as a positive definite quadratic, Newton&amp;rsquo;s method consists of applying equation 4.12 multiple times. NOTE that Newton&amp;rsquo;s method is only appropriate when the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient is not attracted to saddle points unless the gradient points toward them.&lt;/p>
&lt;h2 id="part_ii-why-that--work" >
&lt;div>
&lt;a href="#part_ii-why-that--work">
#
&lt;/a>
Part_II: Why that Work
&lt;/div>
&lt;/h2>
&lt;h3 id="linear-regression-as-maximum-likelihood" >
&lt;div>
&lt;a href="#linear-regression-as-maximum-likelihood">
##
&lt;/a>
Linear Regression as Maximum Likelihood
&lt;/div>
&lt;/h3>
&lt;p>Previously, we motivated linear regression as an algorithm that learns to take an input $x$ and produce an output value $\hat{y}$. The mapping from $x$ to $\hat{y}$ is chosen to minimize mean squared error, a criterion that we introduced more or less arbitrarily. We now revisit linear regression from the point of view of maximum likelihood estimation.&lt;/p>
&lt;p>Instead of producing a single prediction $\hat{y}$, we now think of the model as producing a conditional distribution $p(y | x)$. We can imagine that with an infinitely large training set, we might see several training examples with the same input value $x$ but different values of $y$. The goal of the learning algorithm is now to fit the distribution $p(y | x)$ to all those different $y$ values that are all compatible with $x$.&lt;/p>
&lt;p>To derive the same linear regression algorithm we obtained before, we &lt;strong>define&lt;/strong> $p(y | x) = \mathcal{N}(y; \hat{y}(x; w), \sigma^2)$. In this example, we assume that the variance is fixed to some constant $\sigma^2$ chosen by user.&lt;/p>
&lt;p>Since the examples are assumed to be i.i.d., the conditional log-likelihood is given by&lt;/p>
&lt;p>$$
\begin{split}
&amp;amp;\sum^m_{i=1} log \ p(y^{(i)}|x^{(i)}; \theta) \\
&amp;amp;= -m \ log \ \sigma - \frac{m}{2} log(2 \pi) - \sum^m_{i=1} \frac{||\hat{y}^{(i)} - {y}^{(i)}||^2}{2 \sigma^2},
\end{split}
$$&lt;/p>
&lt;p>where $\hat{y}^{(i)}$ is the output of the linear regression on the $i$-th input $x^{(i)}$ and m is the number of the training examples. Comparing the log-likelihood with the mean squared error,&lt;/p>
&lt;p>$$
MSE_{train} = \frac{1}{m} \sum^m_{i=1} ||\hat{y}^{(i)} - {y}^{(i)}||^2,
$$&lt;/p>
&lt;p>we immediately see that maximizing the log-likelihood with respect to $w$ yields the same estimate of the parameters $w$ as does minimizing the mean squared error. The two criteria have different values but the same location of the optimum.&lt;/p>
&lt;p>This justifies the use of MSE as a maximum likelihood estimation procedure.&lt;/p>
&lt;h4 id="maximum-likelihood-estimation" >
&lt;div>
&lt;a href="#maximum-likelihood-estimation">
###
&lt;/a>
Maximum Likelihood Estimation
&lt;/div>
&lt;/h4>
&lt;p>Rather than guessing that some function might make a good estimator and then analyzing its bias and variance, we would like to have some principle from which we can derive specific functions that are good estimators for different models.&lt;/p>
&lt;p>The most common such principle is the maximum likelihood principle.&lt;/p>
&lt;p>Consider a set of m examples $\mathbb{X} = {x^{(1)}, \cdots, x^{(m)}}$ are i.i.d from true but unknown data-generating distribution $p_{data}(\mathbf{x})$.&lt;/p>
&lt;p>Let $p_{model}(\mathbf{x}; \mathbf{\theta})$ be a parametric family of probability distribution over the same space indexed by $\mathbf{\theta}$. In other words, $p_{model}({x}; \mathbf{\theta})$ maps any configuration $x$ to a real number estimating the true probability $p_{data}({x})$.&lt;/p>
&lt;p>The maximum likelihood estimator for $\mathbf{\theta}$ is then defined as&lt;/p>
&lt;p>$$
\begin{eqnarray}
\mathbf{\theta_{ML}}
\tag{5.56}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} {p_{model}(\mathbb{x}; \mathbf{\theta})} \\
\tag{5.57}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \prod^m_{i=1}{p_{model}(x^{(i)}; \mathbf{\theta})}
\end{eqnarray}
$$&lt;/p>
&lt;p>This product over many probabilities can be inconvenient for various reasons. Such as it&amp;rsquo;s prone to numerical underflow. We observe that taking the logarithm of the likelihood does not change its argmax but does conveniently transform a product into a sum:&lt;/p>
&lt;p>$$
\tag{5.58}
\mathbf{\theta_{ML}} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \sum^m_{i=1}{\text{log} \ p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>Because the argmax does not change when we rescale the cost function, we can divide by
$m$ to obtain a version of the criterion that is expressed as an expectation with
respect to the empirical distribution $\hat{p}_{data}$ defined by the training data:&lt;/p>
&lt;p>FIXME: \mathbb in equation(5.59, 5.60, 5.61) not work.&lt;/p>
&lt;p>$$
\tag{5.59}
\mathbf{\theta_{ML}} = {\underset{\mathbf{\theta}}{\operatorname{argmax}}}\
\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {\text{log} p_{model}(x^{(i)}; \mathbf{\theta})}
$$&lt;/p>
&lt;p>One way to interpret maximum likelihood estimation is to view it as minimizing the
dissimilarity between the empirical distribution $\hat{p}_{data}$, defined by the
training set and the model distribution, with the degree of dissimilarity between the
two measured by the KL divergence. The KL divergence is given by&lt;/p>
&lt;p>$$
\tag{5.60}
D_{KL}(\hat{p}_{data} || p_{model}) = \mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\log \hat{p}_{data}(x) - \log {p}_{model}(x)]}.
$$&lt;/p>
&lt;p>The term on the left is a function only of the data-generating process, not the model.
This means when we train the model to minimize the KL divergence, we need only minimize&lt;/p>
&lt;p>$$
\tag{5.61}
-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} {[\text{log} \hat{p}_{data}(x)]},
$$&lt;/p>
&lt;p>which is of course the same as the maximization in equation 5.59.&lt;/p>
&lt;p>Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions. Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution and the model distribution. For example, MSE is the cross-entropy between the empirical distribution and a Gaussian model.&lt;/p>
&lt;p>We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution $\hat{p}_{data}$. While the optimal $\mathbf{\theta}$ is the same regardless of whatever we are maximizing the likelihood or minimizing the KL divergence, the values of the objective functions are different.&lt;/p>
&lt;p>In software, we often phrase both as minimizing a cost function.&lt;/p>
&lt;p>Maximum likelihood thus becomes minimization of the negative log-likelihood (NLL), or equivalently, minimization of the cross-entropy.&lt;/p>
&lt;p>å…³äºŽKLæ•£åº¦&lt;/p>
&lt;p>KLæ•£åº¦(Kullback-Leibler divergence), äº¦ç§°ç›¸å¯¹ç†µ(relative entropy)æˆ–ä¿¡æ¯æ•£åº¦(information divergence), å¯ç”¨äºŽåº¦é‡ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚. ç»™å®šä¸¤ä¸ªè¿žç»­åž‹æ¦‚çŽ‡åˆ†å¸ƒ$P$å’Œ$Q$, äºŒè€…ä¹‹é—´çš„KLæ•£åº¦å®šä¹‰ä¸º&lt;/p>
&lt;p>$$
\label{eq_kld} \tag{C.34}
KL(P||Q) = \int^{\infty}_{-\infty} p(x) \text{log}\frac{p(x)}{q(x)} dx,
$$&lt;/p>
&lt;p>å…¶ä¸­,$p(x)$å’Œ$q(x)$åˆ†åˆ«æ˜¯$P$å’Œ$Q$çš„æ¦‚çŽ‡å¯†åº¦å‡½æ•°.&lt;/p>
&lt;p>KLæ•£åº¦æ»¡è¶³éžè´Ÿæ€§, å³&lt;/p>
&lt;p>$$
\tag{C.35}
KL(P||Q) \geq 0,
$$&lt;/p>
&lt;p>å½“ä¸”ä»…å½“$P=Q$æ—¶$KL(P||Q)=0$. ä½†æ˜¯, KLæ•£åº¦ä¸æ»¡è¶³å¯¹ç§°æ€§, å³&lt;/p>
&lt;p>$$
\tag{C.36}
KL(P||Q) \neq KL(Q||P),
$$&lt;/p>
&lt;p>å› æ­¤, KLæ•£åº¦ä¸æ˜¯ä¸€ä¸ªåº¦é‡(metric).&lt;/p>
&lt;p>è‹¥å°†KLæ•£åº¦çš„å®šä¹‰($\ref{eq_kld}$)å±•å¼€, å¯å¾—&lt;/p>
&lt;p>$$
\begin{eqnarray}
KL(P||Q)
&amp;amp;=&amp;amp; \int^{\infty}_{-\infty} p(x)\log p(x)dx - \int^{\infty}_{-\infty} p(x)\log q(x)dx \\
\tag{C.37}
&amp;amp;=&amp;amp; -H(P) + H(P, Q),
\end{eqnarray}
$$&lt;/p>
&lt;p>å…¶ä¸­$H(P)$ä¸ºç†µ(entropy), $H(P,Q)$ä¸ºäº¤å‰ç†µ(cross-entropy).&lt;/p>
&lt;h3 id="bayesian-linear-regression" >
&lt;div>
&lt;a href="#bayesian-linear-regression">
##
&lt;/a>
Bayesian Linear Regression
&lt;/div>
&lt;/h3>
&lt;p>So far we have discussed &lt;strong>frequentist statistics&lt;/strong> and approaches based on estimating a single value of $\theta$, then making all predictions thereafter based on that one estimate. An other approach is to consider all possible values of $\theta$ when making a prediction. The latter is the domain of &lt;strong>Bayesian statistics&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>Freqentist: the true parameter value $\theta$ is fixed but unknown, while the point estimate $\hat{\theta}$ is a random variable on account of it being a function of the dataset (which is seen as random).&lt;/li>
&lt;li>Bayesian: the Bayesian uses probability to reflect degrees of certainty in states of knowledge. The dataset is directly observed and so is not random. On the other hand, the true parameter $\theta$ is unknown or uncertain and thus is represented as random variable.&lt;/li>
&lt;/ul>
&lt;p>Before observing the data, we represent our knowledge of $\theta$ using the &lt;strong>prior probability distribution&lt;/strong>, $p(\theta)$ (a.k.a., &amp;ldquo;the prior&amp;rdquo;). Generally, the machine learning practitioner selects a prior distribution that quite broad (i.e., with high entropy, such as uniform distribution) to reflect a high degree of uncertainty in the value of $\theta$ before observing any data.&lt;/p>
&lt;p>Now consider that we have a set of data samples {$x^{(1)}, \ldots, x^{(m)}$}. We can recover the effect of data on our belief about $\theta$ by combining the data likelihood $p(x^{(1)}, \ldots, x^{(m)} | \theta)$ with the prior via Bayes&amp;rsquo; rule:
$$
\tag{5.67}
p(\theta | x^{(1)}, \ldots, x^{(m)}) = \frac{p(x^{(1)}, \ldots, x^{(m)} | \theta) p(\theta)}{p(x^{(1)}, \ldots, x^{(m)})}
$$&lt;/p>
&lt;p>In the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation of the data usually causes the posterior to lose entropy and concentrate around a few highly likely values of the parameters.&lt;/p>
&lt;p>Bayesian estimation offers two important differences from MLE:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Unlike the MLE approach that makes predictions using a point estimate of $\theta$, the Bayesian approach is to make predictions using a full distribution over $\theta$. For example, after observing $m$ examples, the predicted distribution over the next data sample, $x^{(m+1)}$, is given by
$$
\tag{5.68}
p(x^{(m+1)} | x^{(1)}, \ldots, x^{(m)}) = \int p(x^{(m+1)} | \theta) p(\theta | x^{(1)}, \ldots, x^{(m)}) d \theta
$$
Here each value of $\theta$ with positive probability density contributes to the prediction of the next example, with the contribution weighted by the posterior density itself.&lt;/p>
&lt;p>After having observed {$x^{(1)}, \ldots, x^{(m)}$}, if we are still uncertain about the value of $\theta$, then this uncertainty is incorporated into any predictions we might make.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The prior has an influence by shifting probability mass density towards regions of the parameter space that are preferred a priori. In practice, the prior often expresses a preference for models that are simpler or more smooth.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Critics of the Bayesian approach identify the prior as a source of subjective human judgment affecting the predictions.&lt;/p>
&lt;p>Bayesian methods typically generalize much better when limited training data is available but typically suffer from high computational cost when the number of training examples is large.&lt;/p>
&lt;p>&lt;strong>$\color{Green}{\mathbf{Example}}$&lt;/strong>&lt;/p>
&lt;p>Here we consider the Bayesian estimation approach to learning the linear regression parameters. In linear regression, we learn a linear mapping from an input vector $x \in \mathbb{R}^n$ to predict the value of a scalar $y \in \mathbb{R}$. The prediction is parameterized by the vector $w \in \mathbb{R}^n$:
$$
\tag{5.69}
\hat{y} = w^{\mathsf{T}}x.
$$
Given a set of $m$ training samples ($X^{(train)}, y^{(train)}$), we can express the prediction of $y$ over the entire training set as
$$
\tag{5.70}
\hat{y}^{(train)} = X^{(train)}w.
$$
Expressed as a Gaussian conditional distribution on $y^{(train)}$, we have
$$
\begin{eqnarray}
p(y^{(train)} | X^{(train)}, w)
\tag{5.71}
&amp;amp;=&amp;amp; \mathcal{N}(y^{(train)} ; X^{(train)}w, I) \\
\tag{5.72}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(
- \frac{1}{2}(y^{(train)} - X^{(train)}w)^{\mathsf{T}} (y^{(train)} - X^{(train)}w)
\bigg),
\end{eqnarray}
$$
where we follow the standard MSE formulation in assuming that the Gaussian variance on $y$ is one.&lt;/p>
&lt;p>In what follows, to reduce the notational burden, we refer to ($X^{(train)}, y^{(train)}$) as simply ($X, y$).&lt;/p>
&lt;p>To determine the posterior distribution over the model parameter vector $w$, we first need to specify a prior distribution. For real-valued parameters it is common to use a Gaussian as a prior distribution,
$$
\tag{5.73}
p(w) = \mathcal{N}(w; \mu_0, \Lambda_0) \varpropto \text{exp} \bigg(
-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg),
$$
where $\mu_0$ and $\Lambda_0$ are the prior distribution mean vector and covariance matrix respectively. (We assume a diagonal covariance matrix $\Lambda_0 = diag(\lambda_0)$, unless there is a reason to use a particular covariance structure.)&lt;/p>
&lt;p>With the prior thus specified, we can now proceed in determining the &lt;strong>posterior&lt;/strong> distribution over the model parameters:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.74}
&amp;amp;\varpropto&amp;amp; p(y | X, w)p(w) \\
\tag{5.75}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(y - Xw)^{\mathsf{T}} (y - Xw) \bigg) \text{exp} \bigg(-\frac{1}{2}(w - \mu_0)^{\mathsf{T}} \Lambda^{-1} (w - \mu_0) \bigg) \\
\tag{5.76}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} \big( -2y^{\mathsf{T}}Xw + w^{\mathsf{T}}X^{\mathsf{T}}Xw + w^{\mathsf{T}} \Lambda_0^{-1}w - 2\mu_0^{\mathsf{T}}\Lambda_0^{-1}w \big) \bigg)
\end{eqnarray}
$$
We now define $\Lambda_m = (X^{\mathsf{T}}X + \Lambda_0^{-1})^{-1}$ and $\mu_m = \Lambda_m (X^{\mathsf{T}}y + \Lambda_0^{-1} \mu_0)$. Using these new variables, we find that the posterior may be rewritten as a Gaussian distribution:
$$
\begin{eqnarray}
p(w | X, y)
\tag{5.77}
&amp;amp;\varpropto&amp;amp;
\text{exp} \bigg(- \frac{1}{2}(w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) + \frac{1}{2} \mu_m^{\mathsf{T}}\Lambda_m^{-1}\mu_m \bigg) \\
\tag{5.78}
&amp;amp;\varpropto&amp;amp; \text{exp} \bigg(-\frac{1}{2} (w - \mu_m)^{\mathsf{T}} \Lambda_m^{-1}(w - \mu_m) \bigg)
\end{eqnarray}
$$
All terms that do not include the parameter vector $w$ have been omitted; they are implied by the fact that the distribution must be normalized to integrate to 1.&lt;/p>
&lt;p>Equation 3.23 shows how to normalize a multivariate Gaussian distribution:
$$
\tag{3.23}
\mathcal{N}(x; \mu, \Sigma) = \sqrt{\frac{1}{(2\pi)^n \text{det}(\Sigma)}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \Sigma^{-1}(x - \mu) \bigg).
$$
When we wish to evaluate the PDF several times for many different values of the parameters, the covariance is not a computationally efficient way to parametrize the distribution, since we need to invert $\Sigma$ to evaluate the PDF. We can instead use a &lt;strong>precision matrix $\beta$&lt;/strong>:
$$
\tag{3.24}
\mathcal{N}(x; \mu, \beta^{-1}) = \sqrt{\frac{\text{det}(\beta)}{(2\pi)^n}} \text{exp} \bigg(-\frac{1}{2} (x - \mu)^{\mathsf{T}} \beta (x - \mu) \bigg).
$$&lt;/p>
&lt;h4 id="maximum-a-posteriori-map-estimation" >
&lt;div>
&lt;a href="#maximum-a-posteriori-map-estimation">
###
&lt;/a>
Maximum A Posteriori (MAP) Estimation
&lt;/div>
&lt;/h4>
&lt;p>While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter $\theta$, it is still often desirable to have a single point estimate. One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation.&lt;/p>
&lt;p>Rather than simply returning to the MLE, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate. One rational way to do this id to choose the &lt;strong>maximum a posteriori&lt;/strong> (MAP) point estimate. The MAP estimate chooses the point of maximal posterior probability ( or maximal probability density in the more common case of continuous $\theta$):
$$
\tag{5.79}
\theta_{MAP}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {p(\mathbf{\theta} | x)}
= \underset{\mathbf{\theta}}{\operatorname{argmax}} {\text{log} \ p(x | \mathbf{\theta})} + \text{log} \ p(\mathbf{\theta})
$$
We recognize, on the righthand side, $\text{log} \ p(x | \mathbf{\theta})$, that is, the standard log-likelihood term, and $\text{log} \ p(\mathbf{\theta})$, corresponding to the prior distribution.&lt;/p>
&lt;p>As an example, consider a linear regression model with a Gaussian prior on the weights $w$. If this prior is given by $\mathcal{N}(w; 0, \frac{1}{\lambda}I^2)$, then the log-prior term in equation 5.79 is proportional to the familiar $\lambda w^{\mathsf{T}}w$ weight decay penalty, plus a term that does not depend on $w$ and does not affect the learning process. MAP Bayesian inference with a Gaussian prior on the weights thus corresponds to weight decay.&lt;/p>
&lt;h3 id="mle-and-map-æ®Šé€”åŒå½’" >
&lt;div>
&lt;a href="#mle-and-map-%e6%ae%8a%e9%80%94%e5%90%8c%e5%bd%92">
##
&lt;/a>
MLE and MAP: æ®Šé€”åŒå½’
&lt;/div>
&lt;/h3>
&lt;h4 id="binary-variables" >
&lt;div>
&lt;a href="#binary-variables">
###
&lt;/a>
Binary Variables
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Coin flipping: heads = 1, tails = 0 with bias $\mu$
$$
p(X = 1 | \mu) = \mu
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Bernoulli Distribution
$$
Bern(x | \mu) = \mu^x \cdot (1 - \mu)^{1 - x} \\
\mathbf{E}[X] = \mu \\
var(X) = \mu \cdot (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>N coin flips: $X_1, \ldots, X_N$
$$
p(\Sigma_i X_i = m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Binomial Distribution
$$
p(m | N, \mu) = {N \choose m} \mu^m (1 - \mu)^{N - m} \\
\mathbf{E}[\Sigma_i X_i] = N \mu \\
var[\Sigma_i X_i] = N \mu (1 - \mu)
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="the-bias-of-a-coin" >
&lt;div>
&lt;a href="#the-bias-of-a-coin">
###
&lt;/a>
The Bias of a Coin
&lt;/div>
&lt;/h4>
&lt;p>Suppose that we have a coin, and we would like to figure out what the probability is that it will flip up heads.&lt;/p>
&lt;ul>
&lt;li>How should we estimate the bias?&lt;/li>
&lt;/ul>
&lt;p>With these coin flips result: &lt;strong>[tail, head, tail, head, head]&lt;/strong>, our estimate of the bias is: 3/5 (&amp;ldquo;the frequency of heads&amp;rdquo;).&lt;/p>
&lt;ul>
&lt;li>
&lt;p>why is this a good estimate of the bias?&lt;/p>
&lt;p>- how good is this estimation?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$P(Heads) = \theta, \ P(Tails) = 1 - \theta$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Flips are i.i.d.&lt;/p>
&lt;ul>
&lt;li>Independent events&lt;/li>
&lt;li>Identically distributed according to Binomial distribution&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Our training data consists of $\alpha_H$ heads and $\alpha_T$ tails
$$
p(D | \theta) = \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="mle" >
&lt;div>
&lt;a href="#mle">
###
&lt;/a>
MLE
&lt;/div>
&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Data: Observed set of $\alpha_H$ heads and $\alpha_T$ tails&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hypothesis: Coin flips follow a binomial distribution&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Learning: Find the &amp;ldquo;best&amp;rdquo; $\theta$&lt;/p>
&lt;p>Maximum Likelihood Estimation: Choose $\theta$ to maximize probability of $D$ given $\theta$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\hat{\theta}
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ P(D | \theta) \\
&amp;amp;=&amp;amp; \underset{\mathbf{\theta}}{\operatorname{argmax}} \ \text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;ul>
&lt;li>Set derivative to zero, and solve!&lt;/li>
&lt;/ul>
&lt;p>$$
\begin{eqnarray}
\frac{d}{d\theta} \text{ln} \ P(D | \theta)
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\text{ln} \ \theta^{\alpha_H} \cdot (1 - \theta)^{\alpha_T}] \\
&amp;amp;=&amp;amp; \frac{d}{d\theta} [\alpha_H \text{ln} \ \theta + \alpha_T \text{ln} (1 - \theta)] \\
&amp;amp;=&amp;amp; \alpha_H \frac{d}{d\theta} \text{ln} \ \theta + \alpha_T \frac{d}{d\theta} \text{ln} \ (1 - \theta) \\
&amp;amp;=&amp;amp; \frac{\alpha_H}{\theta} - \frac{\alpha_T}{1 - \theta} = 0 \\
\\
\Rightarrow \hat{\theta}_{MLE} &amp;amp;=&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T}
\end{eqnarray}
$$&lt;/p>
&lt;p>As we can see now, that&amp;rsquo;s exactly the &amp;ldquo;Frequency of the heads&amp;rdquo;! In other words, the frequency of heads is exactly the &lt;strong>maximum likelihood estimator&lt;/strong> for this problem.&lt;/p>
&lt;h4 id="map" >
&lt;div>
&lt;a href="#map">
###
&lt;/a>
MAP
&lt;/div>
&lt;/h4>
&lt;p>Suppose we have 5 coin flips all of which are heads, Our estimate of the bias is: ???&lt;/p>
&lt;ul>
&lt;li>MLE would give $\theta_{MLE} = 1$&lt;/li>
&lt;li>This event occurs with probability $1 / 2^5 = 1/32$ for a fair coin&lt;/li>
&lt;li>Are we willing to commit to such a strong conclusion with such little evidence?&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Priors&lt;/strong> are a Bayesian mechanism that allow us to take into account &amp;ldquo;prior&amp;rdquo; knowledge about our belief in the outcome. Rather than estimating a single $\theta$, consider a distribution over possible values of $\theta$ given the data:&lt;/p>
&lt;ul>
&lt;li>Without any data observed, our best guess of $\theta$ is obeyed a Beta(2, 2),&lt;/li>
&lt;li>After we see some data (such as observed flips:[tails, tails]), we update our prior to Beta(3, 2).&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Bayesian Learning&lt;/strong>
$$
\begin{eqnarray}
\tag{L1}
p(\theta | D) = \frac{p(D | \theta) \ p(\theta)}{p(D)} \\
\tag{L2} \label{eq_map}
\Rightarrow p(\theta | D) \varpropto p(D | \theta) \ p(\theta)
\end{eqnarray}
$$
where&lt;/p>
&lt;ul>
&lt;li>$p(\theta | D)$ is the posterior,&lt;/li>
&lt;li>$p(D | \theta)$ is the data likelihood,&lt;/li>
&lt;li>$p(\theta)$ is the prior,&lt;/li>
&lt;li>$p(D)$ is the normalization factor.&lt;/li>
&lt;/ul>
&lt;p>We update the prior according to the observed data to get the posterior by applying Bayes rule.&lt;/p>
&lt;p>&lt;strong>Picking Priors&lt;/strong>&lt;/p>
&lt;p>How do we pick a good prior distribution?&lt;/p>
&lt;ul>
&lt;li>Priors could represent expert domain knowledge&lt;/li>
&lt;li>Statisticians choose them to make the posterior distribution &amp;ldquo;nice&amp;rdquo; (conjugate priors, which makes the posterior the same form as the prior)&lt;/li>
&lt;/ul>
&lt;p>What is a good prior for the bias in the coin flipping problem?&lt;/p>
&lt;ul>
&lt;li>Truncated Gaussian (tough to work with)&lt;/li>
&lt;li>Beta distribution (works well for binary random variables)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Coin Flips with Beta Distribution&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Likelihood function: $p(D | \theta) = \theta^{\alpha_H} (1 - \theta)^{\alpha_T}$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Prior: $p(\theta) = \frac{\theta^{\beta_H - 1} (1 - \theta)^{\beta_T - 1}}{B(\beta_H, \beta_T)} \sim Beta(\beta_H, \beta_T)$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Posterior:
$$
\begin{eqnarray}
p(\theta | D)
&amp;amp;\varpropto&amp;amp; \theta^{\alpha_H}(1 - \theta)^{\alpha_T} \theta^{\beta_H - 1}(1 - \theta)^{\beta_T - 1} \\
&amp;amp;=&amp;amp; \theta^{\alpha_H + \beta_H - 1}(1 - \theta)^{\alpha_T + \beta_T - 1} \\
&amp;amp;=&amp;amp; Beta(\alpha_H + \beta_H, \alpha_T + \beta_T)
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>MAP Estimation&lt;/strong>&lt;/p>
&lt;p>Choosing $\theta$ to maximize the posterior distribution is called &amp;ldquo;maximum a posteriori (MAP)&amp;rdquo; estimation
$$
\theta_{MAP} = \underset{\mathbf{\theta}}{\operatorname{argmax}} \ {p(\mathbf{\theta} | D)}
$$
The only difference between $\theta_{MLE}$ and $\theta_{MAP}$ is that one assumes a &lt;strong>uniform&lt;/strong> prior (MLE) and the other allows an arbitrary prior.&lt;/p>
&lt;blockquote>
&lt;p>Recall that:&lt;/p>
&lt;p>With uniform prior $p(\theta) \varpropto 1$, according to $\ref{eq_map}$, the posterior $p(\theta | D) \varpropto p(D | \theta)$.&lt;/p>
&lt;/blockquote>
&lt;p>Suppose we have 5 coin flips all of which are heads,&lt;/p>
&lt;ul>
&lt;li>
&lt;p>MLE would give $\theta_{MLE} = 1$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>MLE with a Beta(2, 2) prior gives $\theta_{MAP} = \frac{5 + 2 - 1}{5+2+0+2 - 2} = \frac{6}{7} \approx .857$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>As we see more data, the effect of the prior diminishes
$$
\begin{eqnarray}
\theta_{MAP}
&amp;amp;=&amp;amp; \frac{\alpha_H + \beta_H - 1}{\alpha_H + \beta_H + \alpha_T + \beta_T - 2} \\
\\
&amp;amp;\approx&amp;amp; \frac{\alpha_H}{\alpha_H + \alpha_T} \ (\text{for large number of observations})
\end{eqnarray}
$$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="sample-complexity" >
&lt;div>
&lt;a href="#sample-complexity">
###
&lt;/a>
Sample Complexity
&lt;/div>
&lt;/h4>
&lt;p>How many coin flips do we need in order to guarantee that our learned parameter does not differ too much from the true parameter (with high probability)? Say, I want to know the coin parameter $\theta$, within $\epsilon = 0.1$ error with probability at least $1 - \delta = 0.95$.&lt;/p>
&lt;p>Using the Chernoff bound, we have
$$
p(|\theta_{true} - \theta_{MLE}| \geq \epsilon) \leq 2e^{-2N \epsilon^2} \\
\delta \geq 2e^{-2N \epsilon^2} \Rightarrow N \geq \frac{1}{2\epsilon^2} \text{ln} \frac{1}{\delta}
$$&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Convert each variable to standard units. The average of the products gives the correlation coefficient (may be more intuitively in the python code)&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Associated with a unit increase in $x$ there is some average change in $y$. The slope of the regression line estimates this change. The formula for the slope is $\frac{r \times SD_y}{SD_x}$. And the intercept of the regression line is just the predicted value for $y$ when $x$ is $0$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>