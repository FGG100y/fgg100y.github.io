<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>fgg blog</title>
<meta name=description content="

    
        #
    
    误差逆传播算法(error BackPropagation, BP)


BP 算法是迄今最成功的神经网络学习算法。现实任务中使用神经网络时，大多是在使用 BP 算法进行训练。BP 算法不仅可用于多层前馈神经网络(multi-layer feedforward neural networks)1 ，还可以用于其他类型的神经网络，例如训练递归神经网络。但通常说 “BP网络” 时，一般指用 BP 算法训练的多重前馈神经网络2。


    
        ##
    
    BP网络及变量符号


给定训练集 $D = {(x_1, y_1), \ldots, (x_m, y_m)}, x_i \in \R^d, y_i \in \R^l$ ，即输入示例由 $d$ 个属性描述，输出 $l$ 维实值向量。为便于讨论，图5.7 给出了一个拥有 $d$ 个输入神经元、$l$ 个输出神经元、$q$ 个隐层神经元的多层前馈神经网络结构，其中


输出层第 $j$ 个神经元的阈值用 $\theta_j$ 表示，隐层第 $h$ 个神经元的阈值用 $\gamma_h$ 表示；


输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的连接权为 $v_{ih}$ ；


隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的连接权为 $w_hj$ ；


记隐层第 $h$ 个神经元接收到的输入为 $\alpha_h = \sum^d_{i=1} v_{ih} x_i$ ；"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title"></h1></header><div class="post-info noselect"><a class="post-hidden-url u-url" href=/posts/neuralnetworks/xgs_backprop/>/posts/neuralnetworks/xgs_backprop/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#误差逆传播算法error-backpropagation-bp>误差逆传播算法(error BackPropagation, BP)</a><ul><li><a href=#bp网络及变量符号>BP网络及变量符号</a></li><li><a href=#参数的更新>参数的更新</a></li><li><a href=#bp-算法的工作流程>BP 算法的工作流程</a></li></ul></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><h2 id=误差逆传播算法error-backpropagation-bp><div><a href=#%e8%af%af%e5%b7%ae%e9%80%86%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95error-backpropagation-bp>#
</a>误差逆传播算法(error BackPropagation, BP)</div></h2><p>BP 算法是迄今最成功的神经网络学习算法。现实任务中使用神经网络时，大多是在使用 BP 算法进行训练。BP 算法不仅可用于多层前馈神经网络(multi-layer feedforward neural networks)<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> ，还可以用于其他类型的神经网络，例如训练递归神经网络。但通常说 “BP网络” 时，一般指用 BP 算法训练的多重前馈神经网络<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>。</p><h3 id=bp网络及变量符号><div><a href=#bp%e7%bd%91%e7%bb%9c%e5%8f%8a%e5%8f%98%e9%87%8f%e7%ac%a6%e5%8f%b7>##
</a>BP网络及变量符号</div></h3><p>给定训练集 $D = {(x_1, y_1), \ldots, (x_m, y_m)}, x_i \in \R^d, y_i \in \R^l$ ，即输入示例由 $d$ 个属性描述，输出 $l$ 维实值向量。为便于讨论，图5.7 给出了一个拥有 $d$ 个输入神经元、$l$ 个输出神经元、$q$ 个隐层神经元的多层前馈神经网络结构，其中</p><ul><li><p>输出层第 $j$ 个神经元的阈值用 $\theta_j$ 表示，隐层第 $h$ 个神经元的阈值用 $\gamma_h$ 表示；</p></li><li><p>输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的连接权为 $v_{ih}$ ；</p></li><li><p>隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的连接权为 $w_hj$ ；</p></li><li><p>记隐层第 $h$ 个神经元接收到的输入为 $\alpha_h = \sum^d_{i=1} v_{ih} x_i$ ；</p></li><li><p>记输出层第 $j$ 个神经元接收到的输入为 $\beta_j = \sum^q_{h=1} w_{hj} b_h $ ，其中，$b_h$ 为隐层第 $h$ 个神经元的输出；</p></li><li><p>假设隐层和输出层神经元都使用sigmoid函数<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> 作为激活函数<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> 。</p></li></ul><p><img alt="BP network and notations" src=./images/xgs_BP_notations.png></p><p>对训练例 $(x_k, y_k)$ ，假定神经网络的输出为 $\hat{y}<em>k = (\hat{y}^k_1, \ldots, \hat{y}^k_l)$ ，即
$$
\tag{5.3} \label{eq_y_hat}
\hat{y}^k_j = f(\beta_j - \theta_j),
$$
则网络在 $(x_k, y_k)$ 上均方误差为
$$
\tag{5.4}
E_k = {1 \over 2} \sum^l</em>{j=1} (\hat{y}^k_j - {y}^k_j)^2 .
$$
图5.7的网络中有 $(d + l + 1)q + l$ 个参数需确定：</p><ul><li>输入层到隐层的 $d \times q$ 个权值；</li><li>隐层到输出层的 $q \times l$ 个权值;</li><li>$q$ 个隐层神经元的阈值、$l$ 个输出层神经元的阈值。</li></ul><p>BP 是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> 对参数进行更新估计，即任意参数 $v$ 的更新估计式为
$$
\tag{5.5}
v \leftarrow v + \Delta v .
$$</p><h3 id=参数的更新><div><a href=#%e5%8f%82%e6%95%b0%e7%9a%84%e6%9b%b4%e6%96%b0>##
</a>参数的更新</div></h3><p>以下我们以图5.7中隐层到输出层的连接权 $w_{hj}$ 为例来进行推导。</p><p>BP 算法基于梯度下降(gradient descent)策略，以目标的的负梯度方向对参数进行调整。对式(5.4)的误差 $E_k$ ，给定学习率 $\eta$ ，有
$$
\tag{5.6}
\Delta w_{hj} = - \eta {\partial E_k \over \partial w_{hj}}.
$$
注意<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>到 $w_{hj}$ 先影响到第 $j$ 个输出层神经元的输入值 $\beta_j$ ，再影响到其输出值 $\hat{y}^k_j$ ，然后影响到 $E_k$ ，有
$$
\tag{5.7}
\frac{\partial E_k}{\partial w_{hj}} =
\frac{\partial E_k}{\partial \hat{y}^k_j}
\cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j}
\cdot \frac{\partial \beta_j}{\partial w_{hj}} .
$$
根据 $\beta_j$ 的<a href=/posts/neuralnetworks/xgs_backprop/###BP网络及变量符号>定义</a>，显然有
$$
\tag{5.8}
\frac{\partial \beta_j}{\partial w_{hj}} = b_h .
$$
sigmoid函数有一个很好的性质：
$$
\tag{5.9}
f&rsquo;(x) = f(x)(1 - f(x)),
$$
于是，根据式(5.4)和式(5.3)，有
$$
\begin{eqnarray}
g_j
&=& - \frac{\partial E_k}{\partial \hat{y}^k_j} \cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j} \
&=& - (\hat{y}^k_j - {y}^k_j) f&rsquo;(\beta_j - \theta_j) \
\tag{5.10} \label{eq_output_gradient}
&=& \hat{y}^k_j(1 - \hat{y}^k_j) (\hat{y}^k_j - {y}^k_j) .
\end{eqnarray}
$$
将式(5.10)和(5.8)代入式(5.7)，再代入式(5.6)，就得到了 BP 算法中关于 $w_{hj}$ 的更新公式
$$
\tag{5.11} \label{eq_weights_hj}
\Delta w_{hj} = \eta g_j b_h .
$$
类似可得
$$
\begin{eqnarray}
\tag{5.12}
\Delta \theta_j &=& - \eta g_j, \
\tag{5.13}
\Delta v_{ih} &=& \eta e_h x_i, \
\tag{5.14} \label{eq_threshold_hidden}
\Delta \gamma_h &=& - \eta e_h, \
\end{eqnarray}
$$
其中
$$
\begin{eqnarray}
e_h
&=& - \frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha_h} \
&=& - \sum^l_{j=1} \frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} f&rsquo;(\alpha_h - \gamma_h) \
&=& \sum^l_{j=1} w_{hj} g_j f&rsquo;(\alpha_h - \gamma_h) \
\tag{5.15} \label{eq_hidden_gradient}
&=& b_h (1 - b_h) \sum^l_{j=1} w_{hj} g_j .
\end{eqnarray}
$$
学习率 $\eta \in (0, 1)$ 控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢。</p><p>有时为了做精细调节，可令式(5.11)与(5.12)使用 $\eta_1$ ，式(5.13)与(5.14)使用 $\eta_2$ ，两者未必相等。</p><h3 id=bp-算法的工作流程><div><a href=#bp-%e7%ae%97%e6%b3%95%e7%9a%84%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b>##
</a>BP 算法的工作流程</div></h3><p>对每个训练样例，BP 算法执行以下操作：先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；然后计算输出层的误差(第4-5行)，再将误差逆向传播至隐层神经元(第6行)，最后根据隐层神经元的误差来对连接权和阈值进行调整(第7行)。该迭代过程循环进行，直到达到某些停止条件为止。</p><hr><p><code>BP 算法工作流程</code></p><hr><p><strong>输入</strong>：训练集 $D = {(x_k, y_k)}^m_{k=1}$ ；</p><p>​ 学习率 $\eta$ .</p><p><strong>过程</strong>：</p><p>1：在 $(0,1)$ 范围内随机初始化网络中所有连接权和阈值</p><p>2：<strong>repeat</strong></p><p>3: <strong>for all</strong> $(x_k, y_k) \in D$ <strong>do</strong></p><p>4: 根据当前参数和式($\ref{eq_y_hat}$)计算当前样本的输出 $\hat{y}_k$ ；</p><p>5: 根据式($\ref{eq_output_gradient}$)计算输出层神经元的梯度项 $g_j$ ；</p><p>6: 根据式($\ref{eq_hidden_gradient}$)计算隐层神经元的梯度项 $e_h$ ；</p><p>7: 根据式($\ref{eq_weights_hj}$)-($\ref{eq_threshold_hidden}$)更新连接权 $w_{hj}, v_{ih}$ 与阈值 $\theta_j, \gamma_h$</p><p>8: <strong>end for</strong></p><p>9: <strong>unitl</strong> 达到停止条件</p><p><strong>输出</strong>：连接权与阈值确定的多层前馈神经网络</p><hr><p><strong>需要注意的是</strong>，BP 算法的目标是要最小化训练集 $D$ 上的累积误差
$$
\tag{5.16}
E = {1 \over m} \sum^m_{k=1} E_k ,
$$
但我们上面介绍的 “标准BP算法” 每次仅针对一个训练样例更新连接权和阈值，也就是说 <code>BP 算法工作流程</code> 中算法的更新规则是基于单个的 $E_k$ 推导而得。如果类似地推导出基于累积误差最小化的更新规则，就得到了 “累积误差逆传播(accumulated error backpropagation)” 算法。累积BP算法与标准BP算法都很常用。一般来说，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新的效果可能出现 “抵消” 现象。因此，为了达到同样的累积误差极小点，标准BP算法往往需要进行更多次数的迭代。累积BP算法直接针对累积误差最小化，它在读取整个训练集 $D$ 一遍后才对参数进行更新，其参数更新频率低得多。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP算法往往会更快获得较好的解，尤其是在训练集 $D$ 非常大时更为明显<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> <sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>神经网络层级结构的一种，每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接。“前馈”并不意味着网络中信号不能向后传，而是指网络拓扑结构上不存在环或回路。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>The term back-propagation (BP) is often misunderstood as meaning the whole learning algorithm for multi layer neural networks. Actually, back-propagation refers only to the method for computing the gradient, while another algorithm, such SGD, is used to perform learning using this gradient.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>$\text{sigmoid}(x) = {1 \over 1 + e^{-x}}$ ，对数几率函数是典型的sigmoid函数，它把可能在较大范围内变化的输入值挤压到 $(0,1)$ 输出值范围内，因此有时也称为 “挤压函数(squashing function)”。&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>激活函数也称为 “响应函数”，理想的激活函数是阶跃函数 $\text{sgn}(x)<em>{x |x &lt; 0} = 0; \text{sgn}(x)</em>{x |x \ge 0} =1$，它将输入值映射为输出值 &ldquo;0&rdquo; 或 “1”，显然 “1” 对应于神经元兴奋，“0” 对应于神经元兴奋。然而，阶跃函数具有不连续(在零点处)、不光滑等不太友好的性质，因此实际常用sigmoid函数作为激活函数。&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>感知机(Perceptron)由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是 M-P 神经元，也称为 “阈值逻辑单元(threshold logic unit)”。感知机的学习规则非常简单，对训练样例 $(x, y)$ ，若当前感知机的输出为 $\hat{y}$ ，则感知机权重将这样调整：$w_i \leftarrow w_i + \Delta w_i$ ，其中 $\Delta w_i = \eta (y - \hat{y}) x_i$ ，其中 $\eta \in (0,1)$ 是学习率(learning rate)。可以看出，若感知机对训练样例 $(x, y)$ 预测正确，即 $\hat{y} = y$ ，则感知机不发生变化，否则将根据错误的程度进行权值调整。&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>这就是 “链式法则”。&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>标准BP算法和累积BP算法的区别类似于随机梯度下降(SGD)与标准梯度下降之间的区别。&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>感知机的参数更新规则和BP算法的参数更新规则式(5.11)-(5.14) 都是基于梯度下降。&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/ml101/treebasedmodels/treemodels/>Book Notes: Tree-based Models</a></div><div class="right pagination-item disabled"></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>