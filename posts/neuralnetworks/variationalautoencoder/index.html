<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>变分自编码器（VAE） - fgg blog</title>
<meta name=description content="变分自编码器（VAE）是一种生成模型，结合了自编码器和概率图模型的思想。"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">变分自编码器（VAE）</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-05-01>2024-05-01</time></div><a class="post-hidden-url u-url" href=/posts/neuralnetworks/variationalautoencoder/>/posts/neuralnetworks/variationalautoencoder/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/auto-encoder/>#Auto-Encoder</a></li><li><a href=/tags/variational-inference/>#Variational Inference</a></li><li><a href=/tags/reparametrization-trick/>#Reparametrization-Trick</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#自编码器>自编码器</a></li><li><a href=#变分自编码器>变分自编码器</a></li><li><a href=#重参数化技巧reparametrization-trick>重参数化技巧（reparametrization trick）</a></li><li><a href=#beta-vae-disentangled-vae>$\beta$-VAE (Disentangled VAE)</a></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><h1 id=变分自编码器veriational-autoencoder-vae><div><a href=#%e5%8f%98%e5%88%86%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8veriational-autoencoder-vae>##
</a>变分自编码器：Veriational AutoEncoder (VAE)</div></h1><h2 id=自编码器><div><a href=#%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8>#
</a>自编码器</div></h2><p>自编码器通常用于数据压缩，也就是将高维数据映射到低维空间，然后通过解码器对压缩后的数据进
行重构（尝试恢复原数据）。例如：在训练阶段，将214x214的图片通过解码器压缩为100维的向量，
然后用解码器对这个100维向量进行重构，重构的目标是生成的图片与原图片越接近越好。经过大量
数据训练后，模型将学会对数据进行压缩。</p><p>图像去噪(de-noicing)：如果在训练过程，给输入数据加入噪音信号，重构目标是原图片，则模型同
时学会降噪；</p><p>图像分割(segmentation)：如果在训练过程，输入数据不变，而重构目标变成图像区块，则模型学会
分割；</p><p>神经填充(neural inpainting)：如果在训练过程，直接对图片部分内容打码，而重构目标是原图片，
则模型学会还原被打码部分(最近闹得沸沸扬扬的“一键消衣，无中生胸”大约是此类模型技术)；</p><p><img alt=IMG_AE src=/posts/neuralnetworks/variationalautoencoder/images/AutoEncoder.png></p><h2 id=变分自编码器><div><a href=#%e5%8f%98%e5%88%86%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8>#
</a>变分自编码器</div></h2><p>变分自编码器（VAE）是一种生成模型，结合了自编码器和概率图模型的思想。</p><p>变分自编码器中的模型架构与自编码器一样，区别主要在于生成的低维表示的方式，自编码器
是生成固定长度的向量，然后传递给解码器；但变分自编码器则生成一个潜在空间中的概率分布
（laten space distribution），然后从这个分布采样得到的数据再传递给解码器。</p><p>变分自编码器中的“变分”其实是“概率推断”（probabilistic inference）中的一种技术，称为“变分
推断（variational inference）。在概率图模型中，通常需要计算后验概率分布（posterior dist.），
即贝叶斯分析中的后验概率。而对于复杂的概率模型，这个后验概率难以直接计算。
解决这个后验概率计算的思路有两种：一是MCMC方法；二就是变分推断。两者都是近似计算方法。</p><ul><li><p>MCMC（马尔科夫链蒙特卡洛）是通过构建满足平稳分布为目标分布的马尔科夫链，然后待马尔科夫
链收敛到平稳分布后，就从这个分布抽样。</p></li><li><p>VI（变分推断）就是通过优化一个参数化的近似分布来近似真实的后验分布，试图将推断问题转变
成最优化问题。更多关于VI的内容请移步<a href=../2024-09-01-variational_inference.md>这里</a>。</p></li></ul><p>VAE的工作流程如下：</p><ol><li><p>编码器（Encoder）：将输入数据映射到潜在空间中的分布参数。这个过程可以理解为将输入数据
编码成潜在空间中的概率分布，而不是直接映射到一个确定的点。</p></li><li><p>采样（Sampling）：从潜在空间的分布中采样一个点，作为潜在表示（latent representation）。</p></li><li><p>解码器（Decoder）：将潜在表示解码为输出数据的概率分布。与编码器相对应，解码器将潜在表
示映射回原始数据的分布参数。</p></li><li><p>重构损失（Reconstruction Loss）：衡量重构数据与原始数据之间的差异，通常使用重构误差或
者交叉熵来衡量。</p></li><li><p>KL 散度损失（KL Divergence Loss）：用于度量编码器输出的潜在分布与预设的先验分布（通常
是高斯分布）之间的差异，促使模型学习到合理的潜在表示。</p></li><li><p>总损失（Total Loss）：重构损失和KL散度损失的加权和，用于训练模型。</p></li></ol><p><img alt=IMG_AE src=/posts/neuralnetworks/variationalautoencoder/images/AE.webp></p><p><img alt=IMG_VAE src=/posts/neuralnetworks/variationalautoencoder/images/VAE.webp></p><h2 id=重参数化技巧reparametrization-trick><div><a href=#%e9%87%8d%e5%8f%82%e6%95%b0%e5%8c%96%e6%8a%80%e5%b7%a7reparametrization-trick>#
</a>重参数化技巧（reparametrization trick）</div></h2><p>在标准的VAE中，编码器网络通常会输出潜在空间中的均值（mean）和标准差（standard
deviation），然后通过从该分布中采样来生成潜在表示。然而，直接从均值和标准差中采样是不可
微的，这导致了无法直接使用梯度下降来训练模型。</p><p>Reparametrization Trick 的关键思想是重新参数化潜在表示的采样过程，使得采样操作与网络参数
之间的关系变得可导。具体而言，潜在表示 $\mathbf{z}$ 通过一个确定的变换从一个固定的标准高
斯分布中采样得到，然后通过编码器网络的输出来计算这个变换的参数。这个过程可以表示为：</p><p>$$
z = \mu + \sigma \odot \epsilon
$$</p><p>其中，$\mu$ 是编码器网络输出的均值，$\sigma$ 是输出的标准差，$\epsilon$ 是从标准正态分布 $N(0, 1)$ 中采样得到的噪声。</p><p>通过这种重新参数化，$\mathbf{z}$ 的采样过程与模型参数的梯度相关，从而使得可以直接使用梯
度下降算法来优化模型参数。这种技巧允许我们在训练过程中，通过反向传播算法直接更新编码器和
解码器的参数，从而优化VAE模型。</p><p><img alt=IMG_VAE_trick src=/posts/neuralnetworks/variationalautoencoder/images/VAE_reparametrization_trick.png></p><h2 id=beta-vae-disentangled-vae><div><a href=#beta-vae-disentangled-vae>#
</a>$\beta$-VAE (Disentangled VAE)</div></h2><p>在损失函数中，使用一个超参数 $\beta$ 乘以 KL-散度损失。也就是：</p><p>$$
Loss = \text{Reconstruction Loss} + \beta \times \text{KL Divergence}
$$</p><p>其中，β 是一个超参数，用于平衡重构损失和 KL 散度之间的重要性。通过调整 β 的值，可以控制
模型对潜在表示的约束程度。当 β=1 时，与标准的VAE相等，而当 β 小于1 时，模型更加关注于重
构损失，从而更加注重数据的重建；当 β 大于1 时，模型更加关注于 KL 散度，从而更加注重潜在
表示的独立性和结构性。</p></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/dsp101/spectral_decomposition/>spectral_decomposition</a></div><div class="right pagination-item"><a href=/posts/llms/llm_faqs/>LLMs_interview_faq</a></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>