<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>ResNet - fgg blog</title>
<meta name=description content="思维实验：
在浅层网络结构的基础上（比如20层），往后面直接添加更多的同映射隐层（identity layers），
得到的深层网络（比如50层）理论上效果应该不会变差。但实验结果说明，它真会变差。意味着：
SGD算法无法找到使得更深层网络性能不变差的参数。
残差网络架构可以解决这个问题。


    
        #
    
    Why, What, and How



Deeper neural networks are more difficult to train. We present a residual learning
framework to ease the training of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learning residual functions with
reference to the layer inputs, instead of learning unreferenced functions.

训练一个深度（足够深的）神经网络是一件很难的事情（2015年）。
使用“残差”神经网络架构可以更容易地训练足够深的神经网络。
“残差”架构就是把这些中间层作为一个学习输入与输出的残差的函数。

就是说：增加的隐层去学习 $h(x) - x$（残差）而不是 $h(x)$，而输出的是当前隐层的输出加上前
一层的输出 $x$ （同时也是当前层的输入）。"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">ResNet</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-06-29>2024-06-29</time></div><a class="post-hidden-url u-url" href=/posts/neuralnetworks/resnet/>/posts/neuralnetworks/resnet/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/resnet/>#Resnet</a></li><li><a href=/tags/bottlenet-block/>#Bottlenet-Block</a></li><li><a href=/tags/residual/>#Residual</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#why-what-and-how>Why, What, and How</a></li><li><a href=#残差网络结构>残差网络结构</a></li><li><a href=#残差连接为什么有用>残差连接为什么有用？</a></li><li><a href=#此残差非彼残差resnet-vs-gdbt>此残差非彼残差（ResNet VS GDBT）</a></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><p>思维实验：
在浅层网络结构的基础上（比如20层），往后面直接添加更多的同映射隐层（identity layers），
得到的深层网络（比如50层）理论上效果应该不会变差。但实验结果说明，它真会变差。意味着：
SGD算法无法找到使得更深层网络性能不变差的参数。</p><p>残差网络架构可以解决这个问题。</p><h2 id=why-what-and-how><div><a href=#why-what-and-how>#
</a>Why, What, and How</div></h2><blockquote><p>Deeper neural networks are more difficult to train. We present a residual learning
framework to ease the training of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learning residual functions with
reference to the layer inputs, instead of learning unreferenced functions.</p></blockquote><p>训练一个深度（足够深的）神经网络是一件很难的事情（2015年）。
使用“残差”神经网络架构可以更容易地训练足够深的神经网络。
“残差”架构就是把这些中间层作为一个学习输入与输出的残差的函数。</p><p><img alt="ResNet Block" src=images/resnet_learning_block.png></p><p>就是说：增加的隐层去学习 $h(x) - x$（残差）而不是 $h(x)$，而输出的是当前隐层的输出加上前
一层的输出 $x$ （同时也是当前层的输入）。</p><h2 id=残差网络结构><div><a href=#%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84>#
</a>残差网络结构</div></h2><p>残差连接（aka, shortcut connection）在输入和输出的维度相同情况下可以直接使用如下结构块：</p><p>$$
y = F(x, {W_i}) + x.
$$</p><p>当想要增加维度时，有两种方式来对齐输入和输出的维度：</p><ul><li><p>残差连接进行同映射操作，然后对增加的维度进行补零操作（padding zeros)</p></li><li><p>残差连接进行线性投影操作 $y = F(x, {W_i}) + W_s x.$ 来对齐维度（使用 1x1 卷积实现）</p></li></ul><p>两种情况下，当残差连接的是不同尺寸的特征图（feature map）时，使用步幅为2的卷积操作。
（通道数翻倍，则高宽减半，因此使用步幅=2（stride=2）的卷积来保证维数对齐）</p><p><img alt=bottlenet-block src=/posts/neuralnetworks/resnet/images/resnet_bottlenet_building_block.png></p><p><img alt=resnet-arch src=/posts/neuralnetworks/resnet/images/resnet_arch_for_imagenet.png></p><h2 id=残差连接为什么有用><div><a href=#%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5%e4%b8%ba%e4%bb%80%e4%b9%88%e6%9c%89%e7%94%a8>#
</a>残差连接为什么有用？</div></h2><ol><li><p>Easier Gradient Flow</p><ul><li>缓解梯度消失/梯度爆炸问题</li><li>残差连接使得梯度更稳定</li></ul></li><li><p>更容易学习同映射函数</p></li><li><p>提升最优化和收敛效果</p></li><li><p>更好的特征传播（feature propagation）</p></li></ol><h2 id=此残差非彼残差resnet-vs-gdbt><div><a href=#%e6%ad%a4%e6%ae%8b%e5%b7%ae%e9%9d%9e%e5%bd%bc%e6%ae%8b%e5%b7%aeresnet-vs-gdbt>#
</a>此残差非彼残差（ResNet VS GDBT）</div></h2><p>GBDT（梯度提升决策树）中的残差（residual）是指预测值与真实值之间的差值。</p><p>ResNet 中的残差（residual），是神经网络尝试去学习特征图（feature map）的残差（是网络块输出与输入之间的差值）。</p><p>Paper: <a href=https://arxiv.org/pdf/1512.03385>https://arxiv.org/pdf/1512.03385</a></p></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/dsp101/2024-07-03-discrete_fourier_transform/>Discrete_Fourier_Transform</a></div><div class="right pagination-item"><a href=/posts/neuralnetworks/2024-06-28-knowledge_distillation2/>knowledge_distillation2</a></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>