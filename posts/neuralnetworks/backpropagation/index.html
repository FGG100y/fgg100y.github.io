<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>前向传播_反向传播_计算图 - fgg blog</title>
<meta name=description content="

前向传播（forward pass）指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。


反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。简
言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。该算法存储了
计算某些参数梯度时所需的任何中间变量（偏导数）。


计算图：具体直接看实例，图1，图2，图3


问题：前向传播具体是计算什么？存储什么？反向传播怎么计算梯度？

图1（即图4.7.1）来自《动手学深度学习》一书。本节主要内容均来自此书对应章节。
简单网络（单隐藏层网络）相对应的计算图，其中正方形表示变量，圆圈表示操作符。左下角表示输
入，右上角表示输出。注意显示数据流的箭头方向主要是向右和向上的。

图2 Computational graph of the BatchNorm-Layer. From left to right, following the black
arrows flows the forward pass. The inputs are a matrix X and gamma and beta as vectors.
From right to left, following the red arrows flows the backward pass which distributes
the gradient from above layer to gamma and beta and all the way back to the input.
(来自此博客)"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">前向传播_反向传播_计算图</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-10-31>2024-10-31</time></div><a class="post-hidden-url u-url" href=/posts/neuralnetworks/backpropagation/>/posts/neuralnetworks/backpropagation/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/backprop/>#Backprop</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#前向传播具体计算的内容包括>前向传播具体计算的内容包括：</a></li><li><a href=#故事的另一部分>故事的另一部分</a></li><li><a href=#整个故事框架梯度下降算法和反向传播算法>整个故事框架：梯度下降算法和反向传播算法</a><ul><li><a href=#1-计算损失函数>1. 计算损失函数</a></li><li><a href=#2-反向传播计算损失对权重的梯度>2. 反向传播：计算损失对权重的梯度</a></li><li><a href=#3-梯度下降更新权重>3. 梯度下降更新权重</a></li><li><a href=#4-不同的优化算法>4. 不同的优化算法</a></li><li><a href=#5-迭代更新>5. 迭代更新</a></li></ul></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><ul><li><p>前向传播（forward pass）指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。</p></li><li><p>反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。简
言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。该算法存储了
计算某些参数梯度时所需的任何中间变量（偏导数）。</p></li><li><p>计算图：具体直接看实例，图1，图2，图3</p></li></ul><p><strong>问题：前向传播具体是计算什么？存储什么？反向传播怎么计算梯度？</strong></p><p><img alt=forward-pass src=/posts/neuralnetworks/backpropagation/images/d2l-forward-pass.png></p><p>图1（即图4.7.1）来自《动手学深度学习》一书。本节主要内容均来自此书对应章节。
简单网络（单隐藏层网络）相对应的计算图，其中正方形表示变量，圆圈表示操作符。左下角表示输
入，右上角表示输出。注意显示数据流的箭头方向主要是向右和向上的。</p><p><img alt=backward-pass src=/posts/neuralnetworks/backpropagation/images/Backprop-circuit.png>
图2 Computational graph of the BatchNorm-Layer. From left to right, following the black
arrows flows the forward pass. The inputs are a matrix X and gamma and beta as vectors.
From right to left, following the red arrows flows the backward pass which distributes
the gradient from above layer to gamma and beta and all the way back to the input.
(来自<a href=https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html>此博客</a>)</p><h2 id=前向传播具体计算的内容包括><div><a href=#%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad%e5%85%b7%e4%bd%93%e8%ae%a1%e7%ae%97%e7%9a%84%e5%86%85%e5%ae%b9%e5%8c%85%e6%8b%ac>#
</a>前向传播具体计算的内容包括：</div></h2><p>中间变量$z$:
$$
z = W^{(1)} x
$$</p><p>输入样本是 $x \in \mathbb{R}^d$，隐藏层的权重参数 $W^{(1)} \in \mathbb{R}^{h \times d}$</p><p>将中间变量$z \in \mathbb{R}^h$通过激活函数$\phi$后，可得到隐藏层激活向量：
$$
h = \phi(z)
$$</p><p>假设输出层参数只有权重$W^{(2)} \in \mathbb{R}^{q \times h}$，以下式子可计算输出层变量，
这是一个长度为$q$的向量：
$$
o = W^{(2)} h
$$</p><p>记损失函数为$\text{loss_fn()}$，样本标签为$y$（通常是独热编码向量），可以计算单个数据样本的误差：
$$
L = \text{loss_fn}(o, y)
$$</p><p>根据$L_2$正则化的定义<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，给定超参数$\lambda$，正则化项为
$$
s = \frac{\lambda}{2} (||W^{(1)}||^{2}_{F} + {||W^{(2)}||}^{2}_{F})
$$</p><p>其中矩阵的Frobenius范数<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>是将矩阵展平（flatten）为向量后应用$L_2$范数的结果。最后，模
型在给定数据样本上的正则化损失为：
$$
J = L + s
$$</p><p>通常将$J$称为目标函数（objective function）。也就是用于最优化（optimization）求解的函数。</p><p><img alt=forward-pass src=/posts/neuralnetworks/backpropagation/images/d2l-forward-pass.png></p><p>以上只是前向传播故事的一部分（出于方便入门理解的初衷？），实际上，前向传播过程还会计算和
存储本地梯度值（local gradient）&mldr;</p><h2 id=故事的另一部分><div><a href=#%e6%95%85%e4%ba%8b%e7%9a%84%e5%8f%a6%e4%b8%80%e9%83%a8%e5%88%86>#
</a>故事的另一部分</div></h2><p><img alt=local-gradient src=/posts/neuralnetworks/backpropagation/images/cs231n-local-gradient.png></p><p>图3来自CS231n课程的视频截图（卡帕西老师讲的课），在每个节点（激活函数）都可以计算节点输
出（$z$）对输入（$x$, $y$）的“本地”偏导数（“local gradient”），因为在当前节点，只要知道
了输入和输出是什么，就可以计算出“本地”偏导数。那么，如图中所示，在反向传播过程中，（例如）
倒数第一个节点就是计算损失函数$L$对节点输出$z$的偏导数$\frac{\partial L}{\partial z}$，
那怎么计算输入对损失函数的影响程度呢？链式法则。利用已经计算好存起来的本地梯度值
（$\frac{\partial z}{\partial x}$），通过链式法则可以快速计算得到节点输入$x$或$y$对损
失函数$L$的偏导数（如图所示）。这个过程是递归式的。</p><p><strong>反向传播的目的就是计算梯度，而计算梯度就是为了更新权重。</strong></p><p>(注:
1.针对单隐藏层的反向传播过程的每一步的计算公式细节，请《动手学深度学习》第4章。
2.如果对理论层面的内容感兴趣可以看看我的<a href=/posts/neuralnetworks/xgsbackprop/>另一个博客</a>，或直接参考“西瓜书<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>”的相关章节)</p><h2 id=整个故事框架梯度下降算法和反向传播算法><div><a href=#%e6%95%b4%e4%b8%aa%e6%95%85%e4%ba%8b%e6%a1%86%e6%9e%b6%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%ae%97%e6%b3%95%e5%92%8c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95>#
</a>整个故事框架：梯度下降算法和反向传播算法</div></h2><p>神经网络的权重更新是通过<strong>梯度下降</strong>或其变体算法实现的。梯度下降的核心思想是通过反向传播
计算出损失函数对每个权重的梯度，然后利用这些梯度信息调整权重，使损失逐步减小（最优化过
程），从而提高模型的预测能力。以下是权重更新的详细步骤：</p><h3 id=1-计算损失函数><div><a href=#1-%e8%ae%a1%e7%ae%97%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0>##
</a>1. 计算损失函数</div></h3><p>在前向传播完成后，神经网络会计算<strong>损失函数</strong> $L$，它衡量模型预测值与真实值之间的差异。例
如，常用的损失函数包括均方误差（用于回归任务）和交叉熵损失（用于分类任务）。</p><h3 id=2-反向传播计算损失对权重的梯度><div><a href=#2-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e8%ae%a1%e7%ae%97%e6%8d%9f%e5%a4%b1%e5%af%b9%e6%9d%83%e9%87%8d%e7%9a%84%e6%a2%af%e5%ba%a6>##
</a>2. 反向传播：计算损失对权重的梯度</div></h3><p>在反向传播过程中，通过链式法则计算损失对每层权重的梯度。对于每个权重矩阵 $W$ ，我们会得
到一个梯度矩阵 $\frac{\partial L}{\partial W}$，这个矩阵表示损失函数 $L$ 对 $W$ 的敏感度。</p><h3 id=3-梯度下降更新权重><div><a href=#3-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%9b%b4%e6%96%b0%e6%9d%83%e9%87%8d>##
</a>3. 梯度下降更新权重</div></h3><p>使用梯度下降算法更新权重。对于每个权重 $W$，更新规则为：</p><p>$$
W = W - \eta \cdot \frac{\partial L}{\partial W}
$$</p><p>其中：</p><ul><li>$\eta$ 是<strong>学习率</strong>，决定了每次更新的步长大小。</li><li>$\frac{\partial L}{\partial W}$ 是损失对权重的梯度。</li></ul><h3 id=4-不同的优化算法><div><a href=#4-%e4%b8%8d%e5%90%8c%e7%9a%84%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95>##
</a>4. 不同的优化算法</div></h3><p>经典的梯度下降可以分为以下几种方式：</p><ul><li><strong>批量梯度下降（Batch Gradient Descent）</strong>：在整个训练数据集上计算梯度并更新权重。这种
方法计算较慢但更新稳定。</li><li><strong>随机梯度下降（Stochastic Gradient Descent, SGD）</strong>：对每个样本计算一次梯度并更新权重，
计算速度快，但会引入噪声，使得更新方向不稳定。</li><li><strong>小批量梯度下降（Mini-Batch Gradient Descent）</strong>：在一个小批量上计算梯度和更新权重，
兼顾了效率和稳定性。</li></ul><p>此外，还有一些常见的梯度优化算法的变体，用来提高收敛速度和稳定性：</p><ul><li><p><strong>动量法（Momentum）</strong>：在更新时加入上一次的梯度，缓解方向震荡并加速收敛。</p></li><li><p><strong>Adam（Adaptive Moment Estimation）</strong>：结合动量法和RMSProp，根据过去的梯度平均值和均
方根值调整学习率，使得每个权重的更新幅度自适应。</p></li><li><p><strong>RMSProp</strong>：通过历史梯度的平方和来调整学习率，防止震荡，特别适合处理稀疏梯度。</p></li></ul><h3 id=5-迭代更新><div><a href=#5-%e8%bf%ad%e4%bb%a3%e6%9b%b4%e6%96%b0>##
</a>5. 迭代更新</div></h3><p>在整个训练过程中，通过不断进行前向传播、反向传播和梯度更新，模型的权重逐渐优化，从而使损
失函数逐步减小。这个迭代过程会持续进行，直到满足训练结束的条件（如达到设定的轮次或损失足
够小）。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>$L_2$正则化，也称为“权重衰减（weight decay）”技术，通过函数与零的距离来衡量函数的
复杂度，一种度量方法是通过线性函数$f(x)=W^Tx$中的权重向量的某个范数来度量其复杂性，例
如$||W||^2$。在物理意义上，$L_2$ 范数表示向量从原点到点$x= [x_1,x_2,…,x_n]$的欧几里得
距离。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>矩阵的Frobenius范数（Frobenius Norm）是矩阵的一个范数，用来测量矩阵的大小或总能量。
对于矩阵A，Frobenius 范数定义为矩阵中所有元素平方和的平方根，也就是可以看作是将矩阵展
平为一个一维向量后，计算其$L_2$范数的结果。&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>周志华的《机器学习》一书。&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><h3 class="read-next-title noselect"></h3><ul class="read-next-posts noselect"><li><a href=/posts/neuralnetworks/xgsbackprop/>西瓜书_误差逆传播算法(BP)</a></li></ul><div class="pagination post-pagination"><div class="left pagination-item disabled"></div><div class="right pagination-item"><a href=/posts/personals/momentsoflife/2024-10-27-parallel_quicksort/>parallel_quicksort</a></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>