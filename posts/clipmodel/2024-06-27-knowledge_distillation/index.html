<!doctype html><html lang=zh data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>knowledge_distillation - fgg blog</title>
<meta name=description content="知识蒸馏（Knowledge Distillation）是一种机器学习技术，它通过将大型、复杂的模型（称为教师
模型，Teacher Model）的知识“蒸馏”到小型、简洁的模型（称为学生模型，Student Model）中，从
而实现模型压缩和加速，同时尽可能保持原始模型的性能。这一技术使得模型可以在资源有限的设备
上高效运行，如手机或嵌入式设备。

The method works by incorporating an additional loss into the traditional cross entropy
loss, which is based on the softmax output of the teacher network. The assumption is
that the output activations of a properly trained teacher network carry additional
information that can be leveraged by a student network during training.
"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main></a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/FGG100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>首页</a>
<a href=https://fgg100y.github.io/posts/ title>归档</a>
<a href=https://fgg100y.github.io/tags/ title>标签</a>
<a href=https://fgg100y.github.io/about/ title>关于</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">knowledge_distillation</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-06-27>2024-06-27</time></div><a class="post-hidden-url u-url" href=/posts/clipmodel/2024-06-27-knowledge_distillation/>/posts/clipmodel/2024-06-27-knowledge_distillation/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>map[email:1522009317@qq.com name:fmh]</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/distillation/>#Distillation</a></li><li><a href=/tags/tearch/student-models/>#Tearch/Student Models</a></li></ul></div></div></div><details class="toc noselect"><summary>Table of Contents</summary><div class=inner><nav id=TableOfContents><ul><li><a href=#基本原理>基本原理</a></li><li><a href=#知识蒸馏的三种代码实现>知识蒸馏的三种代码实现：</a></li></ul></nav></div></details><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><p>知识蒸馏（Knowledge Distillation）是一种机器学习技术，它通过将大型、复杂的模型（称为教师
模型，Teacher Model）的知识“蒸馏”到小型、简洁的模型（称为学生模型，Student Model）中，从
而实现模型压缩和加速，同时尽可能保持原始模型的性能。这一技术使得模型可以在资源有限的设备
上高效运行，如手机或嵌入式设备。</p><blockquote><p>The method works by incorporating an additional loss into the traditional cross entropy
loss, which is based on the softmax output of the teacher network. The assumption is
that the output activations of a properly trained teacher network carry additional
information that can be leveraged by a student network during training.</p></blockquote><h2 id=基本原理><div><a href=#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86>#
</a>基本原理</div></h2><p>知识转移：核心思想是通过让学生模型模仿教师模型的输出行为，不仅包括硬分类标签，还有软概率
分布（softmax概率），这样可以传递更多关于数据分布的信息。软标签相比硬标签含有更多关于数
据不确定性及类间关系的信息，有助于学生模型学习更细腻的决策边界。</p><p>特征蒸馏：除了输出层的知识外，泛化知识蒸馏还可以涉及中间层特征的学习，即学生模型试图学习
教师模型的高层特征表示。这有助于提升学生模型的泛化能力，因为它学会了如何从输入数据中提取
更有用的特征。</p><p>关系蒸馏：强调保持教师和学生模型对于输入样本间关系的理解一致性。这意味着学生模型不仅要学
会单个样本的处理，还要理解样本之间的相对关系，这对于一些需要理解复杂上下文的任务尤为重要。</p><p><strong>知识蒸馏过程</strong>：</p><ol><li><p><strong>训练教师模型</strong>：首先，使用大量数据和计算资源训练一个高性能的深度神经网络（教师模
型）。这个模型可能包含数百万甚至数十亿个参数，但它在分类任务上的表现非常出色。</p></li><li><p><strong>生成软标签</strong>：教师模型在对输入数据进行预测时，不仅仅给出最终的分类结果，还会给出各
类别的概率分布（通常通过softmax层获得）。这些概率分布被称为“软标签”，它们包含了额外的
信息，比如类别的不确定性。</p></li><li><p><strong>训练学生模型</strong>：接下来，使用教师模型的软标签和实际的硬标签（即数据的真实类别）来训
练学生模型。学生模型的架构设计得更简单，参数量远小于教师模型。训练过程中，学生模型不
仅要学习模仿硬标签，还要通过损失函数（如KL散度或交叉熵）尽量接近教师模型的软标签输出。</p></li><li><p><strong>温度参数调整</strong>：在生成软标签时，有时会引入一个“温度”参数来调整概率分布的平滑程度。
高温可以使软标签更加平滑，促进学生模型学习到教师模型的决策边界；低温则使得软标签接近
硬标签，但可能会丢失教师模型的一些细微决策信息。</p></li></ol><h2 id=知识蒸馏的三种代码实现><div><a href=#%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f%e7%9a%84%e4%b8%89%e7%a7%8d%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>#
</a>知识蒸馏的三种代码实现：</div></h2><ol><li>知识转移：基于 softmax output 软标签</li></ol><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimizer<span style=color:#ff6ac1>.</span>zero_grad()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Forward pass with the teacher model - do not save gradients here as we do not change the teacher&#39;s weights</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>with</span> torch<span style=color:#ff6ac1>.</span>no_grad():
</span></span><span style=display:flex><span>    teacher_logits <span style=color:#ff6ac1>=</span> teacher(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Forward pass with the student model</span>
</span></span><span style=display:flex><span>student_logits <span style=color:#ff6ac1>=</span> student(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e>#Soften the student logits by applying softmax first and log() second</span>
</span></span><span style=display:flex><span>soft_targets <span style=color:#ff6ac1>=</span> nn<span style=color:#ff6ac1>.</span>functional<span style=color:#ff6ac1>.</span>softmax(teacher_logits <span style=color:#ff6ac1>/</span> T, dim<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>soft_prob <span style=color:#ff6ac1>=</span> nn<span style=color:#ff6ac1>.</span>functional<span style=color:#ff6ac1>.</span>log_softmax(student_logits <span style=color:#ff6ac1>/</span> T, dim<span style=color:#ff6ac1>=-</span><span style=color:#ff9f43>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper &#34;Distilling the knowledge in a neural network&#34;</span>
</span></span><span style=display:flex><span>soft_targets_loss <span style=color:#ff6ac1>=</span> torch<span style=color:#ff6ac1>.</span>sum(soft_targets <span style=color:#ff6ac1>*</span> (soft_targets<span style=color:#ff6ac1>.</span>log() <span style=color:#ff6ac1>-</span> soft_prob)) <span style=color:#ff6ac1>/</span> soft_prob<span style=color:#ff6ac1>.</span>size()[<span style=color:#ff9f43>0</span>] <span style=color:#ff6ac1>*</span> (T<span style=color:#ff6ac1>**</span><span style=color:#ff9f43>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate the true label loss</span>
</span></span><span style=display:flex><span>label_loss <span style=color:#ff6ac1>=</span> ce_loss(student_logits, labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Weighted sum of the two losses</span>
</span></span><span style=display:flex><span>loss <span style=color:#ff6ac1>=</span> soft_target_loss_weight <span style=color:#ff6ac1>*</span> soft_targets_loss <span style=color:#ff6ac1>+</span> ce_loss_weight <span style=color:#ff6ac1>*</span> label_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss<span style=color:#ff6ac1>.</span>backward()
</span></span><span style=display:flex><span>optimizer<span style=color:#ff6ac1>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># ...</span>
</span></span></code></pre></div><ol start=2><li>特征蒸馏：基于 hidden state 余弦相似度</li></ol><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimizer<span style=color:#ff6ac1>.</span>zero_grad()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Forward pass with the teacher model and keep only the hidden representation</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>with</span> torch<span style=color:#ff6ac1>.</span>no_grad():
</span></span><span style=display:flex><span>    _, teacher_hidden_representation <span style=color:#ff6ac1>=</span> teacher(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Forward pass with the student model</span>
</span></span><span style=display:flex><span>student_logits, student_hidden_representation <span style=color:#ff6ac1>=</span> student(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.</span>
</span></span><span style=display:flex><span>hidden_rep_loss <span style=color:#ff6ac1>=</span> cosine_loss(student_hidden_representation, teacher_hidden_representation, target<span style=color:#ff6ac1>=</span>torch<span style=color:#ff6ac1>.</span>ones(inputs<span style=color:#ff6ac1>.</span>size(<span style=color:#ff9f43>0</span>))<span style=color:#ff6ac1>.</span>to(device))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate the true label loss</span>
</span></span><span style=display:flex><span>label_loss <span style=color:#ff6ac1>=</span> ce_loss(student_logits, labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Weighted sum of the two losses</span>
</span></span><span style=display:flex><span>loss <span style=color:#ff6ac1>=</span> hidden_rep_loss_weight <span style=color:#ff6ac1>*</span> hidden_rep_loss <span style=color:#ff6ac1>+</span> ce_loss_weight <span style=color:#ff6ac1>*</span> label_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss<span style=color:#ff6ac1>.</span>backward()
</span></span><span style=display:flex><span>optimizer<span style=color:#ff6ac1>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># ...</span>
</span></span></code></pre></div><ol start=3><li>特征蒸馏：基于 Intermediate regressor MSE损失</li></ol><div class=highlight><pre tabindex=0 style=color:#e2e4e5;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimizer<span style=color:#ff6ac1>.</span>zero_grad()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Again ignore teacher logits</span>
</span></span><span style=display:flex><span><span style=color:#ff6ac1>with</span> torch<span style=color:#ff6ac1>.</span>no_grad():
</span></span><span style=display:flex><span>    _, teacher_feature_map <span style=color:#ff6ac1>=</span> teacher(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Forward pass with the student model</span>
</span></span><span style=display:flex><span>student_logits, regressor_feature_map <span style=color:#ff6ac1>=</span> student(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate the loss</span>
</span></span><span style=display:flex><span>hidden_rep_loss <span style=color:#ff6ac1>=</span> mse_loss(regressor_feature_map, teacher_feature_map)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Calculate the true label loss</span>
</span></span><span style=display:flex><span>label_loss <span style=color:#ff6ac1>=</span> ce_loss(student_logits, labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># Weighted sum of the two losses</span>
</span></span><span style=display:flex><span>loss <span style=color:#ff6ac1>=</span> feature_map_weight <span style=color:#ff6ac1>*</span> hidden_rep_loss <span style=color:#ff6ac1>+</span> ce_loss_weight <span style=color:#ff6ac1>*</span> label_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss<span style=color:#ff6ac1>.</span>backward()
</span></span><span style=display:flex><span>optimizer<span style=color:#ff6ac1>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#78787e># ...</span>
</span></span></code></pre></div><p>具体参考：https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html?highlight=distill</p></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/clipmodel/2024-06-28-knowledge_distillation2/>knowledge_distillation2</a></div><div class="right pagination-item"><a href=/posts/lossfunctions/2024-06-27-mixup_beyond_empirical_risk_minimization/>mixup_beyond_ERM</a></div></div></main><footer class="common-footer noselect"><ul class=language-select><li>Chinese</li><li><a href=/en/>English</a></li></ul><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div><a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>map[email:1522009317@qq.com name:fmh]</a></p></footer></div></body></html>