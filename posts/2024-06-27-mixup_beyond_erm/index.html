<!doctype html><html lang=en data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>mixup_beyond_ERM - fgg blog</title>
<meta name=description content="

    
        ##
    
    Empirical Risk Minimazation (ERM)


经验风险最小化, Empirical Risk Minimazation principle (Vapnik, 1998)

基于ERM训练模型：亦即在训练数据集上学习以最小化其平均误差。
当前SOTA模型的参数量随着训练数据集规模增大而线性增加。

而经典VC学习理论（learning theory, Vapnik & Chervonenkis, 1971）表明：只要学习器的参数量
不随着训练样本数量增加，则基于ERM学习一定会收敛(convergence, i.e., good generalization
to new data)。亦即：模型的复杂度（参数量规模）相对于训练数据规模应该是固定的或者变动不大。"><link rel=icon type=image/x-icon href=https://fgg100y.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://fgg100y.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=/css/style.min.184a655c5ad8596648622468e6696abf0cf0a2cf8266df17b4f7a36fe9c97551.css integrity="sha256-GEplXFrYWWZIYiRo5mlqvwzwos+CZt8XtPejb+nJdVE="><link rel=stylesheet href=/css/style.min.c4c04b3ef88e3d619ad4c7ee5e03048422bc55c4fefdc1f07657c1133670aa22.css integrity="sha256-xMBLPviOPWGa1MfuXgMEhCK8VcT+/cHwdlfBEzZwqiI="><link rel=stylesheet href=/css/style.min.21c5d8fe0a79d623b0adc1ce4bd4f6dd2c05cd939c9aaaa966ba7186b1464f4d.css integrity="sha256-IcXY/gp51iOwrcHOS9T23SwFzZOcmqqpZrpxhrFGT00="><link rel=stylesheet href=/css/style.min.863b4356f5ce53525ab2482f84c47476c4618984b9726e576c244225ebda1bcc.css integrity="sha256-hjtDVvXOU1JaskgvhMR0dsRhiYS5cm5XbCRCJevaG8w=" crossorigin=anonymous><script src=/js/script.min.08f04d96386c73c9bf4d160333f8f448c05a6e01c06770542ee0e013954ce930.js type=text/javascript integrity="sha256-CPBNljhsc8m/TRYDM/j0SMBabgHAZ3BULuDgE5VM6TA="></script><link rel=stylesheet href=/css/custom.css></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><div class=header-top-left><h1 class="site-title noselect"><a href=/>fgg blog</a></h1><div class=theme-switcher><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-sun-high"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14.828 14.828A4 4 0 109.172 9.172a4 4 0 005.656 5.656z"/><path d="M6.343 17.657l-1.414 1.414"/><path d="M6.343 6.343 4.929 4.929"/><path d="M17.657 6.343l1.414-1.414"/><path d="M17.657 17.657l1.414 1.414"/><path d="M4 12H2"/><path d="M12 4V2"/><path d="M20 12h2"/><path d="M12 20v2"/></svg></span></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");function switchTheme(){currentTheme=currentTheme==="dark"?"light":"dark",localStorage&&localStorage.setItem(STORAGE_KEY,currentTheme),document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))}const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeGiscusTheme(currentTheme),document.body.dispatchEvent(new CustomEvent(currentTheme+"-theme-set"))};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme==="auto"?(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)):document.documentElement.setAttribute("data-theme",currentTheme),switchButton&&switchButton.addEventListener("click",switchTheme,!1),showContent()});function detectCurrentScheme(){return localStorage!==null&&localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}function changeGiscusTheme(e){function t(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}t({setConfig:{theme:e}})}</script><ul class="social-icons noselect"><li><a href=https://github.com/fgg100y title=Github rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-github"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></span></a></li><li><a href=/index.xml title=RSS rel=me><span class=inline-svg><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></span></a></li></ul></div><div class=header-top-right></div></div><nav class=noselect><a href=https://fgg100y.github.io/ title>Home</a>
<a href=https://fgg100y.github.io/posts/ title>Posts</a>
<a href=https://fgg100y.github.io/tags/ title>Tags</a>
<a href=https://fgg100y.github.io/about/ title>About</a></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">mixup_beyond_ERM</h1></header><div class="post-info noselect"><div class="post-date dt-published"><time datetime=2024-06-27>2024-06-27</time></div><a class="post-hidden-url u-url" href=/posts/2024-06-27-mixup_beyond_erm/>/posts/2024-06-27-mixup_beyond_erm/</a>
<a href=https://fgg100y.github.io/ class="p-name p-author post-hidden-author h-card" rel=me>fmh</a><div class=post-taxonomies><ul class=post-tags><li><a href=/tags/mixup/>#Mixup</a></li><li><a href=/tags/erm/>#ERM</a></li><li><a href=/tags/vrm/>#VRM</a></li></ul></div></div></div><script>var toc=document.querySelector(".toc");toc&&toc.addEventListener("click",function(){event.target.tagName!=="A"&&(event.preventDefault(),this.open?(this.open=!1,this.classList.remove("expanded")):(this.open=!0,this.classList.add("expanded")))})</script><div class="content e-content"><h1 id=empirical-risk-minimazation-erm><div><a href=#empirical-risk-minimazation-erm>##
</a>Empirical Risk Minimazation (ERM)</div></h1><p>经验风险最小化, Empirical Risk Minimazation principle (Vapnik, 1998)</p><ol><li>基于ERM训练模型：亦即在训练数据集上学习以最小化其平均误差。</li><li>当前SOTA模型的参数量随着训练数据集规模增大而线性增加。</li></ol><p>而经典VC学习理论（learning theory, Vapnik & Chervonenkis, 1971）表明：只要学习器的参数量
不随着训练样本数量增加，则基于ERM学习一定会收敛(convergence, i.e., good generalization
to new data)。亦即：模型的复杂度（参数量规模）相对于训练数据规模应该是固定的或者变动不大。</p><p>这就造成了这样的矛盾：</p><ul><li><p>经典学习理论认为：想要ERM有效，则模型的参数量应该保持一定大小，而不是随着训练数据集规模增加而增加；</p></li><li><p>然而在实际任务中：SOTA模型的参数量是与训练数据量保持线性增加的。</p></li></ul><p>实际上，一方面ERM允许大的神经网络模型“记住”训练样本（即使用了很强的正则化约束，
strong regularization)，另一方面ERM训练得到的模型在训练分布之外的样本上的预测结果差异巨
大（即使被预测的样本仅仅发生了相对微小的改变，adversarial examples）。</p><p>那么，有没有别的模型训练最优化准则呢？-> Vicinal Risk Minimization (VRM).</p><blockquote><p>In VRM, human knowledge is required to describe a vicinity or neighborhood around each
example in the training data. Then, additional virtual examples can be drawn from the
vicinity distribution of the training examples to enlarge the support of the training
distribution.</p></blockquote><p>例如，在图像分类任务中，图像增强通常包括轻微的旋转、翻转、缩放等操作，这其实是图像的近邻
集（vicinity set）。这些数据增强通常提升了模型的泛化性能。但这些操作通常也是数据依赖的。</p><p>Mixup 提供了一种数据无关（data-agnostic）的增强方法，它构造虚拟样本的方式如下：</p><p>$$
\begin{eqnarray}
\tilde{x} &=& \lambda x_i + (1 - \lambda) x_j, \text{where } x_i, x_j \text{ are raw input vectors} \\
\tilde{y} &=& \lambda y_i + (1 - \lambda) y_j, \text{where } y_i, y_j \text{ are one-hot label encoding}
\end{eqnarray}
$$</p><p>$(x_i, y_i)$ 和 $(x_j, y_j)$ 是从训练集中随机抽取的两个样例。$\lambda \in [0, 1]$。</p><p>因此，mixup通过结合先验知识扩展了训练分布，即特征向量的线性插值应导致相关目标的线性插值。
mixup可以用几行代码实现，并且只引入最小的计算开销。</p><p>Mixup: beyond_ERM, <a href=https://arxiv.org/pdf/1710.09412>https://arxiv.org/pdf/1710.09412</a></p></div></article><div class="pagination post-pagination"><div class="left pagination-item"><a href=/posts/2024-06-27-knowledge_distillation/>knowledge_distillation</a></div><div class="right pagination-item"><a href=/posts/2024-06-16-wifi%E8%BF%9E%E4%B8%8D%E4%B8%8A_%E4%BD%86%E6%89%8B%E6%9C%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E4%BB%A5/>路由器wifi连不上_但手机流量可以</a></div></div></main><footer class="common-footer noselect"><div class=common-footer-bottom><div style=display:flex;align-items:center;gap:8px>© fmh, 2024</div><div style=display:flex;align-items:center></div><div>Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/Junyi-99/hugo-theme-anubis2>Anubis2</a>.<br></div></div><p class="h-card vcard"><a href=https://fgg100y.github.io/ class="p-name u-url url fn" rel=me>fmh</a></p></footer></div></body></html>